Title: NVIDIA GeForce RTX Launch Event at Gamescom 2018
Publish_date: 2018-09-04
Length: 3847
Views: 16750
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/-ZefAW-Anno/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: -ZefAW-Anno

--- Transcript ---

ladies and gentlemen please welcome
nvidia founder and CEO jensen huang
welcome to the launch of the geforce gtx
1180 I have never seen anything that
leaked this much the good news is you're
gonna be surprised this is a historic
moment for computer graphics for as long
as there has been computer graphics has
been the dream of computer scientists to
generate with computers photorealistic
images well we considered the holy grail
of computer graphics
first of all light is coming in from the
window it's bouncing all over this room
the vast majority of this room is being
illuminated indirectly whenever light
touches a material that material is
rendered and simulated physically the
material is metallic or dielectric is
either smooth or rough the roughness who
come from microwave surface structures
that bounces light absorbs light
diffuses light in a very different way
some of it is glossy some of it is matte
and as a result this entire room looks
alive as light is bouncing around this
entire room it doesn't strike every
surface the same way some of it is
occluded from light we call it ambient
occlusion you also see that there's a
desk lamp and it's shining through the
glass and because the glass has
curvature in shape light refracts around
it and accumulates in a way that we call
caustics that little bright reflection
on the table is particularly attractive
the shadow of phasma coming from all
different directions
it's called Umbra and penumbra Umbra is
where the shadow is completely occluded
and penumbra is where it's partially
occluded and so you see the sense of
soft shadows she's reflected in the
mirror even the parts that you don't see
you know how mirrors work you could see
behind things often times and video
games we can't do that
as light travels through those gummy
bears bounces around inside the gelatin
and then reflects and refracts it back
out called subsurface scattering all of
those little tiny effects are really
really hard to do and the good news is
in 1979 a computer scientist named
Turner Whitted
now a researcher at Nvidia described an
algorithm that was both powerful and
elegant
he called it multi bounce recursive
ray-tracing by starting from your eye
instead of from a light source where
light is emitted because the vast
majority the light doesn't reach your
eyes so there's no reason to trace them
he traced light backwards inverse ray
tracing through the eye through the
pixel through every single pixel on the
screen would be sent forward and if it
were to strike a triangle it would then
send off a shadow ray to decide whether
it's in shadow that shadow ray would go
towards to the light sources if there
was something else in the way if it
strikes another triangle it would
determine that it was in shadow and if
it doesn't then it would accumulate the
color and the intensity of that light
would then be used to shade that surface
if that beam of light if that ray goes
through another pixel or to strike a
reflective surface it would generate a
reflection ray that reflection ray would
be basically the same incident angle
relative to that normal of that surface
where it struck and then it would go off
and decide whether it strikes another
triangle and if it were to strike
another triangle it would create shadow
race it would determine whether that
particular surface was in shadow or
reflection rays and such and if it was
to hit into a surface that is
translucent then it would generate
refraction race based upon Snell's law
and that this algorithm this basic
algorithm would recursively run and
eventually he would generate this
beautiful image in 1979 he generated
this image and it took one and a half
hours on a million dollar VAX computer
basically he was generating 60 pixels
per second instead of 60 frames per
second he was generating 60
pixels per second and so the question
then is how long would it take for us
from that time for us to be able to
realize this incredibly beautiful
picture there I just showed you a second
ago well if you look at our technology
curve has been nothing short of
extraordinary
because of the extraordinary demand of
video games and the scale and the size
and the vastness of this entire industry
it has propelled one of the most fast
advancing technology in the history of
computer science if you look at this
curve moore's law has been the
foundational thought the foundational
idea of computer science for 35 years
and yet during this entire time since
our beginning we have been moving at ten
times more long if you go back to when
Moore's law was following historical
levels of increasing performance ten
times every five years or a hundred
times every 10 years our GPU technology
was advancing at a thousand times every
10 years well if you were to plot out
this trend and you were to think about
how many rays it would take you had
billions of rays we had to send into
that room
even though we're not simulating in most
of the race only the ones that are
coming into our eyes we're sending in
billions of rays into that room we got
we've got diffuse rays that we have to
do we have reflection rays refreshing
race and it's bouncing all over the
scene and you have hundreds of polygons
hundreds and millions of polygons the
amount of computation necessary is
easily in the several peda flops while
several petaflop s-- were already at the
teraflops today so it would take
approximately ten more years
approximately ten more years at our
current rate of progress to go from
teraflops to peda flops well we didn't
want to wait that long and as you know
architecture is the single greatest
lever in computer graphics
it is the single greatest lever of GPUs
so what are the interesting new ideas we
could create that allows us to achieve
as effectively 1,000 times more
performance ten years earlier and so we
invented the nvidia RT x the nvidia RT x
as a platform consisting of architecture
in software and SDKs and libraries that
allows us to combine different types of
rendering technology into one unified
and cohesive platform of course it
should always take advantage of
rasterization rasterization is so
incredibly powerful and so incredibly
efficient but we also need to include
ray tracing
however just ray tracing at brute-force
level simply won't get us there
it simply won't get us there in fact
I'll show you something later that's
just really amazing unless we use this
technology we discovered about six seven
years ago called deep learning or
artificial intelligence we can finally
create images along with ray tracing
that allows us to achieve the levels
that we otherwise can't imagine now we
leverage two fundamental rendering
technologies you know very well
rasterization which is the way that we
do things today we start with the 3d
world and we take the 3d world
information the vertices the triangles
and we project 3d world vertices into a
2d world into our screen space that
projection leaves the silhouette of a
triangle and we have to decide and test
which one of the pixels are within that
triangle one of the most beautiful
things about rasterization is that you
could paralyze an enormous amount of
calculation and nothing is more
beautiful than being able to do things
in parallel that's one of the reasons
why the GPU is so incredibly powerful at
parallel processing it started with the
idea of doing things in parallel it also
has the ability to do things incredibly
high resolution because high resolution
is just more things in parallel and so
rasterization was incredibly powerful
one of its limitations however is that
every single time you want to light the
world you have to project light from
that light and if we had two or three
lights that's fine if you have an area
light that's effectively infinite number
of lights and if everything in the room
were to reflect light and everything in
the room became a lighting surface
otherwise known as global illumination
in the world of global illumination all
of a sudden the number of lights that
you would have to cast in the side that
scene is just insane and therefore on
well this is where ray-tracing is
particularly effective because
ray-tracing using Turner Whitted s--
approach simply traces the number of
pixels that comes into your eyes the
number of rays that ultimately reach
your eyes
the vast majority and raised in the
world simply never-never are reaching
our eyes and so we immediately reject
them
whereas rasterization is taking 3d world
data projecting it in 2d ray tracing is
sending out a ray of light intersecting
the pixel at the screen space and enters
into the world looking for a triangle
that it would intersect the two basic
approaches both has its strengths and
weaknesses in the case of ray tracing in
order to rate traces in our entire room
the number of rays the trillions of rays
the hundreds of millions of polygons the
amount of testing that you have to do
just to figure out which ray intersect
at which polygon is just extraordinary
you simply can't do it well we put this
platform together and we worked with
Microsoft to create the Direct X ray
tracing otherwise known as Microsoft DXR
and then we worked with epic to
integrate r-tx DXR into the Unreal
Engine the ue4 engine and we found
somebody with extraordinary art assets
Industrial Light magic and together we
created the world's first real-time ray
tracing demonstration and never been
done before and we announced it this
year at GDC and I think it's worthwhile
to take a look at it and I want to show
you something special about it in just a
second okay so this is our demonstration
[Music]
I heard kylo Ren destroyed the one over
in D sector in trouble
what you saw was physically based
materials it was sitting in an elevator
shaft where there were area lights and
those area lights were moving as the
elevator was moving the area lights
would illuminate the surfaces and the
shadows would be soft using area lights
and incredibly difficult to rendered
shadows and notice the inner reflection
there's a reflection off of phasma
there's inner reflection reflection of
reflection of phasma on her gun and
there's reflection of her reflection of
her gun on her chest and if you look at
the stormtroopers the illumination and
reflections are all completely in
real-time doing this is simply
impractical today the way that you would
do it of course is that you would fake a
whole bunch of lights that would slow
down performance it still won't look
right and all the shadows will look
bandy second you would create all kinds
of what is called reflection probes made
out of cube maps but unfortunately
they're all moving so every single scene
you're gonna have to create a whole
bunch more reflection probes a whole
bunch more cube Maps basically starting
from the point where you would like to
have to see reflection and rendering out
from that point every single direction
turning into a cube reflection map those
tricks simply are now practical with a
scene of this level of fidelity well it
took for Tesla V 100's the
same GPUs that are powering five of the
world's top seven supercomputers today
in a $68,000 deep learning supercomputer
that we may call the DG x and so $68,000
to finally render running the RT x
platform real-time ray tracing at about
20 some-odd frames per second well when
we announced that people got so excited
about it they said how do we buy that DG
x the price point is slightly out of
reach for most gamers and so today I
would like to make my first announcement
starting immediately DG x 3000 easy
payments of 1995 we'll throw in the
first game for free well it turns out
what you were looking at just now was
not running on that it was running on
this little thing this is called Turing
the most advanced GPU we've ever done
ten years in the making the greatest
leap since we created CUDA and computer
graphics will never be the same again
inside Turing is 18.9 billion
transistors it has three brand-new
processors the SM is completely brand
new has the ability to do independent
floating-point and integer operations
and a reason for that is sometimes your
shading for color sometimes you're using
integer operations to calculate
addresses so that you could run your
shader program to do things like
subsurface scattering and as more and
more special effects are to happen the
SMS address calculation part of the
logic becomes much more complex fourteen
teraflops and fourteen integer
operations per second I like to short
shorten it just to 14 tips so fourteen
teraflops and fourteen tips it also has
the ability to do this thing variable
rate shading so that we could do things
like fovea to rendering focusing all of
the horsepower to GPU where your eyes
are focused instead of wasting it evenly
across the entire space if you're moving
we might be able to adapt the place
where we focus
your shading horsepower as a result your
effective giving getting a several X
boost in your shading performance the
second is a processor we call the RT
core RT stands for ray tracing core it's
ten gig erase per second I love saying
that 10 Giga rays per second a 1080 Ti
the fastest GPU in the world today does
1.21 gigawatts
a 1080 Ti and then we have a new
processor called tensor core which runs
AI processing like crazy a hundred and
ten floating-point teraflops or because
as multiple precision to being the type
of networks you're running you might be
able to use lower precision or mixed
precision as a result 440 tops with four
bit integer all of this so that we could
do hybrid rendering and run RTX well so
the question is how does this all turn
out so if you look at the performance it
looks kind of like this so Turing is
built for RTX the first thing I showed
you was originally ran a dgx with four
vultus 55 milliseconds about 20 frames
per second and this is what Turing looks
like one turn this has simply never
happened before doing computer graphics
essentially a supercomputer replaced by
one GPU within one generation if you
were to look at Pascal this is what
Pascal looks like the fastest GPU on the
planet 1080 TI 308 milliseconds versus
45 basically about 8x so the question is
what's happening under the hood this is
a new computing model and so there's a
new way to think about performance it's
a new computing model so there needs to
be new metrics back in the good old days
the vast majority of computation was
done in shading we're basically shading
the color or we're running a program in
the shader and basically we're shading
but in the future we're going to be
doing lighting we're going to be doing
all kinds of image processing and
so the pipeline is completely different
and if a pipeline is different the
platform software's got to be different
the architecture of the GPU has to be
different and you have to measure it
differently we have a new way of
measuring and let me talk to you about
how we're gonna do that so if you look
at this frame this is abstracted of
course but it's not too far off
basically within the frame that I was
showing you earlier we're doing ray
tracing pretty much all the time and so
all those rays are being bouncing all
over the place and eventually we
accumulate the color the intensity
whether it's directly or from something
nearby and we shade that surface and so
we're ray tracing all the time and
that's with our arty core that's the
amount of time the fraction of time it
within 22 milliseconds that an r/t core
is doing it imagine if we didn't have
the arty core multiply that by 10
shading is shading we're doing shading
incredibly well one of the things that's
really great about Turing shader is that
we can now overlap or run independently
floating point integer and as a result
it's call it one and a half times more
performant than it used to be and then
for DNN processing what everything is
done we could use it to generate pixels
that we haven't finished to generate
using artificial intelligence the pixel
that we haven't finished or use
artificial intelligence to generate
resolution that we otherwise couldn't
have it is possible to filter
information but it's very very difficult
to generate new information that looks
right and so with artificial
intelligence we can now for the very
first time generate missing pixels that
are actually right and we could do it in
real time because we have a tensor core
if this is what's going on in the frame
then what's happening inside our chip
the first part of that processing the
Turing SM and the RT core are both
concurrently running later on we're
shading and running other parts of the
program the shader program and so those
two parts are running the F P and the
integer and then finally when everything
is done we're going to use artificial
intelligence all of our hundred and ten
teraflops 110 teraflops think of that
it's basically ten 1080 T is dedicated
to
doing one thing which is artificial
intelligence and so if you just simply
do the math you have about fourteen
teraflops of shader math you have a
hundred and ten teraflops effectively of
ray tracing 14 plus 14 independent
teraflops and tips and then lastly a
hundred and ten FP sixteen tensor core
processing floating-point operations per
second when you take that and do the
weighted average on it it's basically 78
tera
r TX ops per second and so the way to
think about performance in the future is
to figure out a way to weight all of
these different types of processors
compare that to a tight next our highest
end of approximately twelve seventy
eight versus 12 when you're running ray
tracing that's what it's going to look
like seventy eight versus 12 well one of
the most important miracles is simply
the testing the concept is simple you're
sending an array into the scene and
you're trying to figure out which one of
those triangles out of a hundred million
that intersects well you could simply
just walk across the screen and see
which one you eventually touch well you
got to go a hundred million times or
what you could do is you create a
deceleration data structure called a
bound bounding volume hierarchy
basically it's like pinning suppose I
was asked to go pick something out I had
to go pick one of a thousand I'm gonna
put ten bins
within each one of them I'll have a
hundred things within each box
I'll have ten boxes with ten things each
and I started looking for that thing out
of those boxes and I discovered that is
one of those larger boxes then I can
ignore every other box and then once I
find that within the larger boxes if I
find it within one of the ten smaller
boxes I can ignore all of the other
boxes and then if I can find it within
that box I know it's one of the ten it's
the same idea here we're sending out
this Ray we're trying to figure out
which one of the triangles that
intersects with and once it intersects
with a bounding box which is a large
structure which has a whole bunch of
other bounding boxes within it which
other boxes within it and within that
there's a whole bunch of
triangles once we intersect one of the
bounding boxes we know all the other
bounding boxes are ignored and then we
figure out which one of the bounding
boxes within and then once we figure out
which one of the bounding boxes within
we have to figure out which one of the
triangles that happens to be well the
amount of mathematics necessary to do
this it's just shocking and to do it
precisely so that the image quality is
perfect it's really really hard and took
us 10 years to do this
and so the arti corps was invented
well the Arctic or invention had to go
along with all the software layer on top
of it and what we call eventually the RT
X platform at SIGGRAPH we're really
pleased to see research being done in
this area our researchers and the
researchers at the search for
extraordinary experience I love that
seed and so they worked on our TX with
our researchers to figure out how it
could accelerate how could create new
computer graphics and everything from
deferred shading and direct shadows and
lighting and reflections and global
illumination and ambient occlusion
transparency and translucency
translucency and transparency are just
incredibly hard to do with rasterization
because the concept of death is very
difficult to discern and post-processing
as you could see for each one of these
operations we're using different parts
of the chip exactly as I described
earlier the effective ops the effective
computational capability six-times bolta
six times volta six time invidia v100
and so that kind of puts it in
perspective let's take a look at some of
the stuff this is computer graphics
today physically-based material direct
lighting you could see the shadows
they're hard shadows we could try to
trick it by blurring it a little bit but
it's still going to be basically that
and this is what our TX looks like
before r-tx after r-tx
oh no this is just the beginning I'm
just warming you guys up okay so so this
is ganache ohia
he's one of our dev Tech's amazing
computer graphics engineer this is what
he was able to do with r-tx off of
course this scene is incredibly hard and
reason for that is because you can't use
tricks like screen space reflection here
you could put some reflection probes but
it's just incredibly hard here because
things are being reflected all over the
place there's refraction and you've got
this crystal ball here where it should
reflect and refract and then there's
that chrome ball in the back in this
empty glass box and it's got area light
coming in which is really hard to do and
so in this particular case Ignacio is
doing his best to emulate the area
lights but the shadows are ugly you know
this is this is the limits of today's
computer graphics
now we could fake a lot of stuff we
could fake a lot of stuff but by arting
it up and cause you to not look at it
but in the final analysis this is what
the limits of today's computer graphics
is now Ignacio let's turn our TX on look
what happens the refraction of the of
the glass globe there is just incredible
and look at this
the little caustics on the ground the
area lights cast the soft shadow the
Umbra and the penumbra notice of the
graphics card the shadow of the chrome
ball looks right now there's reflection
of the glass box in the r-tx quadrille
r-tx
you see quadrille r-tx from that sphere
and you see the reflection of the chrome
ball with a little bit of rough surface
to give you a sense of material
everything just kind of looks right even
the glass is casting a shadow hey let's
go back to last one again oh yeah yeah I
know now of course we got to add a whole
bunch of paint and a whole bunch of
stuff to make it look right or we could
do it this way RTX on
but this this could possibly be a
photograph right not just show me what
you got
oh come on okay
pretty incredible oh yeah oh you have to
turn on a spotlight on top of that
everything just works everything just
works
look glass reflects and refracts and
magnifies just like it's supposed to and
just have to turn it on and so the
benefit of r-tx is just turned it on
thank you good job and so now I'm going
to show you something we're going to
turn it on together and what you're
about to see is completely in real-time
and it's running on one touring GPU it's
running on one Turing GPU ladies and
gentlemen the name of our demo is called
Sol Sol
[Music]
[Music]
[Music]
[Music]
what do you guys think
[Applause]
[Music]
the nvidia r-tx running on top of a
turing GPU hybrid rendering real-time
ray tracing for the very first time deep
learning is the single most powerful
computer technology that has come on to
the scene in the last 30 years deep
learning is a field of machine learning
where using a large amount of data you
could train a supremely large neural
network how to do things you train these
neural networks with all of this massive
amounts of data which is otherwise
examples on supercomputers and this is
the reason why voice recognition natural
language understanding photo tagging
image recognition all of these amazing
feats of software are now finally
possible it is the reason why
self-driving cars is even within reason
to fathom deep learning is changing one
industry after another and people
consider this the fourth Industrial
Revolution it is going to enable AI and
computers to write software by itself an
Nvidia we're doing all kinds of research
around deep learning we have a we have a
gigantic supercomputer that all of our
researchers use to develop software for
example you could teach a neural network
how to colorize by just showing showing
this neural network examples of this is
a black and white this is how you
colorize it this is black and white this
is how you colorize it it eventually
learned what the pattern is and you can
give it a black and white image and it
color eise's it you could teach a
network how to take a low resolution
image and make a higher resolution
because you and I can look at this image
and say to ourselves you know what I bet
I know what pixel each one of those
pixels are and so you could use a low
resolution image input and eventually
get an output of a high resolution image
by going the other way which is starting
with a high resolution image and give
this neural network the opportunity to
generate that and whenever it's wrong
you correct it and so you literally
corrected and corrected and it tries to
guess and guess and guess and it
corrects itself until one day it's a
to figure out what pixels to generate we
could teach it to take a low resolution
CT scan and not only increase the
resolution of it but also segment it
figure out what organ is what from that
blurry blue image okay you watch this we
taught it how to segment how to identify
which organ is what and put a different
color around it this is gonna
revolutionize medical imaging you could
take a sketch and turn it into 3d so you
take this neural network you give it a
whole bunch of examples and we give it a
chance to try and try and try and try
again trillions and trillions of times
on a supercomputer eventually it trains
and does this amazing thing and then it
creates this new network model with
hundreds of millions of parameters or
hundreds of gigaflops of operation
necessary to perform its task we could
teach it to do these amazing things
however there are so many architectures
and it takes so much processing to do it
in real time and so we created this
thing called a tensor core they're whole
bunch of em on our chip imagine that a
1080 Ti is eleven teraflops this is a
hundred and ten teraflops basically it
would take ten full 1080 t eyes to keep
up with the tensor core processor that's
all over the turing GPU we taught the
neural network this thing and we're
calling it the nvidia dl SS deep
learning super sample basically works
like this we create a whole bunch of
super super high resolution images with
lots and lots of samples so we jitter it
and we've basically 64 amazing images
and we create hundreds of thousands of
those and then we put an image in here
it's called an auto encoder
convolutional auto encoder it has to be
temporally stable because we want to do
this for live action and so the network
has to remember part of the past and
we're gonna give it a image and we're
saying go generate this amazing image on
the output so pretty soon we'll be able
to give it a new image that's never seen
we trained it we trained it and every
time we train it but guess is wrong we
tell it what's what's the right answer
we propagate it back
it takes another image it guesses it
again if it turns out to be wrong we
propagate it back and we sit there we
just simply loop on it on a
supercomputer we'd run a trillions of
times eventually you put it in the image
and it creates this beautiful image out
you take an image of lower-resolution
say 1440p and it creates this beautiful
image that's 4k essentially what deep
learning is doing is codifying the
memories of a supercomputer and it puts
it into this neural network basically an
image generation brain all of a sudden
it knows when it sees this image a
better version of it is that and so in
the future every game would have had the
benefit of supercomputer pre-processing
before you play it and we call that
platform the Nvidia and GX in the future
computer graphics will merge with neural
net processing we call it neural
graphics acceleration ngx it's a
framework for doing all kinds of amazing
image processing you saw earlier
super-resolution when we've invented
super-slow-mo we're gonna do all kinds
of amazing things that makes video games
and the images that come out of these
computer graphics CGI look even more
amazing and then we have this AI model
that we then OTA to you you download it
into your driver into this plug-in
interface called ng X and the neural
networks are changing all the time
they're getting better and better and we
give you we put it up in the cloud and
you can decide whether you want to
enhance your images or not then you run
that on touring ngx
driver this is all part of the RTX
platform and you run it on the run on
the chip
well the supercomputer looks like this
this is the machine that is generating
all of those networks and basically
before you even run your game on turing
there are a bunch of supercomputers
that's training these models you use
that and you train this network called
the nvidia dl SS and then comes up with
a model like this and this is 4k taa you
guys know the unreal
engin epics demo called infiltrators
just beautiful incredibly taxing on
systems taa is the anti-aliasing
technology we invented which is temporal
it takes last frame the motion vectors
and if tries to figure out what's the
best combination to create beautiful
edges look at that because it's temporal
and the motion vector so many things are
moving you see a disjointed hinge over
here and this is for KD LSS look at this
that's just perfect
well for the very first time because of
this because we could take a lower
resolution image and because we could
train a neural network with all kinds of
super high resolution and super high
quality images this neuro network if
runs on a bat out of hell
processor called a tensor core could
then in real time enhance images in real
time generate pixels it had never seen
before I'm going to show you something
this is infiltrator running on one GPU
at 78 frames per second in 4k at a
quality that has never been seen before
a 1080 Ti the fastest GPU in the world
can currently do about 30 something ok
from 30 something what you're about to
see is 4k now I'm going to lock it to 60
Hertz because this display happens to be
V synced
it's not amazing it's just silky smooth
one Turing GPU twice the performance of
the highest-end GPU in the world today
1080 TI running at 4k with beautiful
image quality well you know we should
take a look at what Turing does and r-tx
does for games you guys want to see some
games kata what do you have for us
so shadow the true mater is the first up
it's the latest and it really obviously
this time the game has Laura Croft going
on epic adventure in Mesoamerica and
South America
we work for Crystal Dynamics NIC season
and I'd also Montreal and and we're
excited to be able to show it so one of
the things of course Lara Croft you're
in shadow all the time it's all about
shadows and one of the challenges with
shadows as you know there's just there
they're too crisp and so they have these
hard shadows and we use contact
hardening and we use percentage
closeness shadows they're kind of blurry
and fuzzy and noisy and when you put a
whole bunch of them together it just
becomes a big mess and so this what
you're looking at is raytrace shadow and
that's the first demo we're gonna show
you so what we did is we worked with
nixies and I toss Montreal to bring
real-time ray traced shadows shadows the
tomb raider and so the thing to look at
right now with our TX off this is really
hard to do and that's why they don't do
it at all which is dynamic lights and
shadows these are dynamic point lights
they're really expensive to do in
current rendering techniques basically
you have to cache shadows in every
directions from the light and this is
goes back to what you were saying
earlier about having to put a cube map
here in a cue map here and every time
you move you have to redo all that
rendering so let's put our TX to the
test and have it just do it for us
the beautiful thing about ray-tracing is
when you turn on the light it just does
the right thing physically and because
we're tracing these rays physically it
should perform according to what we
expect
yeah and though the Umbra's we're right
behind where is nice and nice and dark
it's correct and where did penumbra
where you have the softness of the
shadow because not all of the shadows
occluded part of it is lit you get some
softness now let's take you to you were
talking about some of those area lights
earlier you want to look at some
examples of those yeah well here's a big
example this is a credibly lit scene
this whole scene is amazing it's
beautiful and so what we have here is we
have two cone lights and two area lights
those area lights are basically
rectangular shapes the neon lights above
the stage and what do you notice is the
shadows below are hard like you could
keep talking and this is
state-of-the-art for real-time graphics
right now
and I got to say it cuz the guys are
sitting right next to me who made those
shadows and they are bred this is the
best we have right and they look and
they look beautiful yeah they look great
till now and the way that that's
simulated those area lights are
simulated essentially as a couple of
point lights and that's why you see such
hard shadows let me simulate the area
lights like area lights mm-hmm because
we're a traced it and you independent of
where where the Rays bounced around
eventually if it were to hit some part
of that area light it gets lit and so
ray tracing is a much more cost
effective in fact however very difficult
to do a way of doing area lights and
let's turn this on and take take a look
so you get the nice blend of the color
and the light is that beautiful guys and
talking to the artists that do this so
tell me shadow is another color to them
and they also told me that shadow and
light is what they use to set the mood
and tone of their environments and so
that's how important shadows are and it
feels like we just gave them a lot more
depth for it guys like let's take a look
at an exclusive trailer of the shadow of
the Tomb Raider
that's awesome
[Applause]
I can't wait that's gonna be a wonderful
game alright what's up next
well another blockbuster and this time
we have Metro Exodus by 4a games oh man
yeah it's developed by 4a games they're
super enthusiastic and very
forward-thinking so we're not surprised
that we have them here today to show
something awesome thinking we're gonna
showcase with this demo here is
something that is just really really
hard and so lighting this room using
traditional computer graphics lighting
is just incredibly hard and this is the
perfect showcase for global illumination
so mat this is GI this is GI on and so
just as I was describing earlier the
light is coming in through the window
it's bouncing around inside and as a
result weird lighting look at that where
that corner is darker that corner over
there is completely dark and that light
right above right above the windshield
is nicely lit it does exactly what you
expected to do and it does it all by
itself this is the way computer graphics
basically works now of course we could
create a whole bunch more fake lights we
can create a whole bunch more fake
lights but in this room if you create a
more fake lights using spotlights you'll
get all these different areas that are
lit too intensely and others that are
not and it looks like you basically
turned on a whole bunch of lights now
what you could do is turn on basically a
artificial global illumination light
which is basically a fake ambient light
because they can't do in two we can't do
indirect lighting back in the past as in
yesterday we had to put a fake light in
here constant intensity constant color
throughout this entire room as a result
the places that should be dark like back
there isn't dark the places that should
be more bright is the same intensity so
this is wrong and let's look at right
so so Matt our friend our friends are
now able to have like creepy monsters
literally squatting in the corner yeah
take a look in the rafters in the
ceiling when r-tx is on there black I
could hide anything in there but if I'm
lighting it ambulance e-everything up
there when we have our TX on we can make
the mood in the environment perfect by
just letting the normal light in the
room games will never be the same no sir
so scary let's take a look at another
exclusive trailer shall we Metro
[Music]
[Music]
[Applause]
Wow you guys want to see an amazing game
I can't wait for this this is probably
one of my family's favorite games I
[Applause]
think we have with us Jonas and
Christian from DICE we've been working
together on implementing RT x into
battlefield 5 so we're gonna show you
some pretty amazing stuff
ok Jonas let's go take it away yep
Christian right I'm Christian I'm tech
director for engine group at dice and
next to me we have Jonas media and video
editor let's start here in a very zoomed
in view of the soldiers eye and we're
going to observe something that we've
never really been able to do before
you're not so leave it to you take it
away
yeah what you're seeing there I think
firing off screen being reflected in the
characters I know you guys know this is
not possible with screen space
reflection right Jonas exactly because
the fire is not on the screen so now if
we turn around and we didn't see the
environment we can see that the tank
muzzle flash not only reflects in the
eye it reflects in the entire
environment in the tram windows there
and within the tank itself moving here
so now if we keep moving forward yeah
that's right so of course one of the big
challenges of is asar is also a conflict
surfaces like this so now with our TX on
if we set off an explosion behind the
tank here but next to the car
oh come on reflecting accurately within
the car
that's impossible course how would you
how would you do that with our TX off
yeah how does that look like this here
and it's of course it's updating
dynamically everything reflections will
never be the same again look at the
reflections off the ground guys it just
it just yeah so you can see wow it just
happens with si sorry if it's in the
screen space you still get some of the
reflections upon the car but due to the
nature of SSR it disappears as it goes
away but because ray-tracing just works
you just get the expected result how you
think you would see it so
the next thing we have to show you is
the baked scary crocodile tank the
Churchill and if we make that one shoot
its flamethrower across the scene here
then you will again see that reflecting
upon the surface and on the soldiers
paddling it out here in Rotterdam so now
if we look down in the ground with our
TX on will see the flame will see the
soldiers moving will see everything and
now if I turn our TX off we lose
everything
we lose all the detail all the context
of what's happening in the scene but
with our TX on you just get a much more
cohesive image and a better
understanding of what is happening
around you so one thing we also pay
particularly much attention to in
battlefields it's just the weapons yeah
accuracy making sure the slight of the
smallest detail is correct you can see
even there the flames are reflecting
properly in the wood of this gun here
and then in the windows in the back now
if I turn our TX off again you lose all
this detail so ya know Jonas Jonas what
you guys have done is all of your guns
all of the objects are modeled
physically exactly physically based and
so so the the the gun has wood which is
diffused is a dielectric material you
have metal which is reflective and the
metal has sub structure in a
microstructure in it so there's some
roughness in it and so without doing
anything at all because it's physically
based we shoot a ray into that scene it
intersted intersects with a triangle on
that piece of metal
on the site and it figures out I need to
bunk generate another reflection wait
ray and it traces its way to the flame
which is coming by and when it comes
back it accumulates on the surface of
shades of perfectly all by itself ladies
and gentlemen the magic of ray tracing
exactly and there's more so what you see
now are in objects that are really close
by but ray tracing works for more yet so
up here we've got two c-47 airplanes
that's been shot down they're crashing
with or on fire so now if we go over to
the car here and you can also see the
flame reflecting in the car nicely over
there so if we keep moving forward and
then look down on this reflective car
right here we will still be able to see
the plane yeah if we move back here we
can see the plane reflecting in the car
if I move this lens flare Wow lens flare
works good with our jigsaw I love is
everything up yeah look at that Wow and
Jonas you guys did a great job with for
now reflection look how look when the
angle is just right it's just a perfect
mirror looks so here it's a good example
on transparent services with fernell and
PBR rendering the more of an angle
you're at the stronger the reflection
gets and now with what we had before on
these ones for just cube maps which
they're also static and but with
Rachel's you get a lot more detail you
see the entire environment and of course
as I said the the old-school cute maps
they are also static so say that we were
to destroy this building here make a
tank shoot it with rage racing on you
will be able to see frostbites and
battlefields destruction system within
the window moving dynamically and now to
end it all off RTX i mean it works on
the entire scene so what if we were to
shoot off a v1 rocket here
and soom out get a good view of the
entire street of rotterdam and then take
a look at the windows all the places
that you would expect to reflect for now
turn out the time again and we just wait
patiently for the v1 to hit the ground
and there that's awesome great good job
you guys good job that's really great
and and it's amazing what you guys did
so fast I know you guys work super hard
but it's amazing how you guys
implemented this stuff so fast I mean
that's one of the benefits of rate
racing you know the benefits of
ray-tracing is because things are going
to behave physically you should be able
to turn things on if the rest of your
system and your models are done properly
you could turn these these effects on
relatively easily I mean still takes a
great deal of great engineering but
comparing to doing it the traditional
way of arting everything into existence
the amount of engineering that has to be
done just significantly lowere going
forward you know hopefully using the
laws of physics when you guys create a
scene things would just look right and
behave accordingly and so this is really
just such a great achievement okay you
guys made an amazing trailer let's take
a look at
[Music]
Wow you guys killed it you guys killed
it real-time ray-tracing
battlefield 5 and we have a great
surprise for you today battlefield 5
open beta September 6th there are so
many other r-tx games coming your way
developers all over the world are
working hard on our Qi Xing we're gonna
have all kinds of amazing games coming
your way well I guess your question is
this what are you going to run it on
[Music]
[Music]
[Music]
[Music]
[Music]
ladies and gentlemen 10 years in the
making the g-force r-tx 20 series
computer graphics reinvented the
craftsmanship is unbelievable
everything from the power regulation and
the thermal management system it is
designed for overclocking crazy amounts
of overclocking not to mention it is
just so quiet even when you're a maximum
overclocked at maximum overclock it
sounds like 1/5 the audio levels as a
1080 Ti and we're announcing three
models today
the r-tx 27 with six Giga race per
second five times that of a Titan X 45
trillion
RTX ops per second that is several times
the performance of a Titan X 8 gigabyte
frame buffer r-tx 2088 Giga raise per
second 60 trillion
RTX ops about six times the performance
of a Titan X for ray-tracing
eight gigabytes and then the RT x 20 80
TI 10 Giga rays I love Giga race 78
trillion RTX ops 11 gigabyte frame
buffer starting at $4.99 three orders
today on shelf everywhere September 20th
the nvidia r-tx family quadrille r-tx
8000 two GPUs connected by MV link
turning it into one large virtual GPUs
96 gigabyte frame buffer 166 trillion
RTX operations 160 666 that is basically
16 tight nexus $20,000 now just imagine
that is twice the rate racing
performance of a $68,000 dgx station
then you have the geforce r-tx family
the 20 atti from $9.99 the 2080 from 699
and the 2070 from $4.99 and a 27 t is
higher performance than the $1200 Titan
XP
[Applause]
the biggest generational leap in the
history of computer graphics well that's
our show the g-force r-tx the first to
implement the nvidia r-tx platform
reinventing computer graphics using this
hybrid rendering mode of rasterization
and ray tracing compute using CUDA and
artificial intelligence and it generates
beautiful images deterring the largest
generational leap in the history of our
company the most important new GPU we've
created in 10 years since CUDA 78 tera
RTX ops seventy eight trillion ops
basically changing things so
dramatically we have to change the way
that we talk about performance because
in the future you're gonna raytrace
parts of image
you're gonna rasterize parts of the
image you're going to use post
processing processing on some of the
image and you're gonna use artificial
intelligence to generate all kinds of
pixels that are impossible to generate
otherwise and the the GeForce r-tx 20
series starting at $4.99 available
everywhere on September 20th what do you
guys think
[Applause]
I have one more surprise for you
everything you see here is all
completely in real time this is a
historic moment computer graphics has
been reinvented thank you very much
[Music]
[Music]
[Music]
[Music]
[Music]
[Music]
Title: GPU Technology Conference Keynote Oct 2020 | Part 4: "AI: Software that Writes Software"
Publish_date: 2020-10-05
Length: 843
Views: 160564
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/04E3EjsQLYo/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 04E3EjsQLYo

--- Transcript ---

 
AI is writing software that achieves results no human-written software can.
The scalability of this approach has started a race to create ever larger, more complex and more capable AI.
The chart shows the number of days to train a model on a 1 PetaFLOPS computer.
Computation required to train state-of-the-art models has increased 30,000 times in less than 5 years
That's doubling every couple of months!
The capabilities of AI are incredible:
NVIDIA's Natural Language Understanding AI took the RACE reading comprehension test,
consisting of 30,000 passages and 100,000 multiple choice questions.
The average human scored 73%.
Expert humans score 95%.
NVIDIA's Megatron-BERT scored 91%.
Facebook AI Research developed a Transformer-based chatbot
with knowledge, personality, and empathy
that half of the users tested actually preferred.
Researchers at Caltech developed a reinforcement learning-based drone flight-control-system
that flies smoothly through turbulence
and changes in terrain.
Other researchers at Caltech,
used deep learning in quantum chemistry
to predict molecular-protein reaction energy.
Sped up simulation by 1000 times!
TUM - Technical University of Munich, Google, NVIDIA, and Oak Ridge National Lab trained language models
that predicted the 3D structure of a protein just by reading its amino acid sequence.
The 3D shape is used to predict whether a chemical compound will bond to the protein.
Researchers at NVIDIA used deep reinforcement learning to train the control system
of a 4-legged robot to adapt its gait across multiple terrains, even if the surface is in motion.
The progress of AI is incredible and like magic.
The magic of AI also comes with its incredible computing challenges -
enormous models and training datasets that push the limits of every aspect of the computer
- compute, storage, networking.
NVIDIA is democratizing AI.
There are three pillars to NVIDIA AI:
First, data processing, feature engineering, and training, at any scale.
One GPU, multi-GPU, to multi-node. Any framework, any model, in any cloud.
Second, Inference is a great computing challenge.
NVIDIA TensorRT is a graph-optimizing compiler that targets an NVIDIA inference computer
while optimizing for a bunch of competing constraints accuracy, throughput, response time, memory size.
We are releasing TensorRT 7.2 over 2000 kernel- and layer-type optimizations.
TensorRT has been downloaded 1.3M times,
used by 16,000 companies,
operating over 300 services around the world today.
NVIDIA Triton Inference Server is used by data centers to deploy AI models.
NVIDIA Transfer Learning Tool lets customers take pre-trained models and refine them for their own datasets
to optimize for their usage domain
like refining a general language model for healthcare or engineering speak.
Third, AI application frameworks, pre-trained models, transfer learning, runtime engines
for some of the most challenging AI applications
like self-driving cars, robotics, drug discovery, conversational AI and recommender systems.
Thousands of NVIDIA engineers are working on NVIDIA AI today.
And thousands of companies around the world run on NVIDIA AI.
The software stack for AI in accelerated computing is complex.
The stacks for each domain are different
Rendering is different than genomics,
different than data analytics,
and different than deep learning.
The stacks for each computing environment virtualized or scale-out or microservices are also different.
NGC is our cloud for containerized stacks always up to date, always optimized.
NGC runs on all NVIDIA-certified OEM systems and clouds.
NGC has been a huge success -
1 million downloads in just a couple of years,
250,000 users,
growing 4x year over year.
Cloud-Service-Providers have asked us to integrate NGC into their clouds.
So today we are announcing that NGC will be also be available in Azure, AWS, and GCP marketplaces.
NGC will be easily discoverable in CSP clouds and always up to date with the master registry.
With this, you can be assured to get the best performance and the lowest cloud compute cost.
And your cloud and on-prem stacks will be consistent.
Engineering is required so we'll get there as fast as we can.
In the meantime, the optimized stacks are on NGC.
Billions of people are interacting with cloud services hundreds to thousands of times each day.
Each interaction launches a query, a recommendation, an ad, a piece of news, and often all the above.
Trillions of AI services are processed each day.
Processing the AI on NVIDIA is the most performant and cost effective.
This year is the 10th anniversary of the first NVIDIA GPU in the AWS cloud.
Since AWS, every cloud now offers NVIDIA GPU,
and the aggregate compute throughput has increased 10x every 2 years.
This year, we will ship over 166 exaops of inference compute to CSPs more than 6x that we shipped last year.
We estimate that the aggregate NVIDIA GPU-inference-compute in the cloud now exceeds that of all cloud CPUs.
With this trend, in a couple of two-three years, NVIDIA GPUs will represent 90% of the total cloud inference compute.
Any AI application and service can now rely on NVIDIA Inference.
We are past the tipping point.
NVIDIA AI Inference is operating services for companies in a broad range of industries from cloud, financial, and consumer services,
autonomous vehicles, robotics, healthcare, agriculture, and retail.
Take for example, Square, they are using NVIDIA to process conversational AI for businesses to schedule appointments with customers.
Twitter is using NVIDIA AI to understand the torrent of live videos.
Ebay does millions of item image-searches a day with NVIDIA AI.
GE Healthcare uses NVIDIA AI to infer measurements on cardiovascular ultrasound.
Zoox, the robotaxi group of Amazon, uses NVIDIA AI to do perception for cameras, lidars, and radars.
Today we are announcing that Microsoft is adopting NVIDIA AI on Azure
to power Smart Experiences in Microsoft Office.
The world's most popular productivity application, used by hundreds of millions, will now be AI-assisted.
The first features will include smart grammar correction, Q&A, text prediction.
Because of the volume of users and the instant response needed for a good experience,
Office will be connected to NVIDIA GPUs in Azure.
With NVIDIA GPUs, responses take less than 200ms
and our throughput lets Microsoft scale to millions of simultaneous users.
The inference pipeline consists of NVIDIA V100 in Azure, running the ONNX runtime, on NVIDIA Triton Inference Server.
Cybercrime costs the global economy nearly a trillion dollars a year about 1% of the $140 trillion global economy.
American Express does 8 billion transactions a year,
totaling $1 trillion dollars for their 115 million cardholders
Huge numbers!
Using NVIDIA AI, American Express processes tens of millions of transactions daily, each within 2 milliseconds to detect fraud instantly.
AI fraud detection is going to save the financial industry hundreds of billions each year.
NVIDIA AI makes this possible.
Modern cloud services are composed of microservice containers and orchestrated by Kubernetes.
Microservices like Automatic Speech Recognition, Natural Language Understanding,
text to speech, recommenders, database queries
are reused by many applications all at the same time.
These microservices are running on servers throughout the data center and scaled up-and-down to meet dynamic workloads
while delivering good response time.
Modern data centers are scalable, composable, and accelerated
with servers of different types like CPU, GPU, and storage each optimal
for certain types of microservices.
Let me show you one of the most advanced AI applications running end-to-end.
All of the technology components are state-of-the-art and ready to for you to customize and deploy in the cloud or on-prem.
This is SpeechSquad, a benchmark we developed to measure the response and throughput of conversational AI.
SpeechSquad asks a question,
understands the question, searches for the correct answer, and generates a natural human voice response.
It consists of speech recognition, Q&A using BERT, and text to speech.
SpeechSquad is open-sourced on Github.
First, let's run SpeechSquad on a CPU
When did the Germanic tribes claim territory in north and west Europe?
In the middle period of classical antiquity
Each of the ASR, BERT, and TTS services are assigned to its own CPU.
The response time is over 600ms. CSPs want response times in about 300ms.
For a CPU, that's not enough time to use a neural network model
so we are using Kaldi for speech recognition and MaryTTS for speaking.
That's why CPU sounds mechanical
Now let's run SpeechSquad on Jarvis on GPU.
What is the highest court in European Union law?
The European Court of Justice
Again, we assign each of the ASR, BERT, and TTS microservices to GPUs.
The latency is now 300ms, twice as fast as a CPU.
And GPU can deliver a much more human-quality speech.
We're using NVIDIA TacoTron speech synthesis and NVIDIA Waveglow voice encoding
100x more computationally intensive than Kaldi and MaryTTS.
However, it sounds a lot more natural.
Let's scale this service to support 1,000 streams.
NVIDIA Triton scales up 15 A100s.
Notice we are getting super linear scaling.
This is because the Triton inference server is doing load
balancing of the microservices.
It takes over 100 CPU nodes to support 1,000 streams.
Scaling to 112 A100s deliver nearly 7,200 streams.
This would require 1000 CPUs.
So, with A100, conversational AI is half the latency, a third the cost, and much more natural sounding.
What did I tell you? The more you buy, the more you save.
I want to thank GCP for letting us run the experiment on their cloud A100s and CPUs.
GCP is a fabulous cloud infrastructure.
NVIDIA Jarvis,  our state-of-the-art conversational AI application is now in open beta.
Jarvis is an AI with conversational skills and trainable only with large AI infrastructures like DGX SuperPOD.
Its neural text to speech is human-like.
Its natural language understanding is our reading comprehension record setting Megatron-BERT
And the application is fully customizable.
Live video calls are one of the highest traffic on the internet today.
For work, social, school, virtual events, doctor visits video conferencing is now the most important application for many people.
Today we are announcing NVIDIA Maxine. a cloud-native streaming video AI platform for applications like video calls.
AI can do magic for video calls.
Using AI, we can perceive the important features of a face, send only the changes of the features, and re-animate your face at the receiver
This reduces bandwidth by a factor of 10.
AI can reorient your face so you're making eye contact with each person on the call, individually.
Your face is regenerated to make eye contact with each person.
AI can realistically animate an avatar based on just the words you are speaking.
Yes, there was recently a sign of extraterrestrial life on Venus. What else do you want to know?
AI can remove background noise, super res, see better in low light, replace your background, and even re-light your face.
And with Jarvis conversational AI, we can do real-time language translation and closed captioning
one person is heard but everyone can
simultaneously talk what they say is closed captioned or texted.
With Jarvis and Maxine, we have an opportunity to revolutionize video conferencing of today and invent the virtual presence of tomorrow.
NVIDIA AI inference applications are coming from every industry.
Natural language understanding
Microsoft Office
Instantaneous fraud detection
Conversational AI
AI video calls
Robotics
NVIDIA AI Inference is in every cloud, growing at 10X every couple of years
and have even eclipsed the total inference compute of CPUs in the cloud.
We've reached the tipping point of accelerated AI inference every AI service and application can now be GPU accelerated.
Title: GameWorks VR Gives a Boost to Virtual Reality
Publish_date: 2015-11-19
Length: 187
Views: 93101
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/09dT3vkwXBA/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 09dT3vkwXBA

--- Transcript ---

we feel like virtuality is gonna be the
one of the next biggest things in gaming
and competing there's so much buzz
around it in the industry right now and
we're focused on helping to deliver that
next generation virtuality graphics
platform so the first part of that is
building the world's fastest and best
GPUs with our GeForce GTX graphics
processors the second component is our
client software GeForce experience which
helps gamers get the latest drivers and
the right optimal settings for their VR
games and applications and the third
component is our game works VR SDK which
helps headset and game developers get
better performance lower latency and
better compatibility for their VR games
and applications so there are five
features as part of our game works be
our SDK the first feature is called VR
SLI and as everyone knows SLI is a way
to use multiple GeForce GTX GPUs in
parallel to scale performance with VR we
can have one GPU render the right eye
and one GPU render the left eye to
improve performance for virtual reality
games so the second feature is what we
call multi resonating so with normal
rendering you're drawing a rectangle
image like your desktop monitor looks
like but within virtual reality you're
actually drawing two oval shapes that
are distorted based off of the optics
that are used in the headset and with
multi reading developers can use special
Hardware inside of our GeForce GPUs to
render only the pixels that show up in
the final distorted image and everybody
save anywhere from 20 to 50% of the
pixels John
so third feature is context priority
what context priority does is help
reduce latency by updating the image
that's being rendered based off of the
latest position of your head the fourth
feature is called from buffer rendering
and this is another technique for
reducing latency in virtual reality
with front buffer rendering we can
render directly to the frame that has
been output to the display scanning the
image out as soon as it is warped and
thereby reduce latency the fifth feature
is what we call direct mode which allows
for plug-and-play compatibility between
headsets and g-force based pcs today
when you plug in a headset windows
treats it like a desktop monitor and
will extend your desktop onto that
display but with direct mode in place it
treats it like a VR headset so you only
get your VR games and applications on
that and it really delivers much better
experience so we're working with the
leaders in the industry including CCP
epic valve HTC and oculus to integrate
some of these features and help deliver
better and virtual reality experience
for gamers it's awesome to see so much
virtual reality content come together in
such a short period of time already seen
incredible games like eval carry and
elite dangerous and technology demos
like showdown and back to dinosaur
island how do video we're super excited
about the future of virtual reality and
gaming we're working hard to bring you
guys the best experience possible with
our GeForce GPUs our game works VR
software development kit and GeForce
experience
Title: GeForce Garage: Antec 900 Series, Video 6 – How To Detail Your Rig Using Airbrush
Publish_date: 2015-12-11
Length: 596
Views: 27417
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/0aTmc3yuW18/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 0aTmc3yuW18

--- Transcript ---

hi I'm Andrew with NVIDIA you're
watching GeForce garage airbrushing is a
great way to put the finishing touches
on your build and really tie the theme
all together today in the studio we have
wrongly Christensen from blue horse
studios to show us exactly how it's done
tell us a little bit about your
specialization and what exactly you're
going to show us
yeah we're just going to use some very
basic airbrushing techniques so we're
going to use some stencils to create
some really cool graphics some flames
some font lettering and a few other
things all right well let's get started
alright starting with our airbrush that
we're going to be using today this is my
favorite airbrush though iwata HP CS and
you'll spray from a wide pattern all the
way down to an eyelash depending on your
tip size that you use in the airbrush
and then coming over here got some
pre-cut stuff looks like you guys don't
have a vinyl maker you can just go right
down to your vinyl shop get these
printed out for a couple bucks each
we're going to show you how you use the
transfer tape attached to the vinyl 3ds
out and they get attached to your
project we got some wet-naps we've got a
squeegee for applying the stencil I just
use an old bank card credit card and
just use a little bit of painters tape
over the top of it and works great apply
your graphics onto your substrate or
panel whatever you're using an exacto
knife or applying the graphics a little
bit of painters tape my favorite brand
paint so that we use to create tix
wicked colors these are a water-based
paint we got some some gloves and what
we're gonna be spraying today is the
corsair HX 815 power supply and we're
gonna do some flames and a few other
things on this the Nvidia logo first up
we need to do is go ahead and remove the
factory stickers
all right with the stickers remove we're
just gonna use some alcohol wipes just
to clean up any residue left over from
the sticker okay so the next step is so
we're going to go ahead and weed out our
vinyl graphic so you're just going to
need an exacto blade and just gonna get
under the part that you're gonna want to
spray onto your stencil okay so now with
our graphic weeded out here the negative
that you see in the blue will actually
be sprayed the color that we're going to
choose for the invidia color which is
going to be that lime green kind of
yellowish color and that's going to be
sprayed as we apply the graphic to the
panel here
we're going to go ahead and apply a
little bit of our transfer tape to the
vinyl graphic so what I'm gonna do is
just go ahead and make a nice little
slice down my transfer tape and peel off
a small piece that I need here
just go ahead and cut that down the size
and all this lay it over my graphic you
getting any air bubbles in that's where
your squeegee comes in neck and handy
just go even work most of them out with
your finger at this point just cut off
any excess and we're ready to go ahead
and apply that the graphic to our panel
so the next step is we're going to go
ahead and apply the vinyl graphic to our
power supply and what you want to do is
go ahead and remove the back of the
vinyl just peels up really easily all
right and we just want to go ahead and
apply to the power supply this is where
you want to be careful that you don't
get any bubbles Geographic because if
you have any seams that are lifted then
your you can get paint bleed underneath
your graphic that you don't want paint
to show up on the final work
all right with the graphic lay down I'm
just gonna go ahead and pull up your
transfer tape
and I want to pull down an angle so you
don't pull up the graphic
and you get a perfect stencil to stir
your airbrushing so the next step is
we're going to go ahead and masse the
power supply off we would all want to
get any paint into the fan intakes or
the exhaust making sure this is masked
off correctly before we spray anything
we got the power supply all taped off
here and what we're gonna do is go ahead
and mix up our first colors and spray
our first coats before we can start
spraying we need to go ahead and mix
some paint and for this tutorial we're
just going to go ahead and do a 50/50
mix we're going to go ahead and use a
little bit of the pearl grain and just
go ahead and mix that just a couple
drops a reducer just mix it up make sure
you got a good consistency if it's too
thin and you know that you need to add a
little bit more paint you kind of want
the consistency of skim milk which we
have now we're ready to go ahead and add
our first colors so what I've done is
went ahead and mixed up a few different
colors of the green and different shades
here because what we're gonna do on the
power supply is go ahead and do a
gradient effect and so next step what we
need to do is go ahead and prime our
airbrush and get it ready to spray and
the way that you do that is put a little
bit of a reducer into the gun and just
spray the reducer out the gun and make
sure we don't have any tip clogged and
we're ready to go ahead and add our
first droplets of paint we're going to
start with a white base coat
and we're just going to go ahead and
lightly fog in our first layers of color
what I like to do is hold the brush back
a good 12 to 15 inches because you don't
want the build-up to happen too quick as
you can start watering down on you and
start dripping through the vinyl mask
and getting paint bleeds so you want to
do this step very slowly it's kind of
just creating a tack coat just add a
little bit more paint I only like to
work with a few drops at a time that way
I'm not wasting paint you can always add
more paint but trying to put extra paint
back into your bottles can be kind of
messy okay so that's it for our white
base coat so the next step is to go
ahead and mix our green color what I
like to do is put on a glove because
these do have dyes in them and even
though they're water-based they can
stain your hands we're going to use a
little bit of the pearlized grain and
I'm going to start in the middle of the
graphic and work my way up to a darker
color
all right now we just want to go ahead
and let that color dry for a couple
minutes so the next step we're going to
switch to a little bit of darker color
that we mixed up what I want to do is
kind of keep the center a little bit
lighter than the top and the bottom to
create that kind of gradient effect and
we'll do that on the top end video logo
as well and go ahead and switch to a
little bit of darker color and I'm just
going to work my way around the edges of
the vinyl graphic kind of create a
shadow layer so that's pretty much it
for the layers to create the gradient
what you can do to really make this
graphic pop is go back over those
lighter areas that just add a little bit
of white to your mix to create a really
fine highlight
that's pretty much how you can do a
gradient graphic there and we'll go
ahead and let that dry for a few minutes
before we pull off our graphic and we'll
go ahead and add a few flames to the top
of the power supply to just to finish
the project off alright now that the
piece is dry we're going to go ahead and
remove all the blue painters tape
we can go ahead and remove the vinyl
stencils
alright man nothing says personalization
like custom decals that's super cool
yeah it was a pretty easy graphic to
create we just laid down some vinyl
graphics and then just sprayed over it
with this really cool gradient and then
on the backside here we went ahead and
added some flames to tie in to the rest
of the chassis we did semi flames on the
front and back doors as well so that's
gonna contain with the whole look cool
that's great and of course you know guys
out there you can you do anything you
want with airbrushing which is what is
so cool about it create any kind of
stencil you want any logo a picture of
your cat your mom maybe who knows put it
on anything you want and really ties the
bill together so it's a really cool
customized way to end the build thanks
so much for coming on really appreciate
it yeah it's great being here and thanks
to any video for having me
you're watching g-force garage the
ultimate resource center for designing
building and customizing your GTX PC
at g-force calm slash garage we have a
ton more content for you to check out if
you hadn't had enough if you liked this
video there's gonna be some more coming
up on the screen or right now Boop
right here go ahead and click it and
there's a second video for you to check
out right here whew alright take that no
no seriously check that
Title: NVIDIA Announces New AI Inference Platform
Publish_date: 2018-09-17
Length: 92
Views: 25119
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/0FO-gPs3u-0/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 0FO-gPs3u-0

--- Transcript ---

last night Jensen announced at GGC Japan
an update to our AI inferencing platform
we have a whole new GPU the Tesla t4 a
low profile 75 watt inferencing
accelerator it has up to 260 tear-offs
of AI performance and along with it a
whole new AI software stack called the
tensor RT hyper scale platform included
a new 10 CRT emphasis server for doing
real-time inferencing at scale I'm here
today at the at scale conference where
all the world's hyper scalars and
service providers are coming together to
understand how to deploy AI services and
infrastructure at scale today come on
our head of project maglev talked about
our automotive stack how to build a
self-driving car like one behind me to
process massive amounts of data and
build world-class AI for self-driving
cars it's a huge infrastructure
challenge and one that needs to be
solved at scale as AI gets smarter and
more intelligent the competition gets
bigger and more complex yet the latency
requirements remain the same customers
have to provide a real-time interactive
intelligent service to their customers
by moving to GPUs and moving to
acceleration you can make that new AI
accelerated inference impossible by
building those intelligence services
without any compromise
you
Title: Analyzing Blood Cells in Seconds With Deep Learning
Publish_date: 2017-05-17
Length: 135
Views: 15108
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/0J_XspPlJSY/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 0J_XspPlJSY

--- Transcript ---

[Music]
a phallus is a portable blood
Diagnostics device we use deep learning
in order to rapidly diagnose diseases
like malaria infections even leukemia
the customer gets the Othello's device
they'll plug it into their wall then
they download the app and you take a
prick of blood from your finger you
stick that on the test strip you stick
the test strip into the device and
within a few moments you will get the
full-blown blood report sent on your
phone we're able to take a test that is
currently done in the lab that's very
very expensive in the lab and bring that
to the home environment and make it very
affordable and accessible for everyone
the way it works is that we take
hundreds of images of the sample and
then use our trained classifiers in
order to tag and identify certain kinds
of cells and then based on the counts
and morphology of those cells were able
to make an ende diagnostic that's sent
to the user if you wanted to interrogate
your body at the most intimate level
looking at your immune health that would
be the white blood cell count to bring
that to the home would be a game-changer
the numbers that the customer gets back
right now are the white blood cell count
and the white blood cell differential
ultimately we'll be adding the whole CBC
which includes the red blood cell counts
and their platelet counts on the other
cell types that's really all thanks to
the power of AI because you showed a
little bit of data you augment that data
and the new train your models and within
literally hours you can have a very
accurate and very precise way to
diagnose and identify condition we
really see deep learning as the core
differentiator to our technology as we
do more tests and as the device is
distributed to more people who are able
to get an even more robust and powerful
data set our long-term goal is to make
the costs so low that you can literally
screen entire countries we're going to
really enable rural areas and
undeveloped countries to start
recognizing identifying which diseases
are plaguing their populations and
ensure that resources are allocated in a
way to treat them
system the most accurately as nvidia
continues to innovate in the space every
company amigo system will continue to
benefit from it
[Music]
Title: GANverse3D: Knight Rider KITT Re-created with AI by NVIDIA
Publish_date: 2021-04-16
Length: 59
Views: 66406
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/0PQnrnUIBlU/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 0PQnrnUIBlU

--- Transcript ---

[Music]
nvidia researchers have created a
powerful breakthrough for 2d to 3d
models
now it's possible to create a 3d
animatable model
with a single 2d picture no 3d modeling
software or experience
required it's automatic and easy
we call it ganvers 3d
ready to see this groundbreaking tech in
action we uploaded a picture of kit
the iconic 1980s ai car from knight
rider
and brought it to 21st century
ai-powered life
night industries 2000 here in full
3d i think that got their attention
michael
learn more at the link in our bio
omniverse
really captured my good side all of them
Title: Need For Speed No Limits Dev Diary on SHIELD
Publish_date: 2016-02-04
Length: 197
Views: 25441
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/1EJffgggHFg/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 1EJffgggHFg

--- Transcript ---

you need for speed No Limits is
basically the ultimate mobile game for
street racing that's really for me the
only game where you really feel like
you're steering the car and you're
drifting the car and you're smashing the
car sometimes sort of evading the police
really everything that makes that
fantasy really fun for players
I think it's just amazing to see all the
detail and just how reactive the cars
are to the environments and previously
when we're making mobile games who have
to paint a lot of this in it with static
information but now as the camera in the
cars moving around everything's reacting
appropriately to the likes and the
environment what elevates Need for Speed
no limits compared to other recent
titles on the market is we're really
pushing the fidelity of the game to
reach a console quality level so we're
introducing new shaders on the cars
realistic environment lighting which
really makes the cars pop off the screen
I would give me a game extra kick with
things like look-up tables which gives
it an overall cinematic feel with
calibrating and we're also doing really
fine details like having real-time rain
bounced off the car surface and scatter
around the environments with the hoenn
devices we can not only crank up the
graphics to its full capacity but we can
also maintain a high frame rate
throughout the game which gives the
players at extra sense of speed
the experience on the shield is the most
realistic and it's the closest to what
we intended and in fact during
development we were able to put all of
the effects on such as the physically
based shading high dynamic range
rendering and the new particle system
and once we had everything turned on to
maximum we still had additional Headroom
in the hardware to put more effects in
this is the first time we've just made
the ultimate game for mobile fans so
we're trying to really represent all of
street racing culture the music the cars
so really we control everything from the
storyline the characters and really
importantly like how the game evolves
over time - we're constantly giving the
players new cars new events to play one
of the key things we're doing is a
visual customization system these plays
millions of combinations of body parts
paint tubs and I even rims that they can
use to build their dream cars when you
get a new car you unlock this whole
series of events and it's really about
challenging yourself to make the best
possible you know Toyota Supra the best
possible Ferrari and you really feel
challenged - like I'm gonna make my car
the ultimate I think the Need for Speed
no limits really shines when you have a
tablet device for me that's what I like
to play on the shield Pharaon really
allows players to experience the game as
we intended because the platform was
powerful and mature enough for us to do
that it's been great working with NVIDIA
they've been helping us a lot hand tools
in technology and they really want to
push Android as a a serious gaming
platform what's special for me about
playing Need for Speed you know limits
is literally in getting that sense of
speed you really are driving the car and
you put yourself in that position of
being
a really cool street racer
Title: The deadmau5 Project: Episode 3
Publish_date: 2016-07-22
Length: 277
Views: 68934
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/1FawTgiU0tQ/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 1FawTgiU0tQ

--- Transcript ---

The anticipation of seeing the designs move from 2D to 3D was a long process,
and it was a little gut-wrenching,
because we knew from the beginning we designed stuff that maybe couldn't be built.
We wanted to push the bar from the beginning,
so it was always a little scary to see what comes back.
This project has been really complex.
We're working with designers all over the world.
We've been shipping parts back and forth between the UK and the US and Canada,
and that was just to get things fabricated.
Then they had to get shipped to the US and assembled,
and then once we built everything, it had to get packaged up and shipped over to Canada.
We hope that when it arrives and it's all being unboxed that nothing is damaged or cracked.
So the final stage was at Joel's house.
Safety first.
We showed up, and there was 50 boxes of stuff.
So which one is this?
It's a surprise. Who knows?
The first trick was to unbox every single one of these things and see what's inside.
So how well built are these?
They're pretty solid.
Okay.
So of course there was a little bit of worry that
there might be damage when Joel pulled these out of the boxes for the first time, but luckily there wasn't.
They're lighter than I thought.
Yeah, well if they had GPUs in they'd be really heavy. [Laughter]
Oh, so good.
Yeah, it's god red inserts. It's got all the same shapes as the Mono.
Got it?
Yeah, this one's money.
That is the most gratifying thing in the world.
Oh my god, this is amazing -- this one.
There's rainbow cables inside of it.
That's what makes it. Oh, I can't wait to see them lit up.
This is like the white glove one, you know.
That's sexy.
Dude, I've got to get a bigger table.
When he pulled the systems out and looked at them for the first time,
he was blown away, and he actually said that we surpassed his expectations.
So across the five PCs we designed for Joel,
there are a total of 11 GTX 1080s combined,
and when we shipped the computers to him,
we actually removed the graphics cards so that there's no damage done in transit.
And when we arrived at his house and we started unloading all the parts,
, we had these 11 1080s lined up across the table,
and it just such a magic moment to see that much gaming power in one place.
So the final stage was powering up these awesome machines.
We had each piece lined up perfectly.
We were all set to go. And that's that moment like right before the curtain's open,
or right before kickoff, or, you know, it's the moment of,
"Oh my god. I hope these don't explode."
We all kind of gave each other one more look and said,
"All right, Joel, let's power on."
One after the next we had electrical; we had lights,
GPUs, fans -- everything was running right.
It was a great moment, so we pulled it off.
The livestream lab is kind of a couple of things.
One is it's like an art piece, if you will -- I know, as vain as it sounds
but just showing off the product in some really cool, original cases and build quality.
It's like having a fashion show,
sort of, but a functional one and a way [funner] one.
The other is to showcase games and just to have fun -- fun interacting with fans,
inviting guests, and we'll have a number of that over the next year or two of,
you know just game day -- stream days.
I'm no PC expert expert.
I know a couple things here and there,
but I don't feel like it's going to be a tech channel where we're going to do builds
and all this kind of stuff, more like just us using the product and having a good time doing it.
As far as an experience goes, this was really kind of once in a lifetime
getting the opportunity to work with one of your favorite music producers
and create something really unique for them to share with the world.
I'm super happy with it. I'm very pleased with the project,
and I think Joel is too.
I think we pretty much
hit the red line on all the categories of modding,
and presentation, and that kind of thing, so it's pretty cool.
This project is really all about taking things we love,
working with partners we like,
and making something really cool.
Title: NVIDIA RTX 6000 Ada Generation Accelerates Engineering Simulations
Publish_date: 2022-09-20
Length: 122
Views: 49438
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/1K4w-A8OF_E/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 1K4w-A8OF_E

--- Transcript ---

The NVIDIA RTX 6000 Ada Generation can enable
you to design, simulate, and optimize products
– using the most powerful 3D design & simulation
tools – faster and more interactively than ever before.
Here the car model is set up for a wind tunnel
analysis in Ansys Discovery to perform
external aerodynamic simulation with flow
inlets, pressure outlets, and wall boundary conditions.
Ansys Discovery is built on NVIDIA CUDA 
technology running on NVIDIA RTX GPUs.
"What-if" scenarios can be done at the speed of thought.
Here we change the flow inlet velocity from
30m/s to 5m/s and can instantly view the results.
Exploring multiple design alternatives in real time 
is one of the largest benefits that NVIDIA GPUs bring.
This gives designers the freedom to be creative
and effortlessly try out a wide range of concepts.
With 48GB of memory on our flagship NVIDIA
RTX 6000 Ada Generation, users can easily increase the fidelity
of the solver to perform more accurate simulations
and still obtain the results in near real-time.
Here we can see the model in Ansys Fluent meshed with 20 million cells.
The Ansys Fluent solver supports multi-GPU
acceleration on NVIDIA GPUs significantly
reducing run times for large high-fidelity CFD simulations.
Running 100 iterations of the GPU solver 
takes just 2 minutes on the RTX 6000 
-- up to 5X faster than on a dual-socket CPU.
Once the calculations have converged, users
can visualize and validate the data with 
Discovery Results and/or physical wind tunnel tests.
To learn more about the NVIDIA RTX 6000 Ada Generation 
and how it gives designers the ability to drive
demanding simulation-integrated design workflows,
visit the links in the description of this video.
Title: NVIDIA Executive Keynote | COMPUTEX 2021
Publish_date: 2021-05-31
Length: 3173
Views: 2167477
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/1Kio8Hn8f3U/hq720.jpg?v=60b1b038
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 1Kio8Hn8f3U

--- Transcript ---

[Music]
we miss taiwan and wish we could be
there in person for computex
[Music]
so we created taipei city in microsoft
flight sim and flew in virtually on a
geforce rtx 3080.
[Music]
thousands of mods have been created for
flight sim and they've been downloaded
over 10 million times this mod will be
available just after the keynote
welcome to computex 2021 it's always
great to talk directly to our partners
and this year we have a lot to talk
about i'm going to kick it off with
gaming and then i'll hand it to manavir
das to talk about ai and our enterprise
platforms so let's get started
uh
[Music]
all right
[Music]
[Music]
[Music]
[Music]
[Music]
every person born this year will be a
gamer 140 million of them
in fact gen z prefers video gaming as
their favorite entertainment activity
over just about anything
in taiwan two-thirds of the population
plays video games so it's no surprise
that gaming has transformed into one of
the largest and fastest growing forms of
entertainment and it is here to stay
gaming revenue grew to 180 billion
bigger than cinema music and streaming
video combined
we added 60 million pc gamers to our
ranks and over 10 000 games were
published for them to enjoy
in april the number of concurrent gamers
on steam was at an all-time high of 27
million one and a half times that of
2019.
like any sport people love to watch
others play
100 billion hours of gaming content was
viewed on youtube and more than 430
million people tuned into esports
competitions and it doesn't stop there
minecraft's open world is estimated to
have grown to 4 billion square
kilometers or 8 times the surface of the
planet
valve just announced that the 2021 dota
championship will feature a 40 million
dollar prize pool
roblox had 42 million daily active users
in the first quarter up 80 percent year
over year
and it's not just about playing
there are 45 million professional and
freelance creators fueling an explosion
in digital content 30 million
broadcasters and 75 million higher ed
stem students
all relying on the latest technology to
improve their productivity and quality
of work
over the past 20 years we have built
g-force into the number one gaming
platform with one and a half billion gpu
shipped geforce has fueled the growth of
pc gaming
and we continue to delight our customers
with new ways to game and create
our scale and pace of innovation was
only possible by partnering with the
best companies in the world
together we have transformed pc gaming
ten years ago we introduced optimus
dramatically extending battery life in
gaming laptops all told optimus has
saved over 5.8 billion kilowatt hours of
energy enough to power over half a
million homes for a year
max-q changed the way laptops were built
the performance only available in big
heavy gaming machines was engineered
into thin and light laptops
max q has shed over 8 million pounds
from gaming laptops sold
g-sync introduced stutter-free gaming
with over 20 trillion buttery smooth
pixels now shipped once you game with
g-sync there is no turning back
last year we introduced nvidia reflex
which changed the game for esports
competitors
in call of duty war zone alone reflex
gamers have over 50 million wins
finally nvidia studio is our platform
for creators engineered for the needs of
creative professionals our laptops have
accelerated over 25 billion hours of
content creation
thanks to all of our partners who are
just as excited as we are about
reinventing this market
and are joining us in the next major
leap forward
rtx is a huge reset to computer graphics
a decade in development rtx introduced
the world to real-time ray tracing and
ai for graphics
rtx is now available in a full range of
desktops and laptops in every country
from every oem and even streaming from
the cloud
with over a hundred top games and apps
available today rtx is the new standard
for creators and broadcasters rtx is
accelerating the number one photography
app the number one video editing app and
the number one broadcast app
for gamers rtx is powering the number
one battle royale the number one rpg and
the number one best selling game of all
time
last month we added outriders and call
of duty warzone to the list
and today we have more exciting games to
announce
dying 1983 is the second game in the
dying franchise from the chinese
developer netcom it's a
japanese-inspired horror-themed puzzle
game it's stark and frightening world is
brought to life with rtx ray tracing and
dlss
let's take an exclusive look at dying
1983 with rtx on
[Music]
foreign
[Music]
hello
foreign
[Music]
[Music]
tom clancy's rainbow six siege is one of
the most popular esports games in the
world with over 70 million players
worldwide
in march rainbow six gamers gain the
competitive edge of nvidia reflex
today we are announcing that rtx gamers
will get even more performance with the
addition of dlss
let's take a look
so
new zealand-based developer rocket works
was founded by dean hall best known as
the creator of dayz
their highly anticipated new game icarus
is a stunning multiplayer survival game
set on a savage alien planet today we
are announcing that icarus will come
alive with rtx raytrace global
illumination and dlss when the initial
chapter first cohort launches this fall
let's take a look at some exclusive new
footage
[Music]
[Applause]
[Music]
red dead redemption 2 is one of the most
critically acclaimed games of all time
with more than 275 perfect scores
175 game of the year awards and more
than 37 million copies sold to date we
are happy to announce that rockstar
games will be adding dlss to red dead
redemption 2. coming soon every rtx
gamer will see a free boost in
performance
vr is taking off
vr game revenue is up 70 on steam in
2020 the installed base of pc compatible
vr headsets is expected to exceed 30
million in the next five years
with almost twice the resolution of a
desktop monitor an unforgiving fps
requirements vr is very demanding on the
gpu
and rtx is now coming to vr
no man's sky the popular open world
space game now features dlss vr gamers
can soon play with ultra graphics at 90
frames per second wrench a mechanic
simulator where you repair and maintain
race cars in vr now features rtx ray
tracing and dlss
and the vr survival shooter into the
radius adds dlss to boost performance
and keep you immersed in this dystopian
world
seventy five percent of geforce gamers
play esports we invented nvidia reflex
for them
the difference between winning and
losing is a matter of milliseconds
on average reflex reduces system latency
by 20 milliseconds it gets the pc out of
the way so a gamer's real skill can come
to play
and gamers love it more than 90 percent
of them compete with reflex on
today we are announcing even more nvidia
reflex games gaijin's war thunder the
most popular air land and sea battle
arena with over 40 million gamers
naraka bladepoint the highly anticipated
battle royale slasher from 24
entertainment in the last closed beta it
broke into steam's top five games played
tencent games crossfire hd is a
remastered version of the original with
over 560 million gamers in china and
escape from tarkov the intense survival
shooter from battle state games is one
of the top 10 most played competitive
shooters
coming soon 12 of the top 15 competitive
shooters will feature nvidia reflex
let's take a look at reflex in escape
from tarkov
[Music]
for serious competitors reducing system
latency starts by measuring it from end
to end this requires an ecosystem of
partners
last year we introduced the reflex
latency analyzer a measurement tool
built directly into gaming monitors
today we are adding lenovo viewsonic and
evga
13 partners are offering a total of 15
monitors and 21 compatible mice now more
gamers can simply plug a mouse into the
monitor and accurately measure
end-to-end latency from click to photon
almost 10 years ago we introduced our
kepler architecture and created a new
class of laptops for gamers while the
market reception was great these laptops
were large bulky and had limited battery
life more transportable than portable in
2016 we introduced the pascal
architecture and max q technology a full
system design approach to bringing high
performance to thin and light laptops
performance increased by over 4x and for
the first time high performance gaming
laptops were less than 20 millimeters
thick this year we launched ampere our
second generation rtx it featured our
third generation max-q and delivered
twice the performance of pascal
but the real magic is dlss
gamers have taken notice and geforce
laptops are the fastest growing gaming
platform
laptops are a marvel of engineering no
effort is spared to deliver the highest
performance in a thin and light device
in laptop design system power determines
everything including performance battery
life thinness and acoustics
what if you could get a massive boost in
gaming performance without expending any
more power let's say you are playing
control at 47 frames per second at 1440p
the gpu is rendering almost 4 million
pixels and drawing 80 watts of power
nvidia dlss uses ai-powered tensor cores
to boost frame rates with dlss the gpu
only needs to render a fraction of the
pixels and then uses ai to complete the
frame the result is an image just as
beautiful as native 1440p
and you get one and a half to two times
the performance at the same power
alienware was one of our earliest
partners designing high performance
geforce pcs for gamers
for the past two years we've been
working together to create an amazing
new laptop today we're announcing the
alienware x15 an ultra thin geforce rtx
3080 laptop
powered by max-q technologies including
dynamic boost whisper mode and advanced
optimus and featuring a 1440p display
it's the world's most powerful sub 16
millimeter 15-inch gaming laptop tune in
to the alienware update event on june
1st to learn more this year brought a
record launch for rtx laptops with over
140 models from every oem
starting at 7.99 and featuring max q
there is now an rtx laptop for every
gamer we developed nvidia studio to
address the growing needs of 3d
designers video editors and
photographers these are specially
configured systems optimized and tested
for creator workflows and supported with
a monthly cadence of studio drivers
today we are announcing new studio
laptops from hp and acer the 14-inch hp
envy brings the capabilities of rtx to
an ultra-portable laptop great for
students and creators on the go
the new acer concept d offers a variety
of traditional clamshell options and an
easel sketchboard design to give
creators even more flexibility
gaming has been an inspiration to
digital creators and artists for decades
machinima an art form started in the 90s
uses real-time 3d technologies with game
assets visual effects and character
animation to create short humorous clips
or full-length movies
for these artists we created omniverse
machinima making it easy to produce
cinematic animated stories with advanced
real-time ray tracing and ai
users import their own game assets or
draw from our growing library
they can then apply advanced physics
effects like destruction or fire with
nvidia's blast and flow animate
movements using only a webcam with
wrenches ai pose and use simple audio
tracks to animate character faces with
audio to face the final scenes are
rendered in the omniverse rtx renderer
machinima was just released in beta let
me show you how you can create your own
masterpiece
[Music]
[Music]
[Music]
whoa
look at me
i'm in the omniverse
what are you doing in my kitchen
rtx has been a huge success
and today we are announcing a new
addition to the family
the geforce rtx 3080 ti is our new
flagship gaming gpu based on ampere with
second generation rt cores and third
generation tensor cores ampere is our
greatest generational leap ever
[Music]
[Music]
so
[Music]
[Music]
the 80 thai class of gpus represents the
best of our gaming lineup
the gtx 1080 tie released in 2017 could
tackle all the games of its time but
this well-loved gpu simply can't keep up
with the demands of modern games
the rtx 2080 tie was introduced as the
only way to play in 4k with rtx on
but the production value of games
continues to march forward
new titles like cyberpunk 2077 and
watchdog's legion have elevated realism
demanding even more of the gpu
the rtx 3080 tie is one and a half times
faster than its predecessor and tears
through the latest games with all the
settings cranked up
the rtx 3080 tie is a powerful gpu
with 34 shader teraflops
67 rt teraflops and 273 tensor teraflops
it comes equipped with 12 gigabytes of
ultra fast g6x memory and a 384 bit
memory interface
availability will begin on june 3rd
starting at 1199.
to show you what this beast can do we
have a special announcement
doom is a storied franchise known by a
generation of gamers for pushing the
boundaries of speed power and visual
effects
do maternal the latest in the series
takes this even further based on the id
tech 7 engine it's blistering fast and
beautiful
today we're announcing do maternal is
adopting rtx ray tracing and dlss
here is a first look at doom eternal
with ray tracing at 4k
running on a 3080 tie
[Music]
[Music]
so
and that's not all the rtx 3070 quickly
became our most popular ampere gpu
now we're adding a tie to this class of
geforce with more cores and super fast
g6x memory
today we're announcing the all-new rtx
3070 ti
it's one and a half times faster than a
2070 super and availability will begin
on june 10th starting at 5.99
thanks everyone for your time before i
hand it off to manaver let me summarize
gaming is transforming entertainment and
rtx is changing everything
not just for gamers but for 150 million
creators broadcasters and students
with over a hundred of the top games and
creative apps accelerated rtx is the new
standard
geforce laptops are the fastest growing
gaming platform
now fueled top to bottom by rtx third
generation max q and the performance
boosting magic of dlss
and studio specially designed for
creators extends their reach beyond
gaming
finally the rtx desktop family just got
better with our new flagship gaming gpu
the geforce rtx 3080 ti and the geforce
rtx 3070 tie
every person born today is a gamer
we have an amazing future ahead and we
look forward to building it with all of
you
and here to tell you how we're putting
ai into the hands of every company is
matavir das head of enterprise computing
thank you jeff and thank you everyone
for joining us today
our journey at nvidia began with gaming
but today we are also the driver of the
most powerful technology force of our
time
artificial intelligence
let's take a look at just some of the
ways in which ai powered by nvidia is
improving our world
[Music]
i am
a creator
blending art and technology
to immerse
our senses
[Applause]
i am a healer
helping us take the next step
and see what's possible
[Music]
i am a pioneer
[Music]
finding life-saving answers
[Music]
and pushing the edge
to the outer limits
[Music]
i am a guardian
defending our oceans
and the magnificent creatures that call
them home
i am a protector
helping the earth breathe easier
[Music]
and watching over it for generations to
come
i am a storyteller
giving emotion to words
[Music]
and bringing them to life
i am even the composer
of the music
i am a.i
brought to life by nvidia
deep learning
and brilliant minds
everywhere
and that was just a hint of what ai can
do
taiwan has always been a very special
place for nvidia from system builders
and solution providers to universities
in the government
all have partnered with us
on behalf of jensen and nvidia
thank you to all of our many friends in
taiwan who have worked alongside us to
enable these amazing ai breakthroughs
together we have laid the foundation for
a new computing model based on ai
software that writes software
this foundation includes gpus deployed
in the cloud and in data centers
ai software and sdks tailored to the
world's largest industries
and an ecosystem of startups and
developers
now it's time to build a house on top of
this foundation
it is time to democratize ai
by bringing its transformative power to
every company and its customers
there are three fundamental shifts that
will bring ai to every company
the first shift is that every
application used by any company will be
infused with ai
ai is essentially a two-step process
first a rich set of existing data
and known outcomes from that data are
used to train a model
then the model is applied to fresh data
to make fresh judgments
the model is effectively a piece of
software that can be embedded into any
application
as the application runs it queries the
model to make judgments giving it
intelligence as new data becomes
available the model is retrained and
improved automatically making the
application better without human
developers having to create a new
version
a great example of this infusion of ai
is in microsoft office the world's most
popular productivity application
microsoft is adding smart experiences
such as smart grammar correction q a and
text prediction to office
with nvidia gpus microsoft was able to
improve responsiveness to less than
one-fifth of a second
enabling real-time grammar correction
and at one-third the cost of the same
work on cpus
going forward every application will
have to be built in this way or be left
behind
now this is not the first time we've
seen this kind of shift
for example consider the introduction of
the graphical user interface the gui
once the transformative value of gui
became evident every command line
application had to adopt gui or be left
behind
in the same way every application will
need to be infused with ai
and all of these applications will need
the most performant power efficient and
cost-effective systems to run on
regardless of form factor location from
edge to cloud
every system will need a gpu
the second shift is that every data
center used by any company will be
infused with ai
computing is increasingly being driven
by large amounts of data
neither the data nor the computation
done on it can fit on one server
therefore the flow of data through the
network becomes crucial to both the
capability and the security of the data
center
a new kind of hardware is needed that
sits on the data path and intelligently
optimizes inspects and protects the data
and protects the applications from one
another
this new hardware is the data processing
unit or dpu
every server will need a dpu
the third shift is that the life cycle
of every product produced by any company
will be optimized by ai
consider the very common and
old-fashioned example of soap yes soap
going forward a soap company will design
and tailor its soap product portfolio by
analyzing rich data on the usage and
experience of the customer base
the soap manufacturing process will be
automated and optimized with ai
customers who use soap
pretty much the entire population of the
world will be engaged socially and sold
to online
smart soap dispensers will track usage
and automatically order refills
and all the data from the customer
experience will be fed back
to guide the design and supply chain of
the soap
this soap company and every other
company will need a new form of i.t
and it infrastructure to operate the ai
optimized life cycle of its product
these are fundamental shifts that will
require massive amounts of accelerated
computing now let's talk about how you
can build on the technology foundation
of nvidia to make this a reality
how you can democratize ai
we start with the hardware
nvidia dgx is the instrument of ai
since its launch in 2016 djx has been
used by companies across industries at
the forefront of ai
the reason for this is that djx was
designed from the ground up as the
optimal system for ai
every component and every interconnect
was designed or chosen with great care
and today dgx incorporates learnings
from hundreds of millions of hours of
usage on thousands of systems across the
planet
the pinnacle of ai capability is the dgx
superpod where multiple dgxs are
clustered together and tuned for ai
based on all of nvidia's learnings
so the first step to democratization is
to make this best of breed machine more
accessible
and more obtainable
to make it more accessible we are
providing a software stack called base
command platform
using kubernetes this software allows
administrators to share this powerful
supercomputer across an organization of
data scientists and a mix of workloads
it provides users with a simple
interface for ai
and it allows organizations to
understand and optimize their usage of
this valuable equipment
we have been using base command within
nvidia for many years
sharing superpods across thousands of
data scientists who have run over a
million jobs
to make it more obtainable we are
introducing a subscription model for
superpod
customers can now experience the
capability of a superpod or a smaller
part of a super pod for months at a time
this is a hosted offering in which
nvidia will manage the infrastructure
reducing the operational burden on
customers
the offering includes powerful all-flash
storage and integrated data management
from netapp the pioneer of enterprise
nas storage
netapp is synonymous with enterprise
great storage with over 38 000 customers
today on premises and in the cloud the
subscription model is in early access
now for example adobe is already using
the infrastructure to train style gan a
powerful image generation ai
the model will open up for paid access
this summer
with this offering we expect that many
more customers will be able to
experience the unique capability of a
super pod without the upfront investment
from there they can graduate to more
permanent infrastructure at scale either
their own superpod or in the public
cloud
speaking of the cloud for some time now
we have worked with our cloud partners
to offer powerful gpu instances in the
cloud
leveraging the internal components of
dgx
we are proud to announce today that
amazon and google are working with us to
enable base command platform for
clusters formed from these cloud
instances
this work offers the promise of a true
hybrid ai experience for customers
write once run anywhere
but even as we democratize access to dgx
the larger objective is to decompose dgx
into its smallest ai optimized parts
so that system manufacturers can compose
those parts into different form factors
for different scenarios
while adding unique value add
capabilities for their customers
first we productize the gpu board that
combines multiple gpus into a tightly
interconnected computing fabric
this is hgx
then we offered the core gpu from the
board as a standalone pcie card this is
the a100
now we have further broken down the a100
into smaller form factor gpus such as
the a30 that draw less power and are
lower cost while still providing
powerful acceleration
and finally we have productized the same
bluefield 2 dpu that is inside dgx
superpod
now usable in any server
with this we have now enabled a complete
ecosystem of form factors from
supercomputers to pizza box servers
workstations and edge devices
these form factors can be used in a
variety of scenarios from compute
to graphics to virtual desktops to data
center infrastructure
all made better by ai
hgx boards are the engine of
supercomputing
a100 and 830 are optimized for compute
a40 and a10 are best for a mix of
compute and graphics
and a16 combines multiple small gpus
into a great form factor for vdi
in order to help system manufacturers
create ai optimized designs
and to ensure that the systems can be
relied on by customers we created nvidia
certified a program for servers that
incorporate gpu acceleration
nvidia certified provides blueprints for
system design as well as test suites so
that system manufacturers can validate
their designs
today we are announcing the expansion of
nvidia certified to systems with nvidia
bluefield dpus
going forward the dpu will be an
essential component of every server in
the data center and at the edge this is
because as shown in the diagram the dpu
offloads accelerates and secures
functions that must otherwise be
performed by the host cpu
last year we announced bluefield 2 our
state-of-the-art dpu
nvidia is working closely with vmware to
use bluefield 2 as the host for the esxi
hypervisor as part of project monterey
this year we released the first version
of doka the sdk of bluefield we expect
that doka will do for the dpu what cuda
has done for the gpu enabling millions
of developers with a long lasting
consistent sdk that they can use from
one generation of bluefield to the next
and we are especially pleased to
announce that the world system
manufacturers are joining us in the dpo
journey by producing state-of-the-art
mainstream servers accelerated by nvidia
bluefield 2.
three of these companies are right here
in taiwan asus gigabyte and qanta
bluefield 2 is especially well suited to
inspecting the network for security
breaches
the typical approach is to run an agent
on the host cpu
this approach consumes host cpu cycles
and handles only a narrow window of the
network traffic
bluefield 2 allows full inspection with
no host cpu overhead nvidia has created
a software platform called morpheus that
uses the bluefield 2 and ai to
automatically detect and address
security breaches today we are
announcing that red hat is working with
nvidia to provide morpheus developer
kits for both openshift and red hat
enterprise linux or rel for short rel is
the most commonly used version of
commercial linux in enterprise data
centers today
cyber security companies will now be
able to use morpheus on rel and
openshift to bring advanced security to
every enterprise data center
but this journey is just beginning we
are already working on bluefield three
22 billion transistors
the first 400 gigabits per second
networking chip 16 arm cpu cores to run
the entire virtualization software stack
bluefield 3 takes security to a whole
new level fully offloading and
accelerating ipsec tls cryptography
secret key management and regular
expression processing whereas bluefield
2 offloaded an equivalent of 30 cpu
cores it would take 300 cpu cores to
secure offload and accelerate the
networking traffic at 400 gbps
this is a 10x leap in performance
bluefield 3
the next generation but the same doka
sdk
today we are also announcing the
expansion of nvidia certified to
accelerated systems with arm-based host
cpus
as the gpu and dpu accelerators take on
more of the compute workload for ai it
becomes useful to view the host cpu as
an orchestrator more so than as the
compute engine
energy efficient arm cpus are well
suited to this task and the open
licensing model of arm inspires
innovators to create products around it
an exemplar of this approach is nvidia's
own arm-based grace cpu which will be
available in 2023
grace is purpose-built for accelerated
computing applications such as ai that
process large amounts of data
two years ago we announced that we were
bringing cuda to arm simplifying the
development of ai and hpc applications
on arm today along with our partner
taiwan-based gigabyte we are happy to
announce a dev kit
this dev kit can be used by application
developers to prepare their gpu
accelerated apps for arm
so there it is
democratizing the hardware for ai with a
wide range of accelerators turned into
myriad form factors by a broad ecosystem
of system builders
103
different server and workstation form
factors from 16 different system
builders
more every day
now let's turn to the software of ai
every ai workflow can be thought of as a
four-step process
in a loop
the first step is to take a large amount
of unstructured data and prepare it by
extracting and organizing its features
the second step is to use the prepared
data to train the models
in many cases the models are then
validated or simulated before being used
in production or in the physical world
and finally the actual use of the models
which in turn generates more data that
is then fed back to the process
this is the recipe of ai
over the years nvidia has applied this
recipe to create application frameworks
for a variety of use cases including
conversational ai drug discovery
self-driving cars
robotics virtual collaboration and
recommender systems the engine of the
internet
these frameworks help companies jump
start their adoption of ai
for example nvidia clara is our
application framework for ai-powered
healthcare it includes sdks and
reference applications for drug
discovery smart hospitals medical
imaging and genomic analysis
one customer that uses clara is
astrazeneca
with clara they created an ai model for
generating new drug candidates that
target a given disease while maximizing
safety this method has seen recent
success with in silicon medicine using
ai to find a new drug in less than two
years
companies across the world are using
these ai workflows from nvidia to
improve their businesses and to help
their customers
a great example from taiwan is tsmc
we all know tsmc as a giant of the
technology ecosystem producing millions
of wafers every year for semiconductor
and technology companies across the
planet
analysts have referred to tsmc as the
hope diamond of the semiconductor
industry
in fact nvidia itself is a very large
partner of tsmc
proactively identifying defective
materials and classifying them to
minimize further defects is a critical
aspect of the chip production process
engineers at tsmc have developed an ai
powered system to automate inspection
and defect classification
their system is 10 times faster and more
accurate compared to the previous human
method
all of these ai workflows are built from
the same essential software libraries
and toolkits now we have brought all
these components together as nvidia ai
enterprise a coherent optimized
certified and supported software
platform for ai
nvidia air enterprise is fully
integrated with vmware vsphere the de
facto standard of enterprise computing
it is licensed priced and supported in
the same way as vsphere for a consistent
experience
and it all runs on nvidia certified
servers
mainstream volume servers that are
constantly racked into enterprise data
centers
71 different servers from 16 vendors and
more every day
but the components of nvidia ai are
useful for more than ai
nvidia ai includes rapids a set of
libraries that accelerate machine
learning on gpus
the same libraries can also accelerate
the core engines of big data processing
such as apache spark
cloudera is the de facto provider of
apache spark to enterprise data centers
around the world with over five exabytes
of data under management
as the technology landscape has changed
cloudera has developed cloudera data
platform
a hybrid data cloud platform that brings
customers forward to new technology
without having to reinvent their big
data pipelines
as we speak the 2000 customers of
cloudera are migrating to cloudera data
platform
we are pleased to say that nvidia and
cloudera have partnered to add
transparent gpu acceleration using
nvidia rapids
a fully integrated solution from
cloudera will be available starting this
summer with the release of cdp version
717.
customers do not need to understand
rapids or change their workflows in any
way in order to obtain the benefits of
this gpu acceleration
an example of an early customer is the
united states internal revenue service
with zero changes to their fraud
detection workflow they were able to
obtain three times the performance
simply by adding gpus to their
mainstream big data servers
this performance improvement translated
to half the cost of ownership for the
same workload
but
even as the world is transformed by ai
another shift is underway amplified by
the global pandemic
it is becoming more and more important
for virtual teams of people in different
locations to collaborate in real time
this is especially true for 3d workflows
from visualizing architecture to
creating photorealistic content for
games and movies to simulating
physically accurate worlds this is a
domain nvidia is very familiar with
and the same gpus and systems we talked
about in the democratization of ai
are in fact the ideal foundation on
which to build a platform for real-time
collaboration and simulation of 3d
designs
this is nvidia omniverse
omniverse makes it easy for teams to
collaboratively design and simulate in
3d
we built omniverse with open standards
based on pixar's usd a powerful file
framework and scene description language
with open standards omniverse enables
teams to connect and work across
multiple design applications or asset
libraries in real time and at the
highest fidelity
just as we did with ai we have put the
functionality of omniverse into an
enterprise-grade software platform
nvidia omniverse enterprise
omniverse enterprise can run on any
nvidia rtx powered infrastructure
from professional laptops to
workstations to gpu accelerated
virtualized or bare metal servers
we are excited to partner with leading
global system providers who will offer
nvidia omniverse enterprise later this
year starting at just fourteen thousand
dollars per year per company
the omniverse subscription comes with
connectors which are plugins for the
world's most important third-party
design applications allowing multiple
designers and creators to use their
preferred tools while simultaneously
editing a scene together
companies around the world and across
industries are already using omniverse
to collaborate in amazing ways
let's see how omniverse is ushering in a
new era of virtual worlds
[Music]
oh
[Music]
[Music]
[Music]
where are you from misty i'm from a
place called omniverse
[Music]
so there you have it
it is time to put accelerated computing
to use for every company
this is a giant opportunity and
responsibility for all of us
what we've talked about today are three
essential ingredients built by nvidia
the hardware foundation from which to
build any system
the software platform for artificial
intelligence the software platform for
collaborative design
together we can use these three
ingredients to transform every company
and yet this is the same hardware and
software technology
that is at the heart of g-force
the gaming gpu
which is where it all began
i'll close with a simple thank you
on behalf of my co-presenter jeff fisher
our ceo jensen wong
and all of nvidia
thank you
thank you for joining us today
thank you for being with us every step
of the way
and thank you for continuing forward
with us on this amazing journey
[Music]
Title: New PhysX FleX features
Publish_date: 2014-12-09
Length: 117
Views: 655133
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/1o0Nuq71gI4/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 1o0Nuq71gI4

--- Transcript ---

physics flex is a particle based
simulation technique for real-time
visual effects so far we showed examples
for rigid body stacking particle piles
soft bodies and fluids in this video
we'll be showing some of the new
advancements within physics flex this
scene shows two-way coupling of cereal
represented by rigid bodies with
particle based fluid simulation the milk
is represented by up to 100,000
particles surface tension and viscous
forces are modelled and because the
cereal pieces have a lower density than
milk they naturally rise to the surface
due to buoyancy the milk surface is
rendered in real time using ellipsoid
splatting and a subsurface scattering
approximation in this smoke simulation
the underlying fluid is modeled as
thousands of particles tracking velocity
and vorticity using particles provides a
sparse representation of the airflow and
allows smoke to interact with the other
particle based objects because this is a
grid free method there are no fixed
boundaries on the fluid field for
rendering we add vectors of thousands of
smoke particles through the underlying
velocity field and render them with
shadowing and a single scattering shader
here are more examples of how flex could
be used
the standalone physics flex library is
available for early adopters please
contact us for more information
Title: Connecting in the Metaverse: The Making of the GTC Keynote
Publish_date: 2021-08-11
Length: 1943
Views: 1439948
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/1qhqZ9ECm70/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 1qhqZ9ECm70

--- Transcript ---

GTC first and foremost is our opportunity
to highlight the amazing work that our engineers and our teams
here at NVIDIA have done all year long.
It's always been filled with the most amazing demos
in deep learning, super computing and computer graphics.
What was different this time was that we used our own platform, Omniverse,
to create the amazing visuals and demos for all aspects of the keynote.
Jensen: Here it is.
Slides for any keynote presentation are critical.
We use these visuals to convey extremely complex ideas to everyone.
In the past, we used to use PowerPoint to create these slides
This year, our slides, our virtual worlds, completely created and rendered in Omniverse.
GTC 2021 was a beast.
There were over a hundred slides that the team and I worked on, and it was a lot.
Because of the type of information that we get,
everything is super technical.
We have charts, we have images, we have systems,
we have a whole bunch of things that we need to fit normally on one slide
that needs to make sense to the audience.
So it's almost like solving a Rubik's cube where it's
an extremely complicated problem that we have to solve.
How can we visually tell this story and make it interesting for the audience?
Once we get the slide figured out and figured out what we want to say,
that's where we bring in the 3D team and start creating mock-ups for them,
and mock-ups can range from Photoshop to PowerPoint
to kind of getting the general visual idea.
When I first joined NVIDIA, we were still using CPU rendering
and each render would take quite a long time to finish.
And this year we full on went into Omniverse.
So everything in the keynote was created in Omniverse
using the real-time engine.
And that allowed us to quickly make changes.
We started Omniverse knowing full well
that we're not going to be able to build
the metaverse ourselves.
There are already great tools out there
that people use every day in every industry
that we want people to continue using.
Omniverse was built with the idea that we can take these existing tools
and new ones that will come into the future
and augment them with our technologies.
I would prepare my assets still in Maya, but then I would export it out
and I would do the layout in Omniverse.
It's quicker because you see in the viewport right away,
the shaded view with a lighting interaction, and it's very speedy.
Content changes come pretty frequently.
I created all these different panel templates that
they can just use and insert the content,
and it really helped us to deal with the amount of work
that was coming down the pipeline.
We went through probably 50 different iterations
for the template that we used,
and it ended up being something really beautiful.
One of the biggest challenges we faced with the slides were very large panels
that sat behind the hardware
and those panels were made of this black lacquer
piano lacquer that were super, super shiny.
So anything that you placed anywhere near it, it would completely reflect it.
We had to be very, very careful about where we placed those lights.
Another slide that we had a challenge with is the whole idea behind Omniverse itself.
It's very, very difficult to take all of the information of
how Omniverse works
and the apps that it uses and the collaboration and everything,
and show that visually on a slide.
For the nucleus part of the Omniverse slide, I think that alone
was 15 different variations.
Because it's so fast and interactive, you can hop on a
Teams meeting or a Zoom call
with a designer and have them watch you,
and they can make changes on the fly.
What if you moved it like more to the right? So then more of the card is lit up.
Okay. Sure.
And then you're able to ask all the questions you want at once
and see the results compared to almost having to work
on pieces by themselves
because you can't do the whole thing at once.
Because it's that many more emails and iterations.
We had two slides in the deck that were icons and text and that's it.
You have to find the balance between, "Okay, this looks elegant,
but does it look three-dimensional?"
And that was kind of a struggle that we went back and forth on.
The designers for the slide wanted to bring those icons into the 3D world.
So I was able to do the geometry in Houdini, export it into Maya.
And once I was able to have the 3D geometry,
I was able to apply my MDLs, this really nice,
beautiful gold material to those icons,
and then port them over to Omniverse and have them
live in a very real environment.
So they're reflecting on the floor,
they're reflecting anything that's beside it.
And because Omniverse is once again, so blisteringly fast,
I was able to get feedback from the designers.
It was definitely one of our big line art challenges that we didn't think
was going to be as complicated because it's just icons.
But the end result looked really, really beautiful.
There's like this triad, this triangle.
You have the model, you have the materials
and you have the lighting.
When all three of those elements are in sync,
you get a beautiful render.
They marry so well in Omniverse because of the MDLs.
MDL, our material definition language, which we've been investing in
for the better part of a decade is a big part of Omniverse.
Every aspect of physics that affects matter
should be described in a material.
They're all part of the super definition of a material.
So materials for this particular GTC keynote was crucial that
they all look photo realistic,
that when you have a diffused material,
it looks like a diffused material.
When you have a highly reflective material, it looks just like that.
And it's beautiful, especially when you're really trying to highlight
the materials that the industrial design team have worked tirelessly over.
The goal of the DGX station animation is to celebrate
our industrial design team and their amazing work.
Unfortunately, we only have three weeks to go from a detailed CAD model to a beautiful cinematic.
We had several people working with me
to develop the look and feel of the animation,
the lighting camera angles, and such.
We have only so much time to build, refine, iterate.
So we set up the entire scene within Omniverse.
Omniverse allowed me to make changes fairly quickly,
either in Maya or in Omniverse directly.
We just bring in the props from Maya through
the Omniverse converter and place lights
according to the needs, the look, and the feel of the project.
The CAD file was already prepped before this animation.
We spent about a day taking the DGX station
and converting it over to Omniverse MDLs.
Building out the environment took roughly three to four days.
All in all that process normally takes about a week,
taking the CAD file to a usable model,
and then about a day or two to get the shaders applied
and then rigged up for animation.
With an animation like this, moving a light just a little bit, has a huge impact.
And then being able to see that impact through the entire animation
is really advantageous.
With other packages, I would have to set up the scene,
compose the lighting for what I would assume would be correct,
then render out a frame,
wait 10, 15 minutes, and then get a result
and then adjust lighting accordingly.
Omniverse last year wasn't as stable as it is now.
It's gotten better because of the feedback we've had with
the Omniverse team,
the ability to insert features specifically for our team.
During development, we ran into a bug where the motion blur wasn't
working the way we expected it to.
The deadline was coming up quick within the next couple of days,
so we reached out to the Omniverse team.
A few of us made some suggestions.
I made a suggestion
to nacho on how we can do a quick hack
that would get us actually extremely high quality motion blur.
It's typically not done this way because the rendering is slower when you do it.
But our renderer is so fast and it's so accelerated that we could afford that.
And it actually gives you a better quality motion blur,
Nacho hack that into the renderer as a Python script
in an hour or two and it's still there.
Start to finish, it was roughly three weeks.
It was a real crunch all the way through.
You have a certain set amount of time,
so it'll always be delivered in that time
but the quality is what you end up sacrificing.
You have to start rendering sooner.
So you have to make compromises as far as look and feel
and not have as much refinement.
And in this case, we were able to use 10 machines
to render the entire animation overnight.
So not only did we show off our photorealistic materials
and lighting at this year's GTC,
but we also highlighted our highly accurate
real-time physics engine, PhysX.
PhysX actually predates NVIDIA's acquisition of a company called Ageia.
And with it, we got one of the world's finest teams
writing physics simulators for real time.
The primary focus of PhysX was to accelerate physics inside video games.
But a few years back, it started becoming clear to us
that the value of real-time high-performance physics
is now more than just video games.
We were starting to enter markets like robotics and autonomous vehicles.
And with PhysX 5.0, the latest version of PhysX that we have,
that's integrated into Omniverse,
we've been adding high performance simulations
for different aspects of physics that generally we don't see in real time.
In this year's GTC, we showed all these amazing demos
with all the PhysX technology,
such as fluid dynamic, amazing FEM soft body simulation,
and the highly accurate articulation system for robotic simulation
and large scale rigid body simulation in the GPU-accelerated platform.
My favorite one would be the skeleton hand picking up the cup,
moving the liquid into one cup to the others.
And the reason why I love that demo so much,
it's because it's extremely challenging
to simulate those scenes in real time.
So for example, in this scene, what we're looking at is
we have an articulate skeleton hand
and then each hand has five fingertips
and each fingertip is FEM-based soft body simulation.
So we're using that constraint system and touch those fingertips
into the rigid body
and using friction to lift up the cup.
Each finger tip has 6,000 tetrahedrals
and inside the cup, that is our particle system tech.
Just in that small amount of water
it actually has 20,000 particles inside.
Basically, 
why we are making this demo?
It made us realize lots of technical details we have to address.
For example, the fluid dynamics, because the particle is such a tiny particle,
it's a hard to make it stable in a cup.
So normally when you're doing the particle simulation, right,
even though the particle, the simulation is not stable,
which means the particles are jumping around,
but you can hide it by using a nice render on top of that.
However, in that scene, we don't have a proper render in there.
So if the particle is not stable, you will see like a bubbling effect in there,
but you see the particle is extremely smooth.
So during the process of making this demo,
we actually improved our fundamental tech dramatically.
Physics to us is, is extremely important.
We've had a renewed effort to not only make our physics extremely fast,
but also make it extremely accurate.
With games,
accuracy and fidelity is not as important,
but for tasks like robotics and training AIs,
the fidelity is extremely important
If the AI brain that we're building to put inside these robots
learn physics in a world where physics works differently
or incorrectly, not like the real one,
they're not likely to be successful once we move their brains into the real world.
When building modern factories, robotics are an essential element of the design,
but the only way to really know that your design is correct is to test it out.
And the only way to test it out today is to actually build it
and then try it.
With a digital twin, you get to make those design decisions
very early in the process
and have many iterations when it's still inexpensive and safe to do.
What we demonstrated, along with BMW, was a
vision for the factory of the future,
where we believe the tools and the workflows, the data,
everything will be streamlined by connecting it all
to our Omniverse platform,
we are able to bring significant improvement in efficiency
of the factory design, optimization, and operation.
The BMW factory of the future is a real life plant
located in Regensburg, Germany.
This factory is gigantic. It's a monster.
It's 600,000 square feet,
and it's just packed to the gills with stuff.
Two billion triangles, roughly.
There's tens of thousands of objects.
There's thousands of materials, and all those materials are all PDR with multiple textures.
There's hundreds of full resolution cars and slings.
There's 120 digital humans doing their digital human thing.
There's autonomous robots learning about their environment,
becoming optimal at what they do.
Then there's a whole other section of robots that is being driven by
data from the factory,
no animation happening to their articulation.
It's all just being fed straight from the factory into Omniverse.
So data starts in its CAD package, just like usual.
You send it into Omniverse and then as updates happen in the CAD package,
those updates are just sent into Omniverse.
And it's not like the whole thing has to come over every time.
It's just the differences that are sent at that point.
So Omniverse has the capability to ingest E57 format point clouds,
and it does it on the fly.
So BMW is able to provide us E57s, and we just simply
dropped that into Omniverse, and then
we went through the process of updating, creating real-time lighting,
creating real-time materials and textures
to create a one-to-one digital twin of the actual factory,
both in fidelity and behavior.
Materials play a heroic role in that.
NVIDIA's physically accurate MDL programmable shaders
are purpose-built to represent the real world.
For robots, in order for them to understand the world,
that world really needs to be represented in a physically accurate way.
And it goes way beyond form. Just because you have a good looking 3D model,
really doesn't tell the robot anything.
It needs to know, color, reflectivity, roughness,
metalness, light emission, subsurface scattering.
They all play crucial roles in the identification and the validation
of proper materials of any given object or work set.
Traditionally, when we talk about materials,
normally we're referring to the simulation of
the physics of light and matter,
but materials actually extend past that.
We also want to simulate other aspects of physics
like sound and how it interacts with the materials in a
building like this.
So material definitions also apply to other domains outside of the visual spectrum.
People are really at the core of assembling a car.
The way it works is that there're a bunch of stations along the line,
and every station is responsible for putting things
like the muffler or some other car part.
So it matters how far they are from the thing,
what tools they have to use to secure it.
Being able to simulate those movements,
in the context of the digital factory, we had an Xsens suit on,
I was here in my living room.
I was streaming that out through Motion Builder,
which was connected live into Omniverse
and all of a sudden
I'm in the world and I'm able to say, "Okay,
I'm having to lean over too far and it's too much strain on my back.
And maybe that should be up here.
And, you know, the order of the materials matters too.
So being able to just simulate that over and over again, till you really get the
right process, it's a complete game changer.
There were quite a few sleepless nights on this one.
All those nights were always filled with a lot of laughter
and comradery and making jokes.
And it actually worked. So that was pretty fantastic.
Factories are moving towards mass customization
and gone are the days where the lines would just spin out the same widgets.
Now consumer preferences are changing
and consumers are demanding more and more custom designed products,
but it's at scale.
So the factories had to be highly reconfigurable.
When a company like BMW has to rapidly reconfigure a factory line
to manufacture a new line of cars,
they have to keep in mind all these different optimizations
of sequencing, timing,
and the shortest path from point A to point B, the intralogistics.
And these intralogistics are fulfilled by robots.
So all of this needs to be simulated before you reconfigure the factories.
And that's where the digital twin and Omniverse would come into play.
So you're able to quickly reconfigure the factories,
find the most optimal configuration to produce the new line of cars,
and then you go about designing it in the real world.
And then you operate the factory.
DRIVE Sim contains essentially a digital twin of certain parts of the world.
We're taking certain areas, certain cities and creating a virtual world
that's a facsimile of the real one where we can test our AIs
that we're building to put inside these cars.
We, in order to, to create environments in DRIVE Sim,
we usually have two ways.
One is a manual approach and the other one is creating an
environment from scan data.
So for GTC, we actually went with a manual approach.
We first identified the entire Stuttgart route
near Daimler facility.
We identified two key locations.
And for those locations, we downloaded the reference data,
all the high res aerial imagery and digital elevation model data.
And then we used a third-party tool called MathWorks Roadrunner,
loaded those data into that tool and created an environment.
So we build the road network, we build the terrain.
What we have in Omniverse is, we have a MathWorks Roadrunner importer
that allows us to import that entire environment in one shot.
And when you ingest that entire environment, we do a lot of
processing on the Omniverse side to make it very photorealistic.
We replace certain assets with high resolution models
where you optimize many areas of the scene for Raytracing.
We also enable
randomization for synthetic data generation
and also for testing drive behaviors,
not just for camera, it's for multiple sensors, such as LIDAR and radar.
So a couple of challenges that we faced to create the environment for GTC,
we have a large tunnel to build, and it was super difficult to get it right.
So we had to drive within the tunnel, get lots of images, reference images, or videos
to see how the tunnel exactly looks from inside.
And then we took all that information and then we had
to model that tunnel from scratch.
We went through the texturing and the Raytracing engine did an excellent job
in simulating all the bounced lighting.
All the lights bounced within the tunnel gave that global illumination.
The sunlight was hitting just the right amount and illuminating
the entrance and the exits of the tunnel.
So it turned out quite well, but it was one of the challenges that we overcame.
The biggest challenge that we have worked with
when working with the DRIVE Sim folks
is really meeting the performance needs that they have
A really important aspect of this is that we're using DRIVE Sim
to train these sensors.
And we have to depict an  accurate version of the world.
The sensors have to believe that they're actually in the real world.
So the accuracy of the lighting and the accuracy of
LIDAR and radar have to be correct.
When we're making DRIVE Sim, it's really important
to be able to make these really beautiful ray traced images
and to be able to do that in high performance.
And so we build up high performance representations
of the scene that are very GPU friendly.
So when you have a Mercedes driving down the street,
we also have little programs that are controlling the cars that tell them how fast to
go and whether they need to turn left or right, or stop at that stoplight.
Then we feed that into PhysX,
and then PhysX does the actual simulation of the vehicle
and the response of the suspension.
What should the wheel angle be?
What should the wheel torques and brake torques be?
We pull those results back and then we feed those to the render 
so that we have accurate physical response.
And so one of the things that's tricky here is because
the DRIVE Sim main loop has to be running
at a really high frame rate.
You have to be able to have the physics interaction happening
and our other interaction happening.
And we do that by having these things be asynchronous.
I remember I forget this was maybe like three weeks out
from when we needed to deliver a recording.
And we went up to record and the vehicles would all move,
but their wheels all stayed behind.
It was fun to watch.
But, um, yeah, we fixed that one.
Now, with this kind of accuracy, we could generate all the synthetic data
with various kinds of lighting and randomization in place.
And we proved that it works
by using these training data and deploying it.
And we saw the Ego car could drive from point A to point B all by itself.
It was all using training data generated from Omniverse for GTC.
The factory of the future and DRIVE Sim demos really shows
Nvidia's passion for pushing ourselves to the limits,
but we didn't stop there.
Jensen challenged us to create a digital twin of his kitchen
and also himself to be featured in the pivotal moment of the keynote.
This kind of undertaking is the epitome of what GTC is.
It combines everything that NVIDIA does in computer graphics, AI, and simulation.
In order to pull this off, our creative engineering and research teams
all had to work together as one.
Prior to COVID, NVIDIA was putting on large events and keynotes
with Jensen on a stage in front of a large audience.
Once COVID hit, we had to rethink what those keynotes would be.
Welcome to NVIDIA GTC 2020, our first kitchen keynote.
So Jensen challenged us to find ways to elevate his kitchen keynote.
So we went off and created a 3D digital twin of his kitchen
that we could find unique ways of bringing his kitchen to life.
We took hundreds of photographs from every angle of his kitchen,
pumped all of those photos  through a photogrammetry application
that would build a coarse model to scale.
And that became the starting point in which our
modelers would add a lot more detail and accuracy to the model.
The kitchen is very complex. If you look closely, you're going to see like
thousands of screws, a lot of hidden objects in the scene
that are actually the fabricated components of the kitchen.
I would say there's probably six to eight thousand objects in the scene.
Polygon-wise, in the hundreds of millions.
There are a few little Easter eggs that we put in there.
There's a coffee grinder with an NVIDIA logo on it.
The olive oil tin, the name was changed to Tao,
which was one of the software products that we announced.
And somewhere around there a Lego Jensen's running around.
So as part of the keynote, not only did we virtualize the kitchen,
but Jensen challenged just with virtualizing himself.
You'll see the kitchen disassemble, and when Jensen
reappears inside of the Holodeck,
it's actually the virtual version of himself.
He proceeds to introduce our new CPU, teleports away,
and then the next time we cut back, we're back to the real Jensen.
I've been in visual effects for over 20 years.
So creating a digital replica of somebody real
is something I'm quite familiar with.
But in this case, it's a little different.
Unlike a film, we're able to capture as much data
as possible of this person.
We have a very minimal set and everything else
had to be done through AI to synthesize.
We've been developing a lot of AI-based generation animation
rendering technology.
And that was the big push to see how far our technology can do
with very minimal data of Jensen.
We do need some scan of Jensen, a 3D scan.
We found a service that had a large truck and inside
the back of the truck were hundreds of DSLR cameras.
We pulled that up to Jensen's house.
He stepped in through the back of the truck.
All right, what's the game plan.
And 3, 2, 1...
Nice, you can relax.
How was it?
It hurt just a little.
In a series of poses, took thousands of photos of him
And that was the base in which we constructed the 3D model.
So once we had the model of Jensen,
now we have to bring him to life.
Two main components to bring him to life
was the facial performance and the body performance.
The facial performance was driven by our technology Audio2Face.
The beige hue on the waters of the loch impressed all.
An automated way of using an audio clip
to drive the facial performance of any likeness character.
And that returned a pretty good result,
but it wasn't pixel perfect and it wasn't photoreal.
We then reached out to the video research team.
They were working on a technology called Face Video to Video.
And what that would do is that would take a photograph of Jensen
and map it to the animated CG version of himself.
This Vid2Vid technology enabled us to bridge that uncanny valley from the CG model
to a photorealistic, convincing version of himself
full screen on camera.
Kevin is like the director.
He planned the whole video shootings and the scene
and where Jensen should be.
Simon's team is more like the main actor.
They created a CG model of Jensen to do the performance, acting, talking.
And our team is more like the make-up men
with AI to put fancy makeup on top of the CG Jensen
to make it realistic.
One challenge in render phase, especially in the Holodeck environment,
is that strong specular highlight on the forehead.
To have a realistic rendering,
the specular highlights should remain in the right position.
It shouldn't rotate with Jensen,
and this is a deep learning method.
You don't have the physics to control how it looks like.
We created a room where the lighting is similar
to the Holodeck in our final CG world.
We had Jensen's stand there and we
take a couple of videos and it was about three, four days
before the keynote.
And we are about to quickly leverage the data to fine-tune our model
and achieve a better illumination control.
So bringing the face to life is just half of the battle.
The body needs to reflect the vocal performance as well.
So we were working on another technology called Audio2Gesture.
Amazing increase in system and memory bandwidth,
the base building block of the modern data center.
What this does is take
the same audio input and output gestural qualities
that are reflected in what he's actually saying.
In order to do this, we hired an actor
to learn Jensen's physicality by studying past keynote events.
We had him get into a motion capture suit,
record eight hours of him reading past transcripts of Jensen keynotes.
At the same time, physically mimicking Jensen's performances.
That eight hours became the training set that would be driven
by Jensen's unique audio delivery for the current keynote.
Now you actually just can provide a voice and they'll have body motion.
We had so many varieties of Jensen to pick from.
We had a 21 versions and each take, each time we do an update,
there's 21 of them to choose from and different varying performances.
It was actually getting to the point where I'm actually casting
because they were all performing well.
The performance will be loaded up to CG Jensen's model to create the CG shots.
And we take the video and turn the face part into realistic Jensen.
The face part is then composed with the CG shots
and the final Jensen including head and body
is put into the kitchen to complete the final production of digital Jensen.
As an AI researcher, I come from a machine learning background
and I was not familiar with the movie production pipeline.
An important thing for me is to feel the pain,
how difficult it is to produce the digital human
and to kind of challenge myself,
how we can create tools to make digital human production a much easier process.
The virtual kitchen and virtual Jensen presentation was done in Omniverse,
along with many other applications that help create the textures,
some of the modeling as well.
And that's the beauty of Omniverse,
It's a collaboration simulation platform that enables us to work
multi-directionally across all these applications
while we can build and develop custom technologies very quickly,
especially AI based technologies,
which there isn't many platforms we can do that on today.
The beautiful thing Omniverse is you don't have to give up those
apps that you've known and love.
I still use Maya, I still use Houdini, and Substance as well,
but I'm also using something that can make it
really, really fast to get beautiful results in almost no time.
To me personally, I feel the quality has increased and we moved into
the photorealistic phase of look and feel.
The RTX render inside of Omniverse allows us to do real-time Raytracing
in a way that I haven't seen in other things on the market.
So I feel like the ability to be able to control our destiny with Omniverse
and be able to build it to be something specific for what they need
I think gives us a lot of power.
We built Omniverse first and foremost for ourselves
here at NVIDIA.
We built it for our simulators,
for autonomous vehicles and robotics,
but also for our creative team.
We started Omniverse with the idea of connecting existing tools
that do 3D things together to build a sort of
what we're now calling the metaverse.
We spend a lot of energy and time and blood, sweat, and tears to
present it in the best light possible.
If we do our jobs right and we build something
as great as we believe it is,
then I believe 20 years from now,
we're still going to be using it,
but it's going to be much more evolved.
With so much groundbreaking stuff here at GTC 21,
I really can't wait to see what the team does next year.
Title: NVIDIA and Intelligent Voice Speech to Text Recognition Using Deep Learning and GPUs
Publish_date: 2016-06-09
Length: 155
Views: 14768
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/1RVAfROVad4/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 1RVAfROVad4

--- Transcript ---

communication within business is
happening on many platforms it's not
just paper anymore
all of those can provide content that
might be relevant to a legal debate
voice is a serial medium so voice is one
of those things where actually in order
to understand what's in it
you can't scan it like a page you can't
kind of quickly look down there you have
to listen to every single bit of it what
intelligent voice is doing for the Revue
industry is it's taking that reasonably
difficult medium and it's giving our
clients an insight into the content
without having to review or listen to
the entirety of a recording most of the
projects that we work on a
time-sensitive and quite often cost
sensitive by simply reducing the volume
of information by half for example we've
just saved the client 50% of their
review course without anyone having to
listen to anything where we operate is
at the what you might call the decoding
in so the bit where you take the speech
and you turn it to text and we're still
the only company in the world who can do
that on NVIDIA GPU technology what the
GPU side of things enable us to do was
to take one single phone call or a video
file or something like that and process
it 30 times faster so if I take a Tesla
k80 card for example that will process a
phone call at 30 times the write that I
can do it on a CPU and now in the last
couple of years we're using deep
learning techniques and that is
revolutionizing the way we do things
most telephone calls take place in a
noisy environment very few people
actually take the time to go for a quiet
corner and then speak very slowly again
you know I when I'm on the telephone I
gavel away and people gavel back at me
and they may be talking over each other
so deep neural networking
enables us to build models which can
cope with a lot of these vagaries so a
lot of what we do is not just taking the
the audio and turning into tags we try
to understand what's being said I think
that GPU technology will underpin
everything we do going forward and I
really mean that
Title: Subscribe to the new NVIDIA GeForce YouTube channel!
Publish_date: 2017-01-03
Length: 48
Views: 18753
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/1SuKRmH4DWc/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLD21gMrOqt0SgoGy5q4fCCHMDAP3g
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 1SuKRmH4DWc

--- Transcript ---

hi everyone if you're a fan of g-force
I'm here to tell you the G Force now has
an official YouTube channel subscribe
now to be the first to say new g source
products exclusive game content and
previews of upcoming titles + strategy
and tips to make you a better gamer and
because it's g-force will also have
shows on tasking pc monix triple-a
titles vr in these major gaming events
and eSports when you subscribe you're on
your way to winning our CES 2017 contest
where you can win your very own geforce
gtx 1080 so head over to the geforce
channel by clicking here and stay tuned
for our upcoming CES coverage from Las
Vegas you won't want to miss it see on
the GeForce channel
[Music]
Title: GTC Europe 2017 - Opening Keynote
Publish_date: 2017-10-17
Length: 7084
Views: 23275
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/1Te9PL46oIE/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLBkhXI0qhTAJUNfNxIwJFCye0ygYw
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 1Te9PL46oIE

--- Transcript ---

I am a visionary exploring a universe of
data to sharpen our view of the most
distant galaxies
in studying black holes to help prove
Einstein's theory of gravitational waves
I am a healer giving doctors the power
to turn mountains of data into
life-saving breakthroughs identifying
lung cancer earlier and fewer false
positives
and finding new ways to bring cures to
market faster I am a helper empowering
the disabled in their homes and breaking
down barriers she's so beautiful who is
she across languages and generations I
am a protector
keeping our data safe from cyber
criminals and helping the lost find
their way home
I am a navigator mapping our world one
millimeter at a time
and making even the largest self-driving
vehicles safer for the long haul I am a
teacher
analyzing half a million player moves
every game to identify strengths and
weaknesses
and a learner discovering new strategies
from complex games I am a creator
learning to paint from the Masters and
applying their styles to create original
works of art I am even the composer of
the music you're hearing
I am a I brought to life by Nvidia deep
learning and brilliant minds everywhere
ladies and gentlemen please welcome
nvidia founder and CEO Jensen Huang
[Applause]
welcome to GTC 2017 GPU computing is the
topic of this conference we've dedicated
our company's lives to advancing this
form of computing to enable
breakthroughs in science all over the
world and this is what GTC is dedicated
to do well we have a lot to talk to you
about today so let's get started there
are two fundamental forces that's
shaping the computer industry today the
first is what is known as the end of
Moore's law as you know very well
Moore's law is propelled by three
fundamental forces over the last 50
years the first is that semiconductor
physics has made it possible for us to
scale transistors making them smaller
and smaller as a result making them
faster as well as denser by making a
denser we have more transistors and when
we have more transistors we can apply
them to improving the architecture in
the design of CPUs over the years we've
made CPUs data pass wider we made it
pipelines deeper we invented all kinds
of great technology like superscalar
technology and out of order execution
each and every one of those generations
we applied those transistors the
increased number of transistors to those
design techniques meanwhile the
frequency of those transistors continued
to increase as a result over the course
of 50 years the microprocessor to CPU
increased in performance by 50 percent
per year or twice every couple of years
while the power of something that just
naturally gets faster and faster over
time without you doing anything about it
is really powerful software developers
develop their software once and every
single year that software got faster
meanwhile they load a more software onto
the microprocessor as a result features
got more rich so you either got you
either received higher per for
moments with the same software or more
functionality at the same speed over
time the computer industry advanced in
that way for 50 years and all Sun over
the last several years that freeride has
come to an end
transistors got so small that
essentially these switches these light
switches are always on to the point
where the amount of leakage
sub-threshold leakage this current
that's going through that transistor
whether it's in a state of 1 or 0 made
it impossible for those transistors to
continue to deliver on the performance
that has been promised and relied upon
for 50 years those transistors no longer
make it possible for us to increase
performance without increasing power
dramatically meanwhile as the number of
transistors continue to grow
microprocessors aren't able to apply
those new transistors to new design
techniques we ran out of ideas
as a result microprocessor architecture
advancement has slowed the transistor
performance no longer increased the
combination of those two factors caused
what is effectively at the end of
Moore's law while simultaneously
something else was happening
simultaneously this new approach to
computing called deep learning emerged
into the world it's been around for
quite a long time but deep learning had
a really interesting handicap now first
of all what enabled you to do what deep
learning enables you to do is whenever
you converse with a computer whenever
you load your photos into into into the
cloud and it magically discovers how it
ought to be sorted by the vacations
you've been to by the people that you
you took the picture of somehow it's
smart about collating grouping finding
those photos for you
it's the technology that's making it
possible for us to develop self-driving
cars
and potentially detect cancer earlier
that fundamental breakthrough has been
around as it turns out quite a long time
and in fact one of the most important
researchers AI researchers is was born
right here in Munich
jurgen schmidhuber was one of the
earliest researchers to have started to
work on artificial intelligence and
artificial neural Nets and he was one of
the first to apply GPUs to deep neural
nets and he was a his lab was able to
win the roadsign detection contest his
lab also was one of the first to started
to use recursive neural nets recurrent
neural Nets and created this new idea
called long-term short-term memory this
neural net architecture that's making it
possible for us to detect sequences and
recognize speech that was done right
here in Munich
well this deep neural net has had a had
a had a handicap and this handicap was
in order for this deep neural net to be
effective it needed an enormous amount
of data it needed to Train this layer
these layers and layers and layers of
neurons and synapses inspired by the
human brain it needed tons and tons of
data to train this network otherwise it
was over fit or otherwise known as
ineffective well in order to train this
neural net with huge amounts of data you
need to do huge amounts of computation
trillions and trillions and trillions of
operations necessary to effectively for
this neural network brain to learn how
to perform an intelligent task like
image recognition or voice recognition
well that handicap that handicap was
overcome with the discovery of the GPU
and so these two factors the end of
Moore's law and the emergence of a new
software development method a new
software technique happened almost
exactly the same time and these two
dynamics turbocharged the adoption of
GPU computing this form of computing
we've been working on for literally a
decade and a half in fact you could say
that ever since the founding of our
company we've focused on only this form
of computing no company of our scale and
size has ever dedicated itself does this
one singular field of computer science
GPU accelerated computing was all we do
now as a company of 12,000 people
we've now dedicated and invested some
close to 30 billion dollars in the
pursuit of advancing GPU accelerated
computing well if you look at the chart
if you look at the chart if we're able
to turbocharge these applications and we
can continue to advance Moore's Law at
the rate that it was promised at the
rate that we had relied on basically a
proxy if I could just summarize it to
approximately 10 times every five years
every five years it would improve by
another factor of 10 every 5 years have
improved by another factor of 10 in 15
years the difference the deficit of
Moore's Law no longer continue to
advance is 1,000 times you just got to
imagine what 1,000 times give you 1,000
times of almost anything I'm going to
put some of that into perspective today
but 1,000 times and this is the promise
of GPU computing because we do computing
in a fundamentally different way and we
rely on a large number of transistors
not faster transistors a large number of
transistors and because we optimize each
and every application with a specialized
function with a specialized processor
and the invention of CUDA made it so
that developers all over the world and
all different fields of science could
access this form of computing we've been
able to accelerate applications
in fact far faster than Moore's law
while deep learning researchers
discovered that and they realized that
using our GPUs they could overcome this
handicap that deep learning had
incredible effectiveness if trained on a
large amount of data which requires
trillions and trillions of operations
well this GPU this GPU here this is the
Nvidia Volta GPU this is the most
advanced processor the world's ever done
imagine nearly ten thousand engineer
years to design this one chip several
billion dollars of R&D the largest
single processor the world has ever made
the Nvidia Volta 21 billion transistors
all effectively put to use there's just
no way to build a microprocessor that's
21 billion transistors large because
there's just you run out of ideas you
don't need that many more transistors
the only way to make that CPU go faster
is with higher clock speeds that's not
true with GPU computing we could apply
parallelism in a very specialized way to
solve algorithms that are very important
to researchers and scientists so this is
Volta 21 billion transistors and a
whopping staggering 120 teraflops well
to put a hardened 20 teraflops into
perspective that's effectively a hundred
cpus this right here replaces
essentially a whole rack of computers
and I'll illustrate that more in a
moment so the end of these these two
factors these two forces the end of
Moore's Law and the emergence of
artificial intelligence where software
writes itself where software writes
itself by torturing itself with huge
amounts of data learning effectively
from digital experience
these two forces when it emerged
turbocharged the adoption of NVIDIA GPU
computing and the numbers show it over
the last five years the number of people
have attended GT sees around the world
has grown by a factor of ten the number
of GPU developers has grown to 600,000
there are 600,000 developers in the
world now who are learning CUDA from the
several hundred universities all over
the world that teaches CUDA from the
hundreds of text books that are written
about it six hundred thousand developers
can surely develop amazing software for
a few billion people to consume so we've
now reached critical mass in GPU
computing development engineers the
number of people who downloaded CUDA the
number of times that it was downloaded
in the last few years almost two million
but the shocking statistic is this half
of it was last year it took us 15 years
to get here and literally in one year's
time the number of CUDA downloads
increased by a factor of two well the
number of applications that we've been
accelerating started out with only one
in the beginning called Nandi molecular
dynamics but over at that time if you
take a look at the applications we now
accelerate of course computer graphics
computer graphics is a rear really rare
type of application this is the highest
computing demanding application in the
world that is also simultaneously high
volume because it's a it's a medium for
artistic expression it is a medium for
one of the most popular applications on
the planet videogames it is the medium
by which hundreds and hundreds of
millions of people enjoy their pastimes
it also happens to be incredibly
technologically demanding and so as a
result these two combinations
hi computation intensity technological
technological intensity and extremely
high volumes came together to gave us
this propellant this enormous R&D budget
that allows us to fund the advancement
of GPUs there's actually not another
processor that I know where the
computational demand is simultaneously
high and the volumes are absolutely
incredible that continues to propel the
development of that technology well
video games today not only look good but
the amount of Technology inside the
simulation of the virtual reality which
is basically physics simulation has
really benefited many other industries
the first of course is scientific
computing the advancement of science all
of our lives are made better for it
societies made better for it and just
about every single field of science that
we know whether it's quantum chemistry
material science fluid dynamics
molecular dynamics imaging weather
simulations energy discovery seismic
processing just about every single field
of science has now benefited from the
advancement of GPUs and then of course
deep learning our architecture is so
pervasive it's so accessible that every
single developer every single deep
learning researcher has jumped onto this
platform and then recently even
databases have been accelerated sequel
databases are going through a complete
reinvention if you could access video
games as fast as you can on a PC why
can't you take the entire enterprises
data put it into one computer and play
your company's database as you would a
video game and surely the developers who
are working in these companies whether
it's map D Connecticut blaze blazing DV
screen technologies they've completely
re-engineered the database
now the reason why the database is so
important is this if you can figure out
a way to access the data and refactor
two data and recreate its graph and it's
interactive it's all of its
relationships your ability to analyze
and seek out dependencies and
relationships and correlations among all
of the things that's happening in the
world is dramatically improved and as a
result on top of these companies these
amazingly fast databases a whole bunch
of analytics companies are starting to
emerge and whereas deep learning
requires an enormous amount of data many
of us don't have as much data in any
particular area in our company and yet
we would like to discover insights from
them their traditional machine learning
algorithms that have now been
accelerated instead of requiring a
high-performance computer a
supercomputer to run your machine
learning algorithms whether it's
clustering k-means clustering or
generalize linear regressions or
gradient boosting or support vector
machines all of these machine learning
algorithms are now being accelerated on
GPUs and so from media and entertainment
scientific computing deep learning which
is revolutionizing just about everything
that we do on the Internet to enterprise
computing is an analytics the GPU is now
find itself with a rich rich suite of
applications that it accelerates I think
there's something like 450 applications
now serving just about every important
industry the industries that we find
ourselves in are some of the most
important industries in the world
high-performance computing of course
internet services of course just about
every time you talk to your phone just
about every single query that you make
every photo search that you do every
recommendation that comes your way has
somehow been accelerated by GPUs we're
in the process of revolutionising the
transportation industry
we've always been part of the design and
simulation part of their workflow now
we're inside the car we also find
ourselves solving one of the greatest
problems challenges in computing
planning imagine hundreds of millions of
cars hundreds of millions of people and
we got to figure out which car and which
person to assign them to ride hailing
medical imaging being revolutionized by
artificial intelligence as we speak and
of course logistics trillions and
trillions of dollars of commerce are
going to be recognized by artificial
intelligence and robotics we find
ourselves solving some of the greatest
challenges in computing today solving
the unsolvable for these industries
where our company is all about creating
tools to enable the breakthrough of
scientists researchers and developers
enabling breakthroughs is the ultimate
purpose of our company we make tools and
so you could just imagine our delight
when two recent just last week announced
Nobel Prize winners the work that they
did the revolutionary work that they did
that we were able to make a contribution
to won the Nobel price of Chemistry
cryogenic electron microscopy they could
literally freeze the molecule in mid
motion and take a picture of it now
taking a picture of a molecule at atomic
scale whether it's an antibody or it's a
virus so that you can understand the
mechanics of biology taking a picture at
the atomic level you've got to imagine
how incredibly tough that is and so
these researchers these international
researchers was able to do that using
this technique called cryo-em and crowd
now makes it possible for us to
understand biology and molecular biology
at a level that nobody could ever
imagine if you could see something in
action imagine our ability to understand
it a hundred years ago
Einstein predicted that gravity was a
wave well as a result of the advances of
some American scientists rain or Wiese
Barry bearish and the Kip Thorne they
detected the world's first the for the
very first time gravitational waves a
disturbance in the universe measured the
slightest signal of a gravitational wave
as a result of two black holes colliding
because of our GPUs they can reconstruct
that image just about every modern
instrument just about every modern
instrument whether it's the astronomic
or the atomic scales just about every
modern instrument a scientific
instrument has a GPU in the back and the
reason for that is this just about every
form of instrument every form of
measurement every form of detection we
know today because we're trying to do it
at astronomic and atomic scales requires
computation just about every single
instrument is a computational instrument
today and without instruments science
can't advance what you see what you see
can give you a great deal of insights so
from the astronaut astronomic to the
atomic now we bring you back to the
human scale our GPU it makes it possible
for us to imagine being somewhere else
well one of the things that we do of
course is beautiful computer graphics
and I'm about to show it to you but
today we're announcing a brand new
product that's called the NVIDIA
holodeck this has been a dream of ours
since the beginning of computer graphics
we've been pursuing this dream for a
very very long time and today we've
taken a very giant step I'm delighted to
show it to you we combined several
things in this holodeck we think of this
is the design lab of this future we
combined several things photorealistic
computer graphics of course but once you
go into a holodeck into a virtual world
you would like it you would like this
world to behave according to the laws of
physics if you touch something you would
like to know you've touched it if you
drop something it should fall to the
ground
if you lift something it should seem
heavy unless you don't want it to be
heavy it should obey the laws of physics
it should allow us to all be in there
you should be able to come into it I
should be able to come into it from
anywhere we are in the world enable
virtual collaboration and then lastly as
we know if there's going to be virtual
there if there's going to be artificial
intelligence in our world there should
definitely be artificial intelligence in
the virtual world and so we wanted to
create this world we call it the
holodeck this virtual world where we can
all be part of it and so why do I show
it to you now guys hey Jenson hey hey
guys
Sean you're the one in the white right
yeah I'm right here in the middle and
I'm here with Lars and Igor hey now the
truth of the matter is you guys aren't
together this is the benefit of holodeck
exactly
you could be almost anywhere and we
could Network into it and you can now
have a shared experience in virtual
reality now ladies and gentlemen what
you're seeing here what you're seeing
here of course is going to be a little
bit limited by the projector when you
get a chance during this show I would
love for you guys to try a holodeck
yourself the graphics that you're seeing
that they're seeing the graphics that
they're seeing runs at 90 frames per
second okay the graphics that they see
runs at 90 frames per second as a result
they could move at will inside this
environment and feel really really
natural they don't have to be in the
place they could be in different rooms
they could be in different states and
most of the time when we're back home
and I'm enjoying holodeck with them the
team is in st. Louis in another building
and yet we all seem like we're together
okay guys show us what you want to show
us sir just because it's a holiday of
course that means we can load in any
environment that our imagined nation
could could bring us so we've created
this environment to do a study of
another object if we want to see
something represented we can bring it
anywhere and really check out the design
Wow that's a beautiful lab it's
certainly not bad but a dominant could
you could you bring the car in for us
please
so Jensen I've seen the inside of your
garage and you're much more of a car guy
than I am so so what do you think well I
don't know pretty proud to drive the
brand new McLaren that's a that's
beautiful okay so because the whole
model is here we just imported it
straight from CAD all of the details and
materials are present in the design the
leather the leather a carbon fiber look
at that the rubber steering wheels looks
like rubber steering wheels the car
paint everything is the original design
the benefit of this of course we're not
trying to build a video game we're
trying to build a lab of the future and
so the lab of the future has to seem
real here's what we imagine someday
you're going to go into this lab and
you're gonna create a product and you're
gonna have a ice helping you and the AI
could be handing you things the AI could
finish the job for you for example you
might be able to design the basic shape
of the car and you say you know what I
want to use off-the-shelf inventory that
I've got in the company and I want you
to finish my car based on the inventory
of parts that we currently have and then
the AI goes and finishes the job comes
up with the blueprints you might also
work with your AI to create the factory
that's going to build this car and so
all the robotic arms and how their
program that robotic arms are learning
how to be robotic arms inside this
environment and you're helping it along
and so when you're done when you're done
not only did you design an amazingly
beautiful car you also created the
entire factory necessary to manufacture
the car okay so that's how that's how we
imagine this as a collaboration tool as
a design tool going forward wow that's
just incredibly beautiful exactly as you
mentioned we're able to go in and change
designs and materials and really explore
what this you know objects about in this
case you know this McLaren is beautiful
but we can also take a look inside of
the car in a way that it would be
difficult to do with a physical
prototype yeah take this cool tools
wrong thank you so we can use this
geometry clipping tool to go inside and
take a peek through the engine cover
into the engine itself and really
explore the objects that we can see Wow
and it's just a marvel of engineering
and of course there's over 30,000
individual components represented in
here inside of the holodeck with us and
we can explore all of them individually
or take them apart
that's every part that's every part now
the thing that's really amazing is when
you're in virtual reality right now
you're looking at it from a third-person
view but when you're in virtual reality
this car is gonna look like a city right
in front of you it's just utterly
shocking the idea of a clay model is
gonna be utterly unnecessary and when
you walk up to the car you touch it you
touch it in virtual reality you're gonna
feel like you've touched it because
there's collision detection and with
with haptics it would touch back to you
okay you've made a physical contact with
that car what else can you guys do so
that's what we wanted to show you it's
kind of a sneak peek into the Nvidia
holodeck now when you guys are inside
the virtual inside this virtual reality
environment you don't have to be in the
same place but you feel like you're in
the same place right exactly I'm right
here with my buds yeah you guys look
great
all right guys thank you very much good
job the NVIDIA holodeck
the original des CAD designs the
original CAD designs completely
photorealistic is physically simulated
so it based the laws of physics you
really feel like you're interacting with
the environment it has virtual team
collaboration it doesn't matter where
anybody is you could put on your headset
come into that virtual environment and
you could be basically have the benefit
of virtual presence and then lastly with
AI integrated into the system you could
create future AIS and I've got some
examples I want to show you later
the system works basically like this you
could take your original CAD designs
without changing it at all
as many piece parts as you like the
original CAD design unmodified because
this isn't a video game this is a design
tool you take that entire CAD database
you could add all kinds of materials to
it if you like you could add all kinds
of materials and shaders and all kinds
of other other textures to it using my
in 3ds Max and then you import that
directly into our into the holodeck now
today we only support my own 3ds max
however this is a plug-in architecture
and when we're intending to support
literally every single CAD package in
the world and on the other side of it
just to illustrate that you could be
literally anywhere the NVIDIA holodeck
early access is now go to our website
the Nvidia comm slash holodeck website
and register for early access okay
the Nvidia holodeck super excited about
that good job you guys
[Applause]
well AI is the killer app the new killer
app of GPUs whereas computer graphics
was the killer app that really propelled
our R&D and really propelled us over the
course the last 25 years we've
discovered a new killer app and this new
killer app as a result of deep learning
is now being felt all over the world the
momentum behind the adoption of deep
learning is really quite staggering if
you look at it from startups with just a
few tens of millions of dollars of
funding just a few years ago five years
ago NGO has now grown to six point six
billion dollars the number of papers
that are published around the world on
deep learning has grown to three
thousand this year now three thousand
the way to think about that is if one
out of a hundred researchers work is
worthy of being published imagine the
number of people who are doing deep
learning research and you could just see
that in the neural network conference
the International neural network
conference nips is neural information
processing systems basically neural
network neural network artificial
intelligence conference over the years
look at the attendees when they register
this year long before literally a month
before the conference starts a general
purpose panic to register i think i
think i think only david hasselhoff in
germany
can attract this level of enthusiasm huh
guys David Hasselhoff he might actually
be in the back I just have haven't come
on as a guest so so you could just see
the the adoption of deep learning and
the momentum around around AI and and
that the type of work that's being done
just spans so many different fields of
computer science and different fields of
science there's a healthcare enormous
amount of research being done there
robotics enormous amount of research
being done there natural language
understanding speech recognition is
taking the words that come out of my
mouth the sound and translating it to
text sound to text speech to text that's
speech recognition but what did I say
and what did I mean that's natural
language understanding natural language
understanding obviously is much much
more complicated a very deep area of AI
lots and lots of work being done there
if natural language understanding could
be cracked could you imagine how it's
going to change the way customer service
is done how we interact with computers
incredible incredible opportunities the
number of research work is just really
shocking and people are solving these
problems that we've for the longest time
felt was impossible to solve and so
today I want to show you a few that are
just kind of fun it's easy to understand
and when you think about it it's there's
the the oh my gosh moment the first
example is done at a video research
what's happening here is a form of
computer graphics that's photorealistic
but takes a long time it's called ray
tracing we're simply following literally
following a ray of photons as it bounces
all over the room eventually coming into
our eyes well it takes a long time to
literally trace each one of those
photons and that's why when you see
those white dots and black dots that's
when
the computer graphics is not complete
well we taught an artificial
intelligence network how to finish the
job and the way to think about that is
if I showed you this incomplete picture
I bet all of you could figure out what
is the best color to put into that dot
somehow we figured it out in our brain
well we taught a computer how to do the
same thing and it literally filled into
noise imagine what you could do with
this thing using the same auto-encoder
architecture you can now take a image
that is very low in resolution and
enhances resolution from absence of
information we can create information
that fundamentally breaks information
theory from the absence of information
we can create information which is
what's happening there and so you can
increase resolution if you have a
photograph that's ruined parts of it was
ruined we can go back and fix it just by
you looking at it if you could figure it
out
we can probably teach at artificial
intelligence how to do the same thing
here's another one you and your men the
avocado is a pear-shaped fruit with
leathery skin smooth edible flesh in a
large stone and so this is what's a
picado is a pear-shaped fruit with
leathery skin to smooth edible flesh and
a large stone so this is this is what's
happening if I could just get you to
stop so this is what's happening imagine
imagine lip-reading you learn how to
look at the animation of somebody's face
and you could imagine the words that is
being said this is the inverse of
lip-reading we had the artificial
intelligence network look at a whole
bunch of videos of somebody talking and
from it it can now infer
what is the animation of the face from
the words spoken
what is the animation of the face from
the word spoken okay
like lip-reading except backwards this
one is called pose estimation pose is
your 3d posture
how are you posed how are you positioned
in 3d space well we have to teach if you
were to look at a video you can probably
infer what is the pose of that character
in 3d space however how do you teach an
artificial intelligence network to do
that how do you write software to look
at a bunch of ones and zeros in a video
and recognize where is the person and
what is that person's three-dimensional
pose from 2d to 3d pose if we were able
to do that we can recreate that
character in 3d world looking at the
video so shining a camera on top of me I
can now capture me and beam me into
virtual reality so imagine the holodeck
next time there'll be a video video
camera just looking at me and somehow I
warp into this virtual reality world
wrench is doing amazing work in pose
estimation this is really cool this is a
3d animation character this is a virtual
reality character and based on whatever
you throw in front of it whatever you
throw in front of it it figures out
what's the best way to animate to
navigate the course you see what's
happening so we learned how to climb
learn how to hop off all completely
based on artificial intelligence and
smart enough not to walk into that and
as a tiptoes through this little tiny
bridge just like we would do a virtual
character taught AI and then this last
one
before I show it to you it's a robot
that learned from an example this is
Peter reveals work at UC Berkeley he
goes into virtual reality he shows the
robot how to stack bricks just a few
times with just a few examples this is
what I want you to do stack bricks and
now it doesn't matter where the bricks
are located robot figured it out
one-shot learning imitation learning
basically how we learn so this is the
actual robot stacking actual bricks and
this is how it learned it in virtual
reality okay just some examples of what
AI is able to do none of these examples
have we been able to write software to
overcome for the longest time just
imagine writing software to solve these
problems and now finally we use this
artificial intelligence approach called
deep learning we feed it tons and tons
of information and somehow these deep
neural Nets learned well the work that
is being done in deep learning is
literally all over the world
there are just so many researchers doing
deep learning work and we're just
delighted that we are the deep learning
platform for modern AI there are all
kinds of platforms that are being used
all kinds of frameworks and tools that
are being used PI torch and paddle
paddle of China chainer of Japan MX net
that Amazon uses cafe to that that
Facebook uses Microsoft's cognitive
toolkits ent K theano
and tensor flow from Google the number
of industries that are advancing AI in
research and now in startups all over
the world we're working with some close
to 2,000 startups all over the world in
all these different industries all
developing on the Nvidia platform now
our strategy and our commitment is to
continue to advance with all of our
might
the ability to develop better smarter
more complex a is faster now the reason
why even though we've advanced computing
now advanced AI research by a factor of
a hundred in the course of the last four
years we've improved performance by a
hundred times in four years time deep
learning has been completely
revolutionized because of the GPUs
however we believe that we need to
improve the performance by so much more
and the reason for that is simply this
we want to make it possible for these
AIS to learn how to write software
itself but to have a eyes create a eyes
itself and therefore we need these
massive supercomputers just churning
away trying all these different types of
algorithms and having software write
software by itself just two weeks ago
just about the entire world's computer
industry adopt the volta our latest
generation GPU Volta is now available in
every single cloud on the planet from
Azure to Google Cloud to Amazon Web
Services Alibaba Baidu $0.10 it's
available everywhere anywhere on the
planet as a result all of these startups
that are working on in videos platform
no longer need to build their own
supercomputer they could go rent one in
the cloud and as they grow and their
workload continues to increase at some
point they might decide to buy their own
supercomputer and every single computer
company in the world now offers the
NVIDIA voltage GPU for their GPU servers
from IBM HP and Dell Cisco Lenovo
hallway basically the NVIDIA GPU is
everywhere it's everywhere in every
computer every single PC and every
single region available in every single
cloud and so we're going to commit to
continue to advance this computing
platform the fact that we have one
architecture
makes it possible for your software
development and for the company that you
create to be able to enjoy growth every
place on earth using one architecture
using one singular architecture you
could develop on NVIDIA and scale on on
NVIDIA all over the world okay so the
world's AI platform well the next major
challenge the Mex next major challenge
is after developing the AI you have to
run the AI developing the AI is called
training running the AI running the deep
neural net is called inference to infer
from input some insight about the future
whether it's classifying the image
recognizing things recognizing voice it
could be predicting what futures the
securities are going to be it could be
detecting fraud somebody who is trying
to enter break into your company so
these deep neural nets are trained on
NVIDIA platforms now the big challenge
for us is to make it run incredibly
efficient across all of the world's
computing platforms now this is this is
an area that some people call I o T but
I see the world developing in three
different ways the first is that the AI
network will run in the cloud and these
computers are massive in scale 10
megawatts 20 megawatts data centers and
they're running and inferring and
basically running queries from billions
and billions of people who are accessing
the cloud at the same time there's
another class of device way on the other
side that we're actually quite familiar
with today
things like fitbit's nest thermostats
your phone all of these little tiny
devices that have a little bit of
intelligence inside and that little
intelligence network could be voice
recognition
it could be image recognition it could
be recognizing your
heart rate measuring your blood pressure
measuring the tremor on a piece of
equipment and recognizing that that
tremor is going to lead to a failure in
about a week's time and so instead of
waiting when that machinery goes down
you might send somebody else to repair
in advance and so these this is called
trail it's called IOT and I believe over
time there'll be trillions of these
devices and then there's a class of
machines in the middle they're not
exactly super computing clouds they're
not intelligent sensors but they're
autonomous machines these machines needs
to operate and be intelligence and do
smart things even when they're not
connected to the cloud at all for
example a self-driving car you would
really really appreciate it if your car
drove safely by itself even without
internet connection it could be a drone
that flies out to seek to find somebody
who's lost and lost in a forest it could
be the drone inspecting pipelines or
going through and looking at looking at
a force fire or it could be somehow
somehow inspecting your your plot of
land and all the food that you've all
the farming that you're doing and
looking for whether there's a disease
that are broken out there could be
manufacturing robots it could be these
little drones that people are starting
to develop for the last mile of delivery
as a result of the Amazon effect where
more and more of us are basically buying
products online and hoping that it would
deliver be deliver to your house the
number of truck drivers in the world
can't possibly serve all of us buying
products online and hoping for it to be
delivered that same time and so
companies all over the world are
building these last mile delivery
delivering pizzas delivering soy sauce
in China delivering a hamburger to your
house
okay so autonomous machines all of these
are going to need AI inside them
and so the question is how are we going
to be able to support a world where you
only have a few mil and watts of power
on the one hand - something that has to
be completely autonomous on the other
hand and then of course data centers the
number of different types of devices
that we have the support is exploding
the networks that we're supporting is
exploding
starting with CNN cnn's convolutional
networks is a neural network that
magically figures out what are the
important features that represents a
particular object or a particular
pattern and it has this ability to to
learn the features by itself and to
generalize generalize that information
so that even if you change the color the
shape slightly the orientation slightly
it still recognizes it to be the thing
that it learned so you could change you
can have a Cappy partly occluded and it
still recognizes it you could have a cat
changed colors wear hat and it still
recognizes it the recurrent neural
networks learn sequences speech etc and
this is particularly I came out recently
from Ian Goodfellow he created this
thing called the generative adversarial
Network you have one network that's
trained to be a master detector you have
another network that's trying to fool
this master detector these two Nero
networks one of them generating fake
fake ones the other one detecting the
real ones so it could be an art piece of
art for example one network generating
fake art fake Picasso while the other
one has been trained to recognize real
Picasso and because the two of them are
competing against each other one trying
to fall it the other one try not to be
full and as they as they've learned and
learn and learn and learn and learn and
develop over time one becomes just
amazing at recognizing Picasso's the
other one becomes amazing at generating
Picasso's two networks now incredibly
powerful the adversarial network and
then lastly direct the recurrent neural
net the reinforcement learning neural
net that was used by Peter Beale and
this is basically trial and error the
network is trying over and over again
randomly initially every time it tries
there's a value function that determines
whether it's doing it well or not so
well whenever it does it well it's
encouraged to move in that direction if
it does not so well it's encouraged to
move away from that direction a value
function determines that reward and
Punishment recurrent neural networks is
how we're going to use is the basic
method for us to Train robotics
explosion of the type of networks the
number of and this is just a just a
brief view of it there are so many
different designs and so many different
architectures coming out
well the complexity of the network is
growing as well this little tiny dot
this complexity chart the size of the
the area of the circle represents the
number of operations times number of
operations and I just simply multiply
the two of them together number of
operations times the amount of memory
that it has to access okay
gobs times bandwidth that little tiny
dot is the original Alex net just a few
years ago that beat every single
computer scientist in the world in image
recognition that's that little tiny dot
that little tiny dot overnight without
learning how to do computer vision learn
computer vision fine enough to be every
single computer vision algorithm expert
on the planet and one image net
completely revolutionize computer vision
if it wasn't because of image net the
work that we're doing here in
self-driving cars and robotics wouldn't
have nearly progressed as fast as it has
and yet look at that over the course of
just four or five years it has grown
than these networks have grown three
hundred and fifty times this is a speech
network has grown three thirty times and
last several years this one's really
exciting language translation I really
believe that in a few more years I'll be
speaking into this microphone in English
and it will come out in German and it
will do it completely in real time and
so as a result that Star Trek universal
translator is about to happen the neuro
machine translation Network goes in
English comes out German the network
complexity has grown so much and so what
we've done is this two weeks ago we
announced two weeks ago we announced a
new type of optimizing compiler it takes
neural networks in it takes newer
networks in and it uses advanced
compiling technology and optimizes the
network for all of the target devices
that would run some of the target
devices are very high performance GPUs
like the one that I showed you Volta a
hundred and twenty tops
two hundred and fifty watts powers of
data center all the way to something
that's a self-driving car to a little
neural network that goes into a little
little robot we call Jetson just a few
watts it takes input from any framework
any architecture of network from any
framework runs it through this
optimizing compiler and it targets
machines that ranges all the way from
several hundred watts to several hundred
milliwatts the NVIDIA tensor RT 3.0 it
basically does this it takes this neural
network that's incredibly complicated
this is just one layer and all the math
that goes through it and we do wait an
activation precision calibration first
and then we fuse the layers some of the
layers can be combined together some of
the tensors can be combined together
some of them can be unlimited all
together and so we fuse the tensors
we do kernel optimization with dynamic
programming tracing almost every single
path to figure out what is the single
best path for optimizing the the speed
and the size and the accuracy all at the
same time and we also of course have to
support multiple streams because
sometimes the machine has multiple
cameras well the result is pretty
amazing so that's what it does and this
is how well does it and so if you take
the CPU we train the neural network
which is done largely on NVIDIA GPUs
today however once the network is
trained it's largely run on CPUs today
and the reason for that is because the
entire world's cloud is powered by CPUs
and so you train that network the
resident 50 now this is 50 layers deep
resonant the CPU plus tensorflow which
is the Google which is Google's
framework runs at a hundred and forty
images per second without ten so RT
without this optimising compiler the
output of tensorflow as we move it into
this volta v100 this GPU here this does
three hundred frames per second however
if you optimize it with tensor RT all of
a sudden that performance just
skyrockets five thousand seven hundred
frames per second because tensor RT
optimizing optimizes for CUDA and
optimizes for the architecture of our
GPU takes advantage of the instructions
that we put in here called tensor cook
tensor core which is optimized for deep
learning for open nmt neuro machine
translation basically language
translation for four sentences per
second
225 without tensor RT to 550 ok but this
is just one dimension of the speed-up
40 times speed-up in imaging a hundred
and forty times speed-up in language but
when you look at the latency how long it
takes
in addition first is how many you could
do at the same time the other is how
long does it take for you to do any one
of them in the case of images we have
the latency in the case of language a
hundred milliseconds versus several
hundred milliseconds will make all the
difference in whether you can talk to a
computer or not having a reasonable
conversation so tensor art III our brand
new optimizing compiler now what does it
look like in a data center this is what
it looks like in a data center so
suppose you had had no GPUs in a data
center no tensor RT this is a hundred
and sixty CPUs a hundred and sixty CPUs
will do 45,000 images per second and
this is basically what's happening in
the cloud today whenever you ask for a
photograph whenever you upload a
photograph this is essentially what's
happening every single one of those
transfers go through some kind of a deep
neural network and it's run on the CPU
cluster like this in this case 65,000
watts so 1500 watts per rack for those
racks about $600,000 without cables in
the top of rack switches and all the
power supplies about $600,000 and this
is what it looks like if you bought a
GPU
I know I practically fell on my seat
[Applause]
you see this before after before after
the more GPUs you buy the more you save
just think about that
the more you buy the more you save
one-sixth the cost 120th the power 120th
the power this will improve your total
cost of ownership a factor an order of
magnitude before after this is the best
demo in the world
this is the best devil in the world this
is the only demo that IT department goes
here do that again our IT department
goes on chess and I love that I love
that I'm gonna make that a screensaver
just go back and forth back and forth
just like that my kids always every time
like you know my kids know how cheap I
am but this is this is a they call this
save the money just won money save the
money
this is save lots of the money okay so
that's what it looks like at a data
center but let's take a look at that
let's look at that in real time what
does it really feel like okay so this is
this is what inferencing is now the
thing is the thing is what's really
remarkable about deep neural Nets this
is this is what you see what the
computer saw what the computer saw was a
whole bunch of once a zeros in three
different layers are G and B they're
just a whole bunch of ones and zeros a
whole mess of them and somehow from that
whole mess that's not a cloud from a
whole mess that's not a Beatle from a
whole mess that's not a cactus and
that's not a butterfly from that whole
mess of ones and zeros somehow it's got
to figure out what these things are and
so Ryan why don't we show sir here you
go okay so this is this is what's what's
happening is this is inference on a cpu
and you could see up there where Utz in
the corner up in the work so so it's
running on a cpu it's doing 4.8 images
second but what are these flowers though
oh let's have a look that's a sword Lily
okay there's a bird of paradise all
right I believe it
moon orchid that's a moon orchid okay
that's why a daisy that's an oxy a
Peruvian Lily okay
lots of lilies a Lenten rose a sad
flower oh I bet that one smells good
looks like it smells good
this is it okay that's what it looks
like a Japanese what antimony okay Ruby
lipt
cattle oh yeah I don't know half of
these a blanket flower okay so guys the
computer saw a bunch of ones and zeros
and it classified these images to be
those things
now here's my theory 100% of you failed
it I wouldn't have called any of them I
would he just set flower flower flower
flower that's a flower that's still a
flower you see no I'm saying that's a
cloud I don't know what that is that's
an animal myth that's Christmas that's
right and so 100% of us would have
failed it and here's this computer
looking at ones and zeroes figuring all
out
utterly superhuman imagine applying this
technology to medical imaging imagine
applying this technology to self-driving
cars imagine applying this technology to
robotics we can finally write software
that allows software to write itself so
that it can solve the otherwise
unsolvable well the problem is four
images per second looks fast here but if
a billion of us are using it
that's gonna be a data center so large
it will melt icebergs and so obviously
we can't do that
and so so I write show show us what what
joy looks like sure that's one GPU and
so this is the difference between the
fastest CPU in the world versus one of
these babies
now that's this is a that's a fast fast
car okay here in Munich here in Germany
this would be a supercar right here look
how fast that is
I just hope it's still right and so
these these GPUs that's a California
Poppy I'll file yellow iris okay so this
is what it looks like on a GPU with
tensor RT incredible speed up
[Applause]
562 in this particular Network we sped
it up 100 times this GPU has replaced
100 servers
that's the shocking amount imagine all
the money you're gonna save run out and
get one right away ok so that's four
images let's take a look at another one
well and so the next one the next one is
a technology that's done by based on the
technology by deep brown there's startup
company their startup company in Silicon
Valley and and what they've done is this
they've taken basically sonic
information and they mapped it into a
computer graphics into an image and they
use CNN's use image recognition on sound
information and as a result their deep
learning voice recognition is not only
incredibly accurate it's also incredibly
fast their application is for companies
who are who are with customer service or
they need to make sure that all of the
transcription or all the phone calls are
done within regulation it could be in
healthcare could be in finance it could
be in law it could be in customer
service hotels it can be retail so many
different types of companies in the
world that can't afford to do voice
recognition in the cloud they have to do
it inside their company and not only
that they want to have to ability to
retain the data the observation from all
of the speech so that they could apply
that information to other AIS to enhance
their business and so the thing that
deep Graham does is a voice recognition
engine that is just lightning lightning
fast now the thing that's really great
is this and so that we created this demo
it goes so fast so super real-time that
you could literally watch any movie and
search for words just search for the
sound of those words it's gonna listen
at super real time it's gonna watch and
listen the super real-time a movie and
find whatever scene we need okay so this
Brian's gonna show you that go ahead run
so you might have got there but we're
going to show it today so this is a
we've transcribed a whole bunch of Game
of Thrones and now we can just go and
search through it so we can find our
favorite scenes no one will take my
dragons take you take my dragons but we
can also find some of our favorite
characters Jon Snow
Jon Snow snow or Oh Jon Snow no oh he
knows nothing you know nothing screwed
you another one yeah do one more
Ned Stark Oh a son called for Ned
Stark's head now the entire that's not
bad not bad but we also search for
common phrases so your grace it was well
struck your grace yes your grace
wow you're great I wonder what these are
your grace
wow that occupies like 90 percent of
dialogue
[Laughter]
okay fantastic so so voice recognition
at hyper real-time at hyper real-time
let me give you let me give you an
example thanks a lot Ryan let me show
you one more
this one's really cool is Monti up here
hey Monte so so the folks at Columbia
Cambridge can sell Cambridge Consulting
Cambridge Consulting created this this
incredible artificial network and and
I'm not teas going to tell you about it
in just a second but we're gonna run it
so fast the inference is done so fast
that this artist this trained artist is
going to be able to collaborate with you
so Monty first of all tell us tell us
what you guys did sure so this is
Vincent here it's a combination of many
different deep neural network
technologies using generative
adversarial network technology but
several of them combined those processed
8000 masterpiece artworks from Western
art from Renaissance through to current
day 8000 is nowhere near enough
traditionally for deep learning through
anything useful we're going to generate
megapixel images here and you would
estimate we should need at least a
thousand times as much but because of
the generative adversarial nature this
challenge that Jensen spoke about
between different networks competing and
improving each other we can squeeze so
much more out of the data than might
traditionally be viewed possible and so
so just very very simply Monty and his
guys has trained this network how to
basically draw how to basically draw how
to generate art like the great artists
okay how to draw and they use this gain
network to basically learn how to draw
and then they used another network
there's a whole there's some different
networks but with but the other ad
network that they have to create is
something that would enhance the
resolution of the of the drawing so that
it's many megapixels so it looks
beautiful okay and so why don't you show
it to us oh okay I'll give you a short
demonstration so if I was just to touch
the pen onto the surface initially we
get kind of random noise there's no
possible way you can guess what I'm
trying to draw there that looks like
inherent thing
where that that's 8,000 artists all
merge together but if I so that went for
perhaps a kind of jagged landscape
something like that you can see in real
time as I do that it's beginning to
perhaps you know coloring it's getting a
hint that the sky is unlikely to be Rock
coloreds I can begin to draw more detail
it comes through more more clearly like
that it's a very very simple picture and
for example I've got a couple is going
from the yellow lines but I drew a
better one earlier that looks a bit more
like that and what's interesting there
if I bring the lines up for you can see
very very simple sketch so this isn't
recognizing objects this doesn't know
that it's a hasn't been trained just on
landscapes if I go and do something
damaging to the picture you can see it's
completely changed its mind on what that
is that's no longer a landscape and even
if I add some kind of super real detail
like that moon there we've now got more
of an illustration out of it it's it's
the same picture I can go in a second I
could do this yeah go on
alright so I I could be an artist please
get going read that okay all right
I think you can see how fast the
inference is running here so this trains
overnight on a DD x1 supercomputer takes
about 14 hours but it's now running you
know sort of real-time frame rate
inference on it on a small box here
we've also trained the network with
slightly different parameters the one
thing we can try some slightly different
styles so it looks like it's a little
bit like Picasso right absolutely
[Applause]
Wow
JH yes right there okay there'll be
$1,000 press the heart
let's try a few of those different
networks passes so there's some that
we'll see that perhaps as a lie no
prints or that have seen more modern
pieces Wow colorful yeah that's
fantastic look at that actually effect
incredibly fun
[Applause]
okay good job Monte thank you thank the
guys for me that's so amazing
alright so so you saw some examples of
us using deep neural networks to solve
problems that otherwise would have been
unsolvable in the past we we created a
platform that makes it possible for
researchers and developers to train
their network and develop their network
wherever they are it could be in the
cloud it could be in their data center
it could be under computer we created
this new tensor RT optimizing compiler
that targets the entire range of all the
processors that we make and as a result
sped up deep learning inference by a
factor of 50 a hundred 150 as a result
saving enormous amounts of money well
creating this deep Learning Network and
solving unsolvable problems has led us
to solving some of the most challenging
and most exciting and in our belief the
most transformational opportunities in
world transportation as you know is an
enormous industry it's the fabric of
society it makes it possible it gives us
freedom makes it possible for us to get
around of course to fundamental
technologies to fundamental technologies
has now inspired car designers all over
the world to do amazing redesigns of
what's possible in the future this is
one example this is the work that is
done an Airbus but if you could imagine
the wing is autonomous the wing is
completely autonomous there's an
autonomous machine inside a wing as a
battery-powered wing the car is
autonomous and this battery it's an
electric vehicle if you want to have a
flying car
you simply summon your wing the wing
flies to you you get disconnected from
the electric drivetrain the wing picks
you up and holy cow imagine the
experience and so so people are starting
to think about these ideas people are
starting to think about these ideas from
the extraordinary from the extraordinary
to
figure out how can we revolutionize
transportation for the last you know for
the 50-mile range which is what this is
intended to do - every single day's
transportation so what we did was we
applied ourselves to create a driving
Network a driving platform and create a
platform by which the entire autonomous
the entire transportation industry could
adopt to create autonomous vehicles our
driving platform consists of several
layers the first layer of course is the
computer the way you compute for an
autonomous machine is fundamentally
different second is an operating system
an operating system that allows it to
take information at real-time from all
the different sensors and process it and
algorithms and libraries we call drive
works that exposes the capability of the
computer to applications on top we call
drive a V drive a V for autonomous
vehicles and the type of algorithms and
the type of applications we have to
develop it's basically in three
categories one you have to figure out
where you are it's the perception
networks where are you in the world
where's everybody else around you
what are you sense the second is
localization based on everything that
you sense you have to figure out where
you are where everybody else is where
they're moving to and track everything
around you and then third figure out
based on all of that reason about what
to do you might have to stop you might
have to divert to mind have to turn
follow a route accelerate pass another
car there's a whole lot of different
actions that you could potentially take
and so we created this platform this
completely platform that is basically
drivable several thousand engineers at
Nvidia are working on this this is an
open platform it is fully functional
safe when it's done it is auto grade and
it employs three fundamental computing
approaches one is deep learning another
computer vision and then the other
is parallel computing we have to do
these algorithms in a whole lot of
different ways and the reason for that
is because we want diversity these cars
are going to allow us to not pay
attention and still drive us safely but
if something were to happen it could be
a systemic problem something in the
algorithm it could be a computer failure
maybe it just got old maybe it broke
maybe it got bumped into even if the
computer failed and even if the
algorithm failed we still have to have a
backup that allows us to drive the
passengers to safety
just imagine that how do you create a
computer that when it fails it still
operates perfectly and so the answer of
course is both having redundancy as well
as diversity we have to solve the
problem we have to solve the same
problem in multiple ways we have to
solve the same problem in multiple ways
one simple example is this one way for
you to avoid one way for you to avoid
hitting something around you is
recognizing and detecting everything
around you and figuring out where they
are and then simply driving where
they're not ok figuring out where
everything around you is and then
driving whether or not the other way of
solving that problem is to teach a
neural network to recognize where is it
safe to drive so one way is to detect
what it's not safe the other way is to
detect what is safe if we have these two
basic approaches for perception then
that provides us some amount of
resilience algorithms like this over and
over again are stacked all over this
computer so that we can provide
redundancy and resilience well let me
show let me show let me show you a home
movie the engineers are working really
hard and the car that we integrate all
of this into is called bb-8 and they're
driving all over California all over the
East Coast there's some 20 cars driving
around and we made a home home movie for
you let me just let's play it now
[Music]
here we're detecting distance the neural
net figured out what the distance is
this is a DNN visualization or another
way of saying it we've created an MRI
MRI cat scan for the brain neural
networks can be trained to recognize
your surroundings it doesn't matter what
the orientation of the cars and the
objects are the neural Nets could
recognize and learn it could you imagine
writing this by hand it's just
impossible recognizing where it's free
to free to walk and safe to walk
it's going to take several different
types of sensors to achieve a level for
self-driving car
this is lidar process this is a computer
vision this is light art using either
deep learning computer vision or lidar
to figure out where you are
what is your trajectory otherwise known
as Eagle motion
what is your emotion line our processing
is point cloud processing very
computationally intensive here we're
localizing ourselves relative to the HD
map using computer vision this is now
the car trying to figure out where is
safe where is it good to drive what are
some of the options that I could choose
and then the car has to choose the best
one using deep learning we can figure
out where is the center of the lane
[Music]
okay
we created the holes we created the
stack so that we can understand how deep
learning works so that we could help
help the industry solve one of the
greatest challenges in the
transportation industry and then this
platform is completely open you could
use you could use all our parts of our
platform whether it's calling on the
libraries that we have developing the
libraries completely yourself
programming directly to CUDA or using
our autonomous vehicle applications that
sits on top of it one of the areas that
is going to revolutionize society and
it's just an incredible way is right
handling you guys know this very well
the utility of a car is dramatically
higher the convenience of moving around
is dramatically higher more and more
people are moving into the urban areas
and so it's less and less convenient to
own their cars and so we see we see all
of us see this revolution that's
happening with ride-sharing or right
hailing the number of companies that are
working on the next generation
autonomous vehicle transportation
services it's just all over the world
there's just some really really exciting
work that's being done here in Germany
howdy
the work that's done by Daimler in
United States
uber lyft of course way Moe in China
Baidu Ford with our go AI GM with crews
start up cause Dukes doing really great
work on new tana me in Singapore and
Yandex in Russia just about every single
geography in the world companies are
trying to figure out how to make these
cars completely drive by themselves
without a driver inside which basically
means this in the worst-case scenario if
a car computer were to fail the
algorithms were to fail
the backup computer will have to
complete the mission the backup computer
will have to completely commit complete
Domitian even if a sensor were to break
even if somebody were to bump into your
rear your rear and somehow dislodged the
sensor so it's no longer calibrated you
still have to finish the mission because
otherwise you would have to
say in a very gentle voice get out
you would have to tell the passengers to
get out because there's nobody driving
the car you can't take over there's no
steering wheel and so the level of
computation that's necessary to solve
this problem is just enormous and so
just give you a sense this is a level
two sensor suite with cameras and radars
and here's a level five sensor suite as
cameras and radars a lot more of them
because they're all completely redundant
and then there's light ours that are
surrounding this car this car is just
packed with sensors if you think about
the work that it has to do it has so
much more resolution because it has to
see farther it has more redundancy that
has to do deep learning on he's got all
these surround light ours that has to do
point out point cloud processing these
cameras and lighters have to localize in
a redundant way they're tracking all of
the objects around the card because it's
likely this car is driving in urban
areas and there are people crossing the
street these cars also our mapping cars
because it's got to update the HD map
that the car is driving within as the
roads change there's no driver and so
every single time it has to have
complete confidence that it can complete
its mission has not have enough
horsepower so that the path planning and
the controls of the car is as smooth as
possible it turns out that in the old
world high-performance engines makes
cars smoother smoother for acceleration
smoother at idle smoother and cruising
in the future the more computational
performance you have the smoother your
ride will be okay there's a very similar
analogy there and then of course sensors
and computers have to have it has to be
fail operate and this computer has to
have enough capacity for software to be
added to this car hundreds of software
engineers are developing these cars at
the same time we simply can't guess
accurately enough how much computational
capacity is necessary and so this is
what they do this is what it looks like
in the trunk
and this is one of the most organized
ones
this is one of the best ones this one is
just it was done so wonderfully look how
clean that is see they realized that
someday somebody was going to take a
picture of their trunk and and so they
did such a nice job but usually several
thousand watts
several thousand watts of computing
several thousand watts of computing
eight high-end and video GPU cards power
these self-driving cars there's no way
to take this into production there's no
way to take this into production and so
we've been working for the last couple
years to create a computer that can
finally allow robot taxis to be deployed
in full production auto grade functional
safe the richness of software and this
computer has to replace all of this and
more and we call that Pegasus today
we're announcing the Nvidia Pegasus the
Pegasus is the world's first computer
designed for production deployment of
robot taxis to help us realize this
vision of the future of Transportation
finally deploying it in real worlds well
the amount of computation is just
absolutely nuts this computer here has
320 Patera operations per second put
that in perspective that's 400 CPUs
imagine imagine 400 CPUs that's
basically 10 racks of servers 10 racks
of servers shrunk down 10 racks of
servers about 20,000 watts shrunk down
into this little tiny thing and it's
Auto grades 500 Watts in the world of
electric vehicles power consumption
directly translates to mileage and range
in the world of taxis that directly
translates to reduction in revenues if
your car is not on the road
it's not making money for you
and so we've got to reduce the energy
efficiency increase the energy
efficiency extend the battery life so
that we can keep these cars on the road
for as long as possible so this is the
brand new Nvidia Pegasus a member of the
Drive px family architectural II
compatible to everything that we've done
so far for level 3 and level 4 ok
everything just runs basically we're
packing a supercomputing data center in
your trunk this is what it looks like
[Applause]
before after before some of my best
demos after what were your best demos
PowerPoint okay so that's the that's the
that's the drive AV and this is the
Pegasus and it's going to enable enable
for the very first time the ability for
all of these development projects to
finally hit the road and go to
production the other thing that we are
announcing today is a brand new AI this
brand-new AI is an SDK we call drive IX
intelligent experience we've basically
done this we fused we fused the AI that
drives the car and all the perception
layers that senses what's around the car
with an AI that's in the car that senses
you the driver and all of the passengers
in the car the sensing the perception of
what's around the car and what's inside
the car and some basic neural networks
that we've already pre trained to track
where your eyes are gazed where your
head is posed read your lips understand
your words all of those type of
capabilities gesture recognition
combined with the perception of what the
car sees okay the combination of that
the fusing of that is going to allow our
customers to write applications that are
really quite magical
you'll be literally be able to walk up
to the car and the car knows exactly who
you are and it already adjusted the
seats opens the car and if you're a
passenger knows who you are and adjust
the seats and changes everything
according to your desires you're walking
up to the trunk because you're carrying
a whole bunch of bags it recognizes that
you just went shopping it's you that
food is going to end up in your trunk
which now has a plate has plenty of
space because the pegas
and you're gonna put your food in the
trunk okay and so um all kinds of
different scenarios are going to be
possible
and so this sdk this sdk is going to be
available in early q4 and basically runs
on drive our drive platform and as a
result hopefully these cars becomes an
AI your car is an AI okay so it's an AI
car knows who you are
understands what you like it recognized
the situation around you recognizes that
maybe you're you're dozing off and you
shouldn't be maybe you're not looking in
the direction of some kids playing in
the street and it warns you say says it
to you nicely that you should be looking
to your right okay whether it's an
autonomous driving mode or not in an
autonomous driving mode your car AI will
look out for you creating the car
computer is just one of the
extraordinary endeavors that these
companies have to take in order to
create the autonomous vehicle fleet of
the future it is an ax absolutely
extraordinary undertaking software
development like it's never been done
computer deployment like it's never been
done the amount of software were writing
the amount of AIS we have to develop and
the fleets that are going to be tested
all over the world is an extraordinary
undertaking and it has a lot of
different pieces today I just wanted to
show you what it takes to build this
self-driving car not just a prototype
building a prototype is fun and easy but
getting ready to deploy the fleet is
incredibly hard first of all you've got
to collect a whole bunch of data and you
have to label them just perfectly for
every single country every single road
structure every single road condition
you do it because it's so important and
you do it because you can we have
hundreds of people doing this at Nvidia
you take that Network and you train it
on the supercomputer that we designed
the supercomputer here is called the DG
x and this GPU goes in here this GPU
goes in here can can we see it and that
can you guys see it okay I can't see you
guys seen it and but this is one of the
GPUs there eight of them here one
petaflop s' one petaflop so that's like
that's like that's that's basically a
whole wall of computers fit into this
one NVIDIA dgx this D GX is used for
training the network okay so we use it
to train the network and run training on
d GX but there are so many road
conditions so many scenarios we simply
can't collect data on because we like
not - we don't want to simulate somebody
jumping in front of your car at the last
moment there's no sense doing that and
so we decided to create a 3d simulation
imagine harnessing all of Nvidia's
capability and photorealistic rendering
and we created a simulator for the car
to try all of these different scenarios
that otherwise would be impossible to do
we call the 3d sim on dgx I've already
shown you drive a V on DP X Drive px and
then lastly recent we sim is like
gigantic undertaking and here's the
reason why it's so important to do it
every single time you update your
network every single time you update
your network before you put it on the
road you would like to know that it's
going to pass in simulation all of the
roads in the world all of the roads in
the world and over a period of time
we're gonna collect up enough data
enough videos for literally every single
road in the world in the world and so we
call this super real-time simulation
with D GX and using the new tensor art
III this is what we can achieve with
just eight Nvidia DG X's which is 8 +
video DG X's ok so this is a picture of
our Saturn 5 supercomputer the reason
why they're all intermixed like that is
because each
one of our DG exes consume about 3000
watts okay that 3000 watts of course
replaces a whole wall of supercomputer
of servers and so we're delighted by
that but we still need a whole bunch
more in order to simulate the world and
so each one of the racks can consist of
about 20,000 kilowatts 18,000 watts and
that allows us to hold about about six
of them and so what's going to happen is
this we collect and we collect miles and
miles and miles of road data and in this
case 300,000 miles we can simulate on
eight of those in five hours 300,000
miles represents 10% 10% of all of the
paved roads and United States all of the
pave roads in the United States and so
basically if you could simulate that in
five hours we could simulate every
single paved road in the United States
in basically two days so when we finally
develop our software and we want to
release it we can simulate the whole
thing in two days now of course that's
if you're perfect the whole point about
simulation is to find bugs and the whole
point about finding bugs is sometimes
you do find them and so we have to do
this over and over and over again and
that's one of the reasons why we have so
many of them so that we could reduce the
Reese emission time down to essentially
hours let me show it to you hey Justin
list this is um this is basically us
running in real time this is running on
one drive px1 Drive px is doing a really
great job it ignored the wiper snowy
road recognizes all the pedestrians the
signs it's doing a great job detecting
things detecting the road okay now let's
go to suppose we want to do this on our
DG axis and this is what's happening and
this is how we take a brand new network
the entire software stack and because
our architecture exactly compatible
between Drive px and dry and D GX
the supercomputer and the car computer
is architectural II in software
compatible we take the software stack we
put it on dgx and we run it at super
real-time okay
and so so that thank you thanks guys
good job
and so that shows you give you a sense
of what it takes to drive to create a
self-driving car from the training of
the neural network on a supercomputer to
the simulation of extreme cases on a
supercomputer to driving on a
supercomputer to testing on a
supercomputer that entire flow that
entire flow is extraordinarily
computationally intensive and so we've
plumbed this entire platform and to end
and it's open to our partners and
developers all over the world there are
now a hundred and forty five startups
working on the nvidia platform they're
building trucks they're building
shuttles they're building cars some of
them level four some of them are level
five some of them - for long-haul
reasons some of them for the last mile
there are autonomous machines coming
from just about every corner of the
world everything that moves in the
future will be autonomously assisted
some of them will still have drivers but
all of them will have AI and so
hopefully it will be a lot more
efficient in the long term and
transportation is gonna get reinvented I
mean there's just some really really
cool things mapping companies that are
using our platform they use our cars for
mapping and they use our platform for
map generation so autonomous machines
the next generation of autonomous
machines is super exciting let me finish
by talking to you about one of the
things that we're working on that's
super exciting and this autonomous
machine is unlike the autonomous vehicle
and the reason for that is the
autonomous vehicle is designed for
collision avoidance autonomous machines
are designed for interaction which means
they largely collide collision is one of
the most important things that
autonomous machines do well we can't
afford to have these with autonomous
machines learn
how to interact with the world by being
in the world and so there are two
fundamental technologies we have to
create the first is a whole new type of
processor we call it project Xavier
project Xavier is in the process in the
process of being taped out we'll have
silicon here pretty soon it
fundamentally changes how computing is
done it's really about real-time sensors
it's really about computing in parallel
using computer vision using sensor
fusion using deep learning and it has to
be incredibly energy efficient 30
tear-offs of operations in 30 watts
that's the first piece the second piece
is a simulation environment we need to
create a world a world a virtual world
where robots can learn how to be robots
they go into this world and they use
reinforcement learning and they try it
over and over and over and over again
until they finally figured out how to be
a productive and useful robot basically
there are three different things we have
to train the network just like we do
with the self-driving car then we put
that network into a virtual environment
where the robot is in and that virtual
environment is powered by a
supercomputer so it could run at super
real time and when you're done with that
network then you could put it on this
processor we call it xavier jack the AI
right into the head and this robot will
literally stand up and start walking
that's our goal and so in order to make
this happen there are two pieces of
technology we have to create one is
Xavier and one is Isaac we're going to
show you Isaac real quick guys this is
the Isaac world
this is Isaac's lab and this is Isaac
learning how to play hockey now you
could see Isaac has two eyes and those
two eyes are basically deep learning
eyes so Isaac's world is accelerated AI
because Isaac has to have AI to start it
has to be able to recognize perceive
perceive the hockey
perceive the puck perceived where the
net is and then it's got to figure out
somehow without us writing any code what
to do
and we gave it a value function and the
value function basically says if you put
the puck in the net that's good if you
don't put the puck in the net try again
okay and so we allow it to try and try
and try and notice it's doing it right
now it's learning how to it's there it's
learning right there now in order to
speed this up a little bit you Gordon is
that you back there yeah yeah we're just
going to switch to fill screen and then
we're going to switch to robot wait just
trained a little bit more so the robot
we just saw there it learned to
basically hit the puck and no more than
that and then we've got another robot
which we've trained for a little bit
longer and this robots learnt to hit the
puck it's just further along the
training program and then we can hit the
puck but it's not very good at getting
it in in the goal so to speed up the
learning even more or what we really
want to do is train it in parallel and
there on the aggregate experience server
lots and lots of robots and so Gordon
here in this case what's happening is we
replicated Isaac and we let them all
train and then we stopped it at some
point we take the smartest Isaac we take
the smartest Isaac and we put the
smartest Isaac and everybody's head and
then we start all over again okay you
repeat rinse and repeat this over and
over and over again running on top of a
supercomputer and all of the Isaac's
they see the puck they see the net they
see they see the the they see their
stick and they have to figure out how to
sync it now ideally if this world this
world looks like our world if this world
looks like our world ideally when we're
done we could literally take the AI
that's in Isaac in the virtual world put
it in the physical Isaac and the
physical Isaac will recognize exactly
what to do into it okay all right so
let's how about one more example
so this is this is sorry this is Isaac
as trained as much as we as we could get
he's quite good at getting in the goal
he's not absolutely a hundred percent
perfect but he's pretty good at playing
hockey and he's much his fame is much
more convincing and he's pretty good
yeah all right not bad good job
okay so autonomous autonomous machines
the next adventure for our industry and
we're seeing so many so much excitement
around Isaac every research organization
we go to they can't wait to get their
arms on Isaac so we're looking forward
to releasing it in the near future okay
so today we talked about several things
we announced the NVIDIA holodeck the
design lab of the future merging fusing
some of the most advanced technologies
and computer graphics today
photorealistic rendering virtual reality
physically real simulated environments
and artificial intelligence the second
thing we talked about is Nvidia the AI
computing platform and how we have
established ourselves in AI put our AI
platform all over the world in every
single cloud in every single data center
and recently we announced tensor art III
which allows us to take these networks
that were trained on these platform to
run in real time with super levels of
efficiency the third thing I talked
about is the Nvidia drive the entire
vision of drive is far more than
building the chip it's the chip is the
operating system it's all the libraries
and the algorithms and all the
applications that make it possible for
autonomous vehicles however it also
includes developing the entire software
stack for a fleet of cars that we would
like to deploy in the near future and
then third and fourth I talked about the
Nvidia DRI px Pegasus the world's first
computer that is able to allow all of
these robot taxi developments happening
all over the world to go to production
and then lastly project Issac our AI
world for robots so that they can learn
how to be robots ladies and gentlemen
I want to thank all of you for coming to
GTC it's going to be a really great GTC
they're looking at all the papers that
are that are here this is a brand new
computing era the work that we're doing
is just so exciting the computer
industry is now able to solve problems
that historically was not solvable thank
you for coming and have a great GTC
[Applause]
[Music]
[Music]
ladies and gentlemen thank you for
attending our keynote lunch will be
served right outside please be sure to
visit our exhibit hall downstairs our
sessions will begin at 1300
[Music]
you
you
you
Title: Ride in NVIDIA's Self-Driving Car - NVIDIA DRIVE Labs Ep. 10
Publish_date: 2019-08-21
Length: 447
Views: 200777
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/1W9q5SjaJTc/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLA2xbqKW_MDW5PBDCaASmyuQXsU0w
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 1W9q5SjaJTc

--- Transcript ---

Today, in a special edition of DRIVE Labs,
we're taking you on an autonomous drive and
we're going to show you the pieces of software
we're building, running together, in the car,
enabling the vehicle to drive itself.
Our pilot is Dennis.
I'm your copilot.
Let's go.
We are now on the road and we'll be engaging
autonomy once we get on the highway, but before
we do that, I want to show you our perception
functionality already in action in the car.
Perception is basically what enables the car
to see.
We take in raw sensor data and translate it
into semantic understanding of the world,
of the scene that we're in.
So take a look at that happening on our front
camera.
We have DriveNet detecting obstacles, the
bounding boxes around the cars.
We have WaitNet detecting the intersection,
the yellow box around everything.
WaitNet also detecting traffic lights and
traffic signs and LightNet is classifying
traffic lights state correctly as red.
We also have signed classification going on
using SignNet.
At the same time, DriveNet is detecting pedestrians
in the cyan bounding boxes on the far side
of the intersection.
We also have OpenRoadNet tracing out the free
space around obstacles on the scene.
And on top of that, we have our object tracking
from frame to frame.
You see the track IDs on the top of each bounding
box.
We also have our camera-based DNN, distance
estimation running, so you see the distance
in meters displayed at the bottom of each
box.
ClearSightNet is also running in the background,
assessing whether and how well the cameras
can see in our four cameras surround perception
set up, on our embedded AGX platform.
All of this rich perception functionality
is what our planning and control software
are going to use to execute the autonomous
driving maneuvers that you're about to see.
We're now getting onto the highway on-ramp
and entering the coverage area of the high
definition map that we're going to use today
for the car to create a route plan that we're
going to follow.
Basically, the car will localize itself onto
the map and create a lane plan that tells
us when it needs to stay in the lane, when
it needs to take a lane change to stay on
the route, and when it needs to take a highway
interchange.
The second thing that's about to happen is
that we're going to transition out of human-driven
mode, driven by Dennis, into autonomous machine
driving mode, where the car is going to drive
us.
So taking a look at the top right of our screen,
we see automatic cruise control, ACC, and
Lane Keep, LK.
When they're both off, Dennis is driving.
When they come on, the car will be driving
us.
So here we go.
Taking a look at the screen Lane Keep is now
on.
ACC is now on.
We're driving fully autonomously.
Dennis, his hands are off the wheel but staying
close for safety reasons and we are officially
starting our autonomous drive.
Okay.
We are now in full autonomy.
The car is keeping us in the lane.
Let's take a look at how that is happening.
That thick green center path that you see.
That is the Path Perception Ensemble, DRIVE
Labs episode one and it is computing, not
just the center path and the edges of our
lane but also the center path and edges of
the left adjacent lane and the right adjacent
lanes.
And we visualize that with different colors.
So green is our Ego lane, left adjacent is
red and right adjacent is blue.
Next, we need to determine which of the obstacles
belong in which of these different lanes.
The way that we do that, we have the bounding
box detections from DriveNet.
We have free space boundary detections from
OpenRoadNet.
Where those two meets is what is called the
object fence and that fence is off where that
object is in space.
We combined this object fence information
with lane geometry information from Path Perception
Ensemble and this now enables us to do obstacle-to-lane
assignment.
The car fence takes on the color of it's assigned
lane.
We are now approaching our first maneuver,
first autonomous maneuver.
The car is letting us know that based on our
route plan, we need to take a lane change
to the right.
Here we go.
The car is performing a surround radar and
camera lane change safety check, and we are
now moving from Lane Keep mode into Speed
Adaptation in order to figure out the speed
profile to get into the next lane and into
Lane Change mode.
Moving from the center path of the current
lane into the center path of the target lane.
And we have now completed that lane change.
Okay.
We're now getting ready for our second set
of autonomous driving maneuvers.
Going straight into the highway interchange
onto 280.
Now, although we know this is coming up based
on localization to the HD map, we will not
be using any clues from the map to actually
navigate this maneuver.
We are handling this using Path Perception
Ensemble only.
Lane Handling mode on the screen is split
because this is a lane split interchange and
now the challenge is going to be for Path
Perception Ensemble to maintain confidence
throughout this interchange because it has
both high curvature and high grade.
But take a look at Path Perception Ensemble.
It's still green, meaning it has high confidence,
that it's navigating this difficult curved
graded highway interchange correctly.
We are now coming up on our next set of autonomous
driving maneuvers to get onto highway 87.
The first thing that we're going to need to
do is another lane change to the right to
get into the correct exit lane, and then handle
another lane split highway interchange, followed
by another lane change under time pressure.
So here we go.
First lane change.
You see Lane Handling mode, go into Speed
Adaptation, finding the lateral path into
the next lane.
Ensemble going from red to green as it's landing
in the target lane.
Finding confidence that it's found the lane.
We have just handled another lane merge and
we are going to have a little bit of grade
profile changes in the road coming up.
Right there, this is why it's important to
have calibration continuously running in the
car.
We see the Lane Handling mode move into Split
Mode.
The car needs to correctly take that lane
split to the right to not unintentionally
exit the route.
Path Perception Ensemble is now navigating
another high curvature interchange.
We see the center path staying green, and
we are now moving right into that third maneuver.
This is a lane change under time pressure.
We don't have a lot of time here to move from
the right lane into the next adjacent left
lane, in order to not incorrectly exit from
our planned route.
So here we go.
We're switching from Keep mode into Speed
Adaptation into Change mode and landing in
the center of the target lane to complete
that set of maneuvers.
And we are now going to complete the rest
of our autonomous route and head back to the
garage.
And we're back.
We hope you enjoyed our autonomous drive today
and enjoyed seeing how our software is enabling
the car to drive itself.
For any questions, reach out to us through
the comments section.
Check out our other DRIVE Labs videos, and
we'll see you next time.
Title: NVIDIA press conference at CES 2014 - part 3
Publish_date: 2014-01-06
Length: 468
Views: 13140
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/22eJdMa-o4Y/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 22eJdMa-o4Y

--- Transcript ---

alright let's talk about chips you know
the Android operating system is
undoubtedly the single most disruptive
force in computing in the last 20 years
whether you're a large company in any
industry or a small startup trying to
build any new gadget the Android
operating system is the most accessible
and the most capable operating system
you can get your hands on it's open
source
no wonder the computing revolution in
China took off on the backs of the
Android operating system and is now the
fastest growing mobile computing market
in the world and ROI started out in the
phone industry completely disrupted the
phone market then it worked its way into
tablets but that's not where it's gonna
end because it's an operating system
that's free to all because it's an
operating system that allows every
company every country every industry to
freely innovate and because it is a
virtually complete operating system by
virtue of the fact that so many of us
make contributions to it there are more
engineers working on Android there are
more engineers working around the world
in the Android ecosystem than any
ecosystem today
and so it stands to reason that the
positive feedback system it's gonna fuel
its disruption at this point it's kind
of hard to imagine stopping it
but it's wonderful for the world when we
think about building tegra our mobile
processor we think about what we can do
to take the Android operating system in
combination take it into new markets
markets where our expertise can
especially serve markets where visual
computing matters we started out by
building the Tegra 2 the world's first
dual core processor at the time people
asked us why would you need a dual core
processor then we took that to Tegra 3
the world's first quad core processor
people asked us why we needed a quad
core processor and it's all very clear
now that in a mobile environment in a
mobile context there are more and more
activities that are happening
simultaneously
so the question is where did we go from
here
now there's some new industries that I
think Android will continue to advance
into and make a contribution to one of
them of course is television
that one's a foregone conclusion it's
not if is simply when 4k televisions are
coming smart televisions are coming it
stands to reason that Android is going
to be a very important factor in that
consoles we happen to believe that the
Android operating system is going to be
the most important platform for game
consoles in the future and the reason
for that is because you have all of your
digital asset in Android why wouldn't
you want to be home
connected to television and then have
access to all of your photographs or all
of your music of all you've all of your
movies and all of your video games it's
simply a matter of time it's simply a
matter of time before Android also
disrupts the video
game console industry and it's the
reason why we built shield so that we
can engage it and start to make a
contribution to it we also believe that
Android would make a difference in the
car industry the car industry is one of
the most exciting industries in the
world for many of us because we love
cars but there's a second reason we
happen to believe that the car will be
your most important mobile computer it
is likely to be our most personal robot
someday it is already our most expensive
concert's consumer electronics owning
and it stands to reason that it's also
going to be the most advanced now it
happens to have some of the most
pioneering minds and some of the
brightest engineers working in that
industry because they also love cars and
so I think this is an area that's going
to make huge contributions and I'll come
back to that in just a moment and so the
question then after Tegra to dual first
the world's first dual core after Tegra
3 the world's first quad core and then
we double the performance of that with
Tegra 4 what's next for us what's next
for us I mean what could we possibly do
I guess we could we could do eight cores
but that seems pretty pedestrian that
seems pretty obvious we could do 12
cores that's more than eight however I
think that maybe we could do better than
that
what we decided to do was we do what's
decided to make Tegra k1 the world's
first 192 core processor
a hundred and ninety two CUDA cores all
programmable by the computer by the by
the by programs all fully programmable
all massively parallel and this is the
first GPU the reason why we decided to
call it Tegra k1 is because this is the
first GPU that took a vast jump from the
previous generation
it's almost inappropriate to call a
Tegra 5 because it's simply not linear
we decided to call it Tegra k1 we
decided to call it Tegra k1 because it's
based on the Kepler architecture as many
of you know Kepler is the most
successful GPU architecture that we have
ever created and it's the most important
GPU architecture the industry had ever
known this architecture because it is so
energy efficient has made it possible
and so so programmable has made it
possible for us to extend the GPU from
not just desktop computers to
workstations but all the way into the
cloud with grid and into supercomputers
with Tesla one singular architecture all
compatible is now able to span computing
from a few watts all the way to
megawatts from a hundred and ninety two
cores all the way to 36 million cores
running our nation's fastest
supercomputer all based on the exact
same architecture called Kepler so we're
really really excited today that with
Tegra we've bridged the gap
Title: NVIDIA GTC 2023 Keynote Teaser
Publish_date: 2023-02-27
Length: 34
Views: 21954
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/2CHT9-EAg6M/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 2CHT9-EAg6M

--- Transcript ---

[Music]
welcome to GTC
I want to show you something amazing
[Music]
beginning Drive pilot
[Music]
how do you create content where content
did not exist
I love it
[Music]
Title: The Art of Collaboration: NVIDIA, Omniverse, and GTC | Documentary Trailer
Publish_date: 2022-07-26
Length: 87
Views: 20412
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/2EBWXhI67Jk/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 2EBWXhI67Jk

--- Transcript ---

gtc is where we come together as a
company to tell our story
i want it to be like a ride i wanted to
be like a roller coaster i wanted to be
exciting so many things going on it was
a miracle that we pulled it off
we wanted to do something more than just
create beautiful renders we wanted to
tell a story it's just mind-boggling for
me that we're able to get all these
things done in the short amount of time
that we had
drive sim is an incredibly powerful
platform it's the most powerful driving
simulator omniverse has really allowed
us to take that next step
forecastnet is an ai based weather
forecasting model it's lightning fast
and at the same time accurate so i
expect even up to a million x speed up
the fact that we can simulate years of
simulated time in a matter of days on a
single gpu is just insane and there's no
really other viable alternative every
single meeting during that period i
guess we started it with a quiz is this
real it's this digital we wanted
something that was fun but also could be
taken seriously a completely ar driven
avatar interacting with jensen himself
ask me anything
all of these talented people that are
contributing to this heart of colombia
it's no different from orchestration
[Music]
see you in omniverse
Title: NVIDIA Special Address at SIGGRAPH 2021
Publish_date: 2021-08-10
Length: 1802
Views: 2173907
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/2fddvn16WIw/hq720.jpg?v=61129b86
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 2fddvn16WIw

--- Transcript ---

[Music]
it all began a lifetime ago
when we first discovered the unity of
computing and graphics
born from visions of wonder
in its most basic form
[Music]
explorations of color
and unlimited new possibilities
[Music]
fueled by curious minds
and the science of expression
[Music]
we were awakened to a new need
the need to create
to feel
to explore
inspired by light
by nature
by science
and by pure entertainment
new realities took shape
as quickly as we could imagine them
taking us to amazing places
and new worlds
[Music]
giving us new perspectives in the blink
of an eye
driving us forward
embracing discovery
[Music]
and inspiring lifetimes of exploration
yet to come
welcome to siggraph 2021 we just got to
see some amazing computer graphics
history where art science and research
come together
you know the very first siggraph was in
1974 in boulder colorado 12 years later
was my first cigarette in san francisco
and in 1993 nvidia's first siggraph was
in anaheim california and since that
time we've had close to 150 papers that
have been presented over 200 talks and
panels countless demos and major product
launches that we dedicate to this
important conference
the most important announcement that
we've had was in 2018 when we launched
rtx the world's first ray tracing
capability in a desktop computer this
has been a dream for many many years the
idea of doing photo realistic ray
tracing in real time
since that time there's been over 125
creative apps and games that have
adopted rtx technology and there's more
coming all the time our research teams
have taken the touring architecture and
extended it in all sorts of areas of ai
whether it's for scientific
visualization
medical imaging or just brand new
breakthroughs in computer graphics we
now have a research team of over 80
people dedicated to advancing this
important architecture
let's take a look at some of the amazing
work that they're doing
hi i'm sonya fiddler a senior director
of research at nvidia focus on 3d deep
learning for omniverse it's super
exciting to share some of the recent
research being done at nvidia at the
heart of graphics is content creation
assets motion worlds as well as
rendering or more generally simulation
today we look at nvidia's latest
research in these two key areas first
we'll look at advances in real-time
rendering visualizing virtual worlds
where light and other elements of
physics work exactly the way they do in
the physical world while being simulated
in real time as rendering becomes more
powerful the graphics community looks
for ways to create more advanced content
at the same time ai is finding its way
into every single area of graphics it's
becoming particularly essential for
content creation
researchers on either side of this
technology advancements in graphics and
ai are craving better results for neural
rendering and more control over content
creation
here are some of the works from the
graphics research team
neural radiance caching allows real-time
rendering of complex dynamic lighting
effects such as light shining through a
tiger's fur
it introduces live ai to rendering which
brings tensor core training and
inference into the heart of real-time
path tracing
neural reflectance fuel textures
these are neural textures that can
represent far more complex materials
than traditional materials
it includes fur hair woven cloth brass
and more
rester gi
this discovery makes path tracing up to
160 times faster by sharing light
between pixels
real-time path tracing
and video researchers combine all the
latest innovations in real-time pass
tracing including neural rendering and
can now real-time pass trace dynamic
words made of billions of triangles
millions of lights and rich materials
stroke strip enables the next generation
of artist drawing tools by letting
computers reconstruct curves from rough
overdrawn strokes in the same way that
humans draw
unicon is a reinforcement learning
motion controller that scales
physics-based animation from a handful
to thousands of realistic motions
we're on the path for ai to be the
powerful tool that can bring us closer
to 100 realism with incredible ease that
makes content creation possible to
everyone continuing on with nvidia
researching gans
let's take a look at a variety of
cutting-edge ai tools for content
creation both 2d and 3d
style gen has revolutionized the quality
of air generated imagery and its
architecture has become the standard for
cutting-edge gann research worldwide
gauguin creates photorealistic images
from segmentation maps which are labeled
sketches that depict the layout of a
scene
gancraft is a neural based rendering
engine that can convert user-created
minecraft worlds to real worlds turning
minecraft gamers to 3d artists
ganor's 3d turns photographs into
animatable 3d models without ever seeing
any 3d examples during ai training
it learns about 3d purely from 2d
imagery such as those found on the web
drivegen is a data-driven neural
simulator that learns to simulate
driving given user controls
it also allows for interactive scene
editing such as adjustment of objects
and weather
with cameo is a neural gant based
talking head rendering engine that can
animate an avatar by using facial cues
as a source
using my facial cues as a guide watch as
ai animates a digital avatar right in
front of your eyes
these two areas of research real-time
graphics and ai will start blending
together in amazing ways in the near
future
it's super exciting to see this synergy
as a researcher
our creative team has made a video of
all the great work nvidia research is
doing in the world of graphics and ai
let's take a look
[Music]
i am a visionary
[Music]
bringing history to life
and adding motion to memories
[Music]
i am painting with the sound of my
voice a peaceful lake surrounded by
trees
and letting art speak for itself
did you know that white rhinos and black
rhinos are both gray
i am changing the way we see the past
[Applause]
[Music]
[Applause]
imagining the present in a new light
[Music]
and creating new dimensions
[Music]
i am redefining storytelling
paving a new path to innovation
and driving toward a better future
for everyone
i am
aei
brought to life by nvidia and brilliant
creators
everywhere
[Music]
that was some incredible work from our
partners and researchers you know it's
our goal to expand rtx into as many
industries as possible today we're
announcing the rtx a2000 this will
expand rtx power into millions of
designers and engineers the innovative
design of the a2000 brings several
industry firsts to a wide range of
professional graphics solutions
including a compact low profile power
efficient design that can fit into a
wide range of workstations including the
rapidly growing segment of small form
factor workstations and it does this
while delivering up to five times the
application performance of the previous
generation
the a2000 is powered by nvidia ampere
gpu architecture this brings the power
of rtx accelerated ray tracing ai and
compute to this segment for the very
first time this will enable millions of
engineers and designers to incorporate
rendering and simulation right into
their existing workflows
this amazing form factor is small how
small
this small it fits in the palm of your
hand
this amazing card will be available in
october of this year
neil stephenson described in the 1990s
novel snow crash where the metaverse is
a collection of shared 3d spaces and
virtually extended physical spaces
extensions of the internet today we have
massive but disconnected twin worlds
from content creation to gaming to
virtual training environments for ai and
factories we built omniverse to connect
these worlds
more content and economics will one day
be virtual than physical we will exist
both in the physical and virtual worlds
nvidia omni versus connecting the open
metaverse
omniverse is a platform that connects
existing workflows using familiar
software applications into a world where
they can share the latest technologies
from nvidia like ai physics and
rendering
on the left collaborators here are
working on different software tools each
composing their part of the scene
whether it's modeling props building the
environment texturing painting lighting
or adding animation and effects they're
connected into omniverse via the
omniverse connector which brings them
into the platform live now in the center
we see the core of omniverse omniverse
nucleus the database and collaboration
engine that enables the interchange of
3d assets and scene descriptions
finally on the right users can portal in
and out of omniverse with workstations
or laptops allowing them to teleport
into the environment with vr or they can
mix with ar and anyone can view the
scene on omniverse by streaming rtx to
their device
omniverse is based on usd
universal scene description it is the
enabling technology that has this
platform come alive you can think of usd
as the html of 3d
it was originally developed by pixar as
a way to unify their production
pipelines and assets across different
software tools and their massive
production teams
even though it started in m e it's
quickly being adopted in other
industries like architecture design
manufacturing and robotics what makes
usd unique is it's not just a file
format it is a full scene description
allowing all the complexities into a 3d
world to be unified and standardized we
based omniverse on usd and other
companies support usd like apple who
supports usd on all of their different
products and like the journey from html
1.0 to html5 usd will continue to evolve
from its nascent state today to a more
complete definition for the virtual
worlds as the community comes together
to ensure a rich and complete standard
apple had been working on a definition
of physics for usd and nvidia was
working on a definition of physics for
usd we came together with pixar and
decided to work together so that there
would be one standardized definition of
physics in usd and i'm happy to report
that that first step rigid body dynamics
has been ratified now there's a long
journey to go and there's more elements
to get done but it shows that the
community will come together to make
this a rich and comprehensive platform
we've been doing a lot with physics on
the omniverse platform and i'd like to
show you some of the work our physics
teams has done take a look
[Applause]
[Music]
so
[Music]
[Music]
[Music]
so
[Music]
so
[Music]
another two elements of the omniverse
platform that are incredibly important
are materials and path trace rendering
we want to take rendering to the next
level and make it available to all the
workflows that are working with omnibus
it's been one thing to see static
imagery and things have been rendered
beautifully and they've done a great job
but where it really gets complex is when
you take on a challenge like a digital
human how do you take the things that
are perfectly imperfect and make them
believable
so what do i mean about perfectly
imperfect well we know what a human
looks like we are familiar yet different
and one very challenging area areas
creating the materials and rendering of
humans high quality realistic rendering
of digital humans has always been labor
intensive and time consuming to render
modeling the realistic appearance of
skin hair eyes clothing all parts of the
digital human extremely challenging this
is digital mark our first reference
digital human that was used to develop a
few of the technologies we're making
available today in omniverse we want to
be able to simplify how we create
realistic digital human appearances so
that anyone can easily create these
effects and later this week there's a
talk specifically designed for digital
humans in omnibus be sure to check it
out
material is a key element to the perfect
rendering today we are introducing
omnisurface a physically based uber
material for rendering complex surfaces
based on mdl
mdl is a core component of the omniverse
platform to physically describe correct
material a portable material definition
language
layers and mixes bsdfs to physically
model a diverse range of materials
including plastic metallic car paint
foliage human skin fabric and much more
it is designed to simplify the look
development for end users like digital
humans
using ai to make 3d easier let's take a
look about how research paves the way
for the next generation workflow ganvers
3d was developed by our research lab in
toronto
again trained purely on a 2d photograph
is manipulated to synthesize multiple
views of thousands of objects in this
case cards the synthesized data set is
then used to train a neural network that
predicts 3d geometry texture and part
segmentation labels from a single
photograph using a differentiable
renderer at its core
for example a single photo of a car
could be turned into a 3d model that can
drive around a virtual scene complete
with realistic headlights taillights and
blinkers
this accelerates 3d model workflow gives
new capabilities to those who don't have
experience in 3d modeling and ganvers 3d
is available today in omniverse
let's take a look at this research in
action here one of our artists
experimentation featuring an ai and
omniverse accelerated workflow using
ganvers 3d extension omniverse create
real illusion character creator 3 and
adobe photoshop
first up using gonverse 3d they were
able to quickly create car assets for
the scenes
next using omniverse real-time physics
allowed them to blow up those assets and
stack them on each other
next using character creator 3 they were
able to pose and concept out the
character for that scene
once that was finalized they used the
omniverse connector to easily bring the
character back into the scene
finally they were able to use our
real-time ray trace renderer for the
highest quality of rendering lighting
and atmospherics
let's talk about the future of digital
twins in virtual worlds
in particular i want to start by talking
about training robots in a virtual world
compared to the physical world
now in the physical world you could plug
a robot into a computer and you can
train it to do certain things and that
robot will learn how to do those things
but in a virtual world you could have
hundreds of robots or thousands of
robots and then using ai you can train
those robots in the virtual world so
that when you take all of those
learnings and download it into the
physical robot it's going to be many
thousands of times smarter
true to reality simulations achieves
faster time to production allowing
software development before hardware
exists
hardware changes that could be
prototypical and validated before even
being built
developing robots in the virtual world
before the physical world has three main
advantages first training the robots
autonomous machines are built on ai
engines that require large data sets
that can be time consuming and costly to
acquire
second testing robots automated testing
ensures software quality and testing and
simulation which allows corner cases to
be validated that are impossible to do
in the real world and finally at scale
multiple robots and large environments
demand a scalable solution
at our recent gtc we showed a vision of
the factory of the future with bmw now
as one of the premier automotive
manufacturing brands bmw produces over
two and a half million vehicles per year
99 of those vehicles are custom from
tailored performance packages to custom
trim and interior options
bmw must continually strive to exceed
the expectations of their most exacting
customers
producing custom vehicles at this scale
requires a tremendous amount of
flexibility and agility in their
manufacturing process
the nvidia omniverse platform has
allowed bmw to generate a completely new
approach to planning highly complex
manufacturing systems
omniverse integrates a wide range of
software applications planning data and
allows for them to have real-time
collaboration across their teams and
geographic locations
autonomous vehicles and other autonomous
machines need ai to perceive the world
around them the world for an autonomous
vehicle is a complicated one
there are street signs other vehicles
pedestrians obstacles weather and so on
training the ai agents to handle all of
these objects and scenarios that it
might encounter on the road requires a
massive amount of properly labeled data
and even while collecting and labeling
this real world data there are some
situations that are dangerous or so
infrequent that you can't even get to
them in your data set
photorealistic synthetic data generated
in omniverse can close the data gap here
and help deliver robust ai
models at gtc we announced that bentley
is building their digital twin called
itwin on the omniverse platform now this
integration allows for engineering grade
millimeter accurate digital content to
be visualized with photorealistic
lighting environmental effects on
multiple devices including web browsers
workstations tablets virtual reality and
augmented reality headsets from anywhere
in the world
the combination of bentley itwin and
nvidia omniverse provides an unmatched
high performance user experience at a
scale that had previously not been
possible let's take a look at the
progress being made
[Music]
[Music]
[Music]
next i want to talk about a new project
called the cognitive mission manager or
cmm for wildfire suppression the cmm
group is an ai visualization research
team at lockheed martin
they have started a long-term project to
develop an ai mission manager to perform
prediction and suppression
recommendations for wildfire management
on the omniverse platform now this is an
extremely important topic and something
that we are all becoming acutely aware
of over the past few summers
omniverse will connect new and existing
ai technology to predict how quickly
fires will spread in which direction and
what role environmental variables like
wind moisture content and the type of
ground cover being burned will impact
the behavior of the fire so firefighting
teams can better respond and reduce the
damaging impact of these fires core
design elements include a federated
system that enables decision making
centrally and at the edge turntable ai
that allows for varying levels of human
intervention and explainable predictions
and course of action recommendations
these are important things that are
being done that save forests and more
importantly could save lives
we announced omniverse enterprise at our
recent gtc and today i'm thrilled to
talk about how we're now moving into a
limited early access of that platform
with a few of our key partners partners
who are helping us to test and validate
omniverse in their workflows
partners like ilm industrial light and
magic have been actively evaluating
omniverse in their workflow and
providing incredible feedback
bmw has been working with us to
visualize the factory of the future
fostering partners one of the leading
architectural firms in the world has
been evaluating omniverse to collaborate
across their different geographic
locations
omniverse is for everyone everywhere
we've had over 50 000 individuals
download the omniverse open beta and
over 400 companies now actively testing
the platform across major industries
industries like architectural
visualization media entertainment game
development research visualization
robotics and of course autonomous
driving omniverse is everywhere
omniverse is a platform that enhances
existing workflows it doesn't replace
them it makes them better by connecting
your existing software products to
omniverse you get to take advantage of
the latest and greatest technologies
that nvidia is developing and by working
together with our pioneers who are
helping evaluate the platform and our
partners who are certifying their
hardware to run the platform and of
course our isv partners who are building
their products to connect to it we are
helping grow an open platform and
connecting the worlds together
next up i'd like to give you some
updates on some of the things we're
doing with our partners and omniverse
nvidia and adobe have been collaborating
on a substance 3d plugin that will
enable substance material support in
omniverse this will unlock a new
material editing workflow that allows
substance materials to be adjusted
directly within omniverse
these materials can be sourced from the
substance 3d asset content platform
or they can be created in substance 3d
applications as an industry standard
substance will strengthen the omniverse
ecosystem by empowering 3d creators with
access to materials from substance 3d
designer and substance 3d sampler
and here's one we're really excited
about tangent blender and nvidia have
collaborated to bring usd support to
blender 3.0 now this will be available
in the main blender branch and what's so
amazing about this is it will bring a
way to connect omniverse to 3 million
blender artists
and along with that blender announcement
i'm happy to tell you that blender will
be available in the omniverse open beta
launcher directly and we will have
support for the omniverse universal
material mapper directly in blender 3.0
and as omniverse is so important for our
customers it's equally important for our
developer community so today we're
extending the nvidia developer program
to now include omniverse with specific
tools examples and tutorials on how to
develop using the omniverse kit sdk this
brings omniverse to over two and a half
million developers currently in the
program and makes it available to our
inception program for startups
we have trained over three hundred
thousand developers across ai
accelerated computing and accelerated
data science this is the first time dli
is offering a free self-paced hands-on
training for the graphics market we are
announcing dli training for omniverse
starting with the getting started with
usd for collaboration 3d workflows
available today at nvidia.com dli this
self-paced course will take you through
the important concepts of layer
composition references and variants that
includes hands-on exercises and live
scripted examples this is the first in a
series of new omniverse courses for
creators and developers
there will be a new teaching kit for 3d
graphics and omniverse based on a
consultation with some of the top film
and animation schools in our studio
education partner program designed for
college and university educators looking
to incorporate graphics and omniverse
into their classroom sign up today for
early access again at nvidia.com dli
nvidia is a computing platform with over
1 billion cuda gpus in the marketplace
and over 250 exa flops in the cloud over
2 000 gpu accelerated applications and
over 27 million downloads of cuda
there's 150 sdks over 8 000 startup
companies in our inception program and
over two and a half million active
developers and this computing platform
which brings together graphics and ai
along with our software and hardware
partners and our pioneers and
researchers we are bringing to you the
reality of the metaverse
this week at siggraph we have a lot of
things in store for you from papers and
panels to incredible demos an art show
and our very first omniverse user group
meeting so i invite you to go to our
website and build out a schedule for a
very busy week ahead from all of us at
nvidia we'd like to thank our hardware
and software partners our researchers
our academic institutes our friends and
especially our families it's been a
crazy year and a half but we look
forward to seeing you all in person real
soon now before we go we have a special
treat for you at this past gtc our
creative team created the holodeck to
create a virtual kitchen for the keynote
that keynote was seen over 20 million
times and this was a collaboration
between 50 nvidians across research
engineering product groups and creative
teams they used omniverse create an
omniverse view and they used third party
products like substance painter maya 3ds
max houdini and davinci resolve and all
of the batch rendering was done with
omniverse farm they were able to
accomplish in weeks what would have
taken months we made a documentary about
this revolutionary keynote and that'll
be coming out later this week here at
siggraph but i want to share with you a
glimpse of the amazing work done by our
creative team
please take a look and enjoy siggraph
and thank you very much for being here
we're doing this awfully early
[Music]
amazing increase in system and memory
bandwidth the basic building block of
the modern data center
[Music]
what i'm about to show you brings
together the latest gpu accelerated
computing
today we're introducing a new kind of
computer
come along
Title: The Uncertain Episode 1 Launch Trailer on SHIELD TV
Publish_date: 2016-09-22
Length: 116
Views: 24781
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/2NSgKYITJzo/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 2NSgKYITJzo

--- Transcript ---

humans many years ago they killed each
other thus eliminating their entire race
leaving behind only machines works of
art and empty buildings
I saw humans only in photographs but I
constantly find artifacts and ancient
sources of information that have
survived until now and that helps me to
learn more about humans thought
processes their culture and way of life
there have been many wars humans are
completely irrational
unlike humans we robots they logically
and act rationally therefore we will
never destroy ourselves our civilization
is based on constant interaction in
order to maintain my good physical form
I create various gadgets that I can
exchange for materials from other robots
two weeks ago the police almost
destroyed me for discovering some
messages on the Internet they broke in
without warning and opened fire
despite the ban imposed by us s
corporation our manufacturer I think we
must study the human race because this
knowledge might be useful in the future
I already know much more about humans
than other world
but there is still a lot to learn about
them
I must find the answer to the main
question
why have humans destroyed their entire
race
you
Title: Fable Legends Developer Interview at E3 2015
Publish_date: 2015-06-18
Length: 165
Views: 12844
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/2oK4cM5Gi7s/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 2oK4cM5Gi7s

--- Transcript ---

Babel has always been this
quintessentially different fantasy
experience it's like a Grimm's fairy
tale story you go in and you explore the
world in these lovely dark woods quirky
characters great sense of humor that's
kind of the sense of a different fantasy
world that we've always been wanted to
create and we're returning to now in
fable legends when we decided we wanted
to reach as large a fan base as possible
Microsoft committed us to the idea of
allowing Xbox one users in Windows 10
users to play together in real time
cross-platform play it's been wonderful
partnership for us between us and the
DirectX 12 and and partners like Nvidia
we've been making use of the new
features of DirectX 12 such as the async
compute being able to run jobs such as
the dynamic GI and physics simulation in
parallel to the rest of the rendering
and that's amazing right that's
basically like extra GPU power that we
can use without compromising the
performance of the game what that
translates to is like more detail bigger
world like more lush room so it really
starts in the concept based we've got
some great concept artists they've been
with Babel for literally a decade and
then our environment our team comes in
and literally spends many man months of
time making the world not just a fun
place to play in from a purely gameplay
point of view but it's just a place that
is so beautiful that I want to spend
time in we're building fable legends on
Unreal Engine 4 and early on we decided
that we would like to build our own
lighting system the concept is just very
simple every single object in the world
actually responds to light source and
and being able to just make a very
natural and mystical world what's
awesome about it is we can use all the
async compute capabilities from Nvidia
to make those jobs to be a lot more
efficient on the cpu and on the GPU like
we're seeing savings of about thirty
three percent right now
we've been running a closed beta for
xbox one for about the last seven or
eight months we've been bringing fans in
and they've been able to contribute
meaningfully to the game starting next
month will be increasing that closed
beta and expanding it to pc on july 29th
you probably know Windows 10 will be
released get that puppy on your computer
and then you can join the closed beta
and start contributing to making this
beautiful world come to life when we
think about PC gamers we know that
they're like passionate bunch like well
I want to build the best system and in
the world I want to be rewarded for it i
want my games to just look awesome and
that's basically our promise it's
basically we are going to scale up our
game to match your system all right
we're talking about 4k and things like
that the image is just one of top
Title: 3 Must-Play New Android Games on NVIDIA SHIELD
Publish_date: 2017-01-24
Length: 121
Views: 28238
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/2oMwrPsWxfs/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 2oMwrPsWxfs

--- Transcript ---

the shield TV just announced at CES
takes full advantage of its mighty Agra
x1 processor
to bring a stunning level of fidelity
and performance to Android gaming and
this year you can make the most of it
with more great titles coming from
serene and indeed beautiful if any
developer Jonathan Blow
second masterpiece the witness available
now exclusively on Android on NVIDIA
shield the witness is a first-person
adventure partly inspired by 90s
bestseller mist which sees you take on
the role of an unnamed wanderer
exploring an island littered with
puzzles and complete enough of these
maze like brain teasers in each of the
games section angel' unlock a laser beam
which in turn guides you closer to your
journey down requiring considerably
faster reaction if the gorilla tomb
raider arriving soon exclusively for
Android on NVIDIA shield oh god run home
each other they're killing people
co-written by Rhianna Pratchett Crystal
Dynamics brutal reimagining of Lara
Croft transformation from young academic
into seasoned survivor remains of
stealing personal adventure filled with
unsavory characters and even more
unstable situations to may deceive Lara
shipwrecked on the island of Yama time
where she was sickly holding her
fledgling skills if she survived a
myriad dangers and get to the bottom of
the islands mystery finally also coming
soon to NVIDIA shield is the persistent
online FPS shadow gun legend in a
humanity dominated galaxy the borderland
fringe colonies begin to go silent
before a full-scale galactic war comes
on during towards civilization much like
a traditional MMORPG you can build your
own character with hundreds of weapons
and armor options and more than 50
unique skills or joining a guild and
taking on challenging raids with your
challenge you can enjoy these amazing
titles along with hundreds of others on
NVIDIA shield so just hit up shield and
video comms to find out more
Title: RedShift Brings NVIDIA’s OptiX AI Denoising Technology to 3D Rendering
Publish_date: 2017-12-05
Length: 69
Views: 17184
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/2vJ_5nPVU0s/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 2vJ_5nPVU0s

--- Transcript ---

hello my name is Panos I am from
redshift redshift makes the redshift
gpu-accelerated renderer which runs on
NVIDIA GPUs here at nips today we're
showing redshift running with the AI the
noise but Nvidia has developed and we're
showing how we can produce noiseless
images within a very few seconds but
this will be super important when there
are editing a scene for example if
they're moving an object or they are
editing a shader or moving the lights or
changing the lighting conditions they
will be able to get like a clean image
within very few seconds while previously
that we have to wait a good 20 seconds a
minute or maybe even several minutes to
get an equivalent result we are excited
to have integrate the AI denoiser from
Nvidia to redshift and we think that AI
in computer graphics is here to stay so
we are very excited to see what other
technologies based on the AI and video
is going to be developing in the future
to help with the performance and the
loop of computer graphics
Title: GTC Spring 2021 Highlights
Publish_date: 2021-04-27
Length: 227
Views: 16028
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/2winsqMwBwg/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 2winsqMwBwg

--- Transcript ---

what a gtc
this is a mountain of technology enjoy
nvidia is now a three chip company with
the addition of gray cpu
focused on giant scale ai and hpc
and for data center infrastructure
processing we announced
bluefield 3 and delca1 we announced new
dgx systems
and new software megatron for giant
transformers
clara for drug discovery and cool
quantum for quantum computing
and with new tools like nvidia tau fleet
command
and triton it is easy for you to
customize and deploy on egs
we announced an important new platform
nvidia egx
with ariel 5g to make ai accessible to
all companies and industries
and drive our end-to-end platform for
the 10 trillion dollar transportation
industry
from oren and atlanta the first 1 000
tops
soc to hyperion 8 a fully operational
reference av car platform we're working
with the industry at
every layer we're building virtual
worlds with nvidia omniverse
a miraculous platform that will help
build the next wave of ai for robotics
and self-driving cars
let me show you what drive av and drive
sim can do
20 years ago all of this was science
fiction
today we're living it in total
1600 talks about the most important
technologies of our time from the
leaders in the field that are shaping
our world
so even if you pre-train a system using
ssl on natural images not
x-rays and then you find your needs on
x-rays it will it will still work better
on x-ray
yes with neural nets that's always
possible they may discover a different
way of doing things that wasn't what you
intended
it's a recurrent neural network which
has feedback
connections it is inspired by the human
brain
that's the cool thing about talking to
neuroscientists is that they're doing
some of the work for us
uh providing us with inspiration we're
in the midst of the biggest
and most fundamental transformation
process
in human history the cameras are very
specifically designed and built into the
previz
so that once we determine what the shot
is we also have determined exactly how
to shoot it
you know at nvidia we stand on the
shoulders of the giants that came before
us and
all things that we do from science
research and computer graphics
what we're doing is powering a new
future one we think of as the fifth wave
of computing
the convergence of iot with ai and 5g
now we're offering ngc pre-trained
models that you can
plug into these applications or ones you
develop like jarvis conversational ai
chest x-ray shows left retro cardiac
capacity
merlin recommender system magazine
virtual conferencing
now let's turn back on all the maxing
features
see that's so much better and morpheus
ai security
with morpheus the chaos becomes
manageable we're working with bmw to
create a future factory
simulated from beginning to end in
omniverse creating a digital
twin welcome to bmw production jensen
i could not be more proud of the
innovations that our collaboration is
bringing to the factories of the future
[Music]
now we have better collaborations
[Applause]
musical experience let's go
[Music]
nvidia is an instrument for you to do
your life's work
Title: GTC 2022 Spring Keynote with NVIDIA CEO Jensen Huang
Publish_date: 2022-03-22
Length: 6054
Views: 10364513
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/39ubNuxnrK8/hq720.jpg?v=6239fc90
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 39ubNuxnrK8

--- Transcript ---

Now approaching your destination.
Hi! Welcome to NVIDIA.
Welcome to GTC!
I hope all of you are well.
We have really big announcements and
cool things to show you today.
But first, let me share the new I AM AI.
It’s a work of love by NVIDIA’s creative team
and beautifully tells the stories of the impactful work you do.
I am a visionary.
Expanding our understanding of the smallest particles,
And the infinite possibilities of the universe.
I am a guardian.
Protecting us on all of our journeys,
And insuring our most precious passengers
make it home safely.
I am a healer.
Searching for hidden threats in every cell,
And delivering precise care with every breath.
I am a helper.
Taking on complex tasks
in the most challenging environments
and giving our crops room to grow.
I am a creator.
Transforming the very fabric of our everyday lives,
And using the creative DNA of the masters
to inspire a new generation of art.
I am a learner.
Taking just minutes to discover how to
crawl, walk, and stand on my own.
I am a storyteller.
Giving emotion to words
and breaking down the language barrier.
I am even the composer of the music.
I am AI.
Brought to life by NVIDIA, deep-learning,
and brilliant minds everywhere.
Doctors can now sequence the human DNA in a couple of hours.
And predict the 3D structure of the DNA
from the amino acid sequence.
Researchers can use computers to
generate new drug candidates.
And inside a computer test the new
drug against a target disease.
AI is learning biology and chemistry, just as AI has learned images, sounds, and language.
Once in the realm of computers, fields like
drug discovery will undergo the same revolution
we are witnessing in other areas impacted by AI.
None of these capabilities were remotely possible a decade ago.
Accelerated computing, at data center scale, and combined with machine learning, has sped-up computing by a Million-X.
Accelerated computing has enabled
revolutionary AI models like the Transformer
and made self-supervised learning possible.
AI has fundamentally changed what software
can make and how you make software.
Companies are processing, refining their data, making AI software, becoming intelligence manufacturers.
Their data centers are becoming AI factories.
The first wave of AI learned perception and inference, like recognizing images, understanding speech,
recommending a video, or an item to buy.
The next wave of AI is robotics – AI planning actions.
 Digital robots, avatars, and physical robots
will perceive, plan and act.
And just as AI frameworks like TensorFlow and PyTorch
have become integral to AI software,
Omniverse will be essential to making robotics software. Omniverse will enable the next wave of AI.
We will talk about the next Million-X and other
dynamics shaping our industry this GTC.
Over the past decade, NVIDIA accelerated computing
delivered a Million-X speed-up in AI
and started the modern AI revolution.
Now, AI will revolutionize all industries.
The CUDA libraries, the NVIDIA SDKs, are at the
heart of accelerated computing.
With each new SDK, new science, new applications, and new industries can tap into the power of NVIDIA computing.
These SDKs tackle the immense complexity at the
intersection of computing, algorithms, and science.
The compound effect of NVIDIA's full-stack approach
resulted in a Million-X speed-up.
Today, NVIDIA accelerates millions of developers and tens of thousands of companies and startups.
GTC is for all of you.
It is always inspiring to see leading computer scientists, AI researchers, roboticists, and autonomous vehicle designers
present their work at GTC.
We can see AI and accelerated computing's expanding reach and impact from the new attendees and talks.
This year, we see Best Buy, Home Depot, Walmart,
Kroeger, and Lowe's working with AI.
LinkedIn, Snap, Salesforce, DoorDash, Pinterest,
ServiceNow, American Express, and Visa
will talk about using AI at scale.
And you can look forward to seeing talks from healthcare companies GSK, AstraZeneca, Merck, Bristol Myers Squibb,
Mayo Clinic, McKesson, and Eli Lilly.
GTC 2022 is going to be terrific.
The GPU revolutionized AI.
Now, AI on GPUs is revolutionizing industries and science.
One of the most impactful to humanity is climate science.
Scientists predict that a supercomputer
a billion times larger than today's
is needed to effectively simulate regional climate change.
Yet, it is vital to predict now the
impact of our industrial decisions
and the effectiveness of mitigation and adaptation strategies.
NVIDIA is going to tackle this grand challenge with our Earth-2, the world’s first AI digital twin supercomputer,
and invent new AI and computing technologies to
give us a Billion-X boost before it’s too late.
There is early evidence we can succeed.
Researchers at NVIDIA, Caltech, Berkeley lab,
Purdue, Michigan, and Rice Universities
have developed a weather forecasting
AI model called FourCastNet.
FourCastNet is a physics-informed deep learning model
that can predict weather events such as hurricanes,
atmospheric rivers, and extreme rain.
FourCastNet learned to predict weather from 40 years of simulation-enhanced ground truth data from ECMWF,
the European Center of Medium Weather Forecasting.
For the first time, a deep learning model has achieved better accuracy and skill on precipitation forecasting
than state-of-the-art numerical models,
and makes predictions 4 to 5 orders of magnitude faster -
what takes a classical numerical
simulation a year now take minutes.
Atmospheric rivers are enormous rivers of water vapor in the sky – each carrying more water than the Amazon.
They provide a key source of precipitation for the western U.S., but these large, powerful storms can also cause
catastrophic flooding and massive snowfalls.
NVIDIA has created a physics-ML model that
emulates the dynamics of global weather patterns
and predicts extreme weather events, like atmospheric rivers, with unprecedented speed and accuracy.
Powered by the Fourier Neural Operator, this GPU-accelerated AI-enabled digital twin, called FourCastNet,
is trained on 10 TB of Earth system data.
Using this data, together with NVIDIA Modulus and Omniverse, we are able to forecast the precise path
of catastrophic atmospheric rivers a full week in advance.
FourCastNet takes only a fraction of a
second on a single NVIDIA GPU.
With such enormous speed, we can generate thousands of simulations, to explore all possible outcomes.
Allowing us to quantify the risk of catastrophic flooding with greater confidence than was ever possible before.
NVIDIA is pioneering accelerated computing,
an approach that demands full-stack expertise.
We built NVIDIA like a computing stack,
a neural network – in four layers –
hardware, system software, platform software, and applications.
Each layer is open to computer makers,
service providers, and developers
to integrate into their offering however best for them.
I will announce new products at each of these layers today.
Let's get started.
The progress of AI is stunning.
Transformers opened self-supervised learning
and unblocked the need for human-labeled data.
We can use enormous training sets with Transformers
and learn more robust
and more complete representations.
Because of Transformers, model and data size grew,
and model skills and accuracy took off.
Google BERT for language understanding, NVIDIA MegaMolBart for drug discovery, and DeepMind AlphaFold
are all breakthroughs traced to Transformers.
Transformers made self-supervised learning possible,
and AI jumped to warp speed.
Natural language understanding models can learn without supervision from vast amounts of text,
which is then refined with a small
amount of human-labeled data
to develop good skills for translation,
Q&A, summarization, writing, and so much more.
Multi-model learning with language supervision has
added another dimension to computer vision.
Reinforcement learning models, like NVIDIA NVCell,
are doing chip layout – AI is building chips.
Like FourCastNet and Orbnet, physics-ML models
are learning physics and quantum physics.
The conditions are prime for
significant breakthroughs in science.
Generative Models are transforming
 creative design, helping build virtual worlds,
and soon, revolutionizing communications.
Like Nerf, Neural Graphics networks that learn 3D representations from 2D images
will elevate photography and help us
create digital twins of our world.
AI is racing in every direction – new architectures,
new learning strategies, larger and more robust models,
 new science, new applications, new industries
– all at the same time.
Here’s an amazing example.
This AI-powered character is animated using a
physics-based reinforcement learning model.
Let’s take a look.
We are using reinforcement learning to develop more life-like and responsive physically simulated characters.
Our character learns to perform life-like motions
by imitating human motion data,
such as walking, running, and sword swings.
Our character is put through an intense
training regimen for 10 years in simulation.
Thanks to NVIDIA’s massively parallel GPU simulator,
this just takes 3 days of real world time.
The character then learns to perform
a large variety of motor skills.
Once the character has been trained, it can use those skills that it has learned to perform more complex tasks.
Here, the character is trained to run to a
target object and knock it over.
We can also steer the character to walk in different directions like you would with a game character.
Our model allows the character to automatically synthesize life-like responsive behaviors to new situations.
We can also control the character
using natural language commands.
For example, we can tell the character
to do a shield bash or swing its sword.
We hope this technology will eventually
make animating simulated characters
as easy and seamless as talking to a real actor.
NVIDIA AI is the engine behind these innovations, and we are all-hands-on-deck to advance the platform –
solving new problems, getting it everywhere,
and making AI more accessible.
NVIDIA AI is a suite of libraries that span the entire AI workflow - from data processing and ETL feature engineering
to graph, classical ML, and deep learning
model training to large-scale inference.
NVIDIA DALI, RAPIDS, cuDNN Triton, and Magnum IO
are among the most popular libraries.
We use the libraries to create specialized AI frameworks
that include state-of-the-art pre-trained models
and data pipelines that make it easy to scale out.
Let me touch on a few of our updates at GTC.
Hundreds of billions of web interactions
a day – like search, shopping, and social –
generate trillions of machine learning model inferences.
NVIDIA Triton is an open-source hyperscale model inference server – the grand central station of AI deployment.
Triton deploys models on every generation of NVIDIA GPUs,
x86 and Arm CPUs
and has interfaces to support accelerators
such as AWS Inferentia.
Triton supports any model – CNNs, RNNs,
Transformers, GNN, decision trees, any framework.
Tensor Flow, PyTorch, Python, ONNX, XGBoost.
Triton supports any query type – real-time, offline,
batched, or streaming audio and video.
Triton supports all ML platforms – AWS, Azure, Google, Alibaba, VMWare, Domino Data Lab, OctoML, and more.
And Triton runs in any location –
cloud, on-prem, edge, or embedded.
Amazon Shopping is doing real-time spell checking with Triton.
And Microsoft is using Triton for its Translator service.
Triton has been downloaded over a
million times by 25,000 customers.
NVIDIA Riva is a state-of-the-art speech AI
that is end-to-end based on deep learning.
Riva is tunable.
Riva is pre-trained with world-class recognition rates,
and then customers can refine with custom data
to learn industry, country, or company-specific jargon.
Riva speech AI is ideal for conversational AI services.
Snap, RingCentral, Kore.ai, and many others are using Riva.
Today we are announcing the general availability of Riva.
Release 2.0 has speech recognition in 7 languages,
neural text to speech with male and female voices,
and custom tuning with our TAO transfer learning tool.
Riva runs on any cloud and anywhere
with NVIDIA GPUs, basically everywhere.
Maxine is an SDK featuring state-of-the-art AI
algorithms for reinventing communications.
Video conferencing encodes, transmits,
then decodes images and sound.
Computer vision will replace image encoding,
and computer graphics will replace image decoding.
Speech recognition will replace audio encoding,
and speech synthesis will replace audio decoding.
Fifty-five years after AT&T demonstrated the Picturephone
at the World's Fair in New York,
AI will reinvent video conferencing.
Remote work is here to stay.
We need virtual live interactions more than ever.
Maxine is an AI model toolkit used by developers
to reinvent communications and collaborations.
Maxine has 30 models today.
The GTC release adds new models for acoustic
echo cancellation and audio super-resolution.
Let’s take a look at what Maxine can do.
NVIDIA Maxine reinvents real-time video
communication with the magic of AI.
Thanks to Maxine, we can now hear and see each other better, and feel more connected and included —
even when language becomes a barrier.
To stay engaged with my audience, Maxine helps me keep eye contact with everyone on the call,
whether it’s one person or one hundred -
and even if I’m reading a script.
(In Spanish) How can you overcome the language barrier with Maxine?
While I don’t speak Spanish, with Maxine’s help, now I can!
(In Spanish) Now I can now speak your language in my own voice. Not bad, is it?
(In Spanish) Magnificent.
(In French) That’s great. But can Maxine translate
into more than one language?
Oh yes, absolutely.
(In French) Maxine also allows me to speak French.
(In French) And many more languages.
(In German) We’d tell you more about Maxine’s magic,
but you’ll have to wait until the next GTC.
(In German) Stay tuned so you don’t miss anything!
(In German) Wonderful! I’ll be there
Recommenders are personalization engines.
The internet has trillions of items and is constantly changing – news, social videos, new products.
How do we even know what is out there?
Recommenders learn the features of items,
your explicit and implicit preferences,
and recommend things likely of interest to you –
a personalized internet.
Advanced recommendation engines drive
the world’s consumer internet services.
In the future, it will also drive financial services,
healthcare services, vacation planners, and much more.
NVIDIA Merlin is an AI framework for recommender systems.
Merlin consists of end-to-end components
of a recommender pipeline,
including feature transforms, retrieval, to ranking models.
With NVIDIA Merlin, companies can quickly build, deploy, and scale state-of-the-art deep learning recommender systems.
Snap uses Merlin to improve ad and
content recommendations while reducing cost by 50%
and decreasing serving latency by 2x.
Tencent Wechat uses Merlin to achieve 4 times lower latency and 10 times throughput for short video recommendations.
Tencent's cost is halved moving from CPU to GPU.
At GTC, we are announcing the 1.0 release,
and general availability of Merlin.
Transformers revolutionized natural language processing.
Training large language models is not for the faint of heart –
it is a grand computer science challenge.
OpenAI’s GPT-3 is 175 billion parameters.
NVIDIA Megatron is 530 billion.
And Google's new Switch Transformer is 1.6 trillion parameters.
Nemo Megatron is a specialized AI framework for training large language models – up to trillions of parameters.
To get the best performance possible on the target infrastructure, Nemo Megatron does automatic data, tensor,
and pipeline parallelism, orchestration and scheduling,
and auto precision adaptation.
Nemo Megatron now supports any NVIDIA systems.
And automatically does hyper-parameter tuning
for your target infrastructure.
Nemo Megatron is also cloud-native and supports Azure,
with AWS coming soon.
AI, the creation and production of intelligence, is a giant undertaking and touches every aspect of computing
and every industry.
NVIDIA AI libraries and SDKs accelerate software, platforms, and services throughout the AI ecosystem.
Even with excellent tools and libraries, developers and NVIDIA must dedicate significant engineering
to ensure performance, scalability, reliability, and security.
So, we have created the NVIDIA AI Accelerated program
to work with developers in the AI ecosystem
to engineer solutions together that
customers can deploy with confidence.
NVIDIA AI democratizes AI so that every industry
and company can apply AI to reinvent themselves.
One of the most impactful is the revolution in digital biology.
AI accelerates DNA sequencing, protein structure prediction, novel drug synthesis, and virtual drug testing.
Funding for AI drug discovery startups surpassed
$40 billion in the past couple of years.
Insilico Medicine has just sent its first AI
discovered drug to enter human clinical trials.
The novel drug and target was discovered in less than 18 months, years faster than previously possible.
The conditions are prime for the digital biology revolution.
I can't imagine a greater purpose for NVIDIA AI.
AI applications like speech, conversation,
customer service, and recommenders
are driving fundamental changes in data center design.
AI data centers process mountains of
continuous data to train and refine AI models.
Raw data comes in, is refined, and intelligence goes out.
Companies are manufacturing intelligence
and operating giant AI factories.
The factory operation is 24/7 and intense –
minor improvements in quality
drive a significant increase in customer
engagement and company profits.
New organizations called MLOps are
showing up in companies around the world.
Their fundamental mission is to efficiently and reliably transform data into predictive models - into intelligence.
The data they process is growing exponentially – the more predictive the model, the more customers engage the services,
the more data is collected.
The computing infrastructure of MLOps is fundamental,
and its engine is the Ampere-architecture A100.
Today we are announcing the next generation.
The engine of the world's AI computing
infrastructure makes a giant leap.
Introducing NVIDIA H100!
The H100 is a massive 80 billion transistor chip
using TSMC 4N process.
We designed the H100 for scale-up and scale-out infrastructures, so bandwidth, memory, networking,
and NVLINK chip-to-chip data rates are vital.
H100 is the first Gen5 PCI-E GPU and the first HBM3 GPU.
A single H100 sustains 40 terabits per second of IO bandwidth.
To put it in perspective, 20 H100s can sustain the
equivalent of the entire world's internet traffic.
The Hopper architecture is a giant leap over Ampere.
Let me highlight 5 groundbreaking inventions.
First, the H100 has incredible performance.
A new Tensor Processing format – FP8.
H100 has:
4 PetaFLOPS of FP8
2 PetaFLOPS of FP16
1 PetaFLOPS of TF32
60 TeraFLOPS of FP64 and FP32
Designed for air and liquid cooling, H100 is also the
first GPU to scale in performance to 700W.
Over the past six years, through Pascal, Volta, Ampere, and now Hopper, we developed technologies to train with FP32,
then FP16, and now FP8.
For AI processing, Hopper H100’s 4 PF of FP8 is an amazing six times the performance of Ampere A100’s FP16 -
our largest generational leap ever.
The Transformer is unquestionably the most
important deep learning model invented.
Hopper introduces a Transformer Engine.
The Hopper Transformer Engine combines a new Tensor Core and software that uses FP8 and FP16 numerical formats,
 and dynamically processes layers of a Transformer network.
Transformer model training can be reduced from weeks to days.
For cloud computing, multi-tenant infrastructure
translates directly to revenues and cost of service.
A service can partition H100 up to 7 instances –
Ampere can also do this.
However, Hopper added complete per-instance
isolation and per-instance IO virtualization
to support multi-tenancy in the cloud.
H100 can host seven cloud tenants,
while A100 can only host one.
Each one is equivalent in performance to two full T4 GPUs,
our most popular cloud inference GPU.
Each Hopper multi-instance supports Confidential Computing with Trusted Execution Environment.
Sensitive data is often encrypted at-rest and in-transit
over the network but unprotected during use.
Data can be an AI model that results from millions of dollars of investment, trained on years of domain knowledge or
company-proprietary data, and is valuable or secret.
Hopper Confidential Computing, a combination
of processor architecture and software,
addresses this gap by protecting
both data and application during use.
Confidential Computing today is only CPU-based.
Hopper introduces the first GPU Confidential Computing.
Hopper Confidential Computing protects the confidentiality and integrity of AI models and algorithms of the owners.
Software developers and services can now distribute
and deploy their proprietary and valuable AI models
on shared or remote infrastructure, protecting their
intellectual property and scaling their business models.
And there’s more.
Hopper introduces a new set of instructions called DPX. Designed to accelerate dynamic programming algorithms.
Many real-world algorithms grow with
combinatorial or exponential complexity.
Examples include -
The famous traveling salesperson optimization problems,
Floyd-Warshall for shortest
route optimization used for mapping,
Smith-Waterman pattern-matching
for gene sequencing and protein folding,
and many graph optimization algorithms.
Dynamic programming breaks complex problems down to simpler subproblems that are solved recursively,
reducing complexity and time to polynomial scale.
Hopper DPX instructions will speed-up
these algorithms up to 40 times.
H100 is the newest engine of AI infrastructures.
H100s are packaged with HBM3 memories using TSMC CoWoS 2.5D packaging and integrated with voltage regulation
into a superchip module called SXM.
Let me now show you how we built up a
state-of-the-art AI computing infrastructure.
8 H100 SXM modules are connected by
4 NVLINK Switch chips on the HGX system board.
The four-super high-speed NVSwitch chips each have 3.6 TFLOPS of SHARP in-network computing,
first invented in Mellanox Quantum Infiniband Switches.
For all-to-all reductions, used extensively in
deep learning and scientific computing,
SHARP effectively boosts bandwidth by 3 times.
The CPU subsystem consists of dual
Gen 5 CPUs and two networking modules,
each with four 400Gbps CX7 IB or 400Gbps
ethernet networking chips.
CX7 has 8 billion transistors and is the
world's most advanced networking chip.
A total of 64 billion transistors deliver
3.2 terabits per second of networking.
Introducing the DGX H100 - our new AI computing system.
DGX has been spectacularly successful and is the AI infrastructure for 8 of the top 10 and 44 of the Fortune 100.
Connected by NVLINK,
DGX makes the eight H100s into one giant GPU:
640 billion transistors
32 petaflops of AI performance
640 GB of HBM3
and 24 terabytes per second of memory bandwidth.
DGX H100 is a spectacular leap.
And there's more!
We have a brand-new way to scale up DGX.
We can connect up to 32 DGXs with NVLINK.
Today, we are announcing the NVIDIA NVLINK Switch system.
For AI factories, DGX is the smallest unit of computing.
With NVLINK Switch system, we can scale up
into one giant 32-node, 256-GPU DGX POD,
with a whopping 20.5 terabytes of HBM3 memory, and 768 terabytes per second of memory bandwidth.
768 terabytes per second!
In comparison, the entire internet is 100 terabytes per second.
Each DGX connects to the NVLINK Switch
with a Quad-Port Optical transceiver.
Each port has eight channels of 100G-PAM4
signaling carrying 100GB per second.
32 NVLINK transceivers connect to a
one rack unit NVLINK Switch system.
The H100 DGX POD is essentially one mind-blowing GPU:
1 exaflops of AI computing
20 TB of HBM3
192 TF of SHARP in-network computing
The bi-section bandwidth moving data
between the GPUs is an amazing 70TB per second.
Multiple H100 DGX PODs connect to our new Quantum-2 400 Gbps Infiniband switch with SHARP in-network computing,
performance isolation, and congestion control to scale to DGX SuperPODS with thousands of H100 GPUs.
Quantum-2 switch is a 57 billion transistor chip with the ability to connect 64 ports of 400gbps each.
DGX SuperPODs are modern AI factories.
We are building Eos, the first Hopper AI factory,
at NVIDIA, and she's going to be a beauty.
18 DGX PODs
576 DGXs
4608 H100 GPUs
At traditional scientific computing, Eos is 275 petaFLOPS or 1.4x faster than the fastest science computer in the US –
the A100 powered Summit.
At AI, Eos is 18.4 Exaflops or 4 times the AI processing
of the world's largest supercomputer –
the Fugaku in Japan.
We expect Eos to be the fastest AI computer in the world.
Eos will be the blueprint for the most advanced AI infrastructure for our OEM and cloud partners.
Partners can take H100 DGX SuperPOD
as a whole or the technology components
at any of the four layers of our platform.
We are standing up Eos now and will be online in a few months.
Let's take a look at Hopper's performance.
The performance boost over Ampere is incredible.
Training Transformer models, the compound benefits
of Hopper's raw horsepower,
Hopper Transformer engine with FP8 Tensor Core
NVLINK with SHARP in-network computing, NVLINK Switch scale-up to 256 GPUs, and the Quantum-2 InfiniBand,
and all of our software results in a 9X speed-up!
Weeks turn to days.
For inferencing large language models, H100 throughput
is up to 30 times higher over A100.
H100 is the most significant leap we've ever delivered.
NVIDIA H100, the new engine of the world's AI infrastructure.
Hopper is going to be a game-changer
for mainstream systems as well.
As you've seen with Hopper HGX and DGX, networking and interconnects are critical to computing –
moving data to keep the lightning-fast GPUs
fed is a most serious concern.
So, how do we bring Hopper's superfast
compute to mainstream servers?
Moving data in traditional servers’ overloads CPU and
system memory and are bottlenecked by PCI-Express.
The solution is to attach the network directly to the GPU.
This is the H100 CNX, combining the most advanced GPU
and the most advanced networking processor, CX7,
into a single module.
Data from the network is DMA'd directly to H100 at 50 gigabytes per second and avoids the bottlenecks at the CPU,
system memory, and multiple passes across PCI express.
H100 CNX avoids bandwidth bottlenecks while freeing the CPU and system memory to process other parts of the application.
An incredible amount of technology in a tiny
little package designed for mainstream servers.
Hopper H100 powers systems at every scale – from the PCI express accelerator for mainstream servers
to DGX, DGX Pod, and DGX SuperPOD.
These systems run NVIDIA HPC and NVIDIA AI
and the rich ecosystem of CUDA libraries.
Let me update you on Grace - our first data center CPU.
I am pleased to report that Grace is progressing
fantastically and on track to ship next year.
We designed Grace to process giant amounts of data.
Grace will be the ideal CPU for AI factories.
And this is Grace-Hopper.
A single superchip module with direct chip-to-chip
connection between the CPU and GPU.
One of the critical enabling technologies of Grace-Hopper is the memory coherent chip-to-chip NVLINK interconnect –
a 900 gigabytes per second link!
But I only told you half the story.
The full Grace is truly amazing.
The Grace CPU can also be a superchip made up of two CPU chips connected, coherently, over NVLINK chip-to-chip.
Grace superchip has 144 CPU cores!
And an insane 1 terabyte per second of memory bandwidth -
over two to three times the top Gen 5 CPUs
that have yet to even ship.
We estimate the Grace superchip to have a SPECint 2017
rate of 740. Nothing close to that ships today.
And the amazing thing is the entire module,
including a terabyte of memory, is only 500 Watts.
We expect the Grace superchip to be the
highest performance and twice the energy efficiency
of the best CPU at that time.
Grace will be amazing at AI, data analytics,
scientific computing, and hyperscale computing.
And Grace will be welcomed by all of NVIDIA's software platforms – NVIDIA RTX, HPC, NVIDIA AI, and Omniverse.
The enabler for Grace-Hopper and Grace superchip
is the ultra-energy-efficient, low-latency,
high-speed memory coherent NVLINK chip-to-chip link.
With NVLINK that scales from die-to-die,
chip-to-chip, and system-to-system,
we can configure Grace and Hopper
to address a large diversity of workloads.
We can create systems with:
A Two-Grace CPU Superchip
A One-Grace, One-Hopper Superchip
A One-Grace, Two-Hopper Superchip
And systems with Two-Grace, Two Hoppers
Two-Grace and Four Hoppers
Two-Grace and Eight Hoppers
The composability of Grace and Hopper’s NVLINK,
and the Gen 5 PCI express switch inside CX7,
give us a vast number of ways to address
customer’s diverse computing needs.
Future NVIDIA chips – CPUs, GPUs, DPUs, NICs, and SOCs –
will integrate NVLINK just like Grace and Hopper.
Our SERDES technology is world-class.
From years of designing high-speed memory interfaces, NVLINKs, and networking switches,
NVIDIA has world-class expertise in high-speed SERDES.
NVIDIA is making NVLINK and SERDES available to customers and partners who want to implement custom chips
that connect to NVIDIA's platforms.
These high-speed links have opened a new world to build semi-custom chips and systems with NVIDIA computing.
NVIDIA has accelerated computing a Million-X
over the past decade by GPU-accelerating algorithms,
optimizing across the full-stack,
and scaling across the entire data center.
The computer science and engineering
is captured in NVIDIA SDKs.
NVIDIA SDKs with CUDA libraries are the
heart and soul of accelerated computing.
NVIDIA SDKs connect us to new challenges in
science and new opportunities in industry.
RAPIDS is a suite of SDKs for data scientists using popular Python APIs for DataFrames, SQL, arrays,
machine learning, and graph analytics.
RAPIDS is one of NVIDIA’s most popular SDKs,
second only to cuDNN for deep learning.
RAPIDS has been downloaded 2M times
and has grown 3 times year-over-year.
It is used by over 5,000 GitHub projects
and over 2,000 Kaggle notebooks,
and is integrated into 35 commercial software packages.
NVIDIA RAPIDS for Spark is a plug-in
for accelerating Apache Spark.
Spark is the leading data processing engine used by 80%
of the Fortune 500 companies.
Users of Spark can transparently
accelerate Spark data-frame and SQL.
Operations that take hours now take minutes.
NVIDIA cuOpt, previously called ReOpt, is an SDK for multi-agent, multi-constraint route planning optimization
used for delivery services or
autonomous mobile robots inside warehouses.
With NVIDIA cuOpt, businesses can, for the first time, do real-time planning of thousands of packages
to thousands of locations in
seconds with world-record accuracy.
Over 175 companies are testing NVIDIA cuOpt.
Graphs are one of the most used data structures to represent real-world data, like maps, social networks, the web,
proteins and molecules, and financial transactions.
The NVIDIA DGL container lets you train large graph neural networks across multiple GPUs and nodes.
NVIDIA Morpheus is a deep learning
framework for cybersecurity.
Morpheus helps cybersecurity developers build and
scale solutions that use deep learning to identify,
capture, and act on threats previously impossible.
Every company needs to move to a Zero Trust architecture.
NVIDIA can for sure use Morpheus.
cuQuantum is an SDK for accelerating
Quantum Circuit Simulators
so researchers can develop quantum
computing algorithms of the future
that are impossible to explore on quantum computers today.
cuQuantum accelerates the top QC
simulators Google Cirq, IBM Qiskit,
Xanadu’s Pennylane, Quantinuum TKET,
and Oak Ridge National Laboratory ExaTN.
cuQuantum on DGX is the ideal
development system for quantum computing.
Aerial is an SDK for CUDA-accelerated
software-defined 5G radio.
With Aerial, any data center, cloud, on-prem,
or edge, can be a 5G radio network
and provision AI services on 5G to places not served by WIFI.
The 6G standard will emerge around 2026.
The megatrends shaping 6G are clear -
hundreds of billions of machines and robots
will be the overwhelming users of the network.
6G is taking shape around a few foundational technologies.
Like networking, 6G will be highly software-defined.
The network will be AI-driven.
Digital twins performing ray tracing
and AI will help optimize the network.
NVIDIA can make contributions in these areas.
We are excited to announce a new framework, Sionna, an AI framework for 6G communications research.
Modulus is an AI framework for developing physics-ML models.
These deep neural network models can learn physics
and make predictions that obey the laws of physics
at many orders of magnitude faster than numerical methods.
We are using Modulus to build the Earth-2 digital twin.
Monai is an open-source AI framework for medical imaging.
The NVIDIA Monai container includes AI-assisted labeling for 2D and 3D models, transfer learning, and autoML training;
and it’s easy to deploy through DICOM.
Monai is used by the world’s top 30 academic
medical centers and has over 250,000 downloads.
FLARE is NVIDIA’s open-source SDK for federated learning, letting researchers collaborate in a privacy-preserving way –
sharing models but not data.
Millions of developers and tens of thousands of companies use NVIDIA SDKs to accelerate their workload.
We updated 60 SDKs with more features
and acceleration at this GTC.
The same NVIDIA systems you own just got faster
and scientists doing Operations Research,
quantum algorithm research, 6G research,
or graph analytics can tap into
NVIDIA acceleration for the first time.
And for the companies doing computer
aided design or engineering,
the software tools you depend on from Ansys, Altair,
Siemens, Synopsys, Cadence, and more,
just got a massive speed-up.
From first-hand experience, it has transformed our engineering.
So, go to NGC, NVIDIA GPU Cloud, and download our SDKs
and frameworks that are full-stack optimized
and data-center-scale accelerated.
The Apollo 13 crew was
136,000 miles from Earth
when a faulty electrical wire caused
one of the two oxygen tanks to explode.
And the now infamous words radioed back to NASA -
“Houston, we’ve had a problem.”
To “work the problem”, NASA engineers tested oxygen-preserving and power-cycling procedures
on a replica of the Odyssey spacecraft.
Apollo 13 would have ended in disaster if not
for the fully functional replica on Earth.
This was an important moment.
NASA realized the power of the replica,
but not everything can have a physical twin.
So NASA coined the term “digital twin,” a living virtual representation of something physical.
Extended to vast scales, a digital twin is a
virtual world that’s connected to the physical world.
And in the context of the internet, it is the next evolution.
And that’s what NVIDIA Omniverse is about – digital twins, virtual worlds, and the next evolution of the internet.
Over 20 years of NVIDIA graphics, physics, simulation, AI, and computing technologies made Omniverse possible.
Simulating the world is the ultimate grand challenge.
Omniverse is a simulation engine of virtual worlds.
Omniverse worlds are physically accurate,
obeying the laws of physics.
Omniverse operates at vast scales.
And Omniverse is sharable, connecting designers,
viewers, AIs, and robots.
But what are the applications of Omniverse?
I will highlight several immediate use-cases today.
Remote Collaboration of designers using different tools.
Sim2Real Gyms where AI and robots learn.
And industrial Digital Twins.
But first, let me show you the
technologies that make Omniverse possible.
Omniverse technology will transform the way you create!
Omniverse is scalable from RTX PCs to large systems.
RTX PCs connected to someone hosting the Omniverse Nucleus are sufficient for creative collaboration.
Industrial digital twins, however, need a new type
of purpose-built computer.
Digital twin simulations involve multiple autonomous systems interacting in the same space-time.
Data centers process data in the
lowest possible time, not precise time.
So for digital twins, the Omniverse software and computer need to be scalable, low latency, and support precise time.
We need to create a synchronous data center.
Just as we have DGX for AI, we now have OVX for Omniverse.
The first-generation NVIDIA OVX Omniverse computer consists of eight NVIDIA A40 RTX GPUs, 3 CX6 200 Gbps NICs,
and dual Intel Ice Lake CPUs.
And the NVIDIA Spectrum-3 200 gigabytes per second switch fabric connects 32 OVX servers to form the OVX SuperPOD.
Most importantly, the network and computers
are synchronized using Precision Timing Protocol,
and RDMA minimizes packet transfer latency.
OVX servers are now available from
the world’s top computer makers.
And for customers wanting to try Omniverse on OVX,
NVIDIA LaunchPads are located around the world.
Generation one OVX are running at NVIDIA and early customers.
We are building our second-generation OVX,
starting with the backbone.
Today, we are introducing the Spectrum-4 switch.
At 51.2 terabytes per second, the 100-billion transistor Spectrum-4 is the most advanced switch ever built.
Spectrum-4 introduces fair bandwidth
distribution across all ports, adaptive routing,
and congestion control for the
highest overall data center throughput.
With CX7 and Bluefield-3 adaptors, and the
DOCA data center infrastructure software,
this is the world’s first 400 gigabytes per second
end-to-end networking platform.
And Spectrum-4 can achieve timing
precision to a few nanoseconds
versus the many milliseconds of jitter in a typical data center – that is a 5 to 6 orders of magnitude improvement.
Hyperscalers will enjoy increased throughput, quality of service, and security, while reducing power and cost.
Spectrum-4 enables a new class of computers for Omniverse digital twins in cloud and edge data centers.
NVIDIA Spectrum-4, the world’s most
advanced ethernet networking platform
and the backbone of our Omniverse
computer, samples in late Q4.
Omniverse is a network-of-networks connecting virtual worlds.
The value of the network amplifies when diverse ecosystems connect through Omniverse into a unified workflow.
Since last year’s GTC, we’ve increased connections from 8 to 82.
We have connectors with Chaos Vray,
Autodesk Arnold, and Blender.
Adobe’s 3D Substance Painter, Epic’s Unreal Engine 5,
and Maxon’s Cinema 4D.
Many developers want to OEM and connect
Omniverse directly into their software suite.
Bentley Systems is the leading infrastructure design, construction, and management platform.
They integrated Omniverse into their LumenRT platform to do interactive, engineering-grade, millimeter-accurate,
4D visualization of massive infrastructure digital twins.
Bentley’s LumenRT with Omniverse is available now.
We are launching a major release at GTC.
Omniverse Kit for building Omniverse extensions,
apps, and microservices.
We are releasing 26 manufacturing format converters to connect industrial workflows to Omniverse.
DeepSearch is an AI search engine for massive, untagged, 3D asset libraries using either natural language or images.
Omniverse Replicator, a framework for generating physically accurate synthetic data to train and test AI models.
And Omnigraph, a multi-GPU, multi-node, compute engine to run data-center-scale Omniverse applications.
And, a very big project, Omniverse Avatar,
a framework for building avatars, or digital humans.
Let me welcome back Toy Jensen,
an application we built with Omniverse Avatar.
Remember, Toy Jensen is not recorded.
He is completely real-time.
Making eye contact, making conversation, and fully animated.
Hey TJ! I have a few questions for you.
Sure! Ask me anything.
What is synthetic biology?
Synthetic Biology is about designing biological systems at multiple levels from individual molecules up to whole cells
and even multicellular assemblies like
tissues and organs to perform specific functions.
It is also known as SynBio which stands for 'synthetic life'.
Can you give me some examples?
Some current uses of synthetic biology involve creating biofuels; engineering bacteria with useful properties not found
in naturally occurring organisms; developing biosensors
capable of detecting environmental toxins;
producing biopharmaceuticals; and manufacturing chemicals via fermentation rather than petrochemicals.
What are its medical applications?
Synthetic biology has many medical applications. For example, synthetic biology is being used to develop new vaccines,
new antibiotics, and new treatments for cancer.
TJ that was great! Can you tell us how you were made?
Avatar is a framework built on the Omniverse platform.
It lets you quickly build and deploy
Avatars like me for a wide range of uses.
It starts with my voice. It is fully synthesized
by Riva’s Text to Speech RADTTS
and is also used to drive my
facial animation using Omniverse Audio2Face.
Next, Omniverse animation’s animation graph
is used to define and govern my movement,
from hand and arm movements to subtle head and body motion.
NVIDIA’s open-source Material Definition Language, MDL,
adds the touches that make my cool jacket
look like synthetic leather and not just plastic, while the RTX renderer brings me to life in high-fidelity—in real-time.
Finally, I can listen and talk to you thanks to the latest in conversational AI technologies from Riva
and our Megatron 530B NLP model, one of the
largest language models ever trained.
Megatron helps me answer all those
tough questions Jensen throws at me.
What’s also exciting is that I can be run from the cloud,
the data center, or any other disaggregated system,
all thanks to Tokkio.
Tokkio is an application built with Omniverse Avatar
and it brings customer service AI to retail stores,
quick-service restaurants, and even the web.
It comes to life using NVIDIA AI models and technology like computer vision, Riva speech AI, and NVIDIA NeMO.
And because it runs on our unified computing framework,
or UCF, Tokkio can scale-out from the cloud
and go wherever customers need helpful avatars like me,
with senses that are fully acute
and responsive, and above all, natural.
I hope you enjoyed a quick overview of how I was made.
Back to you, Jensen!
Today’s AI centers around perception and pattern recognition, like recognizing an image, understanding speech,
suggesting a video to watch, or recommending an item to buy.
The next wave of AI is robotics, where AI will also plan and act.
NVIDIA is building several robotics platforms –
DRIVE for autonomous vehicles,
Isaac for maneuvering and manipulation systems,
Metropolis for autonomous infrastructures,
and Holoscan for robotic medical instruments.
And just as NASA recognized, we will need digital twins in order to operate fleets of robots that are far away.
The workflow of a robotic system is complex.
I’ve simplified it here to four pillars:
Collecting and generating ground truth data,
creating the AI model,
simulating with a digital twin,
operating the robot.
Omniverse is central throughout.
DRIVE is our autonomous vehicle system,
it’s essentially an AI chauffeur.
As with all of our platforms,
NVIDIA DRIVE is full-stack, end-to-end,
and open for developers to use in-whole or in-parts.
For ground truth data, we use our DeepMap HD mapping, human-labeled data, and Omniverse Replicator.
To train the AI models, we use NVIDIA AI and DGX.
DRIVE Sim in Omniverse, running on OVX, is the digital twin.
And DRIVE AV is the autonomous driving application
running on our Orin computer in the car.
Let’s enjoy a ride with the latest build of NVIDIA Drive.
We will take you through a highway and urban route in San Jose.
You can see what the car sees
from the confidence view rendering
We will navigate complex situations
such as crowded intersections,
And your AI chauffeur will be a friendly driving companion.
Welcome Daniel. I see a text from Hubert asking,
“Can you pick me up from the San Jose Civic?”
Should I take you there?
Yes, please.
Okay, taking you to San Jose Civic.
StartDRIVE Pilot.
OK, starting DRIVE Pilot.
Can you tell Hubert we’re on our way?
Sure, I’ll send him a text.
I see Hubert.
Can you please take me to Rivermark Hotel?
Okay, taking you to Rivermark Hotel.
Thanks for picking me up!
Definitely. Start DRIVE Pilot.
Ok, starting DRIVE Pilot.
What building is that there?
That building is San Jose Center for the Performing Arts
What shows are playing there?
Cats is playing tonight.
Can you get me two tickets for Saturday night?
Yes I can.
You have arrived at your destination.
Please park the vehicle.
OK, finding a parking spot.
Hyperion 8 is the hardware architecture of our self-driving car and it’s what we build our entire DRIVE platform on.
It consists of sensors, networks, two chauffeur AV computers, one concierge AI computer, a mission recorder,
and safety and cybersecurity systems.
And it’s open.
Hyperion 8 can achieve full self-driving with a 360-degree camera, radar, lidar, and ultrasonic sensor suite.
Hyperion 8 will ship in Mercedes Benz cars starting in 2024, followed by Jaguar Land Rover in 2025.
Today, we are announcing Hyperion 9
for cars shipping starting in 2026.
Hyperion 9 will have 14 cameras,
9 radars, 3 lidars, and 20 ultrasonics.
Overall, Hyperion 9 will process twice the amount
of sensor data compared to Hyperion 8,
further enhancing safety and extending the
operating domains of full self-driving.
NVIDIA DRIVE Map is a multi-modal map engine
and includes camera, radar, and lidar.
You can localize to each layer of the map independently,
which provides diversity and redundancy
for the highest level of safety.
Drive Map has two map engines – ground truth survey mapping and crowdsourced fleet mapping.
By the end of 2024, we expect to map and create a digital twin of all major highways in North America, Western Europe,
and Asia – about 500,000 kilometers.
The map will be expanded and updated
by millions of passenger cars.
We are building an earth-scale digital twin of our AV fleet
to explore new algorithms and designs,
and test software before deploying to the fleet.
We are developing two methods to simulate scenarios – each reconstructs the world in different ways.
One method starts from NVIDIA Drive Map,
a multi-modal map engine
that creates a highly accurate 3D representation of the world.
The map is loaded into Omniverse.
Buildings, vegetation, and
other roadside objects are generated.
From previous drives, the dynamic objects, cars, and pedestrians, are inferred, localized,
and placed into the digital twin.
Each dynamic object can be animated or
assigned an AI behavior model.
Domain randomization can be applied to generate
diverse and plausible challenging scenarios.
A second approach uses Neural Graphics AI and
Omniverse to transform a pre-recorded drive video
into a reenactable and modifiable drive.
We start by reconstructing the scene in 3D.
Dynamic objects are recognized and removed,
and the background is restored.
After scene reconstruction, we can
change the behavior of existing vehicles
or add fully controllable vehicles
that behave realistically to traffic.
The regenerated drive, with 3D geometry, and physically based materials, allow us to properly re-illuminate the scene,
apply physics, and simulate sensors, like LIDAR.
The pre-recorded scene is now reenactable and can be
used for closed-loop simulation and testing.
DRIVE Map and DRIVE Sim, with AI breakthroughs by NVIDIA research, showcase the power of Omniverse digital twin
to advance the development of autonomous vehicles.
NVIDIA DRIVE Map, DRIVE Sim,
Hyperion 8 with Orin, and DRIVE AV stacks
are available independently or together as a whole.
Electric vehicles have forced a
complete redesign of car architectures.
Future cars will be highly programmable, evolving from many embedded controllers to highly centralized computers.
The AI and AV functionalities will be delivered in
software and enhanced for the life of the car.
NVIDIA Orin has been enormously
successful with companies building this future.
Orin is the ideal centralized AV and AI computer
and is the engine of new-generation EVs,
robotaxis, shuttles, and trucks.
Orin started shipping this month.
Today we are thrilled to announce that BYD,
the second-largest EV maker globally,
will adopt the DRIVE Orin computer for cars
starting production in the first half of 2023.
NVIDIA’s self-driving car computer, software,
and robotics AI is essentially
 the same computing pipeline as
next-generation medical systems.
Let me show you what Holoscan can do for an incredible instrument called a lightsheet microscope.
Invented by Nobel laureate Eric Betzig,
lightsheet microscopes use high-resolution fluorescence
to create a movie of cells moving and dividing, giving researchers the ability to study biology in motion.
The problem is that lightsheet microscopes produce 3TB
of data per hour – the equivalent of 30 4K movies.
It takes up to a day to process the 3TB of data.
With NVIDIA Clara Holoscan,
we can process the data in real-time.
Now with Clara Holoscan and NVIDIA index we can
visualize the entire large volume of living cells in real time
as the data is being recorded directly from the microscope.
Watching these living cancer cells move about,
we can see normal healthy biology
and maligent processes at the same time.
The fluorescent marker rendered in blue marks nuclei,
which we see splitting to form two cells from one cell.
A hallmark of cancer is cell division occurring more frequently and with less error checking than normal, healthy cells.
Using Berkeley’s lattice light sheet microscope,
the ultimate-high resolution
allows scientists to see what is hidden to normal light optics – not seen using traditional microscopes.
As we zoom in, watch cancer cells display what is thought to be a rare event even for cancer cell lines –
see one cell split into 3 cells.
This phenomenon has only been reported
anecdotally in a couple of scientific publications.
Scientists do not yet know what we will see – but this technique, enabled with real-time processing and visualization,
now allows the scientific community
to discover new unseen events like this.
Let’s see what the future has in store.
Clara Holoscan is an open, scalable robotics platform.
Clara Holoscan is designed to the
IEC-62304 medical-grade specification
and for the highest device safety and security level.
The amount of computation in Holoscan is insane.
The core computer is Orin and CX7, with an optional GPU.
Holoscan development platforms are available for early
access customers today, general availability in May,
and medical-grade readiness in Q1 2023.
Future medical devices will be AI instruments,
assisting diagnostics or surgery.
Just as NVIDIA DRIVE is a platform for robotic vehicles, Clara Holoscan is a platform for robotic medical instruments.
We are delighted to see the enthusiasm around Holoscan
and to partner with leading medical device makers
and robotic surgery companies.
The demand for robotics and
automation is increasing exponentially.
Some robots move, and other robots watch things that move.
NVIDIA is working with thousands of customers and developers, building robots for manufacturing, retail, healthcare,
agriculture, construction, airports, and entire cities.
NVIDIA’s robotics platforms consist of Metropolis and Isaac –
Isaac is a platform for things that move.
Metropolis is a stationary robot tracking moving things.
Metropolis and Isaac platforms, like DRIVE, consist of 4 pillars – Ground Truth generation, AI model training,
Omniverse digital twin, and the robot with
robotic software and computer.
Metropolis has been a phenomenal success – has been downloaded 300,000 times, has over 1,000 ecosystem partners,
and operates in over a million facilities,
including USPS, Walmart,
cities including Tel Aviv and London, the Heathrow airport, Veolia recycling plants, and the Gillette football stadium.
And now, customers can use Omniverse to create digital twins of their facilities to drive better safety and efficiency.
Let’s take a look at how the Pepsi company
is using Metropolis and Omniverse.
PepsiCo’s products are enjoyed
1B times a day around the world.
Getting this many products to their 200 regional markets requires over 600 distribution centers.
Improving the efficiency and environmental sustainability of their supply chain is a key goal for Pepsi Co.
To achieve this, they are building digital twins to
simulate their packaging and distribution centers
using NVIDIA Omniverse and Metropolis.
This allows them to test variations in layout
and optimize workflows to accelerate throughput
before making any physical investments.
As new products and processes are introduced, Omniverse Replicator and NVIDIA TAO can be used to create
photorealistic synthetic data to re-train the real-time AI models.
These updated models and optimizations
are then transferred to the physical world.
From here, NVIDIA Metropolis applications monitor
and adjust conveyer belt speed in real-time
using AI enabled computer vision helping prevent
congestion and downtime across miles of conveyor belts.
What’s more, with NVIDIA Fleet Command,
all these applications can be securely deployed
and managed across hundreds of
distribution centers from one central plane.
By leveraging NVIDIA Omniverse, Metropolis, and Fleet Command, PepsiCo is streamlining supply chain operations,
reducing energy usage, and advancing
their mission towards sustainability.
One of the fastest-growing segments of robotics is AMR – autonomous mobile robots –
essentially driverless cars for indoors.
The speed is lower, but the environment is highly unstructured.
There are 10’s of millions of factories, stores, and restaurants.
And 100’s of millions of square feet of
warehouse and fulfillment centers.
Today, we have a major release of Isaac – Isaac for AMRs.
Let me highlight some of the key elements of the release.
Isaac for AMRs, like the DRIVE platform, has four major pillars, each individually available, and completely open.
New NVIDIA DeepMap for Ground Truth generation, NVIDIA AI for training models, a reference AMR robot powered by Orin,
new gems in the Isaac robot stack,
and the new Isaac Sim on Omniverse.
First, Isaac Nova, like DRIVE Hyperion, is a reference AMR robot system on which the entire Isaac stack is built.
Nova has 2 cameras, 2 lidars, 8 ultrasonics,
and 4 fisheye cameras for teleoperation.
We’re announcing Jetson Orin developer
kits are available today.
Nova AMR will be available in Q2.
Nova AMRs can be outfitted with NVIDIA’s new
DeepMap LIDAR mapping system so you can scan
and reconstruct your environment for route
planning and digital twin simulations.
Isaac robot SDK includes perception, localization,
mapping, planning, and navigation modules.
And today we’re announcing major updates to build an AMR.
Isaac includes gems like object and person detection,
3D pose estimation, LIDAR and
visual SLAM localization and mapping,
3D environment reconstruction, free space perception,
dolly docking using reinforcement learning,
a navigation stack, integration with
NVIDIA cuOpt for real-time planning,
and robotic arm motion planning and kinematics, and more.
There is even an SDK for teleoperation.
Finally, Omniverse is used to build
Isaac Replicator for synthetic data generation,
Isaac Gym to train robots, and Isaac Sim for digital twins.
The Isaac development flow integrates Omniverse throughout.
Isaac Gym highlights the importance of
Omniverse’s physics simulation accuracy.
In Isaac Gym, a new robot learns a new skill
by performing it thousands to millions of times
using deep reinforcement learning.
The trained AI brain is then downloaded into the physical robot. And since Omniverse is physically accurate, the robot,
after getting its bearings,
should adopt the skills of its digital twin.
Let’s take a look.
Successful development, training, and testing of
complex robots for real-world applications
demand high-fidelity simulation and accurate physics.
Built on NVIDIA's Omniverse platform, Isaac Sim combines immersive, physically accurate, photorealistic environments
with complex virtual robots.
Let’s look at three very different AI-based robots being developed by our partners using Isaac Sim.
Fraunhofer IML, a technology leader in logistics, uses NVIDIA Isaac Sim for the virtual development of Obelix—
a highly dynamic indoor/outdoor
Autonomous Mobile Robot, or AMR.
After importing over 5400 parts from CAD
and rigging with Omniverse PhysX,
the virtual robot moves just as deftly in
simulation as it does in the real world.
This not only accelerates virtual development,
but also enables scaling to larger scenarios.
Next, Festo, well known for industrial automation,
uses Isaac Sim to develop intelligent skills
for collaborative robots, or cobots, requiring acute awareness of their environment, human partners, and tasks.
Festo uses Cortex, an Isaac Sim tool that
dramatically simplifies programming cobot skills.
For perception, AI models used in this task were trained using only synthetic data generated by Isaac Replicator.
Finally, there is Anymal, a robot dog developed by a leading robotics research group from ETH Zurich and Swiss-Mile.
Using end-to-end GPU accelerated Reinforcement Learning, Anymal, whose feet were replaced with wheels,
learned to 'walk' over urban terrain within minutes rather
than weeks using NVIDIA's Isaac Gym training tool.
The locomotion policy was verified in
Isaac Sim and deployed on a real Anymal.
This is a compelling demonstration of
simulator training for real-world-deployment.
From training perception and policies to hardware-in-loop,
Isaac Sim is the tool to build AI-based robots
that are born in simulation to work and play in the real-world.
Modern fulfillment centers are evolving into technical marvels – facilities operated by humans and robots working together.
The warehouse is also a robot, orchestrating the flow of materials and the route plans of the AMRs inside.
Let’s look at how Amazon uses Omniverse digital twin
to design and optimize their incredible
fulfillment center operations.
Every day, hundreds of Amazon’s facilities
handle tens of millions of packages,
with more than two thirds of these
customer orders handled by robots.
To support this highly complex operation,
we deployed hundreds of thousands mobile drive robots,
and associated storage pods,
which allow us to store far more inventory
in our buildings than traditional shelving.
And which help us move inventory in a safer, more efficient way.
Key to the scaling has been our
ability to simulate these buildings
and understand their performance before we build them.
Let’s look at how NVIDIA Omniverse is helping us
optimize and simplify these processes.
At Amazon Robotics, we are able create full-scale “digital twins” of our warehouses in NVIDIA Omniverse,
helping us optimize warehouse design, train more intelligent robot assistants, and gain operational efficiencies.
In Omniverse, we are uniquely able to aggregate
datasets from many different CAD applications
and visualize these massive models in full-fidelity realism,
enabled by Omniverse's RTX-accelerated
ray tracing, materials, and physics.
Digital twins are an integral part of
future warehouses and factories,
enabling continuous integration and continuous delivery.
With each new software and layout optimization,
we can test in the digital twin before
releasing to the physical warehouse,
preventing system downtime or failure
while maximizing operational efficiencies.
Next, packages of every shape, size, weight, and material
move rapidly through our fulfillment centers.
We use NVIDIA Omniverse to better train
autonomous robotic sorting and picking solutions.
Training these robots' perception systems
accurately enough to prevent system failures
requires massive amounts of high-quality data, but often,
the data doesn't exist, or there isn't enough.
When we introduced more reflective tape to our packing materials, the perception systems failed.
We retrained the models with physically accurate, photoreal synthetic data generated in Omniverse -
indistinguishable from reality - saving weeks of
retraining time and increasing model accuracy.
Finally, with digital twins of our facilities and the ability to quickly and accurately train robot perception systems
we can also better configure human-robot workstations, simulating opportunities for better employee ergonomics.
At Amazon Robotics, NVIDIA Omniverse digital twins are helping us as we reimagine warehouse logistics from end-to-end;
and capturing a significant operational efficiencies which
enable us to deliver more value to our customers.
Just like NASA and Amazon, our customers
in robotics and industrial automation
are realizing the importance of digital twins
and are doing amazing things in Omniverse.
You can see how important Omniverse is
throughout NVIDIA's work in AI and robotics.
The next wave of AI - robotic systems -
needs a platform like Omniverse.
We want Omniverse to reach every one of the 10’s of millions of designers, creators, roboticists, and AI researchers.
So, today, we are announcing Omniverse Cloud.
Just a few clicks and you and your collaborators are connected.
In this demo, you are going to see 4 designers, all working remotely, connected by Omniverse Cloud,
and collaborating to create a virtual world.
Let’s take a look at how easy it works.
3D design is a complex team sport, with different artists, apps and hardware, often working in different locations.
With Omniverse Cloud, it’s much easier.
Using NVIDIA RTX PCs, laptops, and workstations,
designers can work together in real-time.
And if you don’t have an RTX computer, you can stream Omniverse from GeForce Now with just a single click.
Let's watch as an architectural design team reviews a project using Omniverse View in a web conference.
Alright, right here is the 5th floor patio.
To get a better feel for the lighting,
let’s see what this space looks like at noon.
Sure!
Hmm... it’s a little sunny.
Let’s make some adjustments to the trellis.
One sec…
How’s that?
Much better, let’s see if we can get Teresa on.
Yep, sending that link now.
Hey you two!
Hi! So we did a sun study at mid-day and decided
it needed more shade. What do you think?
Yeah that looks great but I still feel like it needs a little more.
What if we add some trees?
Yeah, let’s get TJ to help with that.
Hi, how can I help?
Hey TJ, can you add some medium-sized
trees in planters near the tables?
Sure, let me add that for you.
How does this look?
Nice!... but TJ can we vary the trees, say size and type?
OK, here you go.
Hey TJ, let's go to the bar
and see what it looks like at night.
You got it!
Yeah, that looks great.
I’ll send the link out for approval.
See you in Omniverse.
You may have noticed one of the designers is an AI.
As AIs develop skills, we will not only invite them
to help us design buildings and factories,
but also take orders at restaurants, help customers,
and even answer questions about our health.
Omniverse, for the next wave of AI.
We covered a lot of ground so let me do a fast recap.
We announced new products across NVIDIA’s four-layer stack:
hardware, system software and libraries, software platforms
NVIDIA HPC, NVIDIA AI, and NVIDIA Omniverse;
and AI and robotics application frameworks.
These are driving five dynamics shaping our industry - Million-X computing speed-up, Transformers turbocharging AI,
data centers becoming AI Factories,
exponentially increasing demand for robotics systems,
and digital twins for the next era of AI.
Four layers, five dynamics.
NVIDIA accelerated computing, full-stack engineering at data center scale, has sped-up computing by a Million-X.
A Million-X opens the opportunities to tackle grand challenges, like drug discovery and climate science.
And with the invention of self-learning Transformers,
AI jumped to warp speed.
AI has fundamentally changed what software
can make and how you make software.
Companies build AI by processing and refining data to manufacture intelligence - their data centers are AI factories.
NVIDIA H100 is the new engine of the world’s AI infrastructure.
NVIDIA’s NVLINK Switch system connects up to 32 DGXs into a massive 1 exaflops building block of the AI factory.
Hopper H100 is the biggest generational leap ever - 9 times at-scale training performance over A100
and 30 times large-language-model inference throughput.
Hopper also accelerates mainstream servers.
NVIDIA H100 CNX connects the
network directly into H100
through the most advanced networking chip – NVIDIA CX7.
NVIDIA H100, the engine of the world’s AI infrastructures.
H100 is in production, with availability starting in Q3.
Grace is on track for production next year.
Grace is an amazing superchip –
two CPUs connected over a 900 gigabytes per second
NVLINK chip-to-chip interconnect to make a 144-core CPU
with 1 terabytes per second of memory bandwidth.
Grace is the ideal CPU for the world’s AI infrastructures.
NVLINK now spans die-to-die,
chip-to-chip, and system-to-system,
and gives us a multitude of
Grace-Hopper system configurations–
from dual-Grace, to one-Grace one-Hopper,
to dual-Grace and eight Hoppers.
NVLINK will be coming to all future NVIDIA chips –
CPUs, GPUs, DPUs, and SOCs.
We will also make NVLINK available to customers
and partners to build companion chips.
NVLINK opens a new world of opportunities for
customers to build semi-custom chips and systems
that leverage NVIDIA’s platforms and ecosystems.
NVIDIA AI platform, used by over 25,000 companies worldwide, got major updates.
NVIDIA Omniverse is a platform for virtual worlds, digital twins, and robotic systems - the next wave of AI.
Just as Tensorflow and PyTorch are essential
frameworks for perception-oriented AI,
Omniverse will be integral for action-oriented AI.
And as DGX is the infrastructure of AI factories,
OVX will be the infrastructure of digital twins.
OVX runs Omniverse digital twins for large-scale
simulations with multiple autonomous systems
operating in the same space-time.
The backbone of OVX is its networking fabric.
We announced NVIDIA Spectrum-4
51.2 terabits per second switch,
and with CX7 and Bluefield-3, will be the first end-to-end 400 gigabytes per second networking platform.
Spectrum-4 will sample in Q3.
The next wave of AI is robotic systems that
perceive, plan, and act.
NVIDIA Avatar, Drive, Metropolis, Isaac, and Holoscan are robotics platforms built end-to-end and full-stack
around four pillars – ground truth data generation, AI model training, the robotic stack, and Omniverse digital twin.
The platforms are open for developers
to adopt in part or in whole.
Omniverse is central to our robotics platforms.
And like NASA and Amazon, we and our
customers in robotics and industrial automation
realize the importance of digital twins and Omniverse.
Drive Orin is in full production.
 Issac Orin developer kits are available now.
Clara Holoscan devkits are available in May.
We updated 60 SDKs at this GTC.
For our three million developers, scientists, and AI researchers,
and 10’s of thousands of startups and enterprises,
the same NVIDIA systems you run just got faster.
NVIDIA SDKs serve healthcare, energy, transportation, retail, finance, media, and entertainment –
a combined $100 trillion of industries.
Accelerating across the full-stack and at data center scale,
we will strive for yet another Million-X in the next decade.
I can’t wait to see what the next Million-X brings.
I want to thank NVIDIA developers,
partners, customers, and the NVIDIA families
for the amazing work you do to shape the world.
But don’t leave just yet. Omniverse generated
every rendering and simulation you saw today.
NVIDIA’s amazing creative team would like to take
you on one more trip into Omniverse.
Engage.
Title: GTC 2016: Support for Pascal from Baidu and Google (part 8)
Publish_date: 2016-04-06
Length: 787
Views: 10528
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/3Ev1DlGRr9w/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 3Ev1DlGRr9w

--- Transcript ---

well there's so much more to dgx one the
thing that is important isn't just the
fact that we built something super fast
something super super super cool but the
architecture of it has to enable new
types of applications and one of my
heroes
somebody who before anybody talked about
deep learning before anybody else was
talking about the importance of this new
computing model a researcher was
explaining all of this to me in
languages that I understood explaining
to me the impact of this work Brian
Cannizzaro
where are you where's Brian Brian Cadiz
our is researcher at Baidu
he was formerly one of invidious
greatest researchers he was one of the
reasons why Nvidia got into this whole
business in the first place and you're
my hero I just want to tell you that I
hope it doesn't embarrass you and
I am thoroughly embarrassed now not only
are you are you are you graceful
gracious and just an utterly brilliant
brilliant researcher very few people in
the world knows as much about deep
learning and the intersection of deep
learning and GPUs because he knows so
much about GPUs and today I invited
Brian to come and share with you some of
the research that he's doing and would
you like a little clicker sure I think
Brian's gonna tell you about just a
little bit about the work that he's
doing on Pascal great so at Baidu were
really excited about being able to use
Pascal and NV link to accelerate the
training of our recurrent neural
networks and I wanted to mention a
little bit about what a recurrent neural
network is so convolutional neural
networks are often used for processing
images and video at bi do we care a lot
about sequential data text and speech
and I think these are really useful for
many different kinds of AI problems um
recurrent neural networks have a time
series dependence they operate on time
series data and then they produce a time
series output so for example in speech
recognition we operate on an input
waveform that's sequential samples taken
as time goes on and then we produce a
sequence of characters that represents
what the user said um so this sequential
dependence is the power of RNN because
it gives it memory and it gives it the
ability to understand the dependences in
the data which is which are very
interesting but it also makes the
parallelization of the training process
more difficult um there are two major
ways of paralyzing the neural network
training process one of them we call
model parallelism is basically where you
take a look at your model and you
realize wow there's so many neurons here
I could partition them and assign them
to different processors say for example
I'll put half of them on GPUs zero and
half of them on GPU number one and then
the other kind of parallelism is called
data parallelism where we realize hey
our training set is huge it has
thousands and thousands of hours of of
audio data for example I bet we could
chop that up in pieces and assign
different pieces to different processors
um we have been trying to use model
parallelism for many years on and off
for different applications and it's
always been difficult because of several
things one of them being the
interconnect between processors and so
Pascal is going to really help out in
that regard
not only is each GPU much bigger and
much faster which we we really need but
the interconnect between GPUs is much
better and has it's faster and it also
has new capabilities that we're going to
use for interesting things and I wanted
to give you sort of a sneak peek at some
of the things that we're thinking about
doing with these new capabilities we've
been working on a project for a while
called persistent aren't ends and the
idea but behind persistent aren't ends
is that since the weights are reused
over multiple time steps as the model
iterates sequentially over the data we
can actually keep those on chip keep
those persistent in the register file of
the chip and then do all of the math
without needing to talk to memory nearly
as much and the advantage of this is
that we can reduce the number of
training examples that are required to
keep our GPU busy so instead of needing
64 examples to keep the GPU busy we can
keep it busy with say 8 and running very
close to peak efficiency now the big
limitation of persistent RM ends is that
we need to fit the entire weight matrix
on chip in the register file and so on
Pascal it was about a six megabyte
register file on I'm sorry on Maxwell is
about a six megabyte register file on
Pascal you just saw it was 14 so that's
a big leap that's going to help us but
the thing that's even more exciting to
me is that NV link is going to allow us
to use model parallelism as well so we
can take that matrix and partition it
across the register files of multiple
GPUs and use the synchronization
capabilities that NV link provides in
order to very efficiently communicate
between those GPUs and train our model
so when we when we combine this style of
model parallels
with persistent rnns with data
parallelism since we we need fewer
examples per model to keep the GPUs full
we can actually use more copies of the
model and use wider data parallelism
that's gonna combine to allow us to
scale to even more processors and I
think we're gonna be able to go quite a
bit larger than we've ever been able to
in the past like 30x bigger I also
wanted to point out that my colleague
Greg Deimos who's been doing most of the
work on persistent are n ends is going
to be presenting on Thursday so if
you're interested in learning more about
this topic I'd recommend going to see
his talk but just to summarize we're
really excited about the possibilities
that n V link and Pascal give us to
strong scale our RN n training we think
it's going to make huge progress in our
AI applications that rely on our n ends
because we'll be able to turn through
far more data far more rapidly that's
really awesome thank you very much right
you should be a professor yeah thanks
thirty times bigger model thirty times
bigger model that's the benefit of
Pascal and the benefit of the new
architecture and NV link the next
speakers is has made a contribution that
is just absolutely gigantic absolutely
gigantic you know one of the things I
mentioned earlier these frameworks the
frameworks are basically the design
tools of modern AI by encapsulating all
of the complexity into a nice and easy
to use framework so that anybody can
design you networks is really important
but more than that the folks at Google
and tensorflow Google tensorflow
open-source tensorflow they made one of
the most important new tools available
to everybody for free it's completely
open on github it has the most number of
likes of contributors and users than any
other open-source tool twenty thousand
likes and you know how critical
engineers can be twenty thousand likes
this tool is being used far and wide now
tensorflow
it's completely open sourced
and I really believe that this is going
to democratize AI tensorflow is going to
has the opportunity to move a high
quality tool that has been optimized for
modern computing environments and making
it available for every industry every
developer every researcher so that we
can accelerate the progress of AI I
first learned about tensor flow watching
a talk by Rajat manga and I thought we
invite Rajat to come on stage and tell
you a little bit about it Roger won't
you come up and tell us about your
important work ladies and gentlemen
Roger manga first of all congratulations
on tensorflow
thank you my goodness I mean you guys
literally launched it's just several
weeks ago and it's just swept through
the industry it's been great the
community uptick has been great
everybody seems to love it
and we are really invested in making
sure it continues to be even better and
what is the reason why you guys well
first of all you guys develop this tool
for your own internal development and
the thing that I saw in your
presentation which I thought was really
great is you you uh you said that that
inside Google in just one year's time
and there was a chart that you were
showing wasn't on an exponential growth
yeah like there's currently some over
1200 applications from what I can count
that that are now basically AI
accelerated
deep learning powered and just a year
ago was only 400 I mean how is
tensorflow must have been a vital part
of all the developers inside Google
picking it up right so the tooling
definitely makes it easier you know
we've invested a lot at Google in
tooling to make sure that the developers
and the researchers can really take what
they want to do and scale it up you know
you know it's really expanding to many
many different applications and projects
now for the developers that are
currently using tensorflow what do they
love most about it
so for a lot of the developers they
really want to express all kinds of new
things you know if you have researchers
and want to try out new ideas like the
RNN stuff that Brian just talked about
or convolutional nets or over the last
year or two there's this whole thing
about batch normalization so there are a
lot of new techniques around these
algorithms that developers want to talk
about and retention flow weaves try to
make that really really easy and allow
them to still scale it up it doesn't
matter what data
said it is right it could be motor skill
data set am I gonna use it to train a
robot exactly and so whether it's robot
or voice or images or whatever we come
up with it could be it could be you know
seismic data for it for all we care
I mean just whatever data that data
science would like to use to train a
network and apply that network in
production that's what ultimately
tensorflow is for that's right yeah the
thing that's really really cool is you
guys open sourced it and because you
open sourced it it made it possible for
our engineers I mean if it wasn't
because of that open source it would
have been possible for us to start
working on adapting tensorflow
for dgx one early enough yeah I mean
that's really such an incredible thing
and as a result computer designers all
over the world can do the same and so we
we took we took your open source tensor
flow and we have now adapted it to DG x1
and it's running on DG x1 guys what am I
looking at is this one real here we go
all right so ok can you just come maybe
give us a quick tour of tensorflow sure
so on the left you see it's just the
command line running the pencil flow
stuff and it just shows how quickly the
engineers here at Nvidia have been able
to take tensor flow and like Jensen was
saying really adapted to GTX one in
runners on the right just shows the
training curve you know it's showing
over time how the loss is going down and
it's actually training really on the DG
x1 right now with the same tensor flow
that you might use on your machines and
so this on the bottom just for the
audience what is this is the X and
y-axis right so most of deep learning is
about optimization so if you know
mathematics and optimization you're
trying to minimize or maximize some
value in this case there's a loss that
we're trying to minimize and what you
see over time here in this run this is
running real time how that loss is
actually going down in optimizing
so this happens as you train the loss
going down basically the add the number
of errors that you make over time goes
down and so this loss goes down as well
and so if we have a really fast computer
this loss will go down faster
exactly and if you have a really slow
computer you can wait days for that loss
exact down I guess ok and so this is
really so so Roger so what is your what
is your hopes and dreams for tensorflow
where do you see it going from here so
you know like you said deep learning
a really big thing we really believe in
it we really think that that is the
future right it's going to be part of
every single thing the tensorflow we've
built it really to work with all kinds
of devices you know just just like
you've been able to do a dgx one but
also right in not just data centers but
all the way to your phones and embedded
devices and all kinds of things if you
would love to see the community use it
try it out and use it in all kinds of
different ways and really push it to new
territory stuff that even we haven't
gone to mm-hmm yeah the thing that's
really really quite quite quite
visionary about tensorflow and the
strategy around tensorflow is that in
our case I really believe that CUDA in
our GPU has democratized
high-performance computing but what
tensorflow is going to do is you're
going to democratize deep learning and I
think that's a huge huge contribution to
humanity into computer science I want to
congratulate you thank you all right
thanks a lot
Title: NVIDIA GTC May 2020 Keynote Pt5: NVIDIA Jarvis for Conversational AI
Publish_date: 2020-05-14
Length: 554
Views: 112105
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/3ey76WVYkCI/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 3ey76WVYkCI

--- Transcript ---

in France is the last stage of the
machine learning pipeline this is where
you take the model that you train and
deploy it into services to make
predictions what comes out of the
machine learning pipeline and frameworks
are computational graphs that are
incredibly complex these are gigantic
computational graphs and there are so
many different types of neural network
architectures the computer science of
compiling these computational graphs
into a target machine to run as fast as
possible with all the different types of
numerics Precision's incredibly complex
problem we created an optimizing
compiler tensor RT we're now in our
seventh generation this generation can
handle cnn's and transformers and now we
can handle our Nance as well
tensor our t7 includes over a thousand
optimized kernels and in fact our
developers could do the same for
themselves and create all kinds of
custom kernels and custom neural
networks we support Precision's from F P
32 F P 16 all the way to a bit and four
bit integer tensor RT has been a huge
success the number of developers I use
tensor RT increased 10x the world's top
300 Internet services now have NVIDIA
GPUs in their data centers to do
inference it just kind of shows you how
many industries deep learning and AI is
going to impact one of the most
important applications that we can now
enable is conversational AI it is one of
the most challenging inference tasks
because conversational AI requires
interactive performance the elements
that make up a conversational AI hype
line has achieved tremendous
breakthroughs recently starting from ASR
automatic speech recognition to natural
language understanding to text the
speech speech synthesis and now it's
possible for us to imagine for the very
first time having an interactive low
latency conversational AI pipeline
however the application is very complex
still the models are state-of-the-art
training those models take a tremendous
amount of computation and the ability to
put together all of the models end to
end on top of a computing platform fully
accelerated is something the world's
never done before until now
ladies and gentlemen we're announcing
today
a new application framework we called
Jarvis and because it has the ability
connected to omniverse we have the
ability to create an interactive 3d chat
bot one of the AI models that makes this
entire experience possible is what we
call audio to face it takes an audio
input and it has learned how to animate
a geometric mesh to animate facial
expressions while talking that then
drives our computer graphic system in
omniverse and creates an interactive
chat bot let me now show you our AI
model audio to face the speech air is
coming from a rapper one of our
employees
John della Bona and it is really
fantastic take a look
living for the now longest time allowed
time I keep on switching different
styles you created on a cloud that's
what is on my brow cuz I'm running on
these tracks just to keep them running
back you know the drill to quarterback
and I'm practicing my crab dedicate this
play to Kobe what could be a bigger
legacy the making it a story every day
gets better note down so we got a hold
out I got a green a peg shoulders so
down it's all about the progression the
daily grind is to touch with some dance
lesson we burn it more like a day with a
blessing if you haven't real problems up
your bad for you son it's like 99
many share the same one yeah so keep
your head up and get up I bet tomorrow
is Venice driving to pee oh I see father
that's it a letter once we have audio
the face so then we can connect it to
our conversational AI model where you
can ask it a question
it would understand the question answer
the question and then synthesized speech
from that which goes into our audio to
face model which animates and drives a
3d model we've created a demonstration
we call misty our first interactive 3d
chat bot hello hi I'm misty how are you
I'm doing well thank you for asking what
are you here for I'm here to give you
the latest weather information around
the world tell you interesting facts
about meteorology and keep you company
should I bring an umbrella tomorrow it
is not expected to rain in Santa Clara
California tomorrow afternoon what is
the hottest city in the world the
hottest city in the world is Mecca in
Saudi Arabia where temperatures can
reach 122 degrees Fahrenheit
what is the coldest City the coldest
city in the world is Yakutsk in Russia
with temperatures around minus 40
degrees Fahrenheit in the winter months
tell me something about lightning we see
lightning first because light travels
faster than sound
how come you know so much about weather
I know the weather information from the
internet I have lived for a long time
and have been talking about the weather
for as long as I know where are you from
misty I'm from a place called omniverse
who made you I am created by the
wonderful scientists of Nvidia what
powers you I'm powered by NVIDIA GPU and
the CUDA neurons inside of them Thank
You misty thank you it was a pleasure
talking to you
as you can see the ability to have an
interactive conversation requires the
composition AI to process the speech the
natural language understanding
synthesize and also render the graphics
as fast as possible and you have to
process the entire pipeline and n from
speech recognition language language
understanding text the speech as well as
driving and generating the computer
graphics in just a few hundred
milliseconds in order to feel like
you're having an interactive
conversation and that's what Nvidia
Jarvis is about a multimodal
conversational AI service framework that
simplifies the creation and the
development of conversational AI
services it includes state-of-the-art
models that has been pre pipelined into
these Held's charts optimized to run on
Nvidia's triton imprint server which
runs on top of our GPUs and the
performance is interactive and the
entire pipeline and n is only a few
hundred milliseconds but there's more
Jarvis comes with pre trained
state-of-the-art models these things the
art models have been trained with a
great deal of data and a lot of
computation effect what's available in
NGC the NVIDIA GPU cloud represents
several hundred thousand dgx training
hours if you had one dgx
it would take you something like 10 to
20 years to be able to process all of
the data necessary to train these models
we've done that for you and then it
comes with a new tool called Nemo which
takes a pre train model and augments it
with your data your data could because
of a particular domain it could be in
healthcare or insurance or financial
services or maybe it's a call center for
a particular business and there's
special language or special vocabulary
the Jarvis needs to learn and so you
would take that data and you would train
train the Jarvis models with this tool
we call Nemo this end end pipeline from
pre train models retraining with Nemo
and in the application of all of these
state-of-the-art models all put together
in an Helms chart running on a service
on top of train this entire end-to-end
system we call Jarvis conversational AI
is going to automate conversations and
it's going to make it possible for us to
deploy automated services in all kinds
of new applications whether it's video
conferencing call centers smart speakers
one of my favorite applications is video
conferencing when you have a group
conference one person can talk at a time
it would be nice sometimes if we can
kind of hear multiple people talking so
would it be amazing it whenever anybody
talks it's picked up in closed caption
or translate it in real-time or the end
of a conference a simple summary and
transcription is done video conferencing
with a conversational AI agent is going
to be really transformed we know that
the number of people who are calling in
to call centres these days is growing
and many of the call centers have people
wait half an hour to much longer
Jarvis conversational yeah it's gonna be
one of the most important real-time
inference applications
so that's NVIDIA AI it is now end-to-end
fully accelerated from data processing
with the amount of data growing
exponentially SPARC is more important
than ever and now it's accelerated by
Nvidia second the deep learning and the
model training pipeline we have
accelerator for a long time one of the
particular applications recommender
systems is so complicated we decided to
create an application framework just for
it so that we can democratize
recommender systems we call Nvidia
Merlin and then lastly inference tensor
RT 7.0 is here the number of people who
are downloading it and using it it's
really growing and we're super excited
about that we decided to create an
application framework for every company
to be able to create state of the yard
conversational a on models we call that
Nvidia Jarvis this end-to-end
acceleration platform and some of the
reference applications is what we call
Nvidia AI
Title: NVIDIA SHIELD tablet now with Android 5.0 Lollipop and more
Publish_date: 2014-11-13
Length: 142
Views: 110268
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/4actew0zEv4/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 4actew0zEv4

--- Transcript ---

hi I'm Andrew with NVIDIA by now you've
probably heard of the Nvidia shield
tablet the ultimate tablet for gamers we
designed this incredible device to
deliver PC class graphics powered by the
Tegra k1 processor as well as booming
front-facing audio and precision
controls with the shield controller and
have a blast with your favorite android
games google apps and even stream the
hottest PC titles using our game stream
technology we have an exciting new
software upgrade that will make your
Nvidia shield tablet experience even
better
the shield tablet is one of the first
devices on the market to support
Google's new Android 5.0 lollipop
operating system with this simple
software upgrade you'll get the new
operating system along with new features
from Nvidia the new Android OS comes
with Android runtime also known as a RT
which gives you better app performance
and efficiency and it has better gaming
and graphics capability thanks to
Android extension pack also known as a
EP that brings desktop quality graphics
to your shield tablet with the latest
update we've also improved the Nvidia
dabbler art studio app with tons of new
features and tools that makes it even
easier to create works of art using the
Nvidia direct stylus to the user
interface is completely redesigned to go
with Android lollipop some aterial
design and we've added support for
layers a customizable color palette
adjustable pressure sensitivity and more
undo levels you also get a moveable
stencil to help you draw perfect shapes
and lines you can even share your
painting session live on Twitch or save
it for sharing later all directly from
Nvidia dabbler we've updated the shield
hub interface to align with the new
Android lollipop experience this means
you'll get access to the best games from
shield tablet and your favorite media
apps all from one place we're also
introducing our new and Vidya gride
gaming service to all shield owners for
free during this nationwide exclusive
preview now you can enjoy GeForce GTX
gaming from the cloud on your shield
grid instantly streams a library of
popular PC games like Batman Arkham City
and many more finally we're proud to
announce the sequel to the most popular
game of all time half-life 2 episode 1
powered by Tegra k1 half-life 2 episode
1 brings PC class graphics to end
for the first time and is exclusive to
shield town
so check out the latest Android OS and
new features on your shield tablet head
over to shield Nvidia comm to learn more
Title: How Active Learning Improves Nighttime Pedestrian Detection - NVIDIA DRIVE Labs Ep. 19
Publish_date: 2020-04-08
Length: 110
Views: 24490
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/4aq13pB9s7c/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 4aq13pB9s7c

--- Transcript ---

Today in DRIVE Labs, we are talking about
using an AI technique known as active learning
to improve self-driving car perception performance
in a very challenging and important scenario
– detecting pedestrians at night.
Our active learning method uses a set of independently
trained AI models that automatically flag
unlabeled data frames on which these AI models
experience uncertainty.
In this clip, the colors in the heatmaps indicate
uncertainty in pedestrian detection, with
the warmer colors corresponding to higher
uncertainty.
The uncertainty is measured by information-theoretic
mutual information between the AI models,
such that uncertainty is highest when the
AI models disagree with each other.
The data frames selected by active learning
are then labeled by humans and added to the
DNN training dataset.
In this clip, we have a DriveNet DNN model
trained on manually-selected data on the left,
and a DriveNet model trained with active learning-based
data selection and on the right.
With manual curation, we see that the detection
of the pedestrian holding an umbrella disappears
as soon as a small raindrop falls on the camera
lens.
However, DriveNet trained with active learning
still detects this pedestrian, as well as
the pedestrian on the right most corner of
the image.
And here, with manual curation we have a cartoon
incorrectly detected as a pedestrian, while
active learning helps avoid this false positive
detection and helps the DNN better detect
the group of pedestrians.
And in this clip, we see that the DNN model
trained with active learning consistently
and correctly detects both the motorcycle
and the person riding it, despite the low
visibility conditions.
By automatically selecting the right training
data, active learning enables developers to
build and train better DNNs more quickly and
efficiently.
Title: Top 7 Tips for Survival in Rise of the Tomb Raider
Publish_date: 2016-02-09
Length: 249
Views: 72600
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/4D73HwE35LY/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 4D73HwE35LY

--- Transcript ---

if you've just picked up Lara Croft's
latest romp in Russia then here are
seven things you need to know before you
get your hands mucky with all that
lovely dirt don't worry we'll drop it
off at the British Museum are we done
eBay I don't think they're onto us yet
but never heard of it mate try to be
discreet okay
the stunning environments of rise of the
Tomb Raider are positively frothing with
filthy lucre everywhere you turn
treasure coins caches ammo and natural
resources are to be found but they are
seamlessly blended into the world
thankfully Lara has developed a sixth
sense for such things hit Q every time
you enter a new space and you will
activate a survival instinct
highlighting items and points of
interest in her vicinity P use make the
most of this doesn't just extend to
Hoover of goodies make sure you also
activate survival instinct every time
you spot an enemy in these situations it
can also give a Lara the upper hand in
staying stealthy as it will highlight
enemies in full view of one of the play
in red and isolated enemies in yellow
allowing you to take down a whole group
without anyone ever catching on and
sending a storm of hot lead in your
fleshy direction which brings us neatly
onto combat in general hospital' spotted
you can use a mix of firearms throughout
the day but Lara never feels more
resourceful and powerful when stalking
Nachtwey with a bow and arrow starting
with a makeshift long though you can
slowly upgrade aspects of the bow to
make it even more deadly but make sure
you keep your eyes peeled for strong
boxes which contain the necessary
components to unlock the devastating
compound Lara can also discover an
upgrade multiple weapons within the
handgun rifle and shotgun classes giving
you a wealth of combat strategy options
and ensuring that our intrepid Tomb
Raider remains a devastating war
in any situation it's also important to
keep your quiver well-stocked not just a
quiver in fact but also every pocket and
couch you possess with all nature's
bounty cameras on their feathers and
sapling would have vital for keeping
your supplier deaths to explode while
various animal hides mushrooms antlers
plants or and plenty more besides fulfil
a wealth of other crafting and survival
requirements as soon as you're able to
make sure you craft a pouch to enlarge
your carrying capacity never pass up the
opportunity to be topped up with
whatever mother nature sends in your
direction best of all you don't even
have to visit the campfire to make a lot
of lares with deadly items as ammo and
many explosive and incendiary devices
can be crafted on-the-fly
fashion should hardly be top of the list
of priorities when icing dudes and
hunting relics in Siberian wastes but
some of the items in Lara's
ever-expanding wardrobe do provide a
variety can use proof bus whilst also
looking completely badness some of these
run locked as the story progresses and
can be bought or crafted whereas others
unlock after completing side quests I
left the supplies by the other Tower
such as the battle worn outfit which
gives a boost to health regeneration XP
is earned through a wide variety of
activities and rise of the generator and
it's up to you what to spend it on but
don't forget to make sure you do just
that every time you just a campfire also
be very careful exactly which skills you
spend your valuable points on some of
them are a real waste of effort while
others can have a massive impact on
playstyle
for a more detailed look at our
recommended skill upgrade path check out
our dedicated video guide finally make
sure you never pass up the opportunity
to do what Lara does best raiding tools
scattered throughout the wintery wastes
are optional challenge terms some very
well-hidden which you can go off and
explore outside the main storyline
thing up there although it can be
tempting to ignore a prompt that one is
close by when you're focused on the
narrative arc it is well worth taking
the time to seek them out as they are
chock-full of relics and coins the
puzzles they offer up can be tricky but
solving them will also unlock a skill
adding yet more strings to Lara's
increasingly deadly bow
[Music]
rise of the Tomb Raider is out now on PC
you can grab a free copy by purchasing
select GeForce GPUs or notebooks
Title: SHIELD Tablet: DirectStylus 2
Publish_date: 2014-07-22
Length: 166
Views: 155258
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/4h8k5LLK0jI/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 4h8k5LLK0jI

--- Transcript ---

hey guys we're here to talk about the
direct stylus that comes with the new
and video shield tablet
what makes this stylus experience
different is the tablet's powerful Tegra
k1 processor with its 192 core Kepler
GPU the amazing technology in the stylus
itself and the software that ties it all
together it's the perfect way to make
your mark on everything you do one of
the great benefits of having a stylus on
a tablet is the ability to draw and
paint in a natural way
this is dabbler a cool new app that's
exclusive to the shield tablet with
dabbler you can sketch and draw but
thanks to the power of the Tegra k1
processor and GPU acceleration dabbler
can also do real-time watercolour
painting as you can see this gives you
incredible realism and a smooth natural
flow that you just can't find on any
other tablet the watercolors even react
to gravity and they blend naturally with
other colors the same GPU acceleration
allows for natural texture and shine in
oil paintings here's one that was
already created dabbler makes it easy to
change lighting and make edits it's just
like real oil painting without the mess
another great feature is the direct
stylus launcher the direct stylus
launcher is completely customizable
giving you quick access to the most
important stylus driven apps from the
launcher I'll just choose the right app
this lets you use the direct stylus to
take notes on the flight in your own
handwriting add pictures and even move
text if you're familiar with Evernote
we've conveniently bundled it for you on
your shield tablet Evernote is the
leading note taking a note organizing
app that syncs between all your devices
with Evernote on your shield tablet you
can tap out notes using the on-screen
keyboard use the handwriting recognition
bar or jot down notes freehand whichever
method you choose every note you take
will be searchable through Evernote you
should also know that your shield tablet
can even recognize cursive writing and
it supports 54 different languages
another feature that the stylus is great
for is lasso capture this tool is
available in any application let's just
say I want to capture this section of
the screen here and send it to a friend
I'll choose lasso capture circle what I
want and save it or share it I'm going
to share using Gmail by the way this is
the swipe keyboard which works great
with the direct stylus now let me show
you another way you can use the stylus
let's say you have a PDF you can just
use Adobe Reader here and now I can
easily make notes or if I'm ready
I can just sign on the dotted line and
send it off so these are just some of
the things you'll be able to do with the
amazing technology built into the Nvidia
direct stylus on the new shield tablet I
know you'll discover a lot more
Title: Volta Is Everywhere at GTC 2018
Publish_date: 2018-03-29
Length: 85
Views: 8868
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/4k9JltRtW58/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 4k9JltRtW58

--- Transcript ---

we're super-excited about a recent
announcements about Voltas 32gb memory
and that completely aligns with what
we've been trying to do our customers
have expressed to us the increasing
challenge they have with training their
models within the confines of 16gb 100
systems they're seeing that they're
needing a larger GPU memory footprint
when AWS we have a broad portfolio of
services that worked really well with a
GPU compute offering to basically allow
builders to kind of build the
application they want to build when they
want to build it we're partnering that
build the largest supercomputers in the
world we've got a unique interface that
we Co designed together between our CPU
and the voltage GPU so now you can
enable data scientists to really turn to
more complex larger models and not be
limited to any device so b3 brings a lot
of value and performance over our
previous generation instance which is
Picchu instances with the V 100 GPUs we
are seeing for the 5x performance
improvements that our customers are
recording over a previous generation
instances with the kind of power of
computing that we're introducing we're
gonna be able to solve the most
important most complex challenges in the
world whether that's in healthcare
insecurity in autonomous vehicle we're
now bringing together the kind of
computing power that our customers need
to transform the world
Title: Softbank Group, NVIDIA CEOs on What's Next for AI (Courtesy of SoftBank World 2020)
Publish_date: 2020-10-29
Length: 3873
Views: 46097
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/53LLbIHQmnA/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 53LLbIHQmnA

--- Transcript ---

[Music]
i
am an explorer
searching for the origins of our
universe
[Music]
and charting a safer path to other
worlds
i am a helper
moving us forward one step at a time
and giving voice to every emotion
i love you
i am a healer
modeling the future of medicine
[Music]
and finding a needle in a haystack
when every second counts
i am a visionary
uncovering masterpieces lost to the ages
and finding new adventures in a galaxy
far
far away
i am a builder
[Music]
driving perfection in everything we
create
am even the narrator of the story you
are watching from the story you are
watching and the composer of the music
[Music]
and when the world faces its greatest
challenge
[Music]
[Applause]
i give us the power to take it on
together
[Music]
[Applause]
i am
well jensen it's
it's a very exciting time and uh
uh we saw your the opening video of
conference you just had this month and
uh
it's amazing beautiful video so would
you like to talk a little bit about your
video
your vision what you think
yeah masa thanks for thanks for having
me it's always it's always great to sit
and talk to you about
big ideas in fact in fact in our
in our times together uh we've spoken
about some
very big ideas one of them of course is
artificial intelligence
that is the focus of uh gtc
gtc is our annual developers conference
gtc's our annual developers companies we
you know the company's
the company's founding mission and its
mission today
is to solve problems that ordinary
computers cannot solve
and so we have to choose and we focus on
problems like
computational medicine or science
or robotics or autonomous driving
and of course very very broadly
artificial intelligence
and we we focus on
developing tools and sdks and softwares
and
of course new chips and new systems that
that can help developers be more
productive
and once once a year twice a year
actually
we we do our gtc we used to take it all
over the world
i would fly all over the world starting
in santa clara and then i would go to
japan
and i would go to china i would go to
europe
but now now i uh i sit in my in my
kitchen
and uh and i can go i can go all over
the world
and present to to the developers this
year we had
a record number of talks we
spoke about some very important areas
and and we have we have uh i'd love to
explore some of it with you
one of the one of the major themes
is really helping people realize
uh that artificial intelligence is the
most powerful technology force of our
time
and and when you sit back and think
about the reason why
it's because for the very first time a
computer can write software
by itself software can write software
yeah
yeah and because because because we know
that
that there are many problems we want to
solve that we don't know how to solve
and for the very first time
we can build very powerful computers
that can write software that humans
cannot
right and that that observation and in
fact you observed it very early as well
several years ago you came to talk to me
about it yeah yeah yeah yeah
several years ago you came to texas yes
yes
you observed nvidia's nvidia's work in
artificial intelligence
and we observed that uh this new type of
software
right is uh uh is
written in a very different way it's
created in a very different way
not just by software engineers but
together with software engineers
it's created on very powerful computers
the systems the computers the chips the
software inside
uh are completely new and it's a new way
of doing computer science and
the way of doing computer science and
new way of
of uh creating software uh was
was uh such a great idea that we we
decided to dedicate our whole company to
that's that's very amazing you know
writing software is
is a very tedious effort and you do the
debugging and you have to think
unlike human but now with
uh you know computer writing the
software
we can save a lot of time and uh we can
focus
more on creating something that
and then creative thinking so computer
can help
uh realize those uh thinking into actual
coding
it's amazing it's it helps our
creativity
so much more that's right and
and the the amazing breakthrough was
this you know human creativity of course
is still unparalleled our ability to
come up with unique ideas is still
unparalleled and machines are nowhere
near that right
however a computer can solve a problem
that is so
gigantic in scale that no humans can't
that's right
the computer's advantage is scale
yes size it can look at
look at petabytes and petabytes of data
and observe patterns from inside it a
massive parallel effort right
that's right in a way that no humans
can't right because
humans can only fit so much information
in our head and so the advantage of
ai is the ability to learn from a
gigantic
amount of data the the algorithm itself
is kind of like a universal uh function
approximator yes
it can learn almost any function yeah
and they can learn the function from a
great deal of data
and then that function can predict the
future in you know after it's learned
right
so it can predict it can infer uh from
future
uh future circumstances and so our our
observation
the the great i think the great
contribution we've made
to the industry is realizing that ai
is a brand new type of computer science
the software is different
the system is different the chips are
different the methodology is different
and you need a computer to create the ai
right
and you need a computer to run the am
right
these two types of computers are
fundamentally different than the type of
computers
today right that observation by our
company
almost a decade ago was a groundbreaking
observation yeah
and we but we've now invested probably
25 billion dollars wow 25 billion
in investing in this one area of ai now
some of the things that
that that we've been able to solve
in working with the industry and the
developer conference
we have we have uh some over 2 billion 2
million developers around the world now
this last year grew 600 000
so the rate of our developer is growing
incredibly
we have uh our architecture is called
cuda has been downloaded 20 million
times
20 million times 20 million times in one
year
twin note 20 million times this is great
it's been downloaded 20 million times in
history
but last year has been downloaded six
million times oh like
six million dollars in lasted one year
yeah and
20 million times in history yeah we have
sold
we have now shipped one billion gpus
based upon cuda
and now the rate is going very quickly
uh we have uh
we have uh so that many developers that
many developers are really
you know writing calls and so on right
yeah the benefit of a computing platform
and the exciting thing about computing
platform is of course it's very
difficult to build
if you look at the history of computers
really there's only two main computers
that's being used today
x86 and of course the most popular cpu
in the world is armed
the third computing architecture that
has
over the history over this time the only
third version the only third computer
is nvidia's kuda yeah yeah yeah so this
is this is
very important point okay so as you said
the architecture would change people
people need to understand
about this big change in the concept
because i say
the early stage the first stage of
computing
first it did the calculation
uh instead of people do calculation
slowly
the computer can do calculation very
quickly
and then the second it it can memorize
so much more information then one
human can do so it can have a massive
massive memory
when you have a massive memory that you
want to search
right because there are so many data
that you want to search
out of those massive memory so
the computer was utilized for those
purpose and then the company who are
good at
you know realizing those purpose
made a big success but now
finally computer got the ears
and the eyes right to recognize
your your voice and the speech recognize
uh what you can see with the image
recognition right so that would totally
change
totally change how the computer should
be utilized
and you you are in the center of those
uh
you know deep learning uh for learning
what people can see with the eyes and
the ears
and which we did not have in the
computing world
until just a recently uh
calculation uh computer did it at scale
right memorizing
remembering storing data computer did it
at scale
intelligence recognizing patterns
reasoning planning now the computer is
going to do that at scale exactly and
the speed
intelligence at scale yeah
that's the reason why this age of ai is
such an important time and during our
conversation
you know you and i have spoken about so
many breakthroughs that have come
computer vision is now of course better
than humans
uh speech recognition is now better than
human
but even even summarization is better
right
right and so so um language
understanding is becoming better than
humans
translation is becoming better than
humans and so
the the ability for uh this type
of software to be able to learn from
enormous amounts of data at
extremely large scale and speed is
something that
humans cannot keep up with however we
can take advantage of it to solve some
very big problems so so some
some um people said well computer can
uh see and
listen but the computer cannot
understand
the context some people say that i said
why
you know if human can
understand with a logic if as long as
there is a logic
of course computers should be able to
understand the context because it it's a
bunch of uh come with a bunch of logic
right
yeah one of the things one of the things
that people will say of course is that
the computer cannot
think the concept of thinking is
very difficult to describe what is
thinking is very difficult to describe
just like just like an airplane doesn't
fly like a bird
right the computer can
perform skills that is apparently like
thinking right
it doesn't have to think the way we
think the computer
performs a skill that seemingly is
intelligent that's right it
seems like until right the advantage of
course is that
artificial intelligence can do it 24
hours a day
you could do it at incredible high
speeds you could replicate it
and do it at very large scale you could
recognize patterns that no humans can
because there's too much
information for example uh one of the
one of the major applications for
uh for ai right now
is uh is cyber crime fraud detection
so much of our commerce is moving into
the cloud into digital
cyber crime is very very large the
world's economy is 140 trillion dollars
about one percent of it about one
trillion dollars is lost as a result of
of uh cyber crime we can now use
artificial intelligence to recognize
patterns of intrusion from
many many different sources and shut
down the intrusion instantaneously so
american express recently announced
that they're using nvidia to do fraud
detection
and they can detect fraud in two
milliseconds right
it's two milliseconds and so they can
shut that down at the savings to the
global economy of course is very high
uh you could you could um you know that
that there's a fair amount of loss
um in shopping and retail and the
checkout process is very arduous
and uh inaccurate and um
the retail industry uh loses about 1.5
per year yeah if you think about the
size of the retail industry 1.5
per year and their gross profit their
their gross profit is only 2 percent
right right
yeah so 1.5
inventory right so that you have to
throw away the waste
the the product that you over supply
and people have been thinking
what is the best volume should i should
i
procure how much volume can we
sell this christmas
people have been thinking and that is
thinking people thought oh i am
expert i am most good at than average
other people on the street but as you
say you know
computer can think what is the best mix
what is the best volume that we should
prepare as the storage for the inventory
so that we don't waste right so this is
a part of thinking that computer can
become
expert and with the massive data
of the seasonality the temperature
weather and the unemployment rates
the pandemic all those input of data
you know we the computer can actually
think
what is the best mix of product and the
volume
what is the best price that should be
priced
to win against the competition what is
the best
recommendation should we give to the
each
individual customer based on their hobby
and their buying pattern and the habit
and their budget and so on right
that's exactly right and so if you look
at walmart we work with walmart on their
inventory management system
and their retail checkout system and
their their warehouse logistics system
they have so many stores they have
hundreds of thousands of skus
and every retail store is different
because every retail store is in a
different region
and each region has this regional taste
sometimes there's a
there's a maybe sometimes there's a
there's a hurricane
and all of a sudden they need special
type of things uh maybe sometimes uh
there's a picnic just a picnic or a big
game
all of a sudden uh beer runs out or
certain certain types of uh
foods right now you see that each one of
the regions and each one of the stores
are behaving differently
it's impossible for somebody to sit at
headquarters
to collect the information reason about
what to do
plan the inventory ship it to the stores
in the future
walmart is going to be one giant ai
right
yes yes one giant ai
and that's the exciting part the
exciting time right now
i believe that every company will become
an ai yeah yeah banking is a great
example also right
you know it banking become a giant ai
center which uh just happened to manage
people's wealth right that's right and
of course
we will always have human in the loop we
will have human
it will make recommendations about it
will make recommendations about the
action it's about the take
right and we will apply our judgment yep
we can think about it and we can of
course apply core values or maybe
maybe even though we are going to lose
money uh we believe that it's good for
society right now to help
people through the pandemic and we don't
change the price even though the ai
thinks that we should change the process
yeah because we we
we have a heart we have a heart to be
nice to the other human
to to be helpful to the other human so
we use
utilize ai as a tool for our judgment
for our decision making and
for our happiness and or you know joy
whatever human we we choose
uh which recommendation to take and
sometimes go
against the recommendation because of
the other reasons
that we want to have as a taste
we're seeing now now the you know
of course initially ai started in the
clouds because they collected so much
data
and the the amount of data in the in the
world today on the internet
you have six billion people using the
internet and
the amount of data is so gigantic it is
impossible to even search anymore
how do we know what to look for right
there's so much
there's so much items there's so many
things in the world that there's no way
to look for it
we can't look for news it's possible
news is coming all the time
music is coming all the time videos
coming all the time everything is coming
all the time
and so the internet of course had to
move from search
to recommendation the recommendation
engine
of all of the internet companies is what
drove ai
in the first generation yeah
so the fir the first the first
generation of af when you and i first
started to talk about ai
ai was being developed in the
laboratories we were developing the
computing system
and we were creating the software stack
and the methodologies for creating ai
the phase the the first industrial phase
the industrialization of the technology
was in the cloud
what's happening now and this is the
exciting part
the next phase of ai is now moving into
all the industries
we're working with astrozenica and gsk
this is the announcement that i made
in cambridge the reason why i call the
cambridge one
is because because as you know cambridge
is the
home of computer science the birthplace
of computer science alan turing
and the birthplace of genomics watson
and critics
the discovery of the dna can you imagine
when i built a machine for computational
genomics
to place it in the home of my future
my future home cambridge pretty amazing
it's such a wonderful story so
the timing was perfect i called it
cambridge one uh we announced our
partnership with gsk and astrazeneca to
do
use ai for drug discovery so now ai has
gone into industry for drug discovery
ai went into retail walmart for example
ai went into banking
american express for example we just
announced
ai for microsoft office we worked with
society and his team to connect azure
with nvidia ai to microsoft office
and you could imagine all of the ways
that ai can help us
summarize our emails in the morning
maybe prioritize our work
identify identify very urgent matters
recommend the next action when we type
email of course the grammar will be
always very good
of course there are many things that we
could do with ai
and so now it's in office uh we have uh
ai and agriculture with john deere
in um uh in uh in
robotics in japan with fanuc
you have uh ai in uh
uh in uh of course transportation yeah
the the list of industries now that ai
is going into is really quite phenomenal
and so that i would i would say that the
second generation of ai
is enterprise yeah turning every company
into an ai
third generation and this is this is
where you and i
i uh fantasized and talked about
uh the future of ai the largest
opportunity of course
is when ai moves out to the world
and ai is now autonomous machines
everything will be a robot uh buildings
will be a robot
cars will be a robot there will be two
wheel robots two leg robots two armed
robots
uh the future your future uh um
artificial leg to help people walk is a
robot
yeah your future right your future
wheelchair will be a robot
well i i i think the biggest robot is
the
autonomous car so it's it's a gigantic
gigantic industry and
so people still doubt some there are
some
so many people still doubt can
autonomous
driving would be
safer or as as safe as human drivers
okay you and i are a true believer of
the autonomous
driving but let the audience know
that where are we in the autonomous
driving stage
and five years and then 10 years where
what's the level of autonomous driving
uh you know
capability and safetyness
can you comment on that there are
i would break autonomous driving into
three large categories
the first category is autonomous
vehicles
that is not moving people in
closed environments or even in open
environments but moving slowly
for example delivery grocery there's
nobody inside
just the lettuce and milk and eggs and
it's not very
it's a very slow moving car maybe only
moving at 5 10 15 miles per hour
and uh no harm will come
no harm will come that will be
completely autonomous yeah
even even inside the logistics center
the warehouse
right there will be
hundreds of millions of that just
without even thinking about
humans there will be hundreds of
millions of that per year yeah
second inside inside
heavily mapped areas regions that are
very well understood there will be taxi
services
that are essentially driving on
a invisible rail it's almost like a rail
tram except it's based on ai right
you know of course it's much more
advanced than a tram
but it's driving on digital rails
digital maps
very still very complicated but you can
imagine the technology is possible
in fact uh google just announced right
waymo just announced
that arizona their cars are now
completely driverless
so it has begun the thing about
engineering as you know
is is very well is the moment that
someone achieves it
the fact that it's possible everyone
will everyone else will achieve it
the art of the possible is the first
thing for innovation now that somebody
does it
it will become better lower cost
higher quality longer range incredibly
fast
yeah that's the nature of competition
that's the nature of engineering so
people people
still ask questions about what happens
if the
dog runs on on the street the birth
comes
and the heavy rain and the snow all
those long tail
very rare case can autonomous driving be
safe enough compared to the human
drivers
it will be safer than a human driver i
absolutely agree with you
but there are many other people who
don't believe that yet
yeah we have we have to show it and
prove it but but
there's no question it will be safer and
the third category is passenger cars
right right third categories passenger
cars the thing about passenger cars
is that it doesn't have to be completely
autonomous it just has to make the
driving experience
much more delightful in the future when
you have a self-driving car and it's a
passenger you're driving the car
it's almost like you're driving with
your brain power
your hand is on the steering wheel
you're paying attention
it's almost like you're thinking and the
car is connected to your brain
yeah that is going to be really a
wonderful experience
and so the autonomy there is driving
assistance
to the nth degree but it's driving
assistance
very very advanced driving assistance
and
of course you could tell the car to go
park itself
you know it's only two or three miles
per hour
it will recognize the parking spots it
will recognize the electrical park
itself
it will you can summon the car um when
you're in a
in a traffic jam of course it will drive
by itself and you can
even take a nap if you like and so so
there's a lot of different
uh degrees of autonomy i think that when
people
uh take take uh automobiles and they say
that you need to build a self-driving
car that can drive in any condition
in any country on any road just like a
human
that's unnecessary to achieve to bring
great joy to people right
well i i think
sometime in the in the future the
regular human drivers car
will be limited into the some hobby area
like
we used to have horse riding on the
street
right but now horse riding is limited
to some hobby area right
if you if you ride a horse on the
highway you you will be caught by the
police
and because it will it will jam and
and create you know
noise to the traffic system but uh
because the speed speed of the car is so
much faster
than the horses and so on
i think in the future autonomous driving
becomes the center of the transportation
and human can of course have a hobby
of driving uh but for the fast lane
you know uh things uh autonomous will be
safer
that's definitely yeah that's right
exactly
yeah so we're we're working on
autonomous machines now
of all kinds there there's there's ones
that
swim of course all boats and all ships
will be autonomous
flying autonomous uh you know
so we're working on so many autonomous
machines the ones that gives me the most
joy are the ones that are extensions of
your body
you know for to help people who who need
prosthetics
there's some 30 40 million people around
the world that needs prosthetics
and so now we can give them robotic legs
you know and so i think the robotic
technology there is going to help a
great deal of people
and you're going to have robotic
wheelchairs and just
robotic street seat sweepers and
all over the place so that that's all
age
ai right so the the ai have to be
smart enough in the edge side
that is with a high speed but
also you have the massive parallel
the ai in the crowd and
the massive parallel air in the crowd
and hai shake hand with
the fast communication uh with 5g coming
now
and 67 g hg coming
in in the future that
with a seamless interaction of edge side
ai
and the cloud side ai right
that's exactly right the cloud site ai
will be used
uh for creating the ai
for training yes the then the
the ai will be run at the edge on these
autonomous machines
edge ai right sometimes the edge ai is
moving
sometimes the edge ai is not moving it's
just a building or a street or a
warehouse
or a factory uh it's not moving
and um and they're all edgy eyes
and and these edge ais will operate the
ai
and they will learn because some some
skills it's not perfected
and sometimes they make a mistake we
will they will remember their mistake
send it back to the cloud the cloud will
learn from that mistake
send back the learning to all of the
autonomous machines
and everybody becomes smarter at once
yes
this learning loop this perpetual
learning machine
will learn so fast yeah once it's
deployed it will learn so fast
yeah so so the the the crowd side
is the aggregation of all the
intelligence from the
ai right so it's it's a group thinking
it's like a group thinking exactly
it becomes smarter and smarter and
find out all the exceptional cases
stored
and you know think together right so the
each
individual edge side can also
uh be a input to the
crowd you know thinking
but also execution side on the
on the edge side right that's exactly
right
that's exactly right and so so today
today the
the uh the world from a computer
architecture perspective computer
systems perspective the cloud and the
the ai is x86 and nvidia
and in the in the edge it's armed
that's the reason why combining arm and
nvidia makes so much sense because we
can then bring
nvidia's ai to the popular
c edge cpu in the world yeah yeah yeah
the
the beauty beauty of um is the
distribution power of the to the
many many partners right so some people
are
uh wondering uh you know that
arm has been neutral to provide the
ip to many chipset vendors
and some people argue that after
you acquire um from us and become one
company
with those partners who have been
getting
a license from ip for
arm ip do they can they
continue to get licensed so that they
can
also deploy for many different
applications
create many different chipset system on
chip
would you allow them to continue that
absolutely and in fact the ecosystem
the network this distribution power that
you said
is the most valuable part of iron yeah
the cpu was the original of course the
cpu is fantastic energy efficient
it's improving all the time incredible
computer scientists building the best
cpu in the world
and it's designed for these edge energy
efficient
and yet very high performance but the
the true value of arm
is the ecosystem of arm the 500
companies
that use on today yeah we did the
calculation the
arm would ship out a trillion chipset
trillion chipset in a short wire right
so it is the edge computing side
with your partner as a new owner
we we can provide to the edge
computing and that because upgrades to
the
hai for the for the trillion chipset
that would be amazing amazing
combination
our dream our dream is to bring nvidia
ai
to arm's ecosystem right the only way to
bring it to the ecosystem is through all
of the existing customers licensees and
partners
exactly we would we would like to
offer the licensees more
exactly even more and so of course we
will honor them
with everything that they currently buy
we like them to buy even more
and so so we so there's a business
reason of course
but the vision is to bring to combine
the
the most popular cpu in the world that
is every single edge device on the
planet
and it's it's diversifying into so many
different type of systems
whether it's cars or or like delivery
drones or
cell towers right
all kinds of systems all over the world
we would like to bring the idea
we we provide the tool set to those
enabler
right that create a new
socs for every different applications
that they
they would like to create so we just
provide a tool set
that they can utilize more tools toolbox
to
uh utilize all kinds of available
uh you know integration and technology
but they can create
for each specific game machines the home
appliance
the robotics that flies or run or swim
whatever right so they can they can have
all kinds of creativity for different
applications
but the age ai can also communicate
with your cloud ai so that
each of them becomes smarter again right
that's exactly what that's exactly right
and so once
once the world realized what our vision
was they realized
that oh okay that makes sense and in
fact we're going to offer them
nvidia's unique ai technology and all of
our expertise
if they have the expertise that's
fantastic but if they don't they can
benefit from ours
and so together we can bring the next
era of ai
and realize that now this next
generation as you know
is the next internet is so much larger
today's interview today's interview is
used by several billion people
we use the internet we we touch the
internet
probably several hundred times a day
so the internet is processing several
trillion
ai inferences per day
however some did now the limitation of
the current internet of course is it
takes a long time to
create a human even though population is
quite large it still takes a long time
to create a human
nine months to give birth and then they
have to become adults
it takes a long time the the biological
time frame
for intelligence to use the internet is
very long
however in the future the intelligence
that we can create
based on ai and systems and machines
you can create them as quickly as you
can manufacture with a trillion
chipset that they would autonomously
think and
provide the information and communicate
by themselves
right that's and they will be then
because these are machines they never
get tired
right and because they're working in the
world in the physical world where
something is happening all the time
they will be connected they will be
chirping and talking to the internet
continuously so instead of billions of
people
there will be trillions of autonomous
machines
instead of several hundred times a day
it will be continuously talking to the
internet yeah
the point is the next internet is many
orders of magnitude bigger than this one
totally agree totally agree it's a full
of opportunity
and full of excitement and full of
solutions that mankind could not
solve in the past like this this time in
the world
we are all suffering from pandemic but
you know it is amazing that ai
is now solving uh the
the this drug discovery you know the
anti-antibodies
the vaccines that humans could not uh
create
quick enough ai is helping
is helping creating solutions
right for the vaccine and
all those antibodies uh we could not
solve this quickly without the help of
ai
that's exactly right and and you know if
you look at
if you look at our previous threats in
humanity it's
in the old days oftentimes it's war
and so the defense systems that we
create are based on detection of those
things
you know enemies aircraft radar systems
enemies submarines missiles and things
like that now of course
those threats still exist but the future
threats
are likely biological right and we now
see it
right and these biological threats are
invisible and so the only way that we're
going to be able to solve it
and prevent it from its spread detect it
early
shut it down quickly find a therapy find
a vaccine quickly
the only way to do that at light speed
is with artificial intelligence
yeah so the nation's nations that the
nations are building up defense systems
for the future one of our family company
now
has developed ai system
that can detect heart attack
14 days before you have heart attack
right with a 95 accuracy
so people would no longer die from heart
attack
right that one of the biggest cause of
death in the united states and many
other countries now
became heart attack right and but if you
can detect
a heart attack 14 days before you have
actual heart attack you can prepare you
can go to the doctor see the doctor
you don't you don't do a you know too
much
you know heavy exercise
right and you start be careful
careful not to have you know a sudden
heart attack
is amazing you will be saved right
and for the cancer people would no
longer die
from cancer very soon right all kinds of
uh
drug discovery is being done with a
power of ai
and people will no longer die from car
accidents
right so there were many reasons cause
of death
of human uh which we could not solve in
the past
but with the help of ai
now many one by one those
causes of death is being you know
eliminated or reduced
dramatically so unless we have
the natural disaster
like a big earthquake or tsunami
right the hurricane
and this kind of pandemic that we did
not know
as a new virus but
i think with the power of ai those
unsolvable uh issues disasters that
humans were
facing could be helped dramatically
right so some people say hey oh
that's the enemy of mankind no i said no
no no
it is a big help to the
disaster or issues the uh
unproductivity that mankind used to have
the headache
we used to have so we should be
having a big smile and big excitement
uh welcoming this
revolutionary ai right
there's so many there's so many of
course with every powerful technology
and this is this is this is like you
know
you i don't know if the audience knows
this but i'm gonna say it
okay you're the only person that i know
you're the only person that i know who
was
at the beginning and recognized every
single
computer revolution since the pc
revolution
you brought the pc revolution to japan
uh you brought the internet revolution
to japan
you brought the mobile revolution to
japan you were the first to
recognize the china internet
revolution and you were early in
recognizing the ai revolution
but you you you uh you're the only
person that i know that has
that has that has recognized it
benefited from it every one of these
regular every one of these revolutions
and so you you have to you are too nice
to say that
uh i'm flattered but i am still a little
guy
still having you know
you're just modest but i just wanted to
say that the the
the um uh you know you know well that
in all of the times that we've been in
the computer industry but this is the
most powerful technology force that we
have ever seen
the ability for computers to write
software
there are several characteristics
several fact several impacts that i
think is
is that that i would love society to
recognize
one uh today the most valuable skill
the most valuable skill frankly is
computer
science and computer programming if you
were a computer scientist if you're
a computer programmer it is very likely
you have quite a nice
career it is very likely if you're a
softball programmer
you're doing very well and you're
benefiting from this industrial
revolution
however if you're not a software
programmer
it is more likely that you're being left
behind
programming a computer is not easy and
for a long time we thought that the
right answer for the industry
was to teach everyone how to program a
computer but that's not likely c
plus is hard it doesn't matter
it's just hard and the likelihood that
the world all can program computers
is not likely however for the very first
time
we have advanced computer science so
much
that the computer can program itself
that's the amazing part you
just have to teach it the thing that i'm
excited about
is that we have finally democratized
computing
until now we benefited from computing
the people in the computer science
industry the people in tech have
benefited from computer industry for the
first time
we're going to democratize software
programming it no longer
you don't have to program the computer
you just have to teach the computer yeah
and you just ask you you just ask
you just ask to the computer this is
what i want to do
can you give me solution you know this
is what we i think
the where we should go this is what i
wish to have
right then the computer give us the
solutions
and provide provide the the tools
to make it happen right that's right 0.5
of the earth population are in high tech
and
knows how to program a computer but 99
of the people know how to teach
something teach somebody a skill
a lot of people can teach someone a
skill now you can teach your computer a
skill
and that computer can replicate it and
automate it for you
we have now democratized technology i
think that
that that realization that realization
and the realization that that ai is
ultimately about software writing
software
and it's automating skills it's
automating skills
what are the skills that we can use to
automate
and and that observation uh
by the various industries one after
another after another
i think is going to revolutionize
revolutionized economy yeah
so the the computer the ai become
our assistant right and then
uh next stage it becomes our partner
right and then it becomes our mentor
right in some uh
area but still we can we can
interact with them and the most
important thing
for the success of uh each individual
human
in the future i think is to have a big
dream big wish right
then uh you can realize using bunch of
those
assistants or partners that
we can realize so having a strong wish
right i want i wish to do this
i wish to do that right there if there
are so
issues the problem people are suffering
then you have a wish to help those
people who are suffering
right then you have a strong wish and
you can utilize a bunch of assistance or
companion
uh as the ai helping you to achieve that
wish
so i think i think the the the key
in the future to the success for us
is to have a strong wish strong dream
and uh creativity that
we would like to provide that that has
become a key you have you have described
uh the singular characteristic of many
entrepreneurs right
yeah they have big dream big wish
unreasonable expectations yeah push the
limit right
and somehow somehow i somehow
that that vision is realized uh through
of course hard work and persistence is
surrounded by a lot of amazing people
in the future those big dreams will
still require amazing people but it will
also be supported by a bunch of ais
to help that realize that dream there's
no question you know one of the things
that i'm very excited about
i'm excited about about about um uh
the reason why i'm so that i believe
that this is
the time of ai for japan
i believe deeply that and let me tell
you why
japan's industry is not focused on the
internet it's not focused on computer
science
japan's industry precision engineering
precision machinery
there's large industries in japan those
large industries
haven't been able to benefit from
computer science
until now the automation of those
machinery the automation of robotics the
automation
factories you know these precision
instruments precision
machines that the area where japan just
does incredibly well
world-class that area is now ready for
anyone
and here's here's what i imagine i
imagine that we go into a virtual
reality world
and i pre i demonstrated this i
demonstrated this
at my conference as you can see it's
called omniverse
omniverse this alternative universe
is photorealistic it obeys the laws of
physics
if i drop something it falls to the
ground
it looks like reality and it's infused
with a bunch of ai's
you will create a factory first and this
factory will be filled with robots
those robots you will teach your train
you will program these robots
and these robots will learn how to
manufacture products
that product will be maybe a vacuum
cleaner it's a
robot it's a self-driving car it's a
robot
it's a logistics mover it's a robot
rope this robotic factory will be filled
with robots
that will build robots in virtual
reality
the whole thing will be simulated and
you will you will see
at night when you go to bed the ais will
optimize
optimize the manufacturing process
tomorrow when you come back
the organization of the factory may be
different than when you went to bed
right but it is more optimal than when
you saw it yesterday
it will evolve every day that's right
the ai's are evolving the factory and
when it gets to a good point
you will download the blueprint and we
will make it in our universe
now we have two copies of the factory
one
in the physical reality one in the
virtual reality
they will now be digital twins
every optimization you make here if you
want to improve here
you optimize it here you simulate it
here
and then you can change it in the
physical world
that future of
virtual reality and robotic factories
with robots that are building robots
i believe that is going to change
everything yeah that
that is the concept of metabolism right
the concept of medicare's yeah and it's
right in front of us now
yeah it is yeah as you say japanese
has a great uh uh
you know the horizon and the depths of
uh
mechatronics right uh mecca
and electronics now we need to have
artificial intelligence uh integrated so
that it becomes
very smart robots
you know where many other countries
don't have enough
the ways or depths to make this happen
so we have to give the tools of
artificial intelligence to those people
who have passion
to those uh world uh i think that
that is very very important so you
remember
um you know
four years ago you and i
sitting in the garden of my home
right we spent we spent we spent three
four hours just
you and me alone in the garden having
dinner
uh with a glass of wine we were talking
we were talking about our memory of
steve jobs
right how crazy he was how smart
how genius he was and how enjoy
how much enjoy we had uh with him
right independently but we we had a
common view of him
but with his one
you know art which is the iphone
changed the lifestyle of humanity
right so that is the last 10 years
i think the next 10 years you are the
one jen said
you know this is the reason why we are
having fireside chat
right that you are in the center of
the ai computing in the era of ai
computing you provide
all the tools that the intelligence that
everybody want to utilize have to
utilize
as a new industry new science
new education new communication
i i think the very exciting next 10
years is coming
ahead of us unbelievable actually
unbelievable you know we've been we've
been both of us have been in the
computer industry a very long time
but it has never been this exciting
and the future has never been so clearly
going to be so significant the impact is
so significant
and and i i think that that um
the part that is is almost unimaginable
is that we can almost touch it that the
the
the ai that we've always imagined we're
now seeing it
in action it is working in front of us
doing amazing things and you know what
nvidia has ai researchers now everywhere
ai engineers everywhere
and every single day something
unbelievable is happening
not just not just impressive it's
unbelievable
let me give you an example so i was tell
i was telling i was telling um
eric eric graham we were just talking
and uh you know that video conferencing
is going to be one of the killer apps of
this of the next several years and it's
going to consume
live video consumer video is going to
consume
the vast majority of the world's
internet traffic
and and communicating using the
traditional method of video
is very cumbersome eye contact is on
very difficult to make however instead
of transmitting video
we sh we showed at gtc this brand new ai
it's transmitting perception
so i perceive you on my side
i reconstruct you yeah
and when i reconstruct you you're
looking at my eyes
and if we're having a video conference
with 20 other people
when you are reconstructed for every one
of them
you're looking at everybody's eyes
yeah it's really unbelievable and
because it's reconstructed
using ai reconstructing it in its mind
the amount of bandwidth we can use you
can keep reducing the bandwidth
keep reducing the bandwidth by a factor
of 10
and the ai can still reconstruct yeah
you just input
the movement of the parts right and then
reconstruct
because the ai remembered what you look
like yeah
for example if i close my eyes i can
still have a conversation
i still know you're sitting in front of
me i remember that you're sitting in
front of me so the ai remembered
you because it you know recognized a few
images
and then after that it only perceived
you
and reconstructed on my side i also
showed you natural language
understanding
and speech translation in real time
so you could speak to me in the future
in japanese i could speak to you in
english
you hear japanese i hear english
now the world can speak and communicate
your video conference in a way that it
cannot be
since it's it's such an exciting next 10
years
world i really thank you
i i i want to speak with you next 48
hours uh on and on but
uh we have we have a uh limited time so
uh i'd like to close this today's chat
but but i will remember my life that
you know you and i sit down together
in my garden last time you know
and that was a very exciting moment
today's chat with you again so many
thousands of miles far away but we're
we're talking like in front of each
other
right and
with a help of ai we can help
many many other people's life
and not just for the people but for all
the animals and the living beings
on the earth it's it's exciting world
that we're gonna we're gonna see masa
thank you
very much and uh you remember very well
that four years ago
we were talking about many things that
that what is happening now
many of the things that's happening now
we were talking about four years ago
and and you were talking to me about my
dreams for the company
and you were talking about your dreams
for the future of iran
and and i want to thank you for for uh
entrusting me with art but this is this
is really one of the world's great
jewels
it's a treasure and and when you bought
it it was a treasure
and uh i i i really i really
am honored and appreciated that you're
entrusting with me
uh when you called me when you called me
i told you i was going to be
the last and highest bidder and
i was because i wanted it so badly
and and this is this was a it's a
you know i had to pay you an arm and a
leg for it
but but uh but it's going to be worth
way more than that and and the the
company that we're going to build
uh combining the most popular cpu in the
world
uh the ai computing company in the world
we will build the company for the age of
ui
now this this is just an exciting thing
at the time four years ago when we were
having dinner
uh that was exactly the timing i
i was deciding to acquire um
and that is exactly the timing that i
thought
the combination of arm and nvidia was
some kind of collaboration right that we
talked about many ideas
and will create new
age of computing right uh
as a platform i'm i'm so excited and
happy
that you know that what we were
discussing
uh is becoming true we're gonna make it
so
yeah thank you so much thank you thank
you have a good solving yeah
thank you thank you thank you
Title: GTC 2013: Face Works (2 of 11)
Publish_date: 2013-03-19
Length: 759
Views: 656611
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/5d1ZOYU4gpo/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 5d1ZOYU4gpo

--- Transcript ---

[Music]
simulating the ocean is hard simulating
a face is harder and the reason for that
is because our facial expression is used
to communicate even subtle ones and in
1970 a roboticist in Japan wrote about
this behavior called the uncanny valley
and as he was working on prosthetics and
robotics and his observationally --ss
was that as we improve and increase the
realism of robots we become more
familiar with them as they become more
humanoid and more human-like
in the way they look in the way they
move however at some point at some point
it gets sufficiently real it falls off a
cliff and it gets creepy at some point
it gets creepy now I've shown a few
examples here of some of some of my
favorite movies and and of course
bumblebee and transformers doesn't
resemble any human and so it doesn't
affect us we're familiar with him he's
fine sunny and iRobot fantastic movie
it's fine too because his whole body is
metallic he looks a little bit human
humanoid he looks a little bit like
human he has human facial expressions
but you can tell they're not trying to
emulate a human and then of course
Tintin and Captain Haddock they came
right on that line would you guys say
they came right close to - beautiful it
was wonderful to watch but they came
right to the cliff of creepiness and
then Angelina Jolie went right over it
this is a she was the mother of Grendel
and she is she is a monster and the
entire performance was was just creepy
beyond belief and and that is a perfect
illustration of the uncanny
vally well the reason of course is that
we understand what humans look like we
see them all the time
and so our eyes are so acute we pick on
the subtle list of details now we've
been working on rendering faces for some
time you've seen us ever since GeForce
256 we try it every single generation
because this is an endeavor worthwhile
although it's difficult this is an
endeavor worthwhile and it's the
benefits I'll explain in a second if I
can convey to you if I can impress you
with the upcoming demonstration but
before we show you what the new
technology will do let me show you what
what we did just recently this is this
is dawn dawn is a fairy Don is a
beautifully rendered hair is B this is
Kepler done it took us nearly 20 years
to be able to create what appears to be
a fairy beautifully animated the thing
that's really cool about Don is is her
skin her skin and her hair the way that
the human skin works it doesn't just
bounce light light actually permeates
through our skin scatters underneath our
skin and then comes bouncing out in a
whole lot of different directions that
creates that that kind of life like look
instead of plaster or just brown paint
she's really really quite beautiful the
thing that it was a dawn is a
breakthrough in a lot of ways the depth
of field effects that you see that
create the cinematic look in this in
this up in this particular shot the soft
shadows look at her pores the pump the
bump maps the light maps the specular
maps all of this technology that was
necessary to produce Dawn but let's
let's done if we can have done do a
performance please
and this is where the challenge of
computer graphics comes
and in fact this robot assist in japan
matsuhiro observed that that the uncanny
can't think any valley is much deeper
and has a much greater cliff when a
particular object is in motion
and you can see how how in fact earlier
she was beautiful now you can tell
obviously that she is computer animated
she's still a a wonderfully beautifully
rendered computer animation but it's an
animation nonetheless a little bit
creepy okay thank you now let's just
hold their first just a second curtis
hold there for just a second before I
show you the next one let me tell you
how it's done I the way that the if we
could turn on the light for just a
second let me tell you how it's done
it's a partnership between us and Paul
devack Paul the Bewick's USC ICT lab is
the Institute of Creative Technologies
known as University of Southern
California they invented this
performance capture technology called a
light stage you literally walk into a
sphere it's a room size sphere
surrounded by a whole bunch of cameras
about 156 cameras each one of the
cameras are lit by white LED lights so
that you can simultaneously light every
single direction at the same time it
also has cameras that are polarized and
properly filtered so that it could take
all kinds of different types of extract
different types of information from that
scene you walk into this thing and the
first thing that it does is extract
course geometry it takes a whole bunch
of pictures and from that extracts the
3d geometry it also takes a whole bunch
of light field information so that it
can extract the subtle the subtle
geometries like your pores like your
wrinkles and so on and so forth now here
comes the miracle that's just a
three-dimensional geometry the thing
that happens after that is they then
turn all of these into video and using
video cameras they capture the video in
the
fear of someone performing various
expressions about 30 expressions now
that's that's easy to do that part is
easy to do the mathematical part is to
figure out how to take all of those
videos of 30 different expressions sad
happy confused intense concentration you
name it to take those 30 expressions and
extract from it the smallest library of
mosaics that represent how you move how
you express they extract from all of
this incredible amounts of video an
incredible amounts of camp camerawork
that was done in to 32 gigabytes about
one and a half blu-ray DVDs worth of
digital content that programmatically
that programmatically allows you to
replay almost any expression okay now of
course 32 gigabytes is still too much
for us to perform in real time and so
our team created a new technology called
face works the last thing was called
wave works this is called face works
face works takes this 32 gigabytes of
information and condenses that
compresses it even further to about 400
megabytes and what's left are
three-dimensional meshes that we
articulate using our GPU these meshes
are articulated using the GPU the meshes
are being synthesized and created in
real-time tessellated in real-time and
we take the light maps and the bump maps
and the texture maps we compress all of
it into a new way of rendering facial
expression face works is really a
breakthrough piece of work and well
let's show it to you
this is digital IRA IRA is not a
recording IRA is being processed in real
time on a GP on a GPU called the Titan
just mathematically it takes about 8,000
instructions a 8000 instruction long
program to articulate the geometry all
of the mess meshes and all of the pixel
processing necessary for each and every
pixel each one the 8,000 instructions
call it 5 floating-point operations per
instruction 40000 ops per pixel multiply
that by call it half the screen of HD
and multiply that by 60 Hertz and you
get something along the lines of about 2
teraflops which is about half of the
performance necessary by Titan which is
one of the reasons why we build time so
that we can do things like this now of
course the lighting is is wonderful you
can see that the subtleties of the pores
look at the way the shadow rolls over
his eyes
now imagine imagine each and every one
of us would have one of these scans done
and I frankly think that every important
person on earth how to have it done
could you imagine if Lincoln had this
done and we could literally sit there
and talk to them and listen to them talk
and give that speech and in a fidelity
like this what would it be like if we
could use this for telepresence so
instead of video conferencing our words
are translated into instructions which
you would express or animate the avatar
on the other end we can talk to you from
just about any angle you can move around
and look around
now let's make let's make RSA a few
things just to so IRA what did you have
for breakfast
our yogurt parfait and the whole parfait
was fruit frozen fruit there is no
yogurt it's supposed to be half fruit
and half yogurt the whole thing is
frozen fruit that's pretty amazing huh
that's really amazing
hey let me ask you something have you
heard of shield what do you think of my
shield take my money alright let's make
let's make um well let's see what can I
make you do I show me Zoolander that's
not bad
okay show me um white guy dancing
only all the Asians left only all the
Asians left okay guys that's great
that's digital IR you guys let's put our
hands together for the team good job you
guys and all of that was running on
Titan okay so that's face work advanced
real-time character performance imagine
imagine how how real-time performances
video games or video conferencing
avatars could really be revolutionized
as a result of some technology like that
so that's the first part chapter 1
computer graphics
[Music]
you
Title: The Witcher 3: Wild Hunt - Gamescom 2014
Publish_date: 2014-08-13
Length: 173
Views: 50900
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/5DvVxbmu3OU/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 5DvVxbmu3OU

--- Transcript ---

we are here at gamescom and today we
have the opportunity to check out the
witcher 3. i'm chris ray let's go see
what cd projekt red has to show
six months into the invasion milf guards
legions and piers the heart of the
northern realms we turn to open world in
the witcher 3 wild hunt and the game is
35 times larger than the witcher 2.
so the play through the top in total it
is estimated to over 100 hours but
actually it's very hard to tell at the
moment because there are a lot of layers
of main quests side quests minor quests
and the random encounters of the living
word
in this game
when we have thousands of dialogue
sequences we have hundreds of unique
characters we have dozens of unique
monsters and there are varieties in this
game
the nvidia technology that is used in
wildcards
that impacts the gameplay in a heavy
manner is especially the physics i would
say
but for me personally in terms of
the awesomeness of
and visual appeal of the game as the
hair works
and the fur technology because i'm
responsible for the characters and the
monsters in the game having the nvidia
support makes the true next-gen
game out of this
within the vast open world you will
encounter a lot of quests and most of
them are main quests but there are also
side quests that fill out the gameplay
and also minor quests that i've
mentioned earlier there are even like
mini games like card games fist fighting
horse riding boat races varieties of
ways that you can spend
time in the wild campaign working with
nvidia is very nice we were surprised
actually i was surprised of the support
that
they offered us we received a lot of
help with the guys setting up the
technology for different monsters for
for them to look as awesome as they can
if there was one thing
that i wanted to point out
to the fans about the witcher 3 wild
hunt would be the intense story driven
gameplay in our open world there hasn't
been an action rpg
to date with such an emphasis on the
storage
you
Title: NVIDIA Tegra 4: Developers Bring Next-gen Games to Mobile
Publish_date: 2013-10-07
Length: 146
Views: 7171
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/5oCMJQsizbM/hq720.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AH-CYAC0AWKAgwIABABGGUgVChHMA8=&rs=AOn4CLBlgdU8IH4ffk-f7Dz4ddKU1ZvQCA
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 5oCMJQsizbM

--- Transcript ---

[Music]
[Laughter]
hamilton's great adventure is a puzzle
game it's you hamilton the great
adventure and the bird sasha that goes
on these tours where the gamer has to
you know uh go through all different
kinds of of labyrinths and and have
problem solving skills to be able to
advance to the next level with a key
the most important game for us is real
boxing for us the visual quality was
very much important from the beginning
and that's why we just afford to enhance
the visuals even more that was possible
on different
systems fly hunter origins is a
platformer 3d chase game it's about zach
who is an alien intergalactic fly hunter
on earth to collect flies which are
worth a lot on the intergalactic black
market tegra tegra4 has enabled us to
implement features in the game that we
always wanted such as dynamic shadows
depth of field bloom lighting hdr it
allows us to complete the game with the
visual fidelity that we wanted that we
can't achieve on any other mobile device
because of the computing power that
strogaford gave us we can you know add
much more effects like hdr and you know
even the skin looks better on autograph
or on our boxers
the tiger 4 chip actually enables you to
put in very nice rendering techniques
like cascaded shadow mapping and post
processing making it really a great game
the support that nvidia has given us and
our publisher has been the ability to
take our builds you know give us tips
for optimization we may want to use this
function or that function plus of giving
us feedback on the game itself
nvidia is one is a great partner to work
with and in the end both of us working
great together brings great games to the
market
[Music]
[Music]
you
Title: NVIDIA G-SYNC: First Impressions Tim Sweeney
Publish_date: 2013-12-17
Length: 215
Views: 9898
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/5otyQv-fSOE/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 5otyQv-fSOE

--- Transcript ---

hi everybody this is tom peterson with
nvidia we just got done launching a new
technology called g-sync g-sync puts the
GPU in charge and sinks the monitor to
the GPU so there's no more tearing and
latency is absolutely as low as they can
be today's also exciting because we've
got industry legends here that we're
going to talk to we just showed them
g-sync so i'm going to sync up with them
and see what they really think about the
technology and how it's going to impact
their games so first of all I'd like to
start off with tim sweeney tim is from
Epic Games really thanks for coming
today oh you're open it's a pleasure to
be here Tim when we first brought our
g-sync monitor out to epic and Raleigh
what was your first impression of it and
what were you thinking right away it was
really happy to see in video working on
GSN you know this is a problem we faced
for decades literally since the very
first games we built we had to either
aim to run precisely at 60 frames a
second and never fall less than that or
we had to live with these serious
framerate artifacts you know either you
have to deal with jitter as objects move
around at different rates as frame rates
fluctuate or you have to deal with
tearing as you switch frames in the
middle of the monitor is displaying the
scene Tim I know epic is a major game
engine developer with unreal how is
jisun going to change what you're doing
with your engine yeah you know with
every game we've always faced the start
decision of duly aim for 60 frames a
second or do we accept these horrible
artifacts and run it dirty and we when
we decided to run at 30 we had to make
this really unfortunate conscious
decision to not ever run it more than 30
because there's nothing worse than a
game switching back and forth from 30
which is fairly pleasing to 60 and then
back because when he makes a transition
you see i'll see other big problems
there and so decent really frees us from
having to make this one hard decision
for our entire game and enables us to
you know target a continuum of frame
rates during gameplay in on different
computers that gives each game or the
best experience their computer is
capable of delivering when you're
playing a game you can kind of get into
the zone sometimes just like being
outdoors it's all smooth and it's
immersive why is it so jarring when that
experience breaks down you know I do a
lot of hiking so there are these times
when you're hiking out and then
ones that you hit your head on limb and
like you know it jars your view and you
first notice that not as pain but your
visual field being disrupted telling
your brain something really bad is going
on right now I think seeing these visual
artifacts and games just triggers this
low-level brain response that tells you
something's very wrong just you know the
same sort of effects that make you you
know you know trigger fear or writer for
ya fear or clenching or which make you
dizzy or disoriented it's the lack of
cues that indicate your and a realistic
scene and everything's okay interesting
Jim how will an end user really benefit
from g-sync are they gonna see a
difference you know all of our pc games
have a lot of options you know to
control detail and frame rate and so a
gamer who wants 60 will be covered again
r wants 30 will be covered more
importantly everybody in between and the
people who don't really know what they
want but just what the gay and be as
good as possible will be spared the
artifact of tearing if their hardware
supports too soon when you look at a
game running with G sanket is just
buttery smooth it's smooth in the way
that a blu-ray movie is smooth and just
makes the play experience that much more
immersive and impressive
you
Title: Taking AI to New Heights - Skydio | Season 1 Episode 8 | I AM AI Docuseries
Publish_date: 2018-09-10
Length: 299
Views: 14210
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/5p7SGo5f1so/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 5p7SGo5f1so

--- Transcript ---

as an armchair adventurer I love the
excitement and immersion of action
sports cinematography aerial drone
cameras can add a level of scale and
dimensionality that's truly awesome but
successful piloting to get those epic
shots is no easy task that's why one
California company is creating a smarter
drone high-margin touch with Nvidia and
this is I am a on the team at Sky do is
developing AI technology to pilot its
autonomous drone the r1 I caught up with
their CEO to find out more
yeah so the initial idea was really very
simple like we looked out and saw all
the cool concepts that people were
excited about with drones but we just
felt like there was a huge gap between
what people wanted to do and what was
possible with existing products most of
the things that seem like they're gonna
be really valuable and exciting in the
long run rely on the drone being able to
fly itself and that is a set of really
hard technical problems but it's stuff
that we knew a lot about we loved
working on and it just felt like there
was a huge opportunity if we could do a
good job on it are you going to show me
this thing yeah so we have the sky do r1
here
oh man that thing is tiny so there's a
lot going on here but the key thing is
it's been built for full autonomy from
the ground up so it actually has 13
cameras on it so it has eight around the
perimeter and as two on top two on
bottom and then this is the user video
camera so this is what captures the
video that the user actually gets so
this is 4k high quality and the rest of
the cameras are used for for visual
navigation
[Music]
do you need to be able to trust the
drone to fly itself that's just like a
fundamental basic capability that we
think opens up a new world of
possibilities you need to have a
three-dimensional understanding of the
world which comes from the perception
and the computer vision but then you
also need to know what to do given all
that information in order for the drone
to figure out what it should do it
actually predicts about four seconds
into the future it looks at what the
person around it is going to do it looks
at the structure of the environment and
then it uses all that information to
figure out where it can go to fly in a
safe and useful way
[Music]
so how does the r-1 navigate through the
real world and keep up with its subject
sky do uses a convolutional neural
network to locate people in the images
from the main video camera the r1 then
builds a unique visual identifier that
lets it stay aligned to the selected
subject it then uses additional neural
networks and geometric algorithms to
build a 3-dimensional understanding of
the subject and its environment the
Arwen's understanding of the subject is
further enhanced through the use of a
recurrent neural network
the network uses previous states to
predict the motion of the subject up to
four seconds in advance to plan an ideal
flight
[Music]
we do have a lot of data on our drone
that we really need to make sense of in
order to avoid obstacles and track the
subject and plan a path in a small
amount of time there between frames and
so we do that using an onboard NVIDIA
tx1 processor that allows us to have
both the CPU and a GPU in order to
compute where things are in the world
really efficiently in a very small
amount of time so we build up a 3d map
and run neural networks on the GPU and
if that allows us to do things in
parallel very efficiently
so how does AI and autonomy make for a
safer drone it's a tricky balance
between pushing the state of Technology
and having it be actually robust and
trustworthy enough to put in the hands
of a consumer or a business
we're built from the ground up around
enabling autonomy we can show that
flying around people around obstacles
and we have the technology to be safe
more of these types of things can be
enabled and open up industries there's a
tremendous amount of complexity in
flight in flying systems in taking cool
video really what we've tried to do here
is use AI to manage all that complexity
with our AI software systems such that
the end user experience is is really
simple really powerful I think the thing
that's really most exciting to us is
we're really just at the beginning of
this journey our vision for drones is
that they should be sleek super
intelligent beautiful flying assistants
that we're comfortable using every day
for all kinds of interesting cool stuff
[Music]
Title: GeForce Garage: Antec 900 Series, Video 5 – How To Use CNC to Make Custom Parts
Publish_date: 2015-11-24
Length: 948
Views: 21676
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/5yXa_aMA6qY/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 5yXa_aMA6qY

--- Transcript ---

hi I'm Andrew with Nvidia and you're
watching GeForce garage on today's
episode we're gonna take a little field
trip to another garage where my buddy
Thomas from mainframe Kustoms is going
to show us how to use a CNC machine to
cut out some really cool custom case
badges hi welcome what a sweet spot
thanks so much for having us can't wait
to see what we're gonna do today today
we're going to be making some side
markers for the Nvidia case out of
acrylic
I've laminated together some black and
UV green and we'll be cutting those out
and actually lighting them up with LEDs
so what are we gonna be cutting them on
is this the piece of equipment that
we're gonna be using here this is a
small CNC machine 2 foot square running
a standard hand router and it's all
under computer control and this is the
computer that you use to control the CNC
mill so what software do you use to
create the designs I create the designs
in AutoCAD I then export that into cam
BAM okay which is computer-aided
machining that generates all the code
that you then feed into here and then
this this interprets the code and moves
the machine so let's get started on the
design then so Thomas it looks like you
have the design for our custom badge can
you kind of walk us through the process
of how you came up with this design and
how you interact with the program here
well this is AutoCAD and the design is
actually just two dimensional lines
which are then pulled into three
dimensions by extrusion and then
subtracted basically a lot like the
machining process itself you draw an
outline and then you tell it to take out
that part sweet so the Nvidia name there
that part is going to be extrude
that will actually be cut out and then
there's gonna be an inlay piece in in it
as well as a little light oh and so when
you take this file or this design from
AutoCAD and move it to the cam program
what exactly does that process look like
so what I'm gonna do right now is I'm
just gonna pull the design itself out to
a separate drawing file I'll do a an
export on that so this is just the logo
file itself nothing else in it except
out so this is a direct exchange file
which is is file interchange format that
cam in CAD software can use to send
information back and forth so that's
that this is cam software this is cam
BAM all the line work in here is just
two-dimensional stuff so there is no
depth to it or anything so what I want
to do is I usually refer back to this
this will show me how deep I want to cut
for different things I'm actually gonna
use two different cutters for this I'm
gonna use a larger cutter to cut down
quicker and then a smaller cutter to
finish cleaning up and to do the small
designs this lettering here and these
small parts are actually gonna be done
with a 1/32 inch diameter cutter there's
a lip around the outside and then this
is recessed down and then this is
recessed down further so I'll use an
eighth inch and take most of that out
and then come back with the
thirty-second inch okay so I need to
determine the depths of things that I'm
gonna do first thing I'm gonna do is cut
this whole thing down to this lower
level okay so that's the thickness of my
sheet now so now my Z level is at point
two to four so I'm going down 20
thousandths of an inch is if you touch
off on the top of the material you run
the risk of crashing the machine because
everything you're programming is e
negative if you touch off to the table
everything is e positive so anything
ends up being z- it's a red flag because
you're then shaping your table yeah you
don't want to cut the table up so
there's the geometry I want to use and
I'm going to pocket it all out now it
adds a machining operation down here
it's gonna instruct it anything in that
area I want to just completely take it
all out and then you go through and you
tell it you know more info
so I'm gonna go down here tool pick my
tool from the list I'm gonna use a 1/8
inch Oh flew 10 mil and then you want to
give it a clearance plane what that is
is where the bit will retract to after
it's done cutting so you want to keep
that above the level of anything you're
using to hold the park down so my part
is about a quarter of an inch thick so I
want my clearance plane to be more than
an eighth of an inch okay and then my
target depth 2 to 4 stock surface 0.25
the depth increment is if you're going
to take off less than the amount of
depth that you want to go it'll go cut
everything and it'll step down and do it
again you'll see all depth increment a
lot more with the 32nd of an inch bit
because it can't take that much cutting
force then you tell it what feed rate
you're gonna run and feed rate is the
movement speed it's how fast it'll move
when it's cutting and then a plunge feed
rate is how fast it'll go down into the
material when it's starting to cut and
I'll go ahead and leave that at 10 and
then you can tell it how you want it to
lead in so I'm going to do a spiral move
like 4 degrees and then we can generate
the tool pass on there now you can see
there's that ramp in from your clearance
height down to it and then that's the
path that the nose of the tool is going
to take that's the center the very end
of the tool so that's what your
programming is the the very tip of the
tool and then it takes care of the
offset so if I look at this from above
you can see it's offset in so it doesn't
follow this edge it goes inwards on by
half the the tool thickness so step one
is I've got that cut out too depth
step two is I've got several more
pockets that are cut down from that so
I'm gonna check that location there and
that's 0.179 so I'm gonna select this it
uses the same tool pocket and then I can
take this and copy/paste the format
which brought overall the tool
information so then I just need to
change my depth so 0.179 and stock surf
I can tell it that my stock surface is
lower 0.224 so what it'll do is rather
than having to ramp that whole thing
it'll just come down close to the
surface and then Ram
and there's the toolpath for that and
then that's actually cutting to depths
of 0.03 give it a little more depth so
[Music]
it's only gonna generate one tool path
for that will take less time to cut
basically you start big and move down
you're gonna take the most material and
then work down to taking the least
amount of material right now I'm using a
larger cutter so it's gonna take out
what it can with the bigger tool where I
can use higher speed take more material
out cut deeper and then go down to the
finer detail tool afterwards when I'm on
the smaller tool I'm gonna take another
pass around the inside of this to make
it a little sharper because you notice
this has a sharp corner in it but you're
using a rotary cutter you can't go all
the way into that corner and with an
8-inch bit it's gonna leave a lot more
there than 1/32 of an inch bit so I'll
run a couple of extra passes around that
to go further into those corners and
take more out so this one I just didn't
model it that way this one over here I
actually modeled it so it looks like
what it's gonna be done with one with
the cutter when you select more than one
it'll show you the path between them
where right now it's not showing me the
path between the other ones but I can
reorder these as well so I can move that
down so it'll cut pocket one first and
then pocket three and then pocket two if
I want to do it that way so you can
reorder the cuts you can also segregate
them out so I'm only gonna run all the
cuts for this tool and then I'll do a
second program with the other tool that
gives you a chance to then change the
tool out go back and restart it with
another another program and and run that
tool path all right Thomas so we took
our design from CAD into cam for
instructions and now what do we do with
those instructions well I'm gonna load
them into the software and this is the
entire is the instructions on the screen
for generating the part and then we're
gonna set the Machine up first off we've
got to attach the material to the table
[Music]
[Applause]
now I need the right cutter you have a
variety of different cutters where this
one the cutter is actually straight and
that's what we'll be using for the most
of its eight inch so I put my eighth
inch cutter in here this machine we have
a jog control here where if I hold down
slow medium or fast I can then move the
x and y axis and the z okay so how do
you tell the machine where the sort of
zero point or starting point of the
material is so this is where I'm going
to place the the X and y 0 ok so that
makes sure that I'm not gonna go cut
through a screw so I usually move it a
little in from the edge on the control
here I'll select the x-axis this
coordinate now for X is going to be 0
right ok you can see it shifted on my
screen here so then here's why there's
zero and then you can see on the screen
where the tool actually is by this icon
and then what I want to do is touch off
for the length of the tool but so what I
do now to touch this off is I'm gonna
touch it to the top of the board so I'll
call that zero because everything I
programmed is from up from that so
anything below zero it's cutting into
the table anything above it's cutting
into my part so I'm gonna go slow jog
the tool down until I'm as close as I
dare to be and then the piece of paper
is basically to use as a feeler gauge
here and then I can go in here at z axis
and then I can move in increments of
five ten thousandths of an inch
see it's dragging okay now I don't move
that's my Z zero interesting so that is
now all set up make sure my router is
turned back on set my speed and it is
now ready to run
[Music]
[Applause]
[Music]
[Applause]
that's it for that tool I'm gonna grab
the shopvac and I can pull all that off
of there if we look here you can kind of
start to see the outline of the design
but some of the finer parts aren't cut
out yet because we're gonna use a
smaller diameter tool and that is 1/32
of an inch so now we just swap the tools
and reset the program with the new
instructions for the second tool ready
all systems go
[Applause]
[Music]
[Music]
[Music]
[Music]
there we go sweet that thing looks
awesome so this is what it looks like
kind of just right out of the machine
and so now what's the next step to kind
of clean it up and finish the process
just a little hand finishing I'll cut
off a couple more pieces that didn't cut
out and sand it a little bit to remove
some of the tooling marks and buff off
all the leftover tabs as far as lighting
how exactly are we gonna light this
thing I'm gonna actually put it into a
fixture on the machine so I can recut
the backside
I'll trench it down a little bit and
insert some blue LED strips and blue
will kind of make the loo makes the
green really pop okay cool
and that's what you were showing me in
there on your system where the orange
comes through with the with the blue
lighting behind it
yeah that's all blue lighting but
everything shows on okay super cool man
well thanks so much this is really
awesome thanks for letting us into your
home and your workshop thanks for coming
all right take care
[Music]
Wow thanks so much to Thomas and his
family for letting us spend an entire
afternoon in a super-sweet garage on the
next episode we're gonna learn how to
finish off our custom parts in case
through the sweet magic of the air
branch you're watching t4s garage the
ultimate resource center for designing
building and customizing your GTX PC
[Music]
at g-force calm slash garage we have a
ton more content for you to check out if
you hadn't had enough if you liked this
video there's gonna be some more coming
up on the screen or right now OOP right
here go ahead and click it and there's a
second video for you to check out right
here whoo all right take that no no
seriously check that
Title: NVIDIA Omniverse - Designing, Optimizing and Operating the Factory of the Future
Publish_date: 2021-04-13
Length: 277
Views: 173146
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/6-DaWgg4zF8/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 6-DaWgg4zF8

--- Transcript ---

Welcome to BMW production, Jensen.
I'm pleased to show you why BMW sets the 
standards for innovation and flexibility.
Our collaboration with NVIDIA Omniverse and NVIDIA AI  
leads into a new era of digitalization 
of automobile production.
Fantastic to be with you, Milan. I'm excited 
to do this virtual factory visit with you.
We are inside the digital twin of BMW's 
assembly system, powered by Omniverse.
For the first time, we are able to 
have our entire factory in simulation.
Global teams can collaborate using different 
software packages like Revit, Catia,  
or point clouds to design and 
plan the factory in real-time 3D.
The capability to operate in a perfect simulation 
revolutionizes BMW's planning processes.
Are those gaskets still in the right place for you?  Yeah...
BMW regularly reconfigures its factories 
to accommodate new vehicle launches.
Here we see two planning experts  
located in different parts of the world 
testing a new line design in Omniverse.
One of them wormholes into an assembly 
simulation with a motion capture suit
records task movements while the other 
expert adjusts the line design in real time.
They work together to optimize the line 
as well as worker ergonomics and safety  
Can you tell how far i have to bend down there? 
So we'll get you a taller one, but there's.... Yeah, that's perfect
We would like to be able to do this at scale in simulation
That's exactly why NVIDIA has 
digital humans for simulation.  
Digital humans are trained with data from real 
associates. You could then use digital humans  
and simulation to test new workflows 
for worker ergonomics and efficiency.
Now your factories employ 57,000 
people that share workspace  
with many robots designed to make their 
jobs easier. Let's talk about them.
You're right, Jensen. Robots are 
crucial for a modern production system.  
With NVIDIA Isaac robotics platform
BMW is deploying a fleet of intelligent robots  
for logistics to improve the 
material flow in our production.
This agility is necessary since we 
produce 2.5  million vehicles per year, 
 
99% of them are custom.
Synthetic data generation and domain randomization  
available in Isaac are key to 
bootstrapping machine learning.
Isaac Sim generates millions of synthetic images 
and varies the environment to teach the robots.
Domain randomization can generate an infinite permutation 
of photorealistic objects, textures, orientations, and lighting conditions.
It is ideal for generating ground truth whether 
for detection, segmentation, or depth perception.
Let me show you an example of how we can 
combine it all to operate your factory.   
With NVIDIA's Fleet Command,
your associates can securely orchestrate robots  
and other devices in the factory from Mission Control.
They can monitor, in real time, 
complex manufacturing cells,  
update software over the air, launch robot missions, and 
tele-operate.  When a robot needs a helping hand  
an alert can be sent to Mission Control and one of 
your associates can take control to help the robot.
We're in the digital twin of one of your factories 
but you have 30 others spread across 15 countries.
The scale of BMW production is impressive, Milan.
Indeed, Jensen. The scale and 
complexity of our production network  
requires BMW to constantly innovate.
I am happy about the tight 
collaboration between our two companies.
NVIDIA Omniverse and NVIDIA AI give us 
the chance to simulate all 31 factories  
in our production network. These new 
innovations will reduce the planning times  
improve flexibility and precision and in the end 
produce 30 percent more efficient planning processes.
Milan, I could not be more proud of the 
innovations that our collaboration is  
bringing to the factories of the future. 
I appreciate you hosting me for a virtual  
visit of the digital twin of your 
BMW production. It is a work of art!
Title: SC19: NVIDIA CEO Jensen Huang on the Expanding Universe of HPC
Publish_date: 2019-11-18
Length: 6696
Views: 56946
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/69nEEpdEJzU/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 69nEEpdEJzU

--- Transcript ---

I am AI I am a creator freeing our
imaginations
and breathing life into our wildest
dreams
I am a guardian
keeping us safe on our way home
and wherever a curiosity takes us
I am a visionary
[Music]
anticipating the needs of others
[Music]
and simplifying our busy lives
I am a protector keeping our most
magnificent creatures out of harm's way
[Music]
and helping our heroes make it home
safely I am a healer
decoding the secrets from within
providing precision when every second
counts
I am an innovator
finding smarter answers to complex tasks
working in harmony to lighten the load
[Music]
and driving perfection in everything we
create
I have even the composer of the music
you are hearing
a
brought to life by Nvidia deep learning
and brilliant mind everywhere
[Music]
ladies and gentlemen please welcome
nvidia founder and CEO jensen huang
[Applause]
welcome to supercomputing 2019 our
universe is expanding at the speed of
light
[Music]
several of you noticed the four notes
what's the excuse for the rest of you
[Applause]
that is practically our national anthem
the HPC universe is expanding in every
single direction at the same time when
we first came to this conference the HPC
world consisted of supercomputing higher
education both doing science and the
only industry involved was really oil
and gas today higher high performance
computing reaches out to Internet
companies developing AI conversational a
I help you recommend information music
videos groceries products otherwise too
much information on internet for you to
search it's used by self-driving car
companies developing the AI for their
cars
as well as inside the car itself and for
all of you got here with uber or lyft it
was the system that helped them
orchestrate drivers and writers HPC is
literally everywhere today it's in
supercomputing centers it's in the cloud
it's at the edge one of the most
important and the most exciting
developments that we're seeing
high-performance computing today even
the computational methods are changing
first it started with first principled
mathematics simulation approaches now
more and more you're seeing data driven
approaches and the fusion of the two all
of these changes are happening at
exactly the same time I'm going to try
to touch on all of it today
so I better get going and videus purpose
is to advance this form of computing we
call an accelerated computing when
accelerated computing platform company
we apply this computing platform this
incredible capability in three basic
fields in computer graphics in
simulation of physics and artificial
intelligence in all of those fields the
one commonality is simulation we're
simulating light in the world were
simulating physics we're simulating
human intelligence accelerated computing
has made so much progress in the last
decade and a half since we really
proposed the idea of using it for
general-purpose computing and a progress
is really quite amazing but it all
starts with computer graphics this is
just an area that is still progressing
an enormous rates and we're making
enormous contributions this year last
year this year actually last year we
introduced a brand new method of
computer graphics we call r-tx r-tx is
the world's first real-time ray tracer
something that we didn't expect to do
frankly for another 10 years an
algorithm that was discovered by one of
our researchers some 35 years ago we
finally made real-time this last year we
call it RTX let me show it to you this
is our TX this is a beautiful
Lamborghini well basically what we're
doing is we're simulating a light race
as it bounces around the environment and
as it bounces around the environment and
intersects with geometry depending on
the geometry it reflects or refracts it
might get absorbed depending on the
material the subatomic structure of the
material or the photoelectric properties
of the material it may decide to become
quite reflective or it could scatter in
a whole lot of different directions all
of these properties are now simulated in
real time
we also do this in the physically-based
right way so that it conserves energy
and as a result computer graphics looks
photo-real everything that you're seeing
here from the shadows the reflections
the lighting the ambient occlusion that
little crease of dark shadow that you
see between geometries the ambient
occlusion the global illumination light
bouncing around eventually reaching
those vents near the back of the car all
of these different effects are now
computed in real time and as a result
things look photorealistic what are we
moving around a bit are you guys moving
all right look at that all completely in
real time the reflections off of the car
paint we're simulating the car paint
like it's car paint the material on the
ground the cement all the subtle shadows
the lights or what we call area lights
which are really really hard to do
because lights are coming from every
single direction and therefore the arm
brass and the penumbra's of the shadow
comes together in a really soft and
delightful way lighting shadows all
behaving according to the way you would
expect the natural lighting and the
natural reflection of the car paint off
the ground
isn't that beautiful you guys what do
you guys think
[Applause]
what you're looking at right here is
running on one g-force r-tx graphics
card this is one GPU we're now able to
do real-time ray tracing on one GPU
something that would have taken an
entire cluster of GPUs just a couple of
years ago or in entire rooms of CPU
servers to be able to render photo
realistically like this okay that's
fantastic thank you very much good job
[Applause]
RTX gosh it's just beautiful you sit
here and just stare at this for a while
computer graphics still the driving
force of a lot of things we do gives us
incredible joy and it was because of
driving towards photorealistic
computer graphics that ultimately led
the GPU to become the most powerful
processor in the world and then led us
to this conference it led us to working
on high performance computing
we did a lot in high performance
computing last year we made a lot of
progress this year 40 plus so new
supercomputers on the top 500 our GPU
powered by Nvidia we're super proud of
that but you know what were super proud
of what when incredibly proud of is the
fact that we introduced the world's
first to AI supercomputers it was a big
surprise to everybody were working with
indeed our logic was that not all
simulations need to be physically
informed sometimes in fact we have so
many different physics models that are
cobbled together into a meso scale
simulator we might be able to learn that
from data to predict a future we might
be to use AI fused with first principle
computation methods to accelerate
simulations in a way that otherwise
would have been impossible and so we
introduced the world's first AI super
compute
and the results were absolutely amazing
right off the bat the summit was able to
achieve high fidelity climate simulator
simulation result and they learned how
to detect extreme weather patterns using
a neural network and it was the largest
run of a training model ever done
exceeded XO flops and so we were able to
make tensorflow distribute across a very
large number of GPUs and train a neural
network to detect extreme weather
patterns by doing so we should be able
to in the future
notice future extreme weather patterns
developing and as a result provide early
warnings to get people out of the way or
to alert businesses alert ships to avoid
it and save people's lives
this one's incredible the folks at Oak
Ridge and the VA we're trying to figure
out why it is that only 10% of the
people who are using opiate
opioid drugs become addicted to it and
so they took hundreds of millions of
examples of genetics information and it
compared and look for patterns and
several hundred trillion quadrillion
comparisons were made in real-time and
as a result they were able to go see
what are the different generic genetic
patterns out of our three billion or so
human genome pairs that contribute to
addiction this one's interesting well
this one I used the a trainer called
Mendel which is a evolution genetics
evolutions search model that allowed
them to discover a new a design a new
model that was able to predict and
detect cancer on on these slides that
that biopsy slides that they were they
were taking and so here here's the
here's the amazing thing
20 million people are diagnosed with
cancer each year and so they're taking
these BioSpace biopsy slides and these
biopsy slides are a hundred thousand
pixels by hundred thousand pixels so
much larger than any photograph you
would take and they're scanning these
biopsy slides and they're coming off
these scanners at about two per minute
however using state-of-the-art neural
networks it would take us about half an
hour to detect whether it has cancer or
not and each one of these biopsies has
ten of these slides so each year the
United States would have to process
about 200 million slides if we didn't
figure out a way to make a neural
network detect the cancer accurately and
quickly then the ability to process
these slides would be limited and
obviously 200 million slides would be a
great pressure on the entire healthcare
system and so they used this Mendel
learner to go and explored the large
space of different models and a whole
bunch of models were being trained at
the same time and somehow they
discovered one that was both accurate
and fast they discovered a model that
was 16 times faster than today's
state-of-the-art this one is at the
Hanford Site where the the work started
on the Manhattan Project when the
Manhattan Project stopped in the late
80s and the work that was done there
stopped create developing weapons-grade
new tone plutonium and left tens of
millions of gallons of radioactive waste
underground and about a hundred square
miles of surface water was contaminated
and so the cleaning crew is trying to
figure out whether their contamination
containment process was effective and so
they drilled
these wells about a thousand of them in
this hundred square mile area and these
Geiger counters are down there trying to
figure out whether the radioactivity is
subsiding or spreading well they came to
the conclusion that they needed over a
million of these wells and so the method
in order to properly detect whether the
radioactivity is subsiding or that it's
spreading and so they came to the
conclusion that the way to do that was
potentially use artificial intelligence
that is in Spa informed by physics that
they know and then trained with the
ground truth and tested with the ground
truth that's collected from there
thousand wells to create a new type of
neural network they call physics
informed Gans and it would produce
information it would produce data that's
plausible this is one of the really
amazing things about the game work
that's been done recently using
conditional Gans and the information
that you do have and it could be it
could be informed by physics or
something else
other other pre known patterns and it
could learn by itself unsupervised ways
to imagine plausible examples of data
like it and so they were able to
generate data and essentially simulate
what they would have detected from a
million different sensors that was in
fact only collected from a thousand all
of these examples were computation done
at the petascale level this is the type
of science that we would have had to
wait for several more years to be able
to accomplish and now using artificial
intelligence and this new type of
computation we call tensor cores were
able to achieve petascale science today
the most important achievements of our
company that benefits all of you is a
combination of a full-stack optimization
most people think that the work that we
do starts with the GPU and it does and
in fact coming up with a great processor
it's the beginning of the journey of
accelerated computing but it takes a
full stack and it takes a full village
in the entire ecosystem including all of
you to help us accelerate science our
stack basically looks like this we have
the cuda stack the CUDA X stack which is
this is our architecture of our
general-purpose computing GPU our CUDA X
is a collection of libraries depending
on the different domains that we're
trying to accelerate and then the
applications on top are application
specific vertical application specific
domain frameworks that are used by our
partners and customers and developers to
create their applications this last year
this is a small sampling of the
different releases that we've had could
attend now supports arm and is now
interoperable with graphics so that's a
great release nickel now has the ability
to scale up to 24,000 nodes 24,000 GPUs
Dalio now supports tensorflow 2.0 tensor
RT 6.0 our sixth generation this is our
optimizing neural neural network
computational graph optimization tool in
run time tensor RT now supports RN ends
and also company a coverlet
conversational ai model natural language
understanding model we call Bert qu DNN
7.6 the latest release has the ability
to understand Bert models as well as
dynamic shapes who blasts this is our
matrix operations matrix operations
libraries qu solver for dense and linear
solvers
two tensor four linear algebra but
accelerated by our tensor course spark
XG boosts gradient boosted tree machine
learning model Rapids this is our data
science application suite it's open
source from data frame processing to
efficient learning to graph analytics
optics our path tracer and index our 3d
volumetric render all of these look at
the numbers the releases are 6.0 10.2
2.1 7.0 we're dedicated to accelerating
all these different fields of science
and all these different types of
applications for as long as we shall
live and the thing that is really really
cool is if you take a look at this on
one on this suite of applications some
molecular dynamics some chroma quantum
chemistry this is a tensor flow so fluid
dynamics this is on one particular
platform one cpu four GPUs for volta
100s since 2017 this collection of
applications with their associated codes
ran 27 hours two years ago without
changing the hardware at all release
after release after release after
release as we continuously relentlessly
optimize the stack from the libraries to
the solvers to the kernels all the way
up to the applications working with each
and every one of you the applications
got faster and faster and faster almost
27 hours just two years ago is now 10
hours the simple way to think about this
is you either saved three times the
money or for doing your computation or
you're now able to do three times as big
the science this is the ultimate benefit
of accelerated computing that we can
work and innovate at
every single layer of the stack so that
we can ultimately accelerate your
science so this is not possible without
all of you jumping in working at working
with us side by side and as a result
look at the incredible achievements just
software alone we're able to move faster
than Moore's law so I want to thank you
all for that this is probably the single
best understanding of our company we
dedicate ourselves to this I'm super
excited and is super proud of this and
all of our computational mathematicians
and all of our architects and system
software engineers that work with all of
you this is this is ultimately our our
best scorecard the progress that we make
in accelerating science year after year
after year and it benefits the entire
install base because CUDA runs on NVIDIA
GPUs across the board whether it's in
the cloud it's in your supercomputing
centers and your data centers your PC or
in your laptop anything with an NVIDIA
GPU in it whether it's GeForce a Quadro
or tesla it doesn't really matter it
could even be in an embedded system we
call Jetson these applications and all
these benefits accrue to the entire
install base we just make the install
base better and better and better all
the time high performance computing is
expanding in every single direction
it started with simulation of course and
what I just mentioned was largely about
simulation those largely about
simulation and every single time we come
to supercomputing we talk about more
flops and faster simulators and those
are all fantastic things however most of
you will know that all the Sun the world
has changed tremendously and the reason
for that is because the simulations we
do is now spewing and generating so much
data that one of the greatest challenges
is the analyze the results that you get
more and more and more we hear of people
that say they are limiting the size of
their experimentation and simulation
because they simply don't have the
ability to analyze it anyways
and so simulation analytics creating a
simulation that generates 150 200
terabyte terabytes of data now has to go
through the network go through storage
and somehow be read out networking
becomes a gigantic challenge storage
becomes a gigantic challenge and one of
the most exciting developments that
we're seeing right now is putting
intelligent intelligence and computation
at the edge so that we could have all
kinds of rich sensors remote sensing is
going to become software-defined the
type of remote sensing we can do in the
future is just incredible and so all of
a sudden high-performance computing is
moving in every single direction and
even as I mentioned simulation itself is
changing from computational methods to
also data-driven methods all of these
areas are undergoing incredible change
the dynamics that you hear about
artificial intelligence coming into this
industry and helping science be bigger
and better edge computing cloud some
incredible developments in cloud
computing for HPC the single most
impressing in the world I think it's
something like over a hundred billion
devices in the world have this
instruction set call arm is now ready to
come into HPC several dynamics that
makes it very important data analytics
it's like using a spreadsheet on a
hundred terabytes in oh where is that
needle in the haystack data analytics is
one of the greatest challenges in
supercomputing today and all of that
results in understanding IO
at a much deeper level and storage is
now the limiter in so many different
fields of computation we do we have that
in video we have three of the world's
top 500 supercomputers that are running
could
tenuously at our company and we're using
it to train your networks for computer
graphics and imaging and basic research
self-driving cars robotics we're working
with all of you using deep learning to
advance science those supercomputers
those HP seas are running full out and
in every single case the storage is the
limiter just reading data in and out of
those systems has become a great
challenge and so extreme IO IO is now
going to be one of the greatest areas of
innovation and we have to put a lot of
energy into making that better so I'm
gonna touch on each one of these six
things and tell you some of the things
that we're doing AI I condensed
everything about AI into two recent
recent developments and deep learning
into two points the first point of
course is Alex net - or 2012 and I still
remember coming to supercomputing and
showing you some of the early works that
we had with deep learning shortly after
that the incredible breakthrough of
cnn's is one of course that's a deep
deep neural network and so it has the
ability to learn features important
features in a hierarchical way each one
of the layers are differentiable so you
could teach it with stochastic gradient
descent and the front part of that CNN
uses convolution so that it could has
has the ability to find interesting
features important features irrespective
of its orientation or whether it's a
facing slightly different way or it's
scale is a little different it could
identify those important features and
learn it over time through a lot of data
CNN was a really great breakthrough as a
result some of the challenges that we've
always had with computer vision those
impossible applications to write we're
now able to write Alex that was a
breakthrough computer vision has now
achieved superhuman levels since alex
net and CNN the exploration of this
entire space has been absolutely amazing
the type of things that we're able to do
now with deep learning computer vision
is for all of us just incredibly
delightful of course object
cognition object detection
classification are all at superhuman
levels segmentation superhuman levels we
can now use 2d to extract 3d geometry we
can of course use the technology to
enable robots and self-driving cars it
could even imagine images that it has
learned about a particular domain so for
example you could have a schematic of
pixels as you look at on the upper left
up to right there with image generation
this is one of our one of our really
really great works we call Gauguin you
give it a a segment to pixel and it
comes out with a beautiful painting that
that I had to learn from a whole bunch
of other images it could learn 3d pose
not just to Depot's not just pose but 3d
pose where all of the points are in
space and so Alex net has started the
computer vision revolution if you go
back and think about the industries that
this is going to impact whether video
analytics transportation robotics this
one innovation will likely result in
trillions of dollars of industrial
impact over time the largest industries
that we know of today whether it's a
manufacturing or transportation are
going to be affected because of this
that one breakthrough
it happened because it was a neural
network that was in dormant if you will
waiting for technology to happen waiting
for us to come along and then all of a
sudden one day the confluence of the
fact that the internet was there and
there was a large collection of images
that were labeled but most importantly
the computation finally arrived that
made it possible for Alex net to be
trained that was the Big Bang of modern
AI and everything else after that is a
little bit history and so for the last
seven years we've been pursuing computer
vision meanwhile developing this new
area called natural language
understanding
if computer vision is our method of
encoding our understanding of the world
it encodes our understanding of
unstructured data of the world then the
next breakthrough is the encoding of
human knowledge language is the encoding
of human knowledge
it's the h.264 of knowledge all of the
knowledge that we've amassed over time
is encoded in the language that we all
share well in 2019 about August time for
a maybe August timeframe in last
year 2019 or so Google announced a paper
and this model called Burt
bi-directional encoder representation of
a transformer a transformer is one that
learns language but not in sequential
way not one letter and then one word
after another word it learns it
simultaneously using CN N and a this
concept called an attention model and it
has the ability to learn the sentence
the structure in both directions because
it's hard to understand meaning
sometimes Burt went off and inspired a
whole bunch of new language natural
language understanding models and now
NLU has achieved superhuman levels
whereas alex net and recent computer
vision algorithms can detect and
recognize and classify images at
superhuman levels we now have natural
language understanding models that takes
this test called glue which has a whole
bunch of tests like reading
comprehension and has achieved
superhuman levels as well the
implication of this is tremendous
utterly tremendous all of a sudden
things like the ability to answer
questions the ability to translate have
a conversation where the computer
because it understands what is it that
you mean was your intention search is
transformed recommendations are
transformed
the way that we interact with the
computer is going to change in a very
profound way
I can't wait for Bert and other
derivatives like it to essentially
eventually help us understand and decode
the human genome and understand all the
different variations and permutations of
it and mutations of it it's just going
to be around the corner these two
breakthroughs in combination has
implications in all of the fields that
we know how we do science in the future
is going to change I can't wait until
somebody could write me a summary of any
particular field that I'm interested in
and summarize it for me in a way that I
can understand and I can of course dig
into it further if necessary how it
could help healthcare would be
incredible just to present with doctors
the latest breakthroughs in medical
research so that they don't have to code
through all of the latest journals
incredible advances in AI we're really
really proud to have been in the right
place at the right time and the work
that we did the relentless pursuit for
more performance at all times
irrespective of any reason our company
is driven to advance performance for all
the fields of science just relentlessly
it's not based on any particular reason
we're just trying to make it faster all
the time and in fact if you take a look
at our progress this is really quite
amazing our progress over the last five
years five years ago when I was here
talking to you the GPU were shipping at
the time was called Kepler this is
probably one of the most defining GPUs
in high-performance computing it really
assured into the era of accelerated
computing for our company indeed and k80
servers if trained on resident 50 would
have taken 600 hours 600 hours this
would have been something that would
have been impossible for Alex Kirsch F
ski to have done
600 hours is a rather long time to train
a particular model because there's so
many different iterations and
experimentations you have to go through
and so to train this one model just this
one time sweeping through all the very
various hyper parameters learning from
the data training this model 600 hours
we now do it in two
now when you compound all of that it's
basically some 30x over five years
2x every year is essentially 32x and so
this is doubling every single year this
is moving at super Moore's law because
of this relentless pursuit and because
of the programmability of our
architecture and the work that we do
with the entire ecosystem so that every
single model that wants to be trained or
can be trained can be trained on top of
our GPU and therefore it can be
influenced on our GPU we've been able to
achieve excellent performance in the
industry's benchmark this is ml perf
we were number one in training twice in
a row and the inference benchmark just
came out and we led that as well the
fastest the best platform for deep
learning but what's really amazing is
the pace of which is moving this is uh
this is ILA's work at open ai and he's
been tracking the amount of computation
necessary to train the state-of-the-art
models over time now several things are
happening at the same time the first
thing of course is that the models are
getting bigger if the models are getting
bigger then the amount of data that you
need to train it has to be
proportionally better proportionately
larger and the reason for that is
because otherwise it would be under fit
and so you need a lot more data to train
it when these models are pursuing the
theoretical limits of the
state-of-the-art you're gonna try a
whole lot of different experimentation
because you're not exactly sure that's
going to work just like any other
engineering endeavor just like any other
scientific endeavor you're not exactly
sure that it's ever going to work and so
there's a lot of experimentation that
goes along with it but when you just
think about the number of the complex
that's the size of the model the amount
of data that you have to use to train it
and the complexity of the task that they
have to learn how to do the amount of
computation is skyrocketing and you sees
it doubling every three and a half
months and so the net result is in
combination with us moving as super
Moore's Law rates doubling every year
the trend is still doubling every three
months or so well
the net result is machines are getting
larger people are getting larger and
building larger and larger machines and
my sense is that this is a trend that's
going to continue we're going to see
larger and larger systems with more and
more capability and AI is going to drive
our relentless pursuit for more
performance and so everything has to
improve we have to improve our
processors we have to improve our system
architecture we have to improve its
scalability most importantly we have to
improve the software stack so that the
utilization of the computation that we
provide is as high as possible and then
we of course have to create new
algorithms and new models that makes
that are easier to train but all of this
is driving our AI computation
relentlessly ai is also not just in
training if you take a look at the work
that we're doing in nai we're doing an
enormous amount of work in training and
we created this incredible appliance we
call DG x2 with 16 GPUs in it that so
many companies are using for training
but once you train the models maybe you
want to do it in the cloud maybe you
take the mountain out to the edge we
call that system egx and this is an area
that's going through extraordinary
extraordinary excitement and development
some people call it intelligent edge
some people call it edge computing it's
just the edge and then of course
autonomous machines AIS that are
interacting with us in the world some of
it are driving some of it are delivering
they're delivering groceries in the last
mile and so autonomous machines is an
area that's doing going through a lot of
exciting exciting development we have
four basic platform
dgx for training hgx for the cloud
hyperscale egx for the edge and ajax for
autonomous machines let me show you with
you some of the examples of how people
are using AI in high-performance
computing they're gonna come into all
kinds of categories some people are
using AI informed by first principled
physics the first principle physics
equations are actually embedded in the
neural network and as a result our pre
knowledge all of the knowledge that
we've gathered that led to those
equations are now embedded into the
neural network and it gives it a
gigantic head start
physics informed neural networks
some people use enormous simulators with
extremely high precision to train a
neural network and these large simulate
large simulators take an enormous amount
of time to train to run and because the
larger the scale the more precise the
simulation if they could accelerate the
simulator it could actually be more even
though there's estimation approximation
done you means that using the neural
networks the overall simulation could be
more precise and so there meso scale
simulators are used to train neural
networks that are then predicting
outcomes at a much higher pace we see
people using artificial intelligence
networks to study the output of
simulations we see people using
artificial intelligence networks to
guide the simulation to focus in on
where the most interesting activities
are and zoom in on that particular part
of simulation so that we don't waste a
whole lot of computation on a whole
bunch of stuff that don't matter it's
using guiding experimentation
maybe it's used to steer a fusion
reactor so that it keeps it under
control there was a I think the the
folks at
at Princeton created a network called
diffusion RN n diffusion recurrent
neural network this one that's really
interesting this one is using molecular
dynamics simulator
however the quote the quantum chemistry
simulator the DFT simulator using a
Schrodinger's equation simulates the
energy potential of each one of the
molecules and then that neural network
is then placed inside molecular dynamics
simulator to predict what the energy
potential is while it uses Newtonian
physics to simulate the rest of it and
so as a result they've been able to
increase the performance of their
simulator by some five orders of
magnitude and yet still get the benefits
of otherwise would have used shown jure
equation simulators otherwise called
dfts AI for science we use AI for
science we use AI to design our systems
and so we're going to show you an
example of one this is this is in the
category of physics informed neural
networks we call it sim net and Chris
Lam is going to come up to tell you he's
going to tell you what it is that we do
why don't you describe first of all the
method that we use for the neural
network how we train it and then was
given the example of it in action sure
thing so as Jason mentioned we have a
physically informed neural network where
we've embedded the partial differential
equations that govern in this case
coupled fluid transport and heat flow in
an heatsink model that's meant to cool a
chip and we've embedded the orchid you
guys turned up his his mic a bit this
we've embedded the partial differential
differential equations that govern the
physics into the loss function that's
used to train a neural network on this
specific problem so instead of
providing data examples to train on what
we've actually provided is a sampling of
the geometry and the fluid as well as
the parameters such as the pressure or
the temperature of the chip that are in
there and what the neural network is
doing is mapping the boundary conditions
on to a set of plausible physical
outcomes based on complying with the
partial differential equations so
ultimately what we're trying to do here
is take a series of about 2,500
simulations that would take weeks with a
traditional solver on a cluster and
speed it up to interactive rates so in
this case what we're doing here is we're
we're encoding variations of the
geometry of the heat sink where we're
changing the various fin Heights and we
want to figure out what's the optimal
configuration that has the lowest
temperature drop lowest temperature for
the chip with a constraint that the the
pressure drop from the fan cannot exceed
a certain threshold in this case 15
Pascal's and so by creating a surrogate
model training this neural network we
can then run inferences very fast you
can usually infer about once a second on
a dgx to system and within a matter of a
several hours come out with an entire
design space represented here in the
window on the left that shows the
correlation between temperature and
pressure for the various configurations
now christen this case this is this is
really not an approximation per se
I mean in fact that matter is a deep
neural network is a universal function
approximator that's right and so so in a
lot of ways the partial differential
equation that would have gone into the
rest of the simulator has now been
learned into the neural network that's
right so we're very certain because of
the way this was trained with the
partial differential equations that this
is accurately complying with the laws of
physics for every solution that it
outputs you verified against many
different actual other simulators that
this is basically just as accurate as a
traditional simulation and so by going
through this this extra step of taking
your equations
your simulation equations and now
encoding it into a neural network that
extra step does take some time you have
to train it yeah and so but once you do
that this new model is incredibly fast
incredibly it's incredibly fast and as a
result we could simulate much larger
systems if you take a look at this these
simulators with the simulators we used
to do in the past we would just take a
small section of the system
yeah simulate that and and use that to
approximate the behavior of the rest of
the system mm-hm and so people do that
in and crash simulators they do that in
all kinds of Windtunnel simulations they
do that in all kinds of large physics
simulations this is this basic method of
using physics inspired neural networks
has a real potential of allowing us that
taking that extra step of training the
neural network model allows us to now
simulate much much larger models and
quite frankly as a result more accurate
absolutely it's it's applicable to very
broad areas of physics we just chose
this is an interesting example because
we have hand-on experience and we do it
every day at Nvidia
well we chose this one because we
actually need it yeah so that's a good
way good good reason to do it and that
this is beautiful this is beautiful yeah
and one of the benefits of being able to
evaluate different weight guys everybody
this is done in real time this is not a
this is not a video this is probably the
first time you've ever seen fluid
simulations done in real-time computer
graphics is generated in real-time
everything is done completely in real
time no video involved yeah so in this
example we actually found a very
interesting outcome which is a fully
rectangular heatsink wasn't the optimal
design in this case there's actually a
slight peak like a peaked roof that
turned out to be just a little bit more
optimal than something that was
completely square and that kind of
intuitively makes sense because the
hottest portion of the heatsink is
actually in the center of the chip and
there you're putting the surface area
close to where the heat is now you're
trying to balance between having as much
surface area as you can so that the heat
could transfer through those plates but
if you have too much of those plates
then it provides too much back pressure
so the airflow doesn't flow naturally
through it and dependent and those spins
have not designed properly creates
turbulence in the back
absolutely so we've we've integrated in
this into this visualization and
simulation tool and we're using it now
at Nvidia that's really fantastic well
in the future we're going to design
everything in computers and then we'll
take we'll take the blueprint we'll give
it to a robot that was that learned how
to build these things inside a computer
and it would build build our whole
system for us okay well that's the
future by the way that's that's a real
design of our system it's it's it's
beautiful too and so we take we take the
original design and and of our circuit
boards and all of the geometry of all of
the all of the various components and we
we have it all in our database and then
then we can we can simulate our systems
for of course for its functionality for
its performance for its mechanical and
structural rigidity and integrity and of
course its thermal performance now in
the future what also simulates acoustic
performance because all of that needs to
happen right depending on the type of
systems that we create all of it has to
happen in harmony and the next step we
got to do this all immersed in fluid and
the reason for that is because all of
our computers in the future will be
liquid cooled right so this is really
cool good job Chris good John Jay
[Applause]
okay streaming AI there are some amazing
instruments being built the Square
Kilometre Array these incredible
telescopes that are looking at the sky
sigh lights sensors all over the world
measuring temperature pressure vibration
in the future there'll be trillions of
sensors all over the world
there'll be billions and billions of
cameras there be littered and sprinkled
everywhere we'll be looking at
continuous data of the universe and of
the planet continuously we'll have just
an infinite amount of data it'll be
coming at us in real time in some
applications in some applications the
latency that we have to process our and
perform our observations perception and
sensing and recognition could be
classification has to be done in real
time
they could be lidar information they
could be radar information in the future
it could be radio wave information it
could be a whole bunch of cars that are
traveling through a city and somehow we
have to infer something about it in real
time so that we could take the necessary
action or alert the right people maybe
we're using all of this so that we can
improve our signal integrity so that we
could improve our signal fidelity while
dramatically reducing the energy dose
maybe in the future our 5g and 6g radios
are beamforming in real time and the
heuristics are gone it's now completely
based on AI and it's dealing with all
the cars that are coming through and the
people that are walking around and the
connections and that are there and
depending on the structure of the
geometry of the buildings that you're
around it beam forms accordingly we do
this for lidar we do this for square
kilometer arrays we look at different
parts of the sky only if it has two and
maybe one of these days base
on some neutrino detection the telescope
sweeps to a different direction because
maybe a extraordinary event just
happened in the universe all of this
streaming information streaming data and
doing remote sensing an incredible level
is going to enable all kinds of new
applications whether you know
transportation business the
manufacturing business or even in the
retail industry the number of
applications of high-performance
computing as a result of this is quite
extraordinary you know if you go back
and think about all the work that we've
done over the years most of it
contributed to the basic advance of
knowledge which is so important but its
impact on industries tends to be
indirect in several levels the knowledge
we accumulated through science and
discovery we believe ultimately led to
advancements of industries but rarely is
high-performance computing directly
impacting industry until now as I
mentioned earlier with the exception of
oil and gas and seismic processing
high-performance computing really hasn't
affected and hasn't touched most of the
other industries and finally I think
because of the work that we've done
we've done together and particularly
because of AI we're gonna be able to put
these remote sensing systems all over
the world performing all these kinds of
different tasks this is a new type of
high performance computing and I think
in the future you're going to find
supercomputers that are used to train
the models and develop the models but
then we need systems that are at the
edge they're at the edge because that's
where the action is that's where the
data is and they're at the edge because
you can't afford to stream that much
data over the Internet or over any net
back to the supercomputing centers for
processing you want to do the filtering
right there at the edge so that whatever
is coming back or all the relevant
information the meaning information the
useful information that you would like
to collect and store
remote-sensing it's going to become
software-defined and this is an area
that I think can go through enormous
transformation and make several years we
created a whole new type of computer we
called the egx the platform looks the
same and it is basically the same but
it's fundamentally different on many
many directions in many ways first of
all this box is likely to be sitting in
some strange location a strange location
you like never to visit again it could
be it could be underwater it could be on
an iceberg it could be on top of a hill
it could be in far remote regions in
deserts it has to be tamper proof and so
it has to protect and secure data in
place as well as in motion it has to be
managed from afar and orchestrated
independently independently of being
sitting next to it and so you'll be
sitting at your super computing data
center you create your model you should
be able to orchestrate send it out
orchestrate and update the models using
kubernetes all of the models will be
encoded and encrypted all of the data
will be secure every single time it
communicates with you it would have to
add a test and make sure that it's
secure and so this particular high
performance computing system is going to
be based far away from supercomputing
centers and remotely managed and it has
to be secure now so we call it the egx
supercomputing platform that is based
completely on kubernetes and highly
secure the applications of it is really
quite quite phenomenal we've created an
application stack on top we call deep
stream in metropolis basically it's a
streaming computation application
framework it's a streaming copy
an AI application framework it's
essentially like a self-driving car
except no wheels sensor information is
coming in in real-time streaming into
this box as fast as you like it to
stream in just how many NICs would you
like to have
how much computation would you like to
have inside it could have big big boxes
it could be small boxes it could be
little tiny boxes like I mentioned
earlier this little Jetson computer and
these boxes these boxes could sit far
away because they're completely secure
and so we call that deep stream I'm at
metropolis which are the application
frameworks one of the early applications
of this is really a fantastic
application it's used by the USPS the
world's largest logixx logistics center
they process 500 million pieces of mail
a day 500 million pieces of mail a day
some of it has hen written some of some
of the handwriting could be better some
of the handwriting are just wrong and
some of the packages contain things that
we like not to mail and so they need
some AI system to look at and process
all of these incoming pieces of mail 500
million pieces a day the perfect
application for a streaming AI computer
these computers are going to be
perfectly secure there'll be in hundreds
of sites be managed from one place
whenever there's new models maybe new
threats
it could be updated the system never
goes down this is one of those computers
that you can update without it going
down ever and so new AI models will be
deployed to it and I'll continue to run
it runs with the old model until the new
model is is is installed and then it
switches over to the new models there
are all kinds of new applications that
we could we can we can imagine that I've
already spoken about a couple of these
whether it's using one of these systems
manage a whole factory of robotic arms
instead of putting cameras updating all
the robotic arms with cameras on them
and sensors on them
we could put sensors all over the
factory and these sensors all over the
factories will allow these cages to be
taken off and these robots essentially
to be working among us and based on what
other robots are doing and and what are
the other people are moving around and
the other delivery BOTS are happening
these robots could be orchestrated and
managed by one supercomputer sitting
inside that sitting inside that
manufacturing facility the incredible
number of applications all based on high
performance computing and processing AI
in real-time this is really interesting
this is fun to watch
this is how mail gets along surprisingly
brute floors it's a miracle that it gets
there look at that
if we were to realize this is what it
took for mail to get to us we would have
we were too conceived of email a long
time ago we announced at Mobile World
Congress that this particular system is
also ideal for the future 5g edge not
only are we going to put applications on
5g networks that are close to the edge
so that it could be processed a lot
faster so that data doesn't have to
travel long distances over the internet
or because data is secured and data
privacy is so important you can't afford
to have it transported over the internet
and off of your facility those kind of
applications are perfect for this
platform in addition to running the
whole 5g stack this last couple of years
we've been working on accelerating the
5g Rand just as we do signal processing
we do deep learning now we're doing
signal processing for 5g radios and we
could scale to the highest performance
and because it's in the data center and
because the performance and latency is
so low so performing so high in latency
so low it's possible to have a lot fewer
systems because the traffic the workload
could move from data center to data
center so we announced a partnership
with Ericsson Microsoft as you know is
going all-in on the intelligent edge
they're seeing just like we are an
explosive number of applications in
warehouses logistics retail stores 30
trillion dollars where the retail has
the opportunity to improve their
efficiency by just a couple of percent
the benefit to that industry and the
benefits to cost of living is incredible
and so we see the same opportunities
that they do and we partnered together
to bring intelligent edge to the world
let me change gears now talk about
something else as you guys know building
supercomputers is hard and I wish I wish
we had a stop-motion camera on the
building of Summit it would have been
incredible I'm sure I'm sure I'm sure
Jeff Nichols has it but building
supercomputers is super hard and takes a
long time several years into planning
several years in the planning and almost
a year is just a building and then
facilitating the system bringing it up
getting it tuned it's incredibly hard on
the other hand and so researchers don't
have the ability to go and invest and
building their own supercomputer not
only is it expensive it's hard and so
cloud seems like a perfect place to do
it and so we we did we did several
things we took a couple of these
examples of the applications that we
showed you earlier then we benchmark all
the time and we put them in the cloud
and and this is a this is one of the
clouds and it's the it's the CPU that's
recommended for high-performance
computing and it runs these applications
all of these applications in aggregate
takes about 48 hours 48 hours and $152
48 hours and hundred fifty-two dollars
now frankly that doesn't sound so bad
however as you know most of your jobs
most of your simulations take a lot
longer than that
and so taking a lot longer with a lot
more CPUs this bill could really rack up
and rack up fast and so the answer of
course is to accelerate it acceleration
acceleration we've been putting GPUs up
in the cloud now for a couple to three
years and acceleration acceleration is
is is unquestionable the acceleration is
is unquestionable please put your
walkie-talkie to silent
it's okay we're good where these are
friends where among friends and so
acceleration is is is is unquestionable
we know that we could accelerate it
however however the price the price per
hour of a GPU is a lot higher than the
price per hour of a CPU the price per
hour of a GPU is a lot higher than a
price per hour of a CPU and as a result
many tend to use CPUs until recently
that trend is starting to change and the
reason for that is because all
scientists know is not about price per
hour its price per science it's not
price per hour is price per science it's
not cost per hours cost per job and so
the question is what would happen if I
would run this exact same application on
our GPUs and this is what it looks like
on one GPU instance on one GPU instance
on one GPU instance 48 hours become 6
and it cost you 18 dollars even though
when you look at a per dollar per hour
it's a lot higher of course it takes six
times less six times less time on the
other hand even when you use multiple
GPUs on one of the most powerful and
therefore most expensive instances in
the cloud it is actually the most
affordable surprisingly now to you and I
it's not surprising at all the best way
to reduce cost is to get your job done
and get off and so getting your job done
and get off is really the best way to
keep your cost down on clouds and people
are starting to discover that well one
particular group of scientists
discovered it recently and they did
something that is extraordinary they did
something that's extraordinary they're
validating the results and trying to
improve the detectors of neutrinos you
guys know that there's this incredible
incredible sight in neutrino in indicate
neutrino detectors called Ice Cube
in the South Pole neutrinos are these
things called cosmic rays that were
discovered about a hundred years ago and
we finally detected them and they're
neutrinos and they come very rarely but
they are high-energy extremely light has
almost no mass and because it has no
almost no mass it doesn't interact with
anything it doesn't collide with
anything it flies right through
everything and it's in fact so light
that it interacts with the universe so
lightly that it gets here faster than
the speed of light because you know as
you know light interacts with the medium
and as a result neutrino gets here
faster we could detect a neutrino and
might be able to give us early warnings
of something that's happening far out so
that we could focus our energy to go
discover it well that we created a
neutrino detector called the Ice Cube in
South in the South Pole and it's deep in
the eyes one square kilometer one square
kilometer one one kilometre one
kilometer one kilometer down and this
cube of detectors and when a neutrino
strikes an atom a nucleus an ice nucleus
this crystal clear ice underneath or
above underneath the South Pole it emits
light and these these these light
amplifiers collects them up and detects
them they detect you know tens of
neutrinos a year it's not that many they
detect tens of neutrinos and and what
they want to do is improve the quality
of their of their detectors and so they
want to understand when the neutrino
strikes a nucleus and in the midst that
little tiny pulse of blue light as it
travels through the ice they would like
to understand that transport property
better so that they could create better
detectors okay and so they went off and
they launched a gigantic simulator we
and this gigantic simulator was launched
on 52,000 nvidia gpus so somehow they
found the perfect time around the world
where 52,000 nvidia gpus are available
in the cloud and they hit enter
the person that hit enter well it's one
of these guys it's got to be either
Frank or Igor Frank and Igor are here
actually where are you guys
Frank and Igor hey Frank Igor good job
who hit entered the largest single
gpu-accelerated
application ever it launched on 52,000
GPUs it ran in every single country and
every single cloud and my understanding
is is the cost was somewhere between
fifty thousand to two hundred thousand
dollars and and it was on Frank's
Frank's Visa card now now here's here's
the amazing thing in aggregate is 350
pedo flops
350 pedo flops of FP 32 on those 52,000
GPUs which is almost the computation of
summit it's almost the computation of
summit which is which is like you know
what does anybody know the the answer to
this next question is is there like 20
30 megawatts 20 megawatts or so it's
like 20 megawatts or so and and so you
guys hit Enter you you kicked off 52,000
GPUs servers I don't know how many
megawatts were we're there for cranked
up at that moment for an hour or so you
simulate it and then you spawn you you
had to start up all these jobs you got
to retire all these jobs get all the
data back out and altogether was about
two hours incredible simulation now
here's the amazing thing this is kind of
interesting too now it turns out there's
a there's a whole bunch of V 100's in
the world than p1 hundreds and p-40s and
we had several generations of them and
and this is this is not the number of
GPUs this is the events processed per by
GPU type and and as you could imagine it
makes sense since be 100 is so much
faster than P 100 the number of even
though the number of GPUs could be
similar the events that a process was a
lot a lot greater and I think I think
Frank was was telling
telling one of our guys that that in
fact if you did it by cost the 100 is
potentially the cheapest which kind of
makes sense the faster you get something
done the cheaper it is okay and so I've
been I've been known to pass along a
wisdom see the more you buy the more you
save and it's it it is wisdom indeed and
so so for all researchers that would
like to conserve whatever research grant
that you have you know find yourself the
best GPU before you hit enter okay and
so so it starts to rack up alright so
this is a this is a cloud simulation an
HPC cloud is starting to change the
world of high-performance computing and
in fact you you almost wonder what we're
gonna do with top 500 and the reason for
that is because much of top 500
high-performance computing is going to
be done in the cloud in the future and
so we really need to have a system that
allows us to understand that better and
more and more researchers are using this
because it's just easier it's easier
than building a super computing cluster
it's easier than managing it's easier
than then then then then then it's much
much easier to use the cloud and now
that we have the most advanced GPUs up
in the cloud is also very cost effective
and so it's both convenient
cost-effective and you could you could
conserve your your your grants for
research hiring researchers today we're
announcing that we're partnering with
Microsoft to put a large scale and
hopefully larger and larger and larger
scale supercomputers in the cloud and
high performance computing starts with
the processing nodes one GPUs part one
GPU per note many GPUs per node and of
course connected by extremely low
latency connectivity and this is
connected by Mellanox is incredible
InfiniBand and so all of these nodes are
now connect
with InfiniBand and all of it in
aggregate turns into a really fantastic
supercomputer the thing that's really
great is we have this we have this
registry we call the NGC the NVIDIA GPU
cloud it's a registry where we store all
of our latest and updated and optimized
software stacks that I mentioned earlier
during the talk
all of those stacks whether it's for
deep learning or data analytics or
machine learning or molecular dynamics
or quantum chemistry or fluid dynamics
image processing or volumetric rendering
all of that is optimized and opportun
all the time and stored in NGC cloud you
open up an instance you go grab one of
those stacks it's in a container you
launch it on Azure and you're doing
science you're doing science so it's
really really quite fantastic so I want
to I want to thank Microsoft we're
partnering with us on this and putting
in the hands of every researcher in the
world a supercomputer thank you I drank
a can of Diet Mountain Dew before the
talk and every three words I feel like
what do you is there a graceful way of
saying it sorry
whoever wants to give a talk next time
don't drink Mountain Dew all right let
me let me change let me go from the
world's largest computers to the most
energy-efficient computers and so you
guys know that arm is the most pervasive
eisah CPU I said the world's ever known
it's at a hundred and one hundred
billion plus computing devices in the
next several years a very few question
it will cross the trillion trillion
devices mark and now with IOT and
sensors out all over the place and smart
sensors all over the place you know arm
is arm is really really going to
continue to grow and this industry has
has has pursued advancing arm into all
different types of configurations of
high performance computing and people do
that because the CPU is completely open
we use arm we use arm because there are
certain types of computers and we want
to build for example we built xavier
which is the world's first robotics
processor the configuration of it the
way that it communicates the real-time
nature of sensors the proportioning of a
single threaded performance computation
AI computation parallel computation with
CUDA the proportion of all that was so
different we needed we needed the
ability to configure our own computer
well it turns out a lot of people need
the same thing whether it's because
certain increasingly people realize that
high-performance computing is the engine
of the next Industrial Revolution now it
sounds a little cliche and it sounds a
little cliche even as I say it but it's
completely true we automated power in
the first couple generations of eras of
industrial revolution and this time
we're automating automation the ability
to put AI everywhere is truly an
extraordinary event and countries
recognize this and so so uh nations
ourselves in many nations around the
world are investing in building their
own high-performance computing
infrastructure for example euro HPC the
the folks in Japan are building their
own hyper
supercomputers for this very reason
because they they need to advance
supercomputing in the way that they see
the world emerging there are different
types of computers that are being built
some of its done in the edge some was
done in hyper scale cloud some of the
high-performance computers are designed
for very very fast i/o very very fast
storage some of the design so that it
could be incredibly secure and so
everybody has a different motivation for
designing high-performance computing
there used to be one type supercomputers
now there are so many different types as
the universe of HPC expands in literally
every single direction the ability for
people to take a simple
eisah like arm that has the
pervasiveness of arm and to be able to
configure all kinds of different
computers is really quite power quite
powerful and we see all kinds these are
just some of the block diagrams that you
guys might have seen as well the folks
at ampere they call it emag they're
optimizing harpers hyper scale and
storage Amazon they call a graviton it's
four hyper scale and smart Nix Marvel
Thunder x2 hyper scale HPC and storage
Fujitsu has a really incredible
processor they call a 64 FX for
supercomputing and Huawei just recently
announced a really great processor they
call the company 924 big data analytics
and edge and so there's so many
different configurations and everybody
is optimizing for different things and
the amount of i/o that they have is
different the amount of cash they have
is different amount of cores they have
is different some of them have giant
cores some of them has smaller cores a
whole bunch of them and so the the
configuration of it is completely
different and people over the years have
asked us if we couldn't please bring our
CUDA GPUs to this ecosystem and so
several months ago we announced that we
would we would and so we've been working
on
cultivating developing the Kuti
ecosystem forearm and it turns out it
turns out that the lifting is not
terrible because all of the applications
are open source and we work with all the
ecosystem and kuda is is the engineers
that worked on cooter kuda and one of
them is right here chris lamb had done
such a great job that the the porting of
the ecosystem and the cultivating of the
ecosystem forearm has been really
fantastic and we we had a good friend
over at Oakridge start working on it
right away and so so this is based on a
thunder x2 and just basically one Volta
and some of the things that they said
we're fantastically they wrote it they
wrote a paper on it the stack is really
solid straight out of the box on par
with power in x86 this is from Jack
wells and Satoshi a good friend who's
who's a you know provided a lot of
guidance over the years and for the for
the whole HPC industry a new wave of HPC
Nai converged workloads in Japan and the
importance of the arm work that's done
in Japan for their for the national HPC
efforts and the speed ups are all
fantastic the spirits are fantastic and
so today today we're announcing so that
so it's it it seemed like annex it
seemed like an experiment but it wasn't
our company is fully dedicated to this
we just haven't announced too many
things since and we have we have a bunch
of people working on it and so today
we're announcing our first reference
platform the NVIDIA HPC for arm and let
me show it to you
so this thank you so this ladies and
gentlemen our four GPUs and they sit
basically in that chassis up there this
is the configuration and we made it so
that anybody's CPUs anybody's CPUs could
be connected to this okay and so these
are the two CPU boxes
in these boxes that we have here these
are the the Marvel Thunder x2 is really
fantastic CPUs really great i/o and the
single threaded performance is
approximately that of a high-end modern
Xeon CPU and so so the Thunder X 2's two
of them in here and two of them down
here connected through an external cable
for PCI Express and so this way whether
it's an ampere or Fujitsu or anybody
else this is a really wonderful way for
you guys to get connected and each one
of the pairs connect through this to
connect to for voltage GPUs okay it's
connection looks a little bit like that
and so this is our first development
system for our HPC
[Applause]
and it's lighter than you used to be I
think we're making our systems lighter
and lighter must be those simulations
okay so this is what the box looks like
and as you know we design everything in
in digital so it's easy to make them
translucent okay this is this is the
beauty of doing everything in digital
you guys okay we do everything in
digital and and then we just I just
asked him for show me what it looks like
yeah it looks like this and I has four
Mellanox Nick's these are to see X fives
and then we'll have CX sixes next
incredibly high performance and we've
been working with the industry all of
you the the the industry has has really
really been fantastic everybody's
jumping on and and we already have 30
applications from molecular dynamics to
quantum chemistry to imaging to comment
that was a one of the things that I
demonstrated earlier tensorflow so now
arm has AI tensorflow relyin which is a
cryo-electron microscopy imaging system
and all of your favorite and best
programming tools including including
PGI and the cpu the cpu partners are
fantastic and growing and and you know
this is I think this is going to be a
great ecosystem now basically everything
that runs in high-performance computing
should run on any GPU or any CPU as well
and so GPU CPU
it's basically an open system as you
know all of those applications are open
source and could be ported from
application to from platform to platform
to platform and so we now work with IBM
we work with Intel CPUs we work with AMD
CPUs and we work with arm CPUs okay and
so that's our reference platform and
you know it's always good to see it work
now it turns out all of these
applications on top watching them work
is not it's not so bad as grass grow but
but it takes a long time as they should
and so so we chose one that is a fund
watch and it's called VMD now VMD is is
developed by a gentleman named John
Stone now John Stone I like to call him
the great John Stone and so John Stone
John Stone was probably the first CUDA
developer scientific code could a
developer and if not for Nandi being
ported to CUDA quite a long time ago and
when you see him he'll stand up you'll
just be amazing
he looked exactly the same he was a
child then he's a child now and so John
Stone ported AMD to CUDA and gotten
incredible speed ups and so when we came
up with this new new computer we needed
to hey who's gonna try it
hey let's give it to Mikey and so so we
called John Stone and hey John Stone
could you give this take this out for a
world and he says yeah sure why not and
so so he's only had it for a few days
and and let's see what he came up with
all right great John Stone so this would
be the molecular visualization tool that
I developed at University of Illinois
and it's running on an arm machine in
Santa Clara California and you're seeing
it this is a fully featured version of
the program so it has all the same
features it does and all the other
hardware platforms we support our NIH
funded research center develops these
research tools and we want to make them
available to all the different hardware
platforms that the research community
uses and so you're seeing this now
running on a thunder x2 based machine
with two tesla v1 hundreds and it's
showing a live interactive ray tracing
that's being driven by Vishal here
and this ray-tracing is then compressed
in real time and streamed over the
internet and is displayed here in Denver
and what we're showing is a large
photosynthetic organism that lives at
the very bottom of various ponds where
there's not very much light and it does
photosynthesis these little green ring
like structures or chlorophylls and they
basically capture the photons from light
and they that's in the early stage of a
long sequence of operations that take
place they convert that light into
chemical energy in the form of ATP which
is the fuel of all the cell life on
earth and so this is a you know in terms
of photosynthetic machines this is
incredible because this simple thing
simple compared to plants and other
things it is it's energy return on
investment is four times better than the
best engineered thing that humans have
been able to come up with and so we in
our various collaborators all over the
world are interested in studying how
these things work and this is a when
it's simulated on a supercomputer every
single thing you see there has been
simulated on a supercomputer both in
isolation and now in totality this a
grits to hundreds of millions of node
hours over decades and many many many
PhDs and researchers all over the world
working on this so it's really cool to
see this you're seeing an atomic detail
structure and it's like a little Swiss
watch it's amazingly sophisticated and
so here you are you're seeing it running
on an arm machine John nice work I get
ladies and gentlemen I don't think
anybody's ever heard a description of
otherwise normal human know as pond scum
with so much enthusiasm and with okay so
so ladies and gentlemen John Stone
[Applause]
next time next time you're swimming in a
pond and you step on that gooey slippery
stuff on the bottom you'll think of John
Stone from now on I am certain of it
every super computing scientist in the
world when they think pond scum John
Stone I Love You Man
incredible work all right let me talk to
you about Io
our DMA our DMA was was a was really
really pioneered and and brought into
industrial used by Mellanox the ability
to bypass the kernel as as computers
communicate with each other in a large
distributed computing way put infinite
ban on the map and and as a result it is
just a fantastic way to do large scales
large-scale distributed computing
applications and simulations the
challenge the challenge of it is this
when we started adding GPUs to these to
these systems and what we're what I'm
showing here this is basically a this is
basically a DG x2 node it has 16 GPUs 16
GPUs each one of them with one terabyte
per second memory bandwidth so you have
16 terabytes of memory bandwidth
terabytes per second of memory bandwidth
driving half a terabyte of memory ok and
so you can move a lot of data into the
DG x2 for data analytics or deep
learning or scientific computing and
whatnot however if you were to move that
data and you excuse me if you were to
connect multiple dgx systems together to
do a large sales large large-scale
simulation or or a our training training
job the communications between them to
collect intermedia partial results to
reduce them to broadcast them out and
synchronize them with the other nodes
that overhead of communicating becomes
quite quite high and the reason for that
is because the systems are running so
fast each one of the nodes are now
computing so fast and within the node is
communicating with MV link and so the
the commune
patience inside the system is incredibly
fast now all of a sudden the
communications over the system among the
systems becomes the bottleneck and so we
created GPU direct our DMA that many of
you have used already I'm sure and as
part of this API we used to call Nikhil
what we call nickel and nickel basically
uses CUDA to perform our DMA between
between the systems and working with
working with Mellanox next we have the
ability to transfer information from
node to no without ever touching the CPU
the reason why this is so important is
because if you were to send it to the
CPU and back we would cut the bandwidth
in half at least not to mention that the
bandwidth that we're talking about now
hundred gigabytes per second hundred
gigabytes per second 8 NICs
on a DG x2 which is basically 800
gigabits per second or hundred gigabytes
per second that bandwidth is of the
order of the bandwidth of memories on
CPUs and so these CPUs are going to slow
down the data transfer we have the
ability to transfer the data directly
from node to node over GPU direct our
DMA saving the CPU to still orchestrate
and still running the application but
most importantly so that we could
sustain very very high speeds of data
transfer the challenge now the challenge
now is not only is this a problem for
networking this is a problem for storage
the amount of data that we are announced
we're now generating in these large
supercomputing computers is quite
extraordinary and so this is the way it
works today traditional storage you're
now sending storage at 50 gigabytes per
second across 8 NICs and these 8 NICs
the the data goes to the CPU and then it
comes back down to our GPUs we've
invented this technology called GPU
direct storage where now the storage can
stream a hundred gigabytes gigabytes per
second through these 8 NICs directly
into our GPUs the combination the
combination of these two technologies
and many other libraries that will put
into the i/o and multi-node multi-gpu
networking and storage parts of the
stack we're now calling all together the
nvidia magnum io this is an area that's
going to be rich with innovation and
we're gonna put a lot of energy into
helping you move data around the system
in and out of storage between nodes for
networking and we're working with the
ecosystem storage vendors system makers
NIC providers Mellanox and others to
optimize this entire performance now the
question is what does it feel like in
the end the question is what's the feel
like in the end this breakthrough Magnum
IO is one of the biggest things we've
done last year and the reason for that
is almost not nothing else that we've
done collectively as an industry would
have sped something up by a factor of
two three four magnum IO does that
Magnum IO gave us a many X Factor
speed-up and some of the things that
you'll be able to do with it is really
quite remarkable you know that we've
been working on this thing called rapid
this is one of the greatest things we
Det we've done and I'm so proud of it I
spoke at it and spoke about it at length
last year data analytics data processing
is one of the key parts of
supercomputing going forward you can't
just simulate the results you have to
study the results and and getting all of
that data off of disk into the system
for data processing has become a
gigantic bottleneck there's several
parts of Rapids all of this is open
sourced the the several parts of rapid
the first thing is it's built on top of
Apache Aero so that data could be read
in whether it's CSVs or parque and it
could be read into Apache Aero in a
column er and a vectorized format so
that CUDA could read it lightning-fast
second thing is that desk which is also
open source allows us to schedule
multiple GPUs for the very first
the scheduling of a large cluster of
GPUs is a great breakthrough that desk
has provided on top of it we built
Rapids there are several components of
Rapids the first component is the
ability to ingest IO we call it COO IO
is not shown here once you ingest the IO
the data then you go into this thing
called data frame it's basically pandas
change a couple of lines of a couple of
lines of python python code and all of a
sudden it runs on CUDA and it runs on
our GPUs lightning-fast
we call that kudiye kudiye frame ku ml
is basically psychic learn and it's
compatible with X G boost a couple lines
of change again in your Python code and
boom it gets accelerated with ku ml and
then lastly we recently introduced ku
graph which is really getting a lot of
excitement now the ability to process
and Giganta graphs analyze it fly
through it okay and so these stacks are
the first accelerated data analytic
stack the world's first accelerated data
science stack and we see speed ups up to
a hundred and I saw a speed up the other
day seven thousand times well the reason
for that is because you know most most
data analytics programs were developed
when the data is small and you practice
it on small things but pretty soon you
take that application that framework
into high performance computing and
Allison you got 10 terabytes it comes to
a crawl and so the ability to rethink
that whole data analytics pipeline is
really really vital and really important
and we we we're seeing great adoption
all over the world 150,000 downloads
already and there's so many people who
are contributing to this I'm really
delighted to see this and so now data
analytics the acceleration stack is
right there what we need on top of that
is Magnum so that we can move data in
and out of these systems and sometimes
the data is so big it doesn't fit in a
dgx to half a terabyte and we need to
have more more than that and so we need
to of course connect more of them
together and Magnum allows us to do that
as well well here are some examples of
what
to do so this is the folks at Pan Geo
and they they have a reader called
x-rays are and it reads and volumetric
data of the worth the world's high
precision world weather data and they
want to import all this so they can
understand climate change when they
import the data it's like a hundred
terabytes of volumetric data a hundred
terabytes a hundred terabytes once they
move it in they want to do analytics on
it maybe they'll do some you know simple
things like what's the average change in
temperature of this part this cube of
the earth over time over the last three
decades maybe they want to do some
convolution on it and so pan-pan geo
x-ray has been sped up incredibly we we
took took the whole stack out for a for
a drive and we benchmarked in on T BCH
which basically is taking 210 terabyte
data sets and finding relationships
between the two of them okay and so it's
it's a well gosh I think what did it say
went about without without Magnum IO as
the GPU direct storage versus with is
about 20 X speed up and then this is a
structural biology simulator and
basically what it's trying to do is take
all these different time steps that came
out of the simulator and wants to do a
dissimilarity analysis finding these
that create the dissimilarity matrix
from frame to frame to frame to frame so
that they could discover when the
molecules change the least and when it
changes the least that's probably
because I found it the minimum energy
state and it's finally stable and so so
that analysis was sped up tremendously
well I want to show you something that
there's just simply impossible to do in
real time until now okay so in this
particular case you have a DG x2 you've
got you have you have the pan geo excuse
me got Magnum Magnum IO layer you've got
the pan Gio's our reader so
on top of that and then on top of that
you'll do Rapids here you've got the dgx
- you've got the Magnum IO and on top of
that you got CUDA and then you've got
Rapids here here again you've got DJ x2
on top of that you have Magnum and then
you have to the VMD structural biology
analysis tool what I'm about to show you
I'm about to show you is a simulation
that is gigantic and I created a
simulation file a hundred 50 terabytes
large it's a simulation 250 terabytes
large now once the simulation is done we
want to we want to understand what we
simulated and so so in this particular
case the only way to do it is to
visualize it we're gonna visualize a
hundred and fifty terabytes of data I'm
going to fly through her four bytes a
hard 50 terabytes is something along the
lines of 25 thousand DVDs okay we're
going to take 25,000 DVDs we're going to
pile it all into a box I'm going to fly
through every single scene random-access
okay and so so this this next next next
feet is something you should do at home
you just can't all right
and so I'm going to introduce we have a
we have a we have a space engineer among
us actually won't you introduce yourself
sure my name is Ashley Morrison and I'm
an aerospace engineer from NASA Langley
Research Center she is gonna try to
feast trying to figure out how to design
a lander to send us to Mars it takes two
years to get there right you were
telling me it takes like two years to
get there and six people have to live in
what appears to be a condominium as they
land now that's just the last part
before they go they're gonna be like in
a cruise ship they're gonna be discos
there'll be movie theaters because these
six people are gonna have to live in
this thing for two years when
get there they still want to be they
they want to be st. enough to want to
land right and so so so so anyways
ashley is working on this all right so
tell us about tell us about this
adventure tell you what you know what
how about let's show it to them first
yeah okay ladies and gentlemen the Mars
Lander
[Music]
[Music]
[Music]
[Music]
now now actually first of all you know
we when we saw the Curiosity Lander land
on Mars there wasn't a ball of fire
right it was not a ball of fire there
were there were there were some there
were some parachutes and it landed
gracefully on Mars and now now we're
gonna put six people in this thing and
when to send them them to the surface of
Mars and a ball of fire okay so tell us
why we have to do that yeah absolutely
so in order to send humans we're very
fragile were very delicate creatures we
need a lot of things to keep us alive so
it's a lot more mass you need to get to
the surface of Mars so you mentioned the
Curiosity rover it's the size of a
compact car that's about as big as you
can do and no fireball so there were
parachutes deployed at supersonic
conditions to touch down on the surface
when you're talking about payloads this
size this vehicle is now the size of a
two-story house you know it's more than
16 meters in diameter and its largest
dimension it's it's absolutely massive
so you're going from roughly 12,000
miles per hour to zero in a very precise
targeted spot on the surface in less
than seven minutes and to do that you
can't use parachutes anymore so that's
where the ability to simulate the types
of physics for what will be a new
deceleration technology your fireball
really comes into play so that's the
that's the technology change in the
paradigm shift does anybody know how
long it takes to stop from 200 miles an
hour on a car to zero it takes takes
some time yes
and so we're twelve thousand miles per
hour we're gonna we're gonna fly through
this atmosphere is that it's 100 the
atmosphere of Earth and so so at that
speed you're not deploying parachutes
not for a vehicle this size yeah I don't
think the math works and so pretty much
you've got retro boosters and you're
firing these engines and and it in six
minutes time I think you were telling me
that that you would have traveled
another few thousand kilometers
basically the entire width of the United
States and so you're inside the inside
the inside the inside the atmosphere
from
thousand-meter to zero and and you guys
gonna land you're gonna stick the
landing you have to stick the landing
you hope you stick the landing and so
we're gonna send we're gonna send let's
see if I understand the mission okay so
you just got describe it I mean when you
say it out loud just go yeah I could do
that so so you so basically when a sin
we're gonna send six people on a journey
for two years they're gonna watch every
single movie that has ever been made in
two years right they get there they're
gonna they're gonna be flying at this
rock at 12,000 miles per hour and and
unwittingly all of us on a ball of fire
consumes them and and then and then
somehow somebody her name is Ashley
whoo-hoo-hoo-hoo-hoo three decades
earlier did some math that said it's
gonna be fine and you're gonna you're
gonna land you're gonna land this thing
like that you don't even need a joystick
you won't need it because you're not
flying anything at 12,000 miles an hour
yeah okay all right that sounds right
and so that's the mission
[Applause]
okay so before we made DG x2 and Magnum
and port and got indexed to run on this
whole thing index is our distributed
multi node GPU accelerated volumetric
renderer okay that's a very long
description by the time that they were
done they were done explaining to me
several years ago that we needed
something like that I said after that
Jenson what you know what we need we
need a distributed GPU accelerated
volumetric scalable renderer as now I
don't understand what you said and so
but anyways this is the reason what we
did it so that all these simulations
creating terabytes and terabytes and
tens and hundreds of terabytes of data
are going to get put into storage we're
going to scream the data out of storage
and want to fly through all of this data
in real time in real time so that we can
analyze it and so what we need to do is
we need to create a computer that has
the ability to do that we had to create
the i/o stack that could move it in and
out of storage streaming it putting it
directly into the computer while we
visualize it or do analytics on it in
real time the idea that we're going to
have essentially a supercomputing
analytics instrument that sits next to a
supercomputer we're going to do the
simulation on a supercomputer we're
gonna do analytics on a supercomputing
analytics instrument and so that's what
DG x2 is about we've been building this
for some time all the pieces have
finally come together we showed you a
few examples of it we showed you the
miracle of landing on Mars talk to you
about the pan geo x-ray x-ray czar
reader we talked to you about using
optics to visualize molecules in real
time essentially providing a
computational microscope using tensor RT
or AI to inference in real time enormous
database of a pre simulated weather
pattern to go discover extreme weather
and then of course doing data analytics
at a pace and a speed
nobody's ever seen before this is the
benefit of accelerated computing and
finally we've been able to put it
together so this is what we talked about
today the HPC industry the HPC universe
is really expanding and an incredible
rate in every single direction
everything from arm CPUs that will now
have the entire capability if everything
that we've ever done in high-performance
computing to edge computing that has the
ability to now do AI and become make all
of our sensors become software-defined
2d gx2 with the right stack on top of it
now is a computational instrument the
partnership we have with Microsoft to
put world-class supercomputers in the
cloud and of course one of the best
things we've done Nvidia Magnum IO
which now helps tackle one of the giant
bottlenecks of networking and storage
and all of that software is made
available to you fully optimized and
we're going to work on it for as long as
we shall live and in my case quite a lot
of time and so I'm talking about decades
here I'll still be working on this and
accelerating software when when Ashley
sees the first person on Mars in the mid
2030s and so this is this is our
grandest piece of work and something I'm
super proud of and the world of
high-performance computing will never be
the same I want to thank all of you for
your partnership and collaboration over
the years thank you have a great
supercomputer
[Applause]
Title: NVIDIA DGX Station: Power of 400 CPUs
Publish_date: 2017-08-22
Length: 36
Views: 18439
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/6iKV1AQEScc/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 6iKV1AQEScc

--- Transcript ---

hi this is our djx station which we
announced at GPC in May you can see it's
up and running with four of our Tesla
v100 GPUs this is the highest
performance GPU available in the market
today one of the amazing things about
the djx station is because the GP is and
all the other major components are
water-cooled it's whisper quiet so this
brings you server costs performance to
your desk side
Title: NVIDIA RSNA 2021 Special Address: Accelerating AI Innovation in Medical Imaging
Publish_date: 2021-11-29
Length: 2572
Views: 99902
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/6io0uTtgVqs/hq720.jpg?v=61a5039d
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 6io0uTtgVqs

--- Transcript ---

[Music]
[Music]
i am a healer
[Music]
bringing clarity
to our most important questions diabetic
retinopathy can happen if blood sugar
stays too high over a long period of
time and comfort in times of uncertainty
what procedure am i having today you are
having a bronchoscopy
shining a light on a path forward
and finding answers
when every second counts
i am watching over our frontline heroes
creating a safer place
to do their life-saving work
[Music]
guiding them to faster answers
[Music]
and delivering care
wherever it's needed
i am opening new worlds of discovery
and mapping new treatments
for every individual
[Music]
creating a community
to collaborate with thousands
[Music]
and sharing knowledge that can heal
millions
[Music]
i am a i
brought to life by nvidia and brilliant
healers everywhere
[Music]
[Music]
welcome to rsna 2021
i'm david nawalni nvidia's director of
medical business development and
healthcare applications
the opportunity for ai in healthcare
specifically ai and medical imaging has
never been greater
i'm so honored to be here today to have
the opportunity to speak with you all to
share nvidia's value to the healthcare
market
the value to the ecosystem
some recent innovations that nvidia has
announced
and most importantly how they are
actually impacting the development of ai
powered medical imaging applications
the future of health care is
human-centric over the past century the
practice of medicine has become more and
more impersonal
driven in part by the adoption of new
technology and a lot of the clerical
burdens that come along with it
healthcare providers now spend more time
looking at a screen than actually
engaging with their patients
the era of ai and healthcare
will help remove some of these clinical
burdens and give our healthcare
providers superpowers that allow them to
detect disease earlier
make more precise diagnoses
make new physiological observations
and make health care more
patient-centric
i think dr eric topel the director and
founder of the scripps research
translational institute and one of the
most influential physician researchers
in the world said it best with the
following quote
by augmenting human performance ai has
the potential to markedly improve
productivity efficiency
workflow accuracy and speed
both for physicians
and for patients
what i'm most excited about is using the
future
to bring back the past to restore the
care and health care
and i think that's something that we all
can get behind
so with health care costs and physician
burnout rising preventable preventable
medical errors
rising to the third leading cause of
death you can definitely see the
opportunity for artificial intelligence
and health care is absolutely enormous
today the world knows of over ten
thousand diseases each of which need to
be discovered detected monitored and
treated
with more than two million
types of different medical devices
which lead to the development of tens
hundreds of millions of ai models across
the continuum of care what exactly is
needed to create these models
data
data is the raw material needed to
create impactful models
and data is something that's plentiful
in health care
it's crazy to think that 30 percent of
the data generated globally is
healthcare data and that number is
growing at 36 percent a year
as you can see here
some of that data is being put to good
use you're seeing exponential increases
in both the number of research projects
and commercial offerings of a ai enabled
applications
the number of pub pubmed papers on deep
learning and machine learning
increased more than 20x over the course
of the last 10 years the number of fda
approved softwares and medical device
applications has increased more than 30x
since 2014.
the global pandemic has had an
unprecedented what impact on the way we
live
the way we work and the way healthcare
is practiced
and though it's been a tough couple
years if there's any silver lining to
the challenges that we've all been
through
the acceleration of digital adoption and
healthcare
is absolutely phenomenal
according to studies by both mckinsey
and deloitte
digital adoption was accelerated five
years in just a matter of eight weeks at
the start of the pandemic an adoption of
a.i by healthcare organizations has
increased over 73 percent in that same
time
since the start of the pandemic ai has
been used to provide predictive
analytics for earlier interventions
ai for clinical decision support
enabling clinicians to operate more
efficiently and care better for patients
ai chat bots and call centers using nlp
technology helped initially triage
patients
look at their symptoms and save time by
bringing convenience to consumers and
patients
prior to the pandemic the healthcare
industry only scratched the surface of
what a.i could do
postpandemic it's clear
that the era of ai has officially begun
how is nvidia actually making this dream
of ubiquitous ai and healthcare a
reality nvidia is democratizing ai for
the global healthcare industry by
enabling developers to easily adopt
artificial intelligence
from cloud to edge to device by
providing a healthcare specific
application computing platform
that's called clara
nvidia's healthcare-specific full-stack
computing platform allows developers to
quickly and efficiently
integrate ai into their existing devices
develop new ai powered devices
and build training infrastructure
leveraging a common hardware and
software infrastructure
so what exactly is a healthcare specific
accelerated computing platform
really comes in three layers
consists of a base layer of hardware and
systems
think of these as nvidia's dgx egx rtx
jets and platforms
on top of that you have an acceleration
layer
led by the likes of cuda triton tensor
rt and rapids
and the top layer is when you start
getting applications specific
that's the layer of clara that's where
you get application specific sdks
reference applications and pre-trained
models which help remove some of the
undifferentiated heavy lifting from
healthcare and medical solution
developers
additionally nvidia offers enterprise
software for running ai and data
analytics this helps
manage the end end software development
with things like base command ai and
data analytics can be managed with
things like nvidia ai for enterprise and
i'll talk a little bit more about how
that is making an impact in health care
later and all of these applications can
scale across a distributed edge
infrastructure
by being deployed using nvidia fleet
command
so nvidia is really providing an
end-to-end infrastructure in terms of an
accelerated computing platform
application specific reference designs
sdks pre-trained models but then also
providing a foundational software
infrastructure to actually deploy and
build these ai services
so
as you can see nvidia is building a
foundation for the healthcare medical
industry that's going to allow medical
imaging solution providers from
diagnostic imaging devices like ct
mri x-ray and ultrasound to enterprise
imaging applications like pax and vna
all of these can be accelerated using
their ai powered solutions from embedded
devices
to edge to data center to cloud
so throughout this special address i'm
going to take you on a journey from ai
developers
that's going to focus on new innovations
in nvidia that are specific to each of
these following areas
ai and medical devices real time
ai at the edge
enterprise ai at the edge
and ai development in both the data
center and the cloud
our first stop is going to be where data
is both generated and ai inference
algorithms are run leading medical
imaging companies are leveraging
nvidia's accelerated computing platform
and ai infrastructure to deliver
solutions that provide a step function
in patient care
let me share a few examples
nvidia powers the phillips spectral ct
7500 this is a recent mini award winner
for the best new radiological device of
2021.
the spectral ct has demonstrated a
higher sensitivity into in detecting
malignant findings it has improved
readings of incidental findings
this helps minimize the need for
additional confirmatory imaging leading
to a 34 reduction in time to diagnosis a
25 reduction in repeat scans and a 30
reduction in follow-up scans
qt imaging is leveraging nvidia
technology in their quantitative
transmission ultrasound to create a true
3d imaging platform for breast imaging
without harmful radiation or painful
compression
standard slice by slice reconstruction
does not work with an ultrasound but a
3d algorithm transforming volumetric
data to volume image data gives an
exceptionally high resolution high
contrast to noise ratio and
quantitatively accurate
mr type image
but takes 100 times too long using
standard compute architectures
really requires super computer to run
enter nvidia
using cuda along with nvidia a100s
this quantitative transmission
ultrasound is now a reality and it's
really something that is exceptionally
innovative in this specific space
another two applications i want to
highlight the nvidia powered striker
arrow true ct is the only 32
slice point of care ct scanner that
combines self-propelled mobility with
the largest industry's largest
inter-bore it's highly mobile
and provides flexibility in patient
positioning and that's really where
nvidia comes in in terms of being able
to remove the noise and create an
absolutely beautiful image even while
the patient may be moving within uh the
ct scanner
and then also you have the carestream
drx evolution plus system this is a
fully integrated intelligent x-ray room
that sets a benchmark for imaging
quality and performance
and incorporates new features like
ai-powered smart noise cancellation
that again powered by nvidia technology
additionally we highlight a lot of
imaging examples
but
ai computing and 3d graphics are being
used across a variety of different
applications whether it be specific
medical imaging but also endoscopy
medical robotics interventional
radiology and even digital pathology a
lot of applications for the technology
that nvidia is providing
so looking at medical devices and
specifically honing in on medical
imaging applications
everyone seems to be exploring ai and
robotics whether that's from
image reconstruction
to you know beginning with streaming
data uh
image enhancement ai assisted computer
vision
ai assisted
surgical navigation and even ai assisted
physics like signal processing or beam
forming all of these are requiring the
power of ai and accelerated computing
that said medical devices are now going
through a revolution where sensor data
can now be augmented with software
in advanced computing that really enable
these absolutely new innovations
diagnostic imaging radiation therapy
endoscopy medical robotics and even
genomics
all have strikingly similar
processing pipelines one that starts
with sensor data and then is processed
creating a visual output
of clinical decisions that can be made
from
the challenge to implementing this
medical device data pipeline is a
computing platform
that has both the ability to do
real-time processing of large complex
data streams
perform advanced computer graphics
and take advantage of ai skills running
on the device at the edge
while still being upgradable
as ai skills improve
four exceptionally challenging tasks
so
a couple weeks ago
to help the medical device developers
address these challenges
on november 15th at gtc fall nvidia
introduced clara holoscan a new edge
computing platform specifically for the
healthcare medical industry clara
holoscan provides the computational
infrastructure for scalable software
end-to-end processing of streaming
applications all those challenges i just
highlighted in the previous slide
are addressed by clara holoscan
this new application specific computing
platform seamlessly bridges medical
devices with edge servers allowing
developers to create ai microservices
that run ultra low latency streaming
applications on the device while passing
some of the more compute heavy tasks to
data center resources
clear holoscan allows developers to
assemble complex pipelines with graph
composer and intuitive easy to use and
easy to deploy
the output as containers
clear hollow scan is enabled with nvidia
fleet command
allowing devices to be remotely managed
orchestrated and updated
enabling a brand new model software as a
service model for the medical device
medical imaging community
what really this is doing which i think
is
absolutely innovative
is
a lot of data today is being moved to
the cloud and with that you have an
unbelievable ecosystem of microservices
sitting on top of it
there is a need
for an ultra low latency platform at the
edge
and that is really where clear a hollow
scan comes in it's really creating that
device to edge to cloud infrastructure
which i think we all know is the future
of many industries
medical devices healthcare applications
and medical imaging included
so to help ease the development of these
applications in addition to the clara uh
holoscan sdk that we just introduced
we also have a clara hall scan
development platform
this innovative hardware platform's
based on the
jetson agx oren
12 core chip featuring an rtx
a6000 and a connectx 7 high-speed
networking chip
this device also has a variety of i o
hdmi pci e ethernet wi-fi and also gpu
direct rdma for data streaming
so to accelerate the development of
these medical devices we actually
already have been working with a variety
of partners
to create i o cards
that allow
basically allow
video capture for endoscopy video
capture for microscopy
we also have uh front-end devices
specifically for software-defined
ultrasound systems
so with that we're really providing both
a hardware and software platform for the
future of healthcare the future of
software-defined medical devices
so
taking a step back we've talked about
medical devices we've talked about edge
computing
where do all of these
ai skills and algorithms get created
and that's really in the ai training
infrastructure so we can spend a little
bit of time talking about the ai
training infrastructure and then we'll
close out the conversation talking about
the ai enterprise
so
one of the areas where ai infrastructure
and
ai just in general is making its biggest
impact is in cancer research cancer is
one of the most challenging
multi-disciplinary fields utilizes a
huge amount of data driving multiple
decisions across multiple departments
and nvidia's accelerating computing
platforms
like clara and monae are putting the
most important technology in the hands
of the world's leading cancer
researchers
by leveraging nvidia dgx super pods
machine learning insights are able to
guide data-driven decisions and improve
patient outcomes
for example early detection and patient
risk assessment for pancreatic cancer
accelerating radio surgery planning
improved detection of hard to find
cancers via deep learning on digital
pathology
improving screening of breast cancer
patients using machine learning
specifically a research center dkfc has
pioneered many of the deep learning
methods for medical imaging inventing
industry standard biomedical image
segmentation frameworks that are used by
researchers all over the world
let's talk about some of the
technologies that not only these cancer
researchers are using but also many
researchers specifically focused on the
development of ai
nvidia in collaboration with king's
college london and an esteemed open
source foundation of collaborators
have been building the most advanced
medical ai network monae
the medical open network for artificial
intelligence
consists of
specific tools workflows
all accelerating ai application
development
when i consists of tools for data
labeling
optimized training multimodal training
both text and imaging data and when is
expanding to include both batch and
streaming deployments as it moves
forward
in the latest release coming this week
at rsna
it also includes automl specifically for
including researcher throughput
by providing optimal networking search
capabilities
this version will also be the first to
include self supervised learning
workflow in monae and this is going to
allow researchers to generate
pre-training weights with unlabeled data
that can be fine-tuned for downstream
tasks with limited labeled data
monae is in absolutely high demand with
scientists you can see the exponential
growth that it's seeing and averaging
more than 3 000
downloads per week
monai is truly becoming the foundation
of these enterprise machines like nvidia
clara and a growing list of different
data integration partners
leading cloud service providers like aws
microsoft azure and google cloud
platform are all developing medical ai
offerings based on monae
for end to end model life cycle
management ml ops platforms are building
on top of clara monae and taking them
into each state of ai development from
data labeling training federated
learning
to experiment outreach and orchestration
monai truly is the pie torch of health
care and the community continues to grow
and expand its reach
to ensure that when i is optimizing
these training environments
nvidia
organizations
are natively integrated and they can be
run in nvidia's base command platform
using automated and mixed precision
scaling point gpus and multi-mode monae
optimizations to reduce 3d brain tumor
segmentation training by up to 30x from
a single experiment down to
four experiments an hour
pretty amazing acceleration
30x improvement single experiment in a
day to doing four in an hour
that said nvidia data scientists ranked
first second and seventh
among over 2000 submissions in the
september makai brain tumor segmentation
validation phase challenge
the nvidia winners all utilized the open
source pytorch framework of monae
and the final results of this challenge
will be announced at rsna this week
monae truly is the world's most advanced
medical ai framework it's healthcare
specific enterprise supported and
optimized to maximize productivity
we talked a lot about developing some of
these ai algorithms
one of the biggest challenges
is data
the hardest part of ai sometimes isn't
even creating the ai it's generating the
data to create the ai
ai and machine learning are
exceptionally data-hungry processes they
need vast amounts of data in order to
run and deliver meaningful insights
data quality and data preparation become
paramount when ai and ml are involved
and regulated industries like healthcare
data security and data privacy become
even more of a concern
enter federated learning
federated learning is a unique way to
develop
and validate more accurate and
generalizable ai models from a diverse
data set mitigating the risk and
compromising data security and privacy
while leveraging common frameworks like
pytorch tensorflow moni
and also being able to utilize ml
platforms like aws microsoft google
flywheel american college of radiology
rhino health and of course nvidia
federated learning enables ai models to
be built with a consortium of data
providers without data ever leaving the
individual site
it's truly ai models training other ai
models
we all know that more data and better
data leads to a better model federated
learning truly offers a solution
providing the benefits of larger data
sets without the risks of data security
and data privacy which we all know are
paramount in health care applications
federated learning isn't just a dream it
is something that's happening today it's
you know went from coming soon to the
time is truly now
here are a couple of the use cases that
we've been working on the first one
national institute
national cancer institutes early
detection research networks
collaborating to develop ai models for
diagnostic imaging to detect early signs
of pancreatic cancer
erasmus a leading university research
center based in the netherlands is using
nvidia flair
for its ai applications in generic
analysis
one of the most important and actually
made a uh
a presence in nature magazine is a
collaboration between nvidia and
mass general brigham last year to bring
together 20 hospitals from around the
world
to create a federated learning algorithm
and this global algorithm this global
model outperformed any locally trained
model and this was actually published in
nature
minnesota fairview published a machine
learning study specifically
using nvidia flair use real world models
for covid diagnosis
and last melody was a
a global drug discovery consortium based
in the uk that demonstrated how
federated learning techniques could give
pharmaceutical partners the best of both
worlds the ability to leverage a large
collaborative
drug compound data set without
sacrificing data privacy
so how do we actually bring federated
learning products to market
we do it with products
like nvidia flair nvidia is making it
easier than ever for research in the
industry researchers to adopt federated
learning
today
we are announcing nvidia flair
this is nvidia's federated learning
application runtime environment this is
a domain agnostic open source extensible
sdk that allows researchers and data
scientists to adapt
existing
machine learning and deep learning
workflows like pytorch rapids and nemo
to a federated learning paradigm it
enables developers
to use this platform to build secure
privacy preserving offerings for
distributed multi-party collaboration
flair makes it possible for researchers
to run real world collaborations with
different federated learning strategies
to advance state-of-the-art ai
for healthcare
and a variety of other industries it
really enables platform developers to
keep their data secure but really build
the best models they possibly can
nvidia is making nvidia flare open
source
and the main reason to that is to
catalyze
the pace of innovation and really
leverage the community in terms of the
development of federated learning in
this nascent stage really to expand and
grow the market
it aims to create an ecosystem of
federated learning domain researchers
platform developers data scientists and
other open source tools to collectively
mature the technology stack
needed to establish federated learning
is the de facto learning paradigm at the
edge
nvidia flair will be used to power
learning solutions at biomedical
research data platforms at leaders like
flywheel and rhino health
flywheel's exchange platform enables
users to access and share data and
algorithms for biomedical research
manage federated products for analysis
and training and then choose their
preferred federated learning solution
including nvidia flair
and at rhino health a partner and member
of the nvidia inception program
has integrated the nvidia flair sdk into
its federated learning solution
which is helping researchers at mass
general hospital develop an ai model
that more accurately diagnoses brain
aneurysms and is being used by the
national cancer institute's early
detection of research to develop and
validate
medical ai models that identify early
signs of pancreatic cancer
today
the american college of radiology
is introducing
ai lab
with new federated learning capabilities
all of this is based on nv flair to
orchestrate the training the model
sharing and the model aggregation all
sitting on top of the american college
of radiology's acr connect
acr connect is a
developed software that can be installed
on premise and
offers a variety of medical imaging
analysis tools
the ai lab sits on top of acr connect
and really provides even non-technical
users the ability to create and evaluate
ai models through an easily navigatable
graphical user interface
let's walk through how acr is using the
federated learning process
to begin
acr is using a central server to pass a
base model which was developed
previously to perform the same or
similar task but found to have non-ideal
performance
this acr connect instance is at each
participating facility at each facility
the base model is then trained on a
local data set prepared at the site
which is used to create a new
site-specific model new models from each
site will then be passed back to acr for
server integration or server aggregation
the process of aggregation will create a
single ai model it'll be used as the
base model for the next round of
training when repeated over many many
rounds this process will produce a final
ai model that performs well at all
participating facilities
a really excellent example of exactly
how
federated learning should be and could
be and will be used in the future
so at this point now we've talked about
ai medical devices at the edge we've
talked about medical devices we've
talked about uh deployment or i'm sorry
we've talked about uh development
the last piece is the enterprise
how do we actually leverage this
at the enterprise level
and the unique piece is unlike medical
devices and real-time edge computing
platforms and even the ai development
systems which are all developed mainly
by medical device manufacturers
healthcare isvs
enterprise i.t are generally systems
that are owned and managed by a separate
entity in many cases the healthcare
provider organizations and that provides
a whole new set of challenges
the first of these challenges is to
virtualize
an accelerated computing ai
infrastructure that's needed to develop
manage and deploy these next generation
of ai models
it's really time to unify
the hospital data center it's time to
leverage a common infrastructure that
supports both hospital applications
today but also accelerated computing
applications in the future
you can see
moving forward down the path that we're
on
many of these healthcare institutions
went up having a variety of edge
computing boxes they'll end up having a
variety of infrastructure that are all
specific to that medical device are
specific to that healthcare isv
and it'll really be the task of the
healthcare it team to try to own and
manage that
at the end of the day having a
standardized
data center
that's really able to handle all of
these next generation accelerated
computing pieces of infrastructure is
what's going to be key and that's the
answer is converge and accelerate how do
we create a single infrastructure that
hosts all applications for today for
tomorrow a platform that's converged ai
ready application centric
enable
data center scale computing at the
enterprise level
both nvidia and vmware are working
together to transform the data center to
address these challenges and barriers
and bring ai modern workloads to every
single customer that can securely run
modern accelerated workloads alongside
their existing applications it's really
just changing the face of it
and creating a new it that is
specifically for ai
at these healthcare providers
so
this is something again we've introduced
a while ago but we are now bringing
nvidia ai
enterprise
to the healthcare community and really
this is the key to unifying the hospital
data center with an enterprise class ai
operating system for hospitals and
healthcare isvs around the world
and as i mentioned this platform is
called nvidia ai enterprise nvidia
enterprise is optimized
to run ai workloads at near bare metal
performance with new optimizations
for gpu accelerators
on vmware vsphere this includes support
for the newest latest and greatest
ampere architecture and additional
technologies like gpu direct
communications
so vmware is really delivering on the
next generation of cloud infrastructure
for next generation apps
and nvidia is extending ai to every
enterprise every data center every cloud
and every edge
the nvidia enterprise has been certified
by vmware starting with vmware 7
and customers can deploy end-to-end ai
solutions that they know will work and
can be folded into their existing data
center infrastructure
nvidia enterprise support is part of the
software suite so enterprises can ensure
their mission critical applications in
their mission critical projects stay on
track with access to nvidia experts
since the launch at gtc spring nvidia ai
for enterprise has established itself as
a platform for the ai powered
applications and quickly built out an
ecosystem of partners customers and
other collaborators
the first of these partners are oems
like dell hp and a host of others these
oems provide the foundation
ai for enterprise with nvidia certified
systems these nvidia certified systems
conform to nvidia's design best
practices and have passed a set of
certification tests
that best they create best system
configurations for performance minute
bill manageability scalability and
security really providing a great
foundational element to build upon
looking at one of our first examples the
netherland cancer institute one of the
world's leading cancer research and
treatment centers has used nvidia ai
enterprise software
to test ai workloads
on higher precision 3d cancer scans that
are most commonly used today
the netherland cancer institute's ai
model was previously trained on much
lower resolution images
but the higher memory capacity offered
by the nvidia ai enterprise its research
could use high resolution images for
training these improvements help
clinicians much better target the size
and location of the tumor every time a
patient receives treatment
the cancer institute was able to get a
quick start to their project
by leveraging something called nvidia
launch pad which actually provides
immediate access to optimize software
running on an accelerated compute
infrastructure using our colo partner
equinex
it helps customers prototype test
and
start deploying their workloads
in a very very efficient manner
in addition to healthcare providers
medical imaging isvs like icad and vyasa
have embraced nvai enterprise as a
platform to build upon
and specifically validate 2. this not
only speeds up their development time
but it also provides a target gpu
accelerated hardware platform that their
customer base can then take
drive consistency and implement so no
longer do they need to be responsible
for the hardware that their software is
being run on they can just specify an nv
certified system
and nvidia ai for enterprise
and with that
they know that their application will
run
so let's take a quick look at how icann
and vyasa two isvs that have optimized
nvidia ai enterprise
can be easily integrated into existing
hospital environment to accelerate ai
development
one in eight women will develop breast
cancer in their lifetime the key to
improving their outcomes is regular
screenings
ai can dramatically speed up this
process but integrating ai solutions
into existing infrastructure is complex
more than half of ai projects never make
it into production nvidia ai enterprise
helps streamline that integration with
nvidia certified systems and an
end-to-end cloud-native suite of ai and
data analytics software it enables the
rapid deployment management and scaling
of ai workloads with nvidia ai
enterprise both new and existing
applications can run side by side and be
centrally managed in an environment i.t
administrators are already familiar with
let's take a look at two applications
which leverage ai to improve breast
cancer detection and treatment vyasa
uses a deep learning analytics solution
to quickly search and filter patient
records including unstructured records
by simply asking a series of questions
here we filter by age time since the
last visit and other risk factors now
let's look at icat's profound ai
software it can act as an extra pair of
eyes for radiologists reviewing
mammograms here we see a 2017 scan being
compared to a 2018 scan icat's
application is able to automatically
identify an area of suspected cancer and
generate an accurate short-term breast
cancer risk estimation that is
personalized for each woman with nvidia
ai enterprise hospitals can put modern
solutions like vyasa and icad into their
caregivers hands without changing
existing infrastructure to learn more
visit the links in the description
it's always amazing to see the power of
nvidia's accelerated computing
technology and the ai enterprise
infrastructure working together to
create innovative needs of the patients
clinicians and health care professionals
thank you to icann vios for putting
together that demo
as i start to bring this special address
to a close i'd like to just take a quick
review
of what we discussed here today
first and foremost
hopefully you leave this conversation
knowing that nvidia's democratizing ai
for the global healthcare industry
by enabling developers to quickly and
easily and efficiently adopt artificial
intelligence from the cloud to the edge
to the device by providing healthcare
specific accelerated computing platforms
and all of that starts with the
foundation of nvidia clara
clara holoscan
is a new computing platform for the
healthcare industry expanding what clara
has already built specifically
as a computational infrastructure needed
for scalable software-defined
end-to-end processing of streaming data
on medical devices
really again think of this as an
innovation that will continue to evolve
but creating
basically all a platform for
microservices a platform to really uh
create a software as a service model for
ultra low latency edge computing
applications
also note that nvidia in collaboration
with kings college and a open source or
open
source foundation of collaborators is
continuing to drive innovation within
monae
something that is you know really the
the pie torch of health care this is
something that is specific it's going to
become ubiquitous and it's key for
anyone who's looking at building
effective efficient ai models
and then also just what we announced
today
nvidia flair
is an open source sdk specifically
designed to make federated learning
easy and accessible to the entire
community really living up to that
mission of democratizing ai for all of
healthcare
and lastly we talked about what exactly
is nvidia doing in terms of bringing
ai
to the enterprise
and they're doing that
with
nvidia ai enterprise
helping to unify the hospital data
center by providing an enterprise-class
ai operating system to hospitals and
healthcare isvs around the world
so thank you all for taking time under
busy schedules to hear what's new from
nvidia at this rsna
just as the theme of rsna said
2021 is redefining radiology i hope you
can see that nvidia and nvidia
technology is playing a key part in
redefining not only radiology but also
the healthcare industry as a whole
i kindly ask that you tune in to some of
the other talks we have this week from
other nvidia leaders customers partners
and collaborators if you're on site
please check out some of our partners
showcasing nvidia technology in their
booths
for those of you online please visit www
backslash rsna to learn about more of
the products you heard here today
thank you all for your time and i hope
you enjoy rsna 2021
you
Title: NVIDIA CES 2022 Special Address
Publish_date: 2022-01-04
Length: 2911
Views: 3356535
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/6KoscQVKpAQ/hq720.jpg?v=61d482c5
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 6KoscQVKpAQ

--- Transcript ---

Jeff Fisher: Hi everyone.
 Happy New Year and welcome to our special event.
I will kick it off with gaming and creators,
and then Ali Kani will talk about
developments in autonomous vehicles.
So let’s get started...
This, this is beyond a game.
This is an epic journey into a vast unknown
through countless ages and across infinite galaxies
of untold beauty and unfathomable danger.
Where you carve your own path,
never veering, never waning.
Always outnumbered,
never outgunned.
Chasing down destiny
as you peer into the eyes of Defeat
 and quietly say, “You’re next.”
Yes.
This is what you’re here for.
This is why you came.
This is your world.
So, what are you waiting for?
RTX. It’s On.
Gaming is huge.
People no longer ask if it will keep growing.
Of course, it will.
Now the question is—how big will it get?
There are almost 3B gamers in the world,
with 150M more people gaming this year.
Today, the market for games and gaming hardware
 is almost $300B.
Up $70B from just 3 years ago.
Esports continues to be an important driver.
League of Legends Worlds 2021 Finals
 drew 74M peak concurrent viewers.
 The 2021 DOTA2 International paid out a $40M prize pool.
All told, Esports has an online audience of 460M viewers.
And people are tuning into all types of gaming content.
 There are now 10M unique streaming channels on Twitch,
 doubling over the past 2 years.
 750M people watched gaming content online in 2021.
And the gaming universe is expanding into
virtual worlds with their own economies.
Roblox has attracted 200M monthly users.
Lil Nas X hosted a concert inside Roblox in front of 33M fans.
This is also fueling Virtual Reality, where headsets are expected to grow 4 times in the next 4 years.
But it’s not just about playing.
There are 45M professional and freelance creators
 fueling an explosion in digital content.
They will become the builders of our 3D digital worlds.
And as the gaming experience extends into
these vast, open virtual worlds,
some see an opportunity in the trillions.
Over the past 20 years, we have dedicated ourselves to build the best platform for gamers and creators.
Not just to delight our customers, but to develop graphics technologies that enable game developers and creative ISVs to
do some really amazing work.
At the heart of the platform is our GPU, and a history of revolutionary architectures, like RTX.
All told, we have shipped almost 1.5B GPUs.
And on top of the hardware comes a massive investment
in our software stack
that harnesses the GPU to deliver graphics innovations
like game-ready drivers, 
DLSS, GSYNC, Reflex, NVIDIA Broadcast, Max-Q,
 Omniverse, and GeForce NOW.
With billions of devices unable to play the latest games,
 it is no surprise that cloud gaming is expected to grow to
over 100M subscribers by 2024.
For these gamers, we built GeForce NOW.
 Delivering GeForce-class gaming to over 15 million members around the globe
on Chromebook, Macs, SHIELD TV, Android and iOS phones, and under-powered PCs.
GeForce NOW has over 1,100 PC games, 
including 90 free-to-play,
 from the leading PC game stores, including Steam,
Epic Games, Ubisoft Connect and EA Origin.
And extending our partnership with EA,
I am excited to announce that Battlefield 4 and Battlefield 5 
are available on GeForce Now starting today.
Be sure to tune in to GeForce NOW Thursdays, where every week we onboard new games for our members.
We’ve also been working with the world’s largest TV manufacturers to bring GeForce NOW to millions more living rooms.
Last month, we launched a beta version of GFN on LG 2021 webOS Smart TVs.
Today, we’re pleased to announce a partnership with Samsung to integrate GeForce NOW into their Smart TVs.
Starting in Q2, Samsung Smart TVs will be streaming PC-class gaming with GFN.
The rapid expansion of 5G offers the potential for low-latency cloud gaming everywhere.
So we’ve teamed with AT&T, and their Fast, Reliable 5G network, to bring the power of PC gaming to mobile devices.
As a 5G Technical Innovation partner, GeForce NOW on the AT&T 5G Network offers ultra-responsive cloud gaming on-the-go.
We also have an exclusive offer to announce.
Starting today, AT&T customers with a 5G device on a qualifying plan, can get a 6 Month GeForce Now Priority Membership
at no charge.
GeForce Now members get access to the latest GeForce technology.
Our new RTX 3080 tier of service, powered by the GeForce NOW SuperPODs, is the first to support 120FPS at 1440p on PCs,
up to 1600p on MacBooks and 4K with HDR on SHIELD TV.
Available today at just $99 for a 6-month membership.
Play the most demanding PC games, no waiting, no downloads, on the latest GeForce RTX hardware in the cloud.
RTX momentum continues to build.
Ray Tracing and AI are defining the next generation of content.
Since last CES, we have added over 150 RTX games and applications.
And gamers have logged over 1 billion hours of RTX playtime.
The Day Before from Fntastic is an open-world MMO survival set in a deadly, post-pandemic America overrun by
flesh-hungry infected and survivors.
It is in the TOP-10 Most Anticipated Games on the Steam Wishlist.
Today we are announcing The Day Before will launch June 21st with RTX ON, 
including Ray Tracing and DLSS on GeForce and GeForce Now.
Here’s a first look at The Day Before with RTX ON.
With over 70 million players, Tom Clancy’s Rainbow Six Siege
from Ubisoft
has become one of the biggest competitive shooters.
The next game in the series, Rainbow Six Extraction, 
gamers will compete in a tactical co-op FPS as they engage,
contain, and eliminate the mysterious creatures
known as the Archaeans.
Today, we are announcing that Rainbow 6 Extraction,
 launching January 20th, will be RTX ON, featuring DLSS,
and available on GeForce and GeForce Now.
Here is a first look at DLSS in Rainbow Six Extraction.
Escape from Tarkov is an online first-person action game. 
Players battle through the war-torn city of Tarkov, 
which has been sealed off by UN and Russian military forces.
Escape From Tarkov quickly became one of the top 10 most played competitive shooters.
Last June, RTX gamers gained a competitive edge with NVIDIA Reflex.
We are extending our partnership with Battlestate Games to bring more performance with the addition of DLSS.
Let’s take a look.
Dying Light, the tremendously successful open-world action RPG by Techland, 
had over 23M players worldwide.
The highly anticipated sequel, Dying Light 2, will be launching February 4th with RTX ON,
 featuring Ray Tracing and DLSS.
It’s available on both GeForce and GeForce NOW.
Here’s an exclusive look at never before seen RTX ON action.
For competitive gamers, milliseconds are the difference between winning and losing.
We asked gamers to check out the benefits of lower latency gaming
 with our system latency challenge.
Early results show up to 2 times aiming improvement, 
and this is why 8 of the top 10 competitive shooters have integrated
NVIDIA Reflex - our low-latency gaming platform.
And gamers love it. 
Over 20 million GeForce players compete with Reflex ON each month.
And the hardware ecosystem continues to grow.
Our Reflex Latency Analyzer allows players to easily measure system latency with a single click of the mouse. 
We now have over 50 Reflex mice and monitors from 16 partners.
While Reflex is critical for competitive matches, lower latency is important in any game,
 as it better connects the gamer, and their physical actions, to those in the game.
Today, we are announcing 7 more Reflex games, and extending Reflex into new genres.
These include iRacing, where you can steer with better precision in the world's premier online racing simulator;
Rainbow Six Extraction, where Reflex has your 6 as you confront Archaeans;
and Sony’s award winning God of War—
beat the boss with better dodges and combos when it launches on PC on January 14th.
Let’s take a look.
With the global growth in esports, the demand for Esports displays has been doubling each year.
We believe the time is right to take Esports to a new level.
Today, we are announcing a new category of Esports Displays.
1440p, 27” GSYNC and up to 360Hz,
 with new Esports Vibrance and Reflex Latency Analyzer.
These displays are also Dual-Format.
 They can switch to 1080p 25” if that is preferred for certain games.
Here is Seth to fill in the details..
1080p displays have ruled Esports for over a decade,
as the lower resolution has enabled
higher framerates and faster refresh rates.
But our research shows that larger, higher resolution panels improve aiming performance,
providing larger and more detailed targets to acquire.
In fact, 1440p 27" displays can improve aiming by up to 3% over traditional 1080p 24" displays.
For competitive gamers, that's huge.
With today’s high-end GeForce RTX GPUs, 
rendering Esports games well above 360 fps at 1440p
the industry is primed for a change.
Today, NVIDIA is announcing a new class of 1440p 27-inch G-SYNC Esports displays.
These displays feature industry-leading refresh rates of up to 360Hz for the smoothest, lowest latency gaming.
Esports-tuned vibrance color profiles powered by mini-LED backlights and oxide transistors
make targets pop from the screen.
An enhanced, easier-to-use Reflex Analyzer automatically measures system latency,
so gamers can head into competition
with full confidence in their PC’s performance.
And if gamers still want to play a match at 1080p, 
the monitor features a special Dual-format 25” 1080p mode.
The future of Esports monitors is here 
with new displays available soon from AOC, ASUS, MSI, and Viewsonic.
A decade ago, we introduced the 50-class GPU.
Offering gamers great performance, on the latest architecture, at an affordable price.
It was the first step up into GTX gaming.
In fact, 3 of the top 5 GPUs on Steam are GTX 50-class,
and one of the most successful is the GTX 1050.
When it was released in 2016, the GTX 1050 could power the top games of the time at over 60 fps.
But the production value of games continues to rise.
And modern games need much more GPU horsepower.
The GTX 1050 struggles to keep up.
Even a 1050 Ti or the more modern GTX 1650 can’t power the latest games at 60 fps.
Today, we are announcing the RTX 3050.
The GeForce RTX 3050 brings the performance and efficiency of the Ampere architecture
to more gamers than ever before.
It powers the latest games at over 60 fps.
Based on our Ampere architecture, 
the 3050 comes equipped with 2nd generation RT cores for ray tracing
and 3rd Gen Tensor cores for DLSS and AI.
For the first time, you can play ray-traced games on a 50-class GPU at over 60 fps.
Ray tracing is the new standard in gaming and the 3050 makes it more accessible than ever.
The RTX 3050 comes equipped with 8 GB of G6 memory.
Starting at just $249, it will be available worldwide on January 27th from all of our partners.
With 75% of gamers still on GTX GPUs, this is the perfect time to step up to RTX.
We are at the dawn of the next digital frontier.
Interconnected 3D virtual worlds,
whether for commerce, entertainment, creativity, or industry,
are being built today.
These worlds are boundless and will be populated with shops, homes, people, robots, factories, museums…
an infinite amount of 3D content.
And they will be built by an expanding number of creators, collaborating across the globe, 
constructing these amazing virtual environments.
For this future, we’ve built NVIDIA Studio – a fully accelerated platform for creators.
It starts with RTX, the most advanced GPU architecture 
with hardware-acceleration for Ray Tracing, Simulation, and AI,
and NVIDIA high-performance video processors.
The NVIDIA Studio software includes specialized drivers and dozens of SDKs, 
which accelerate over 200 of the industry’s top creative applications.
We also offer our own Studio apps, 
with the latest addition, NVIDIA Omniverse for 3D Design and collaboration.
Today we are announcing that NVIDIA Omniverse is out of Beta 
and generally available to GeForce RTX Studio Creators.
3D content is challenging to produce. It typically requires multiple tools, which are often incompatible 
and then you have to wait for long render times.
Omniverse brings over 20 years of NVIDIA’s groundbreaking work in graphics, AI, simulation, and compute, 
into a single platform to transform 3D workflows.
Let me explain how it works..
Omniverse connects independent 3D design worlds together into a shared virtual space.
The connecting language of Omniverse is USD, or Universal Scene Description,
which you can think of as the “HTML of 3D worlds."
Today, a 3D artist typically works sequentially across multiple applications.
Like 3ds Max for modeling, 
then Substance Painter for texturing, 
and finally Unreal Engine to arrange the scene,
exporting and importing large files many times along the way.
With Omniverse, artists connect their apps and then compose the combined scene using Omniverse Create.
Once in Omniverse, an artist can draw on NVIDIA’s superpowers..
Like Physics, which can let artists use true to reality simulations that obey the laws of physics.
And RTX Renderer, to see the scene in real-time: fully ray traced, or path traced.
Omniverse also lets you collaborate with another artist, 
from across the room or across the globe, 
connecting their favorite apps into a single shared scene.
Changes made by one designer are reflected back to the other artist,
like working in a cloud share document—but in 3D.
This is the future of 3D content creation and how virtual worlds will be built.
We are partnering with over 40 ISVs to transform 3D workflows.
Today, Omniverse is connected to leading applications
 like 3ds Max, Maya, Blender, Unreal Engine, and many more to come.
We now have several new additions to Omniverse.
Today we are announcing an even easier way for artists to collaborate with Nucleus Cloud.
Now in early access, Nucleus Cloud simplifies Omniverse scene sharing—
it’s one-click-to-collaborate and your entire 3D scene is online.
Leading 3D marketplaces are now featuring Omniverse-ready assets. 
Check out the collection of free assets now available
in the Omniverse launcher.
Omniverse Machinima has been a hit with creators who love to game.
It lets you remix and recreate your own game cinematics with thousands of game assets and environments.
We are now adding Mechwarrior5 and Shadow Warrior 3 assets to the Machinima library.
Audio2Face is a revolutionary AI-enabled app that easily animates a 3D face with just an audio track.
We are now supporting blendshape and direct export to Epic’s MetaHuman.
AI, powered by NVIDIA Studio, opens up new capabilities for creators.
Individual artists can now do the work of large teams.
Application developers are quickly embracing the power of AI for content creation.
Adobe Lightroom adds AI to quickly create detailed masks around subjects
Topaz Labs Video Enhance AI uses AI to upscale videos retaining higher details than traditional upscaling algorithms
Adobe 3D Substance Sampler uses AI 
to generate high-quality photoreal 3D materials from photographs.
And Notch uses NVIDIA Maxine pose estimation to enable creators to drive a full-body rig with a single webcam.
NVIDIA’s Studio also features Canvas, 
our application that instantly converts brushstrokes into photorealistic images.
Today, we are announcing a major update to the Canvas AI.
Built from NVIDIA’s GauGAN2 research, the latest app produces images that are 4X higher resolution
with more detailed elements.
It also adds new materials like flowers and bushes.
The new Canvas app is available free to download at nvidia.com
Gamers and creators have made GeForce high-performance laptops the fastest-growing category in PCs.
In the last year alone, 35% more RTX laptops were sold.
And creators have fueled a 3x jump in Studio laptop sales.
We are announcing over 160 new laptops, all built around our RTX 30-series GPUs.
They include a wide range of form factors, like dual-screen, easels, and convertibles.
There are new 14” portable powerhouses and even more G-SYNC and 1440p displays.
As more gamers and creators turn to laptops for their primary PC,
performance expectations continue to increase.
At the same time, they are demanding sleek form factors and longer battery life.
In 2017, we reimagined gaming laptop design with an extreme focus on efficiency.
We developed a system design approach that delivers high performance in thin gaming and studio laptops.
We called it Max-Q.
It has fundamentally changed how laptops are built.
With the help of AI we optimize everything: 
the GPU, CPU, fans, display, board design, laptop construction, 
even optimal settings for games.
Today, we are announcing the 4th generation
 Max-Q technologies.
Let me show you what we’ve done.
On laptops, power is shared between the GPU and CPU.
CPU efficiency is critical for maximizing performance.
So we developed CPU optimizer.
We’ve worked with CPU vendors to create a new, low-level framework enabling the GPU to further optimize the performance,
temperature, and power of next generation CPUs.
As a result, CPU efficiency is improved and power is transferred to the GPU for more gaming performance.
For creators and students who rely on compute-heavy apps like Adobe Premiere, Blender, or Matlab, 
we’ve developed Rapid Core Scaling.
It enables the GPU to sense the real-time demands of the application and use only the cores it needs.
This frees up power that can be used to run the active cores at higher frequencies,
delivering up to 3 times more performance for
intensive creative work on-the-go.
On top of the great performance, laptop gamers also want more battery life.
Battery Boost 2.0 has been totally re-architected.
Now AI controls the whole platform,
 finding the optimal balance of GPU and CPU power usage, 
battery discharge, image quality, and frame rates. 
All in real-time.
The result is great playability on battery,
with up to 70% more battery life.
Our 4th generation of Max-Q technologies, with CPU Optimizer, Rapid Core Scaling, and Battery Boost 2.0,
continues our commitment to bring thin, high-performance laptops to gamers and creators.
Today, we are announcing the RTX 3080 Ti laptop GPU, 
bringing the flagship 80 Ti class of GPUs to laptops
for the first time.
Featuring 16GB of the fastest GDDR6 ever shipped in a laptop,
 the RTX 3080 Ti delivers higher performance than the desktop Titan RTX.
RTX 3080Ti laptops start at $2499.
The RTX 70 family is our fastest-growing class of laptop GPUs.
Introducing the RTX 3070 Ti.
It’s 70% faster than RTX 2070 Super laptops and delivers 100 fps at 1440p.
RTX 3070 Ti laptops start at $1499.
Laptops powered by both of these new GPUs will be available starting February 1st.
For creators looking for the best laptop for your work, we are also announcing new NVIDIA Studio laptops.
With the latest RTX GPUs, these laptops are 7 times faster than the newest Macbook Pro 16 in 3D design.
With RTX hardware-accelerated Ray Tracing, AI and NVIDIA’s high-performance video processor, 
they are the perfect tool for any creator workflow.
Together with NVIDIA Omniverse, Studio laptops are ready to build the virtual worlds of the future.
With over 100 Studio laptop designs, there is a wide variety to meet every creator’s needs.
This includes ultra-portable 14” thin and light,
 perfect for remote workers or students, like the new Razer Blade 14.
Dual display laptops for video editors like the new Asus Zenbook Pro Duo.
And the most powerful laptops for creators, like the new MSI Creator Z17, featuring RTX 3080 Ti.
Let's see what digital artist Jacob Norris can create in 48 hrs with a powerful 3080 Ti laptop…
My name is Jacob Norris. I'm a 3D environment artist working in games and films.
Gaming does inspire a lot of the artwork that I create—
games I’m playing now, and games I’ve played in the past.
I actually get a lot of the inspiration for what I want to work on
while I'm traveling.
Having a laptop with me with a nice big screen, a nice powerful GPU that allows me to work in all the 3D programs I need
just helps me to get my ideas out as they're coming to me.
NVIDIA reached out to me with the opportunity to test out this new laptop, the Razer Blade 17.
I’m going to create a couple of scenes with it.
It's got an NVIDIA GeForce RTX 3080 Ti in this thing, so it's gonna be pretty powerful.
[off-screen]
All right, let's do it.
Sweet.
This is pretty sweet because
actually I've got the entire scene open right now,
which is a huge number of polygons.
It's able to glide around the scene with no issues.
I'm trying to get some of those rocks kind of blending in, 
it'll look a lot better once we get it into Omniverse
and we can get that texture loaded up on top of here.
This is that same landscape scene.
 I'm able to open it in Blender, make those changes to the landscape,
bring it right back into Omniverse and it's a seamless transition.
And I'm able to just easily drag and drop in this dune buggy character that I got from my friends.
We can start playing with some of the sunlight inside the space.
The shadows just change instantly.
It's kind of fun.
So I got the camera in, animations in.
Let's try this out and see how it works.
Yeah, yeah, that's really cool. I like that.
[off-screen]
Jacob, describe your overall experience with this laptop.
I wish you had given this thing to me sooner, my life would have been way easier.
We were able to easily render all this stuff, no problem inside of Omniverse in such a short amount of time.
I feel like I can go anywhere with it and still am able to produce the quality artwork that I could do at home.
I kind of want to run away with this thing.
This is over right?
I’m getting out of here.
You guys...
You guys figure it out.
To recap…
GFN delivers the ultimate cloud platform for PC gamers.
Today we are bringing more games, more devices, and more
networks to the GFN ecosystem.
RTX is the new standard, 
and the GeForce RTX 3050 makes it more accessible than ever.
With 75% of gamers still gaming on GTX GPUs, 
this is the perfect time to step up to RTX.
Laptops are the fastest-growing PC platform.
Today we announced 160 new Laptop designs, 
including our latest Laptop GPUs: the 3080 Ti and 3070 Ti, 
and our 4th Gen Max-Q technologies.
We announced NVIDIA Omniverse for 3D design collaboration
is now generally available for GeForce RTX and Studio Creators.
This is the future of 3D content creation and how virtual worlds will be built.
Over the past 20 years we have dedicated ourselves to building the ultimate platform for gamers and creators.
And today we continue that journey
But there is one more thing.
The RTX 3090Ti. A monster GPU.
40 Shader Teraflops. 78 RT Teraflops. 320 Tensor Teraflops.
24 GB of 21Gb/s G6X memory—the fastest ever.
The RTX 3090Ti. Our next BF GPU. 
Tune in later this month for more details.
Now let me hand it over to Ali Kani.
Thanks Jeff.
I’m here to talk about Autonomous Vehicles. 
It’s perhaps the most intense AI challenge,
but it’s also one with the greatest benefits to society.
It will save lives, make roads less congested
and change the way billions of
people move around the world.
There are 3 core pillars of the machine learning pipeline being developed at NVIDIA to create automated driving
solutions that can be used by the transportation industry.
First is the real-time, AI-based software stack running on an efficient, high performance supercomputer 
that goes inside every vehicle.
Second, there are AI models trained on our DGX Servers in the cloud, 
and finally a simulation platform is required 
and we’ve built it on Omniverse.
It’s used both for synthetic data generation as well as to test and validate the AV system.
This machine learning pipeline is never ending—
—autonomous driving software will need to continuously develop and improve.
New data will be collected and synthetically generated,
models will be retrained in the datacenter,
then validated in simulation, before new software is ultimately updated for use on the road.
We’ve creating this end-to-end automotive platform in modules, so that our partners can use exactly what they need to
speed time to market and build a product that can stay true to their brand.
We have some partners that just buy our chips and core operating system, while developing their own software applications.
Other partners like Mercedes-Benz rely on us across this entire stack
from our self-driving software running on NVIDIA DRIVE computers in each car,
to Training AI models in the Cloud, synthetic data generation, vehicle validation and
testing of new features through simulation, 
which finally get pushed over the air into each Mercedes-Benz vehicle.
Developing an autonomous vehicle requires an entirely new platform architecture and software development process.
Both the hardware and software must be comprehensively tested and validated to ensure they can handle the harsh
conditions of daily driving with the stringent safety and security needs of an automated vehicle.
This is why NVIDIA has built and made open the DRIVE Hyperion platform, which specifies a high-performance computer and
sensor architecture that meets the safety requirements of a fully autonomous vehicle.
Today we are in our 8th generation of Hyperion—
 it has been adopted by hundreds of automakers, truck makers, Tier 1s,
and robotaxi companies.
Hyperion 8 is designed with redundant DRIVE Orin computers,
12 state-of-the-art surround cameras, 9 radar,
12 Ultrasonics, 1 front facing lidar and 3 interior sensing cameras.
It’s architected to be functionally safe, so that if one computer fails,
there is a back up available to ensure that the
autonomous vehicle can drive its passengers to a safe place.
We have spent many years developing a data collection system with high-quality calibration and time synchronization in 4D,
so our partners can leverage NVIDIA’s AV investment 
and focus their resources on their own AI application development.
We have also defined Hyperion with our standard AGX form factor so that customers
can design their autonomous vehicle to scale across generations of their vehicles.
With this standard form factor, an automotive OEM can go to production with a DRIVE Orin computer today, and easily plug
in a hardware and software compatible DRIVE Atlan computer in the future.
Today, I'm pleased to announce that leading automotive Tier 1 suppliers Desay, Flex, Quanta, Valeo and ZF
as DRIVE Hyperion 8 platform scaling partners.
Together these partners provide the automotive ecosystem with world class tier 1 options 
who can manufacture production-ready Hyperion platforms 
with the highest levels of functional safety and security.
We’re very proud of these partnerships as they demonstrate how Hyperion 8 has quickly become the automotive industry’s
most open and adopted platform architecture and you will see it start hitting production as early as this year.
The transportation industry is going through a rapid transformation.
This change started with a move from internal combustion engines to electric motors.
Electric vehicles are not just better for the environment, 
they fundamentally improve the driving experience for consumers.
They drive more quietly, accelerate faster, and they will last longer.
Over the course of the next several decades we will see the majority of cars sold be electric.
This transformation has given way to dozens of New Energy Vehicle startups.
They have reimagined the car, starting with a new vehicle architecture based on 
programmable, software defined computers.
I’m very pleased to announce that our DRIVE Hyperion platform has been adopted by leading New Energy Vehicle OEMs in the
industry.
Volvo-established Polestar is bringing a storied heritage of safe driving into the modern era 
with electric vehicles that emphasize the joy of driving.
The upcoming Polestar 3 SUV features a centralized compute architecture powered by NVIDIA DRIVE.
China—now the world’s largest auto market—has also been very successful driving the transition to NEVs,
with nearly 20% of all cars sold in China this past quarter being electric.
We are really excited to work with the vast majority of the leading NEV companies in China,
including NIO, XPeng, Li Auto, R-Auto,
and SAIC's premium Electric Vehicle Brand - IM.
These new electric cars will get better and better over time with each over-the-air update.
Such companies can benefit from new business models that are software driven.
Rather than selling a car and seeing it degrade once it’s driven out of the dealership,
the tech-based automotive company manages a fleet of vehicles
that will improve with new services and functions offered over the life of a vehicle.
The potential of these relatively young Electric Automotive companies and their innovative business models is really disruptive.
It has resulted in the market valuing them higher than car companies that are well established and produce
millions of vehicles each year.
AI is not just transforming passenger vehicles, 
but also opening up a major opportunity for disruption in the commercial sector.
Over the last decade, and accelerated by the pandemic, 
consumer shopping has dramatically shifted online,
resulting in increased demand for trucking and last mile delivery.
This trend is expected to continue, 
with projections starting at 170B packages being shipped in 2022
and increasing to 280B packages in 2027.
Simultaneously the trucking industry has been experiencing a significant shortage of drivers.
Over the next few years this gap is expected to widen even further,
increasing to a shortage of more than 140,000 drivers
in the US alone by 2027 and more than a million worldwide.
We are now seeing a large number of autonomous driving services being built to enable these opportunities 
that are together essential to our transportation future.
Today, I’m pleased to announce that TuSimple will be building its autonomous trucking platform on NVIDIA DRIVE Orin.
TuSimple is an exciting company that’s meeting the challenges of today’s trucking and logistics industry
with its scalable Autonomous Freight Network.
TuSimple is already working with the leading names in delivery,
including UPS, Navistar and Penske, 
and its technology has already improved arrival times 
for long haul routes of the U.S. Postal Service.
Now, I mentioned earlier that developing AV is a grand AI challenge.
At the heart of AI model development, we need the ability to collect a lot of data and accurately label it to allow our
AI engineers to build accurate and safe AI models.
Labeling real world data can be time-consuming, costly and difficult.
In fact, often the data that we are most interested in using is also data that is the most challenging to label.
Here are some examples from our own data set that are difficult to label including scenes that are dark, blurry or hazy.
Others contain scenes that are challenging to understand like occluded vehicles or pedestrians.
We also have many scenes that are not often encountered - like construction zones.
These issues are the reasons why AV developers need a mix of real data and synthetic data
when developing their AI models.
Because of all these labeling challenges, NVIDIA has built DRIVE Sim Replicator,
a synthetic data generator for autonomous vehicle development.
Replicator helps our AI engineers to build up hard-to-label ground truth data by synthetically generating them from
virtual cameras, lidar and radar sensors within our Omniverse simulation platform.
In this way, engineers can train AI models even before any real data has been collected.
And Replicator can label ground truth in ways that humans cannot—
tracking moving objects across sensors, velocity,
distance, occlusion, and severe weather conditions.
It’s a really powerful tool for the AV developer.
It's accurate, low-cost, and it fills in gaps for data not easily found in the real world.
The industry is just getting started on its path to truly self-driving technology.
Most vehicles today are only able to deliver Level 2 driving,
commonly referred to as Advanced Driver Assistance Systems
that require drivers to still be fully attentive.
We are now starting to see cars classified as Level 3 that allow drivers to take their eyes off the road but still
remain in the driver’s seat.
Some companies are also developing Level 4 and Level 5 solutions that will be truly self-driving—
allowing the driver to become a passenger and even sleep in the back seat.
These include autonomous last mile deliveries. hub to hub trucking, and passenger RoboTaxi.
The complexity of AV software development is ever increasing for these different use cases.
Vehicles targeted for higher levels of autonomy consume much more data,
as they are designed with more sensors,
each with higher resolutions for better accuracy.
AI networks and software architecture also increase in complexity as you require true self driving.
All of these factors result in a large increase in datacenter infrastructure required for self-driving software development.
Companies investing in basic L2 systems can have datacenters that need just one or two thousand GPUs.
And then we see some companies needing upwards of 25 thousand GPUs when developing full self-driving systems.
Now our strategy at NVIDIA is unique to the industry.
We help our partners with computers and software in the car.
We also help them with their infrastructure in the cloud.
And when our partners align to our DRIVE Hyperion architecture, 
we can help them connect their platform in the car with their infrastructure in the cloud seamlessly.
We believe the speed of an OEM's end-to end autonomous driving software development flow 
is the most critical asset and differentiator.
Software-defined vehicles will incorporate two key computers built with NVIDIA DRIVE Orin.
So far today, I’ve talked about the computer for autonomous driving, the DRIVE Chauffeur and how it is an AI-assisted
driving platform that can handle both highway and urban traffic with the utmost safety.
Using the high-performance compute architecture and sensor set of DRIVE Hyperion 8, 
DRIVE Chauffeur can drive you from address to address.
But there is another equally important computer that is needed for a next-generation software-defined vehicle,
and we call it DRIVE Concierge.
With DRIVE Concierge, vehicle occupants have access to intelligent services that are always on,
using NVIDIA Omniverse Avatar for real-time conversational AI.
Welcome Sheryl,
your calendar shows your CES hotel check-in for tonight.
If we leave now we can avoid traffic on 880.
Perfect. Drive me there.
Activating autopilot.
I'm ready to enjoy the ride, but it's a little warm in here.
Should I roll down the windows or turn on the air conditioner?
You know what, open the sunroof.
Opening the sunroof for now.
Omniverse Avatar connects speech AI, computer vision, natural language understanding,
recommendation engines and simulation.
The technology of Omniverse Avatar enables DRIVE Concierge to serve as everyone’s digital assistant,
helping them with recommendations,
to make reservations, safely use mobile devices and provide alerts,
like if a purse is left behind in the vehicle.
It’s personalized to each driver and passenger,
giving every vehicle occupant their own personal concierge.
Are there any important messages for me?
There is one from Jensen. 
He asks, can you meet for dinner tomorrow?
Great. 
Can you find me a Michelin-star restaurant in Las Vegas?
The first result is Nobu located at 3799 Las Vegas Blvd.
Who is the head chef there?
Nobu's head chef is Nobu Matsuhisa.
What is his signature dish?
The signature dish is black cod with miso.
What wine goes well with that?
Nobu's head sommelier recommends a crisp white wine like a Sauvignon blanc with black cod miso.
Sounds like a winner. Are there reservations available for dinner tomorrow?
There is availability for a party of two at 7:30 pm.
Perfect, please book it. 
Can you respond to Jensen to let him know to meet us there tomorrow?
OK, should I send it?
Yes please.
OK, sending.
DRIVE Concierge also needs to be tightly integrated with DRIVE Chauffeur to provide high-quality, 
360-degree, 4D visualization inside the vehicle,
so the driver can comfortably sit back and trust the Chauffeur to safely drive.
DRIVE Chauffeur and Concierge together can do really beautiful things.
As an example, they can serve as a valet to let you out at the entrance of your destination and enable the car to search
for a parking spot on its own.
And then you are ready to leave, you simply summon your Concierge
who will ask your Chauffeur to drive your car back to you.
From AI in the car to AI in the cloud, NVIDIA is paving the way to safer and more efficient transportation.
Now, we're still in the early stages of this exciting journey, 
and I can’t wait to see what our partners develop and deliver
to consumers on our Hyperion 8 platform starting later this year and beyond.
Thank you.
Title: Intersection Detection Using AI-Based Live Perception - NVIDIA DRIVE Labs Ep. 2
Publish_date: 2019-05-10
Length: 85
Views: 21265
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/6MY2xiF52o8/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 6MY2xiF52o8

--- Transcript ---

Today in DRIVE Labs we're going to talk about
NVIDIA DRIVE AV mission 79A, detecting intersections
using live perception only.
And that means using only real time camera
data and our WaitNet deep neural network,
no maps.
What we're seeing here are frame by frame
intersection detections from WaitNet, with
WaitNet using context understanding to detect
the intersection.
You can see that because it detects the intersection
in yellow, well before it detects the individual
features like traffic lights and traffic signs.
This long detection range is really important,
because if the car waits to see a traffic
light or traffic sign before it slows down,
it might not have enough time to stop safely
and comfortably.
Here WaitNet is detecting different types
of intersections.
First, a traffic light controlled one, followed
by a stop sign controlled intersection with
a pedestrian crosswalk.
We are also seeing it detect multiple intersections
per image frame, where it's basically figuring
out which pixels in the image belong to the
closer intersection, and which ones belong
to the one that's further away from us.
Here we see that same principle in action,
even as the road splits.
Here the leading car is fully occluding the
next intersection, but as soon as it becomes
even slightly visible, weight net detects
it.
That was NVIDIA DRIVE AV mission 79A WaitNet
base intersection detection, and it will be
shipping in the NVIDIA DRIVE software 9.0
release.
Title: GPU Technology Conference Keynote Oct 2020 | Part 3: "AI to Fight Disease"
Publish_date: 2020-10-05
Length: 419
Views: 134607
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/6tA5Yt94-Bw/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 6tA5Yt94-Bw

--- Transcript ---

 
Understanding disease is one of the great human endeavors and remains an incredible challenge.
Drug discovery is hard, on average taking over a decade to develop, $2.5 billion dollars in R&D.
And yet 90% of the efforts fail, the cost has doubled every 9 years. This is called Eroom's law.
For the $1.5 trillion dollar pharma industry there is a great deal at stake to accelerate the successful discovery of new drugs.
COVID-19 hits home this urgency.
Using breakthroughs in computer science we can begin to use simulation, in-silico methods, to understand the biological machinery of the
proteins that affect disease and search for lead candidate drugs before the slow process of in-vivo testing.
Drug discovery is hard in every respect.
First, it is hard to find the protein that is implicated in a disease.
Second, it is hard to find a small molecule, chemical compound that can bind with, and to activate or de-active the protein.
Third, getting the small molecule inside the cell is hard as mother nature has created defense systems to protect us from foreign substance.
Finally, what does your body do with the chemical compound? How does it safely absorb, distribute, metabolize, and excrete the compound?
This is called pharmaco-kinetics.
A spectacular arsenal of technologies is deployed in drug discovery.
Genomics is used to choose protein targets responsible for a disease and determine which patient will respond to a drug.
Cryo-Electron-microscope-imaging and 3D reconstruction let us see a protein's structure which determines its function.
Data analytics is used to screen 10^60 potential chemical compounds for fingerprints that might lead to a drug candidate.
Deep learning AI is used to generate and design new leads learning from known chemicals.
Docking predicts affinity, strength of the bond between a chemical and target protein.
Molecular dynamics analyzes the physical motion of a protein and its interactions with a chemical.
Pathology and radiology imaging are used to study the biomarkers, the condition of a disease and effectiveness of a drug.
Natural language processing study publications and health records to find chemicals that might be re-purposed for other disease targets.
Today, I'm pleased to announce the NVIDIA Clara Discovery, a state-of-the-art suite of tools for scientists to discover life-saving drugs.
Where there are popular industry tools, our computer scientists accelerate them.
Where no tools exist, we develop them, like the NVIDIA Parabricks, Clara Imaging, BioMegatron, BioBERT and NVIDIA RAPIDS.
We hope this will bend the trajectory of Eroom's law and lead to faster and more effective discovery of life saving drugs.
The UK is an epicenter of healthcare research.
In fact, Cambridge is the home where Francis Crick and James Watson discovered DNA.
But scientists in the UK need a state-of-the-art computing infrastructure.
So, we're going to build one. It's called Cambridge-1.
400 petaFLOPS of AI performance, it will be the fastest in the UK and top 30 in the world.
Cambridge-1 will host our UK AI and healthcare research collaborations with academia, industry, and startups.
All partners already use NVIDIA computing.
Cambridge-1 lets them do experiments too large for their infrastructure or while theirs are being built.
Our first partners are AstraZeneca, GSK, King's College, NHS, and Oxford Nanopore.
Today we are also announcing a partnership of GSK and NVIDIA to build the world's first AI Drug Discovery Lab.
Let's hear from Dr. Kim Branson, GSK's head of AI.
Understanding human disease has never been more urgent or important than it is today.
This undertaking is bringing together the best minds and the latest breakthroughs in biology.
The complexity of human biology is striking. There are 45,000 genes that make up 500 million protein interactions in a single cell
alone, and the human body has trillions.
With new tools, including genomic sequencers, microscopy, and medical imaging, we can measure this complexity for the first time:
from a genome to a single cell to a protein.
And new perspectives, looking at population-scale data, now capture the vast diversity of human health
while helping us zoom in on an individual's unique genetic makeup and personal history.
GSK has accumulated more data in its first quarter this year than in its entire 300-year history combined.
It's an amount of data no human could understand. And no equation can fully capture.
So, GSK believes that AI and computational biology, especially when combined with our
vast data sources, can break new ground in drug discovery.
But we can't do it alone. That's why GSK is partnering with NVIDIA to build out the world's first dedicated in-house drug discovery AI lab.
If AI is software written by a computer, then companies will need powerful AI computers to write great software.
This insight has transformed NVIDIA and other AI leaders.
I expect every major industry - healthcare, transportation, manufacturing, logistics, retail - will all eventually have
large AI compute infrastructures that collaborate with their software developers and researchers.
But it is challenging to build these systems, to develop the software and the tools to run on these large supercomputers.
So, we built a ready-made AI supercomputer called the NVIDIA DGX SuperPOD.
The DGX SuperPOD is available now, scalable from 20 to 140 DGX systems.
A 140-DGX-configuration is a Top 20 supercomputer in the world
We've also published a SuperPOD blueprint for our large network of partners to replicate using NVIDIA technologies.
Enterprises can go from zero to their valuable AI researchers being productive in just a few weeks, not many months.
The savings in productivity alone will pay for the system.
You've heard me say it. The more you buy, the more you save.
There are many installations going around the world already
CDAC is going to be the most powerful AI supercomputer in India.
Argonne National Lab is using DGX SuperPOD for computational drug screening.
The University of Florida has a system built to train next generation AI researchers and students.
The Wallenberg AI and Software Research is building the largest AI infrastructure in Sweden.
Facebook AI Research Lab has Super DGX for their researchers.
And Naver, Korea's #1 search engine and world's fastest growing messenger platform, is installing SuperPOD to advance natural language understanding.
Title: Reinventing Retail: Lowe's Builds Digital Twins of Stores to Deliver Enhanced Shopping Experiences
Publish_date: 2022-09-22
Length: 122
Views: 19626
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/6uUC63qD9vE/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 6uUC63qD9vE

--- Transcript ---

At Lowe's, we are always looking for ways 
to re-imagine and test the future of our  
store operations and customer experiences. 
With Omniverse, we're taking early steps  
in digital twins and AI to democratize 
store planning for our retail associates.
In two of our stores, digital twins are 
already linked to the physical world  
via streaming data APIs and 
intelligent IoT sensors.
To build our digital twins, we leverage 
a Universal Scene Description pipeline  
that aggregates native CAD from tools 
including Revit, Maya, AutoCAD, and SketchUp.
These will enable our retail 
associates and planners to  
collaborate in real-time to understand 
sales performance and identify anomalies.
And we can run CI/CD processes 
that test and validate  
thousands of store and product layouts -- before 
making any physical changes to our stores.
Each variation uses Omniverse microservices 
to simulate customer traffic flow patterns,  
trained on historical data -- shown here as a heatmap.
In the physical world, these simulations 
can transform how our associates  
reconfigure shelves and aisles. Wearing an 
Omniverse-connected Magic Leap 2 headsets,  
they can see and interact with a full-fidelity 
digital twin in augmented reality.
Magic Leap also uses large AI models for 
perception—streamed from OVX servers—to  
enable accurate object detection and identify 
occluded products on hard-to-reach shelves.
We’re excited to be a pioneer in digital twins for retail.
With Omniverse, we’re pulling store data together  
in ways that have never been possible, 
creating new means of viewing,  
sharing and interacting with that data, and 
giving our associates ultimate superpowers.
Title: Plex Media Server on NVIDIA SHIELD Pro
Publish_date: 2016-06-09
Length: 52
Views: 53292
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/6vUQ9w9M-5U/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 6vUQ9w9M-5U

--- Transcript ---

Today we’re going to introduce you to a whole new side of NVIDIA SHIELD.
The power to serve your personal media collection
to any device,
wherever you go.
SHIELD Pro comes pre-loaded with Plex Media Server,
500GB of internal storage,
and it’s powered by the NVIDIA Tegra X1 processor,
providing fast,
hardware-accelerated transcode.
It’s powerful,
super energy-efficient,
and always on,
making it the perfect solution
for serving your media collection.
Now you can enjoy your home videos, shows, and movies,
music, photos, and more on any device in your home
and even outside your network with remote access.
With Plex built in,
SHIELD Pro is a perfect way to enjoy all of your media
at home
and on the go.
Title: Women in Technology at GTC 2018
Publish_date: 2018-04-04
Length: 47
Views: 983167
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/7A4btc7sQ8U/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 7A4btc7sQ8U

--- Transcript ---

we're here today kicking off the second
day of GTC celebrating all of the great
work that women are doing and how we're
growing our numbers at TTC we're with
female speakers and even all the startup
companies that we get to feature in AI
you know I think artificial intelligence
brings a really interesting opportunity
for not only women but really anyone
from diverse backgrounds to get involved
in technology it's a new way of building
software and it really can be accessed
by and by anyone we're bringing in more
and more female leaders all the time
and you know we're putting everything we
can behind telling the great stories of
women and technology so that we can
spread the word and encourage the next
generation to come along
[Music]
Title: GTC 2018 - NVIDIA CEO Jensen Huang Keynote Supercut
Publish_date: 2018-04-10
Length: 1187
Views: 63878
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/7eVWVNwSMGs/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 7eVWVNwSMGs

--- Transcript ---

[Music]
[Applause]
Jennsen ma we have so much to talk about
today we have a lot of new products to
show you we're talking about mazing
graphics we want to talk about mazing
science amazing AI and amazing robots so
let's get going
computer graphics is the driving force
of the GPU it is computationally
insatiable recreating virtual reality is
one of the most daunting computing tasks
we know and yet on the other hand it is
an enormous industry we want to
visualize information visualized
experiences in all kinds of markets as a
result the technology that we bring to
bear and the size of the market combines
into a gigantic R&D budget it is the
driving force of GPUs this is what
modern computer graphics looks like this
is what a videogame would look like and
in fact it looks pretty amazing the
difference here what I showed you
earlier was generated one frame every
many hours versus what's going on here
produces these images at 4k resolution
at 60 frames per second from one frame
in many hours to 60 frames per second
that fundamental difference has been a
gap we've been trying to close for
literally four decades
it's incredible to see this when you
look at it in motion and so here's an
example of an emotion
[Music]
[Music]
what you just saw was completely
rendered in real time now let's show it
to you this is reflections on
reflections check out captain phasma's
gun reflecting on on phasma's chest
right there you see that and these rays
are bouncing all over this environment
these rays every single one of those
rays are bouncing off the environment
and every time it strikes a surface it
has to figure out do I reflect do i
refract do am i absorbed and how much
does it absorbed the more reflections
and refractions the harder it is this is
how this is recreated instead of a
supercomputer rendering these scenes one
frame every 10 hours this is now running
on one dgx computer with four vultus in
real time this is what we can do now
$68,000 computer versus a supercomputer
so ladies and gentlemen we're announcing
the Nvidia RTX technology this has been
ten years in the making
we also are announcing today the world's
largest GPU ladies and gentlemen this is
the Quadro GV 100 the world's first
workstation GPU based on the Volta
architecture it also is the first one
that has a brand new interconnect
between GPUs call MV linked to the two
GPUs connected through this new
interconnect call MV link is essentially
one giant GPU so these two GPUs working
together it will become a revolutionary
new workstation what makes this special
is for the very first time we can bring
real-time ray tracing to the market
people can actually use it and video to
voltage GPU the r-tx technology the
solvers the architectures the libraries
has now been integrated into three of
the most important rendering api's
one the Nvidia optics to Microsoft's
dx12 extension called DX ray tracing DX
are the work that we did with them is
fantastic and then now it's also going
to be available in OpenGL Vulcan
creators and designers are gonna love
this it is amazing how many frames are
rendered each year we estimated billion
frames are rendered each year with yet
Nvidia r-tx technology and a Quadra GB
100 I believe the number of frames that
will be rendered will jump by a factor
of ten you also get it done faster but
most importantly you'll save money the
more GPUs you buy the more money you
save that's right the more GPUs you buy
the more money you save this is now
common sense now let me illustrate it to
you this is what a render farm would
look like 280 dual CPU servers it
consumes a hundred and sixty eight
thousand watts that's like a hundred and
sixty eight families this is before this
is after this is with the NVIDIA r-tx
technology Quadro GV 100 look at this 14
quad GPU servers 24,000 watts and you
save millions of dollars science needs
supercharge to computers and that's the
reason why we're building supercharge
computers the applications that I love
the most of all of the work we do in HPC
is the work that we do in
revolutionising modern medical imaging
well whether it's CT computer tomography
magnetic resonance ultrasound
mammography which is low-dose x-rays the
positron emission pet each one of these
modalities of medical imaging has been
revolutionized recently using
computational approaches we have an
initiative a project in our company we
called Project Clara it's a medical
imaging supercomputer you could put it
in the data center you can put it in the
cloud but it's possible for us
to actually virtually upgrade every
single medical instrument let me show it
to you okay you take this tradition all
true sound machine that's sitting in a
hospital is already on the network
anyways you stream the ultrasound
information into your data center and
your data center running the stack on
top of a GPU server creates this miracle
on the right and now it's volumetric we
call this the medical imaging
supercomputer new networks are growing
and evolving and an extraordinary rate
what started out just five years ago
with the Elex net was eight layers deep
had millions of parameters well five
short years later thousands of species
have emerged what started out just five
years ago as eight layers and a few
million parameters is now hundreds of
layers and billions of parameters so the
world wants the researchers all over the
world wants just a gigantic GPU and so
ladies and gentlemen today I would like
to announce the world's largest GPUs
this is the world's largest GPU this is
16 Volta equivalents connected by 12
brand-new high throughput switches that
the world has never seen is called MV
switch 14 terabytes per second of
aggregate bandwidth and let's say if a
high-resolution movie is 10 gigabytes
okay so 14 thousand gigabytes 1440
movies more movies and any human has
ever seen could be transferred across
this switch in one second like that
ladies and gentlemen fourteen thousand
movies
every single GPU every single GPU can
communicate to every other single GPU at
20 times the bandwidth of PCI Express so
altogether ladies and gentlemen it looks
like this this is the nvidia d GX - the
world's largest GPU the world's largest
adding card - petaflop s-- 512 gigabytes
and just beautiful this this is
unfortunately we know that hyper scale
data centers are the most complicated
computers the world has ever made
so we've dedicated just an enormous
amount of resources to solving this
problem for inference and so we started
working on tensor RT which takes the
output of these frameworks would you
these massively computation complex
computational graphs and we have to
target for all of the parameters that I
just told you about we've got to read we
got to make that that network as small
as possible as high performance as
possible and yet retain all of its
accuracy today we're announcing the
largest battery of new tools the largest
battery of new algorithms and new
libraries for inference that we've ever
announced first of all tends to RT for
point 4.0 brand new tensor RT and now
has the ability to handle recurrent
neural networks sequence the sequence it
has deep integration into tensor flow we
worked with the team of call D framework
the most popular voice recognition
speech recognition framework called call
D and then lastly Onix a brand new
back-end that supports PI torch supports
MX net supports Windows and has the same
back-end for Windows ml in aggregate
we're going to speed up hyper scale data
centers with our GPUs with this
generation of optimizations a hundred X
now that we have all these accelerated
frameworks and all these accelerated
codes how do we deploy it into the
world's data centers well it turns out
there's this thing called kubernetes
kubernetes is it's an orchestration
layer that orchestrates the workload
that's coming in from the cloud and
orchestrated across the resources of the
hyper scale data center okay let me show
it to you
this is flowers of course running on on
CPUs four images per second and the
fastest skylake that we have this is one
GPU one GPU let's kubernetes this thing
yep we got it running in kubernetes
that's large loads so we need to handle
that load so what we do is we can ask
who Nettie's to say hey let's make
multiple replicas of that same container
and so I'm gonna add eight replicas in
you're gonna see them come in and I'm
gonna add them into the load balancer
and as I add them in you're gonna see it
just get faster Wow not only it can be
run it in our local arm front and center
but we can scale and we can burst into
the cloud so let's take a look at that
right now so I'm gonna add four and more
from the cloud and what I'm gonna do now
is show you self-healing so I'm going to
kill off four of these GPUs and then
those AWS GPUs are gonna jump right in
and the performance is gonna come back
up there they go that one sometimes it's
that fast
the transportation industry is the
largest industry one of the largest
industries in the world ten trillion
dollars large and we believe that
someday everything that moves will be
autonomous or have autonomous
capabilities safety is the most
important thing just imagine this you've
got these cars with all these different
sensors some of our radar that could
sense motion lidar that can measure
depth but don't see very well in light
and snow and fog cameras with super high
resolution but can't see in the dark
infrared that can see in the dark but
doesn't have very high resolution we
take all of these sensors and we fuse
them together into a super sensor but in
order to do this and to compute it in
such a high rate of speed as the car is
moving and to take action so quickly is
incredibly hard and what NVIDIA is
dedicating ourselves to do is to solve
the problem from end to end
this infrastructure inside Nvidia is
called the perception infrastructure is
a massive investment and it's just
something that I'm so proud of at the
end of it it produces networks we
already have 10 networks inside our car
the self-driving car is not one network
it's a whole bunch of networks and then
be a whole bunch more we already have
ten by the time that we ship in a couple
of years call it 20 or 30 and the reason
the reason why it's so hard
we're trying to create a AV computing
system so that the entire transportation
industry can take advantage of this
massive investment that we're making and
create the future of autonomous vehicles
and so we created one architectural
roadmap and it basically starts like
this
two years ago we introduced Drive px
Parker and then we created a four chip
solution for try px 2 which is the
development system for most of the
autonomous vehicles that you see around
the world then two years later this year
we created the drive Xavier basically we
took four chips the computational
capability of those four chips in those
300 watts and we shrunk it into one
miracle chip then from Xavier we added
we created a four chip system to xavier
z' and to Voltas together another
running at a very low temperature and
voltage another 300 watt computer
emerged and that's right here a white
computer is being used in robot taxi
self-driving car system development all
over the world
we're sampling both of them today both
of them will be in production by the end
of this year but we're not stopping here
our next generation is called Oran and
we're gonna take basically those eight
chips to to drive Pegasus's I'm going to
put it into a couple of Oren's it will
create also an amazing processor for
autonomous driving cars auto-grade super
energy efficient fully Ald huge
investment
and videoscribe roadmap civilization
drives about ten trillion miles per
years in the united states 770 accidents
happen from 1 billion miles and so the
question you have to ask yourself is how
confident are you when your car your
fleet of test cars of 20 over 1 year has
driven about a million miles clearly the
amount of coverage scenario coverage and
miles coverage is just not possible in
real life and this is where Nvidia's
skill can really shine we know how to
build virtual reality worlds and and the
key here is that the the fidelity of the
simulation has to be sufficiently high
that the sensor stacked all the software
that we create would just operate as
they would in real life and notice we're
detecting all the cars were detecting
all the lanes but here's the amazing
thing
earlier the video that you saw were real
cars in the real world we didn't change
one line of code iota and it's running
on exactly the same Drive px Drive
xavier and dr pegasus computer that we
would drive in the car this image
generated is generating the world now
this image generator in the context of a
video game this image trainer is a
gaming pc and the person that would be
driving the car we replace that person
with a artificial intelligence AV
computer we call this drive
constellation we're gonna have thousands
of constellations these virtual reality
worlds will all be running
simultaneously and hopefully we could
cover a large large coverage of
scenarios with just 10,000
constellations we can cover 3 billion
miles a year
in the future robots will augment and
will assist us in all kinds of different
places and so we've created a platform
like we've done with drive we called
this Issac today we're gonna release the
first little version of that this is
such a great achievement and so
basically in the future you're going to
be doing simulation as a development
system because these robots have to work
in a three-dimensional world and so we
created a three-dimensional world
simulator simulator we called the Isaac
lab in the Isaac lab we will develop the
perception capability the localization
capability the mapping capability and
the planning capability that is
necessary for robots and autonomous
machines to navigate these complex
worlds when you're done with the
simulator that hardware should be able
to run the entire software stack and we
call that the Isaac SDK
you
[Music]
look at one thing I know I'll show you
with you this is something we've been
working on and and it's just so exciting
I thought that that as a as a close I
would invite you into Nvidia slab okay
but imagine there is a car in the world
somewhere and we need to help it we just
created a virtual reality car we call
this drive lab and this is where we have
a virtual car and we can see sensors
coming in from the vehicle the remote
vehicle that needs help Tim is currently
in holodeck he is looking at this
virtual world and you see the video
that's being piped in that is literally
live let's show that he has control over
the car as well so there's the real car
out there you can turn the steering
wheel a little bit we can see that we've
got come on cut it out well that's fun
okay so I think we've seen this and we
can have them take off and actually
drive the car as you can see he's
blocked there's a vehicle here that's
doing some unloading you can now operate
this vehicle around this obstacle and
maybe take you to a safe spot in a
parking lot for us and hopefully slowly
and safely in a cordoned off area here
and and Tim's view is very broad he can
see all three screens and get a full
perspective of everything that's going
on in the car the future we can we can
represent all kinds of lidar systems and
everything you know like what Tim is
experiencing right now he's sitting
inside the car and he feels like he's
inside a car and he feels the car around
him and that's why he's able to look at
that he parked in a parking space and
they stabbed him ladies and gentlemen
it's great having all of you here thanks
for your support and have a great GTC
you
[Music]
Title: Transforming Graphics with AI | I AM AI
Publish_date: 2020-08-24
Length: 148
Views: 55874
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/7fcDJDd2P-8/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 7fcDJDd2P-8

--- Transcript ---

[Music]
i
am a visionary
revitalizing the past to inspire
the future
[Music]
breaking the constraints of perception
to power dynamic forms of expression
i am realizing artistic visions
inventing unique variations
and reimagining legendary works
as timeless as their creator
[Music]
i am constantly evolving
bringing characters to life i who are
master of the universe
finding freedom in exploration
and poetry in movement
i am always pushing the limits
accelerating excellence
and envisioning a better future
for everyone
[Music]
i am a.i brought to life
by nvidia and brilliant creators
everywhere
Title: Jen-Hsun Huang speaks about GPUs for Microsoft Azure Cloud
Publish_date: 2015-09-29
Length: 114
Views: 17154
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/7Fq21oguB9g/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLDuY3lZICSx9p1DiTnoh_31sVDLzw
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 7Fq21oguB9g

--- Transcript ---

our announcement today is the result of
some amazing work that's done by the
Microsoft Azure team in the Nvidia
graphics team it takes an enormous
amount of innovation in order to put
GPUs into the cloud these are processors
that are intensely high-performance but
want to be very very close to the user
through very important innovations we've
now made it possible for these powerful
GPUs to be included in the cloud first
of course is the intense energy
efficiency focus that is required
because these data centers are really
really large and the cost of managing
the data centers are very high the
second element is the shortest possible
latency so that the computer graphics
that's generated or the computation that
is done is instantly transmitted to the
user and the third a GPU has to be
general-purpose one of the most
important characteristics of the NVIDIA
GPUs is the two decade-long effort that
we put into the compatibility and the
performance optimizations for the very
first time we have virtualized that
entire software stack and integrated it
deeply into the Microsoft Azure cloud as
a result we can be completely compatible
with almost any application that you can
think of it could be an seismic
processing application it could be a car
design application an industrial design
application of product design
application to medical imaging
applications there are no applications
that run on a PC today that I know of
that is not compatible with the Nvidia
graphics stack now that capability is
put into the Microsoft Azure cloud and
as a result any applications can run in
the Azure cloud and be delivered on any
device anywhere for anybody I want to
congratulate Jason and the entire
Microsoft Azure team on this very
important work this wonderful
collaboration I really believe that this
is the beginning of the future of
computing
you
Title: GTC 2015: Introducing the GeForce GTX TITAN X GPU (part 3)
Publish_date: 2015-03-17
Length: 684
Views: 80842
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/7Fs8zCbaROM/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 7Fs8zCbaROM

--- Transcript ---

so
and that's how you announce a new gpu
ladies and gentlemen
the titan x
eight
billion transistors the first g the
first chip we designed the first
graphics chip we designed called the
revo 128 was four
million transistors
four million transistors a small
difference since then this is eight
billion transistors the team did just a
fantastic job
three thousand cuda cores
the highest
single push precision throughput of any
gpu we've ever created
it's designed for single precision is
designed
with a 12 gigabyte frame buffer
and as you will see in just a moment the
reasons why we made such choices for
people who want
double precision we still have titan z
titan z is still the fastest single card
double precision gpu we have
the titan x
is the highest performance single
precision the largest frame buffer the
most advanced gpu architecture ever
created this is all based on maxwell
okay ladies and gentlemen the titan x
now let's take a look at what titan x
can do this is work that we're about to
show you that was done by a company
called epic epic is a long-term partner
of ours their game engine company
they are the game engine you choose
if you're developing a aaa title
now the the animation you're about to
see is completely in real time and it's
running on one titan x everything you're
about to see is real time
running on one titan x
it captures a hundred square miles
of 3d graphics
literally a hundred square miles of 3d
graphics basically
the entire silicon valley
you're not going to see silicon valley
you're going to see something a little
bit more beautiful
a hundred square miles
15
million plants
you're going to see 15 million plants
this entire terrain
was photometrically scanned because no
it's it's impossible for artists to have
captured landscape that beautiful
that
great
any given frame is 20 to 30 million
polygons
everything is an hdr
the camera effects are beautiful
you're going to see physically based
rendering the rocks look like rocks
water looks like water the shadows are
beautiful
you might even notice some rim lighting
on off the shoulders
this animation this rendering
real-time graphics is the most beautiful
that's that i've ever seen and this is
just incredible work by the team at epic
i love their work and they've just
outdone themselves yet again ladies and
gentlemen let's take a look at
so
me
um
so
titan x and unreal engine 4.
you know if you take a look at that
and you just reflect on the best cinema
effects that you've seen over the last
20 years
that rivals it and that helps you
understand where this industry is going
and where we are going
it is the case
that we will continue to advance
the future of computer graphics
make it more beautiful than ever
more wonderful than ever
to the point where stories could be told
that way
that story was simply impossible to tell
without the advancement of computer
graphics
we're so excited by the work
we just take a look at that it just
takes your breath away i'm amazed by the
work
titan x is more than graphics though
titan x because of many of you
many years ago helped me realize the
importance
that it could be used in a new field
of computer science
some people call it machine learning
evolved greatly now it's called deep
learning
we're going to talk a lot about that
today
but one of the things that we did
was focus on this application and
continues to advance it
it was discovered
in the year about 20 probably about 2010
about five years ago maybe even six
years ago when some of the work that was
done at google led to some work that was
done at stanford
with our engineers
and our researchers
discovered that it's possible to put
deep learning
in these deep neural nets on gpus
and we discovered that the acceleration
of training which takes utterly months
it takes months
to train these very complicated networks
with a large amount of data we could
accelerate it dramatically
we were able to achieve in just a week's
time
what non-accelerated platforms
would do in several months at the time
well since then we've all made a lot of
progress
even if you compare it against the
highest end cpu with a 16 core and most
researchers don't have 16 cores by their
side if you had a desktop computer you
had six maybe eight cores but if you had
16 cores training
the now famous alex net
took 43 days
on a cpu
a titan that we introduced a year and a
half ago or so
is able to do it in literally a week
from a month and a half down to a week
now when you speed something up by a
factor of two
speed something up by a factor of five
when it took a few milliseconds to do
that's important
but when you speed something up that
took a month and a half
and you reduced it down to a week's time
it could be the difference
between your willingness your ability
to actually do the work versus not at
all
titan was able to reduce deep learning
training to about a week we then
introduced a new middleware called coup
dnn
with all of our engineers
we found ways to improve the mapping of
these convolutional networks onto our
gpu's architecture
that middleware is called coup dnn has
been downloaded thousands of times one
of the most popular middleware that
we've ever done
and it reduced it even further
now when you think about those charts
the second green dot the second green
bar relative to the first green bar is
really nothing to be that excited about
with the exception of the fact that you
saved a day and a half
a day and a half in the life of a
researcher
a day and a half and then finally with
titan x
we're now able to reduce your turnaround
time your training time from 43 days
to just three days
enormous enormous speed up
it's utterly
utterly life changing and i'm going to
show you some other results in just a
moment but that tells you what titan x's
can do and the reason why we do it
all of that
all of that
in your hands researchers all over the
world
9.99
now
that's right for nine
just think about that 9.99
you'll pay for it in an afternoon
it'll pay for itself in literally an
afternoon surely it'll pay for itself in
the day the most advanced gpu we've ever
created the fast cheap fastest gpu we've
ever made 12 gigabytes of frame buffer
every deep learning scientist is craving
for more memory capacity and titan x
gives you
twice as much
as what we had earlier
the 980
gtx 980 is now from six gigabytes to 12
gigabytes all for 9.99
Title: GeForce Garage: Cross Desk Series, Video 3  – How to Bend Copper Pipe For Liquid Cooling Loops
Publish_date: 2014-09-29
Length: 562
Views: 447829
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/7RJN1XIs0zU/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 7RJN1XIs0zU

--- Transcript ---

hi I'm Andrew with Nvidia and you're
watching GeForce garage today we're
going to talk about liquid cooling your
PC liquid cooling is super awesome
because it's much more efficient than
air cooling it allows you to overclock
your components so you get much more
performance plus it looks badass
most of you are probably familiar with
flexible tubing but today we have
Richard C Rose the Darth Bevis my good
friend from red Harbinger to show us how
to lay some pipe hi I'm Bridget Rose aka
Darth Beavis when you're liquid cooling
your gaming PC you have several options
for the type of tubing you'd like to use
now some of you may be familiar with
liquid cooling systems using flexible
tubing this is very easy to use does
give you a nice aesthetic look it's
clear so you can see the coolant going
through there the next option that we
have would be a rigid acrylic tubing PE
TG so this is a very very strong this
works really well you would heat bend it
again you can see the coolant through
the tubing comes with a really nice
effect if you want to powder coat it's
matching with our color scheme we're
going to use rolled copper tubing we're
going to be able to use some tools to
make some nice straight runs and
actually do nice bends as well so this
is going to allow us to pimp this rig
out and have it be absolutely beautiful
matching with our color scheme perfectly
so what we're planning at our loop we
have some options when you're doing
water cooling you need to be flexible
just like you're reading the wind
sometimes you have to change and
improvise and think on the go so don't
be hard set maybe you have to turn
things around we have some fittings that
we can use 90 degree fittings we have 45
degree fittings or we can just use their
regular fittings and do total bending to
get the look that we're going for so if
you look at the EK water block on the
bridge we have almost a straight shot if
we're to use a ninety degree fitting
going to the bulkhead so we'll do that
we'll have just a slight slight Bend now
the reason that we're not going to do a
straight Bend is that we would have to
jog up and around in several different
directions number one is very difficult
to do with copper tubing number two it
does not give us a clean nice aesthetic
look that we're going
for so we're going to use fittings where
we need to and bends where it's more
appropriate place where it would be
better to use a band would be going from
the CPU to one of the radiators so we'll
come up to a ninety will come down and
do another ninety and we'll go right
into the radiator so you have a lot of
options you know keep these things in
mind be flexible and just pay attention
to detail so the first thing we need our
fittings so we're going to use these pre
will chill revolver fittings for rigid
tubing now these can be used for the
acrylic tubing for the PE TG tubing but
also for the copper tubing the way these
operate simple compression mechanism to
come in three parts the collar the next
part is an o-ring and last we have the
Barb okay so the way that these work you
take your collar slide it over the
tubing next put your o-ring on and you
put on your Barb slide it nice and snug
the tubing in you'll push your o-ring
onto the top of the Barb and you just
twist your collar right on top what will
happen is the collar will cause the
o-ring to compress making a nice
waterproof tight seal and you can just
give it a little partial turn with the
tool if you need to and you're good to
go so to do all this bending obviously
I'm you know a beefcake I could do this
by hand but we're going to use tools
instead we have a variety of tools here
we will not be using all these but I
just wanted to give you a little brief
introduction to show you what's possible
so this is a really nice primo tubing
cutter and then we have a deburring tool
that will deburr the inside and the
outside of the tubing so you have a nice
clean finish so this is a really nice
ratchet set they have all different
types of mandrel so you can bend all
different sizes of tubing it's a ratchet
type so it doesn't take quite as much
strength which can do really nice
90-degree angle bends we're not going to
use that because to use this it works
better if you actually fill the tubing
with sand or water and freeze it if you
like to do hand bending versus using a
bending tool you can use a variety of
spring benders so with these you can
slide one over and Bend you could have
one inside the tubing and over the bend
so the tubes don't collapse these
won't be quite as precise but if you're
just doing a really quick job you can
use these vendors come in many different
price ranges from you know a hundred
dollars up to over a thousand dollars
depending on you know if they're
pneumatic and what type of mechanism
they use we have the economy size bender
can do 1/2 inch and 7/8 inch which is
really large
uber powerful but it takes a lot of
muscles the difficulty bending depends
on many different factors like your type
of copper and the wall thickness this is
a bending that we'll use today so this
one is really good or this more pliable
copper tubing this is a 180 degree
bender you can see that you can bend
almost all the way around you can buy
these 90 degree as well but I think that
if you can have one tool that can do
multiple angle bands you may as well
just go with this one of the downsides
of using coiled copper tubing is that
it's really hard to straighten out so
this company quicks UK made this really
awesome tube straightener that allows
you to get an almost perfect straight
piece and then you can go through and
bend it to the shape that you want okay
so the first thing we need to do is we
need to cut off a piece of copper tubing
so we're going to use our handy dandy
tubing cutter so we'll just put it ever
so slightly on there and this is just a
continual process we just keep
tightening it up a little bit more each
turn making the blade go a little bit
deeper so you just keep turning it and
tightening it ever so slightly
and it is almost through there so the
next thing we'll do is we'll use our
deburring tool to deburr the inside and
the outside of the cut this is important
because you don't want to have pieces of
metal floating through and also some
fittings will catch on the rough edges
so now we have a piece of tubing it is
not straight so the first thing we'll do
is we'll just straighten it out by hand
as best we can get it without the tool
we're going to use our straightener for
clicks UK the hardest part is getting it
started but once you get it started this
work your back and forth motion
so now you see we have a straight piece
where before we had a curved piece so
the next thing we'll do is we'll do our
bend now if you look at this bender this
allows you to bend 45 have 90 and 180 so
you just match it up and you just keep
bending until the zero works up with the
degree of bend that you're trying to
achieve so for this we want to do it 90
so we'll open up the tool will place the
tubing and against this back brace we're
going to line up the zeros and then
we're going to pull it all the way
across to the zero is at the 90 now when
you do this you want to have a nice
smooth motion you don't want to jerk
around a lot so you don't leave as many
marks on the tubing sort of squeeze
around the corner we're going to have
that zero line up with the 90 there we
go
now you will have spring back when
you're bending so you can go just a tad
further because it will Bend back in
that way you'll offset
and you pull that out and now you have a
math degree bit okay now that we have
this bent we're ready to a measure cut
it and fit it into the cross test so 99
Ben's love to go here's one tube done
all right well that was pretty
straightforward well we did take a right
turn so let's get the rest of the tubes
bent and installed see what it looks
like all right done well let's get the
powder coated
man this looks awesome dude it looks
amazing doesn't it yeah what's super
cool about powder-coating is you can
choose any color of the rainbow
to really match your bill yeah I mean we
have a lot of different design elements
but that allows us to tie everything in
and unify it in one beautiful package
rich thanks so much for coming in really
appreciate your help it was a pleasure
don't forget to check out our next
episode where we're gonna make this
thing look even more badass with custom
lighting thanks for watching g-force
garage the ultimate resource center for
designing building and customizing your
GTX PC
Title: NVIDIA DRIVE Concierge With Omniverse Avatar
Publish_date: 2021-11-11
Length: 27
Views: 16126
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/7zKAKgc7Ecs/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 7zKAKgc7Ecs

--- Transcript ---

what kind of driving modes do you have
i have max range super hush sport and
super sport which would you like i don't
want to be late let's go super sport
got it super sport it is remind me when
my range drops below 100 miles
got it i will tell you when the range
has dropped to 100 miles thank you
Title: AstraZeneca: Accelerating Drug Discovery with Machine Learning and AI on Cambridge-1
Publish_date: 2021-09-07
Length: 140
Views: 14448
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/8gQUkD9dm1U/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLC1xQz3Q-gAzKQicqJ5RBix8YOW6A
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 8gQUkD9dm1U

--- Transcript ---

[Music]
as a medicines company we're obviously
really focused on our patients we're
trying to
re-imagine how you do drug discovery in
the 21st century increasingly people are
reimagining drug discovery as an
engineering problem as much as a science
problem so our partnership with nvidia
and the creation of cambridge one
couldn't have come in a better time
within the drug discovery community and
ourselves at astrazeneca we're using
modern machine learning and artificial
intelligence to try and accelerate that
process so that we can get medicines to
people that need them more quickly
we're using machine learning to help us
discover new molecules so in medicinal
chemistry we're using it to enhance our
digital pathology efforts so that we can
screen patients more quickly and many
other points through the drug discovery
life cycle we're trying to learn a
representation of chemistry that the
model can reason over in a very similar
way to a model that learns the structure
of english and a natural language model
it learns the the grammar of english
we're trying to train a model to learn
the grammar of chemistry and this will
help us not just generate new molecules
but understand the similarity between
molecules and generally contribute to
and accelerate the business of finding
new medicines what we've tended to do
because of the computational constraints
that we operate under
is to take a pathology slide tile it
break it up into segments and then
process each of those segments
individually because the image itself is
too large
trouble with that is that there's a lot
of important information
that you get from looking at the whole
slide that a pathologist gets from
looking at the whole slide and it's
really quite a challenge to recover that
information once you split it up and
tiled it so we're hopeful that by
processing the whole slide images in one
go on cambridge one we'll be able to
retain a lot of the information that a
pathologist might use
having one of the most powerful
supercomputers in europe dedicated
entirely to healthcare sat directly on
our doorstep is fantastic not just for
us but for the whole of the uk health
ecosystem and it's going to allow us to
do things that we couldn't do before
Title: GeForce Showcase: Top VR Games for Oculus Rift
Publish_date: 2016-04-19
Length: 193
Views: 342298
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/8nS6b9jEXzU/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 8nS6b9jEXzU

--- Transcript ---

20:16 is all set to be the year of
virtual reality and once your rig is
ready to rock with an NVIDIA GeForce GTX
970 GPU or higher you'll need to know
which titles to try so here's our
rundown of the most anticipated VR games
come in to the oculus rift bundled with
all pre-orders of the oculus rift is the
incredible eval current set in the EVE
Online universe this is a deeply
immersive interstellar dogfighting sim
which pits you against other Ric users
in multiplayer team battles as you
advance in the game you'll be able to
unlock new ships weapons and equipment
to further cement your reputation as the
savior of the galaxy and you'll even
have the voice of Battlestar Galactica's
starbug for company along the way like
this back on terra firma project cars is
already one of the most beautiful and
technologically impressive racing games
out there but the addition of a VR mode
when the game launches on the oculus
store promises to take things up a notch
being able to sit in a cockpit and have
true depth perception makes an enormous
difference to your overall experience
meaning nailing that apex has never felt
so good spinning into a slightly nearer
Earth orbit is the gripping space
survival game adrift if you've ever
wanted to experience the hopelessness of
oscar-winner gravity firsthand then this
is about as close as you're likely to
get suffering from amnesia and a heavily
damaged DVA suit you must navigate the
shatter remains of your space station to
find resources to keep your life just
long enough to effect an escape to earth
if you're after an engrossing VR
narrative title then look no further
than Kronos an adventure RPG steeped in
atmosphere it's easy to control a young
hero who's trying to save his homeland
from an evil dragon by navigating an
ancient labyrinth which opens only once
a year every time you fail you must wait
another full year before reentry and
with each passing season your character
ages and develops new traits
growing older and wiser which affects
how you can then tackle the games
puzzles alternatively if you want to
channel your inner Shackleton then you
need to play edge of nowhere this
third-person action-adventure sees you
travel into the wintery wastes of the
Antarctic in a bid to discover the
whereabouts of the missing expedition T
but nothing is quite as it seems as you
struggle to keep a grip on both your
life and your sanity to finish on a
lighter note we have Lucky's tale i
bundled for free with every oculus rift
this bright and fun platforming
adventure sees you guiding lucky to fox
through numerous luscious environments
where you'll race around collecting
coins battling enemies solving puzzles
and completing mini-games with so much
hardcore gaming surrounding VR from
launch this provides a wonderfully fun
and cheerful challenge and shows off the
potential of the technology to deliver
refreshing and enjoyable experiences
to get the most out of your VR
experience make sure you have an NVIDIA
GeForce GTX 970 GPU or higher
Title: AI-Driven, Physics-Based Character Animation
Publish_date: 2022-03-23
Length: 95
Views: 96313
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/8oIQy6fxfCA/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 8oIQy6fxfCA

--- Transcript ---

Physics simulation has revolutionized how
we animate physical phenomena like cloth and fluids.
But animating characters remains a manual and tedious process.
Traditional animation techniques tend to 
produce stiff and unresponsive behaviors.
We are using reinforcement learning to develop
more life-like and responsive physically simulated characters.
Our character learns to perform life-like
motions by imitating human motion data,
such as walking, running, and sword swings.
Our character is put through an intense training
regimen for 10 years in simulation.
Thanks to NVIDIA’s massively parallel GPU
simulator, this just takes 3 days of real world time. 
The character then learns to perform a large variety of motor skills.
Once the character has been trained, it can use those skills 
that it has learned to perform more complex tasks.
Here, the character is trained to run to a
target object and knock it over.
We can also steer the character to walk in
different directions like you would with a game character.
Our model allows the character to automatically
synthesize life-like responsive behaviors to new situations.
We can also control the character using natural language commands.
For example, we can tell the character to
do a shield bash or swing its sword.
We hope this technology will eventually make
animating simulated characters 
as easy and seamless as talking to a real actor.
Title: Storytelling to Storyliving: ILMxLAB on the Future of Immersive Media
Publish_date: 2019-06-12
Length: 91
Views: 8203
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/9-3n92AzJaI/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLAf9BZFs-Fwf1qpHOSY7WFMd2_gyg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 9-3n92AzJaI

--- Transcript ---

I'm Vicky top-spec I'm the executive in
charge of Iowa Mac's lab which is with
this film's immersive entertainment
division virtual reality and augmented
or mixed reality really offer unique
opportunities because they can do things
that traditional media cannot there is a
sense of the power of being there you
really are inside the world we've never
been able to do that before it's always
been a distance between us and the
screen so that is this idea of moving
from storytelling to story living the
collaboration between Nvidia epic and
ILM xlab around star wars reflections
was a really really important step as we
move toward this idea of photo real
immersive experiences what our TX ray
tracing actually enables is for us to
add reflections and soft shadows and all
things that cause us to really believe
the world that we are now stepping
inside in a way that was never before
possible and and videos managed to bring
the cost down so rather than operating
on a very Hana machine it's actually
accessible not just to us but to
everyone that's engaged in storytelling
as the power increases and the
accessibility also increases the kinds
of stories the kinds of experiences that
we're going to be able to create and
invite people into are going to be
something that we've never experienced
before
Title: ARK: Survival Evolved Developer Interview at E3 2015
Publish_date: 2015-06-17
Length: 163
Views: 43348
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/91cDH-kg9B4/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 91cDH-kg9B4

--- Transcript ---

my name is Kate Hendricks and I am the
senior technical and gameplay designer
for Studio wildcard and I'm here to talk
about Ark survival evolved studio wild
card consists of a core group of about
nine developers the studio is a mix of
passion and really dedication to making
games that we want to play things that
we really find fun a lot of games out
there have this formula of antagonism
between players and we decided to
instead go with a model where antagonism
against the world is the primary driving
force behind the gameplay
maybe a character just wants to build
and so they've dumped a bunch of points
into their crafting stats and their
weight capacity and we've got just a
huge number of things that you can craft
in the game and we're always adding more
others however could choose to
specialize as warriors and get all of
the weapons and all of the explosives so
the the skills and the crafting have
been really guided around trying to
create a multiplayer experience as
opposed to again find a guy bludgeon him
with a rock and be done with it all of
the tools that Unreal 4 provides are
just better I would never go back at
this point you can edit any part of the
engine you want and that's had a huge
hand in doing things like incorporating
some of Nvidia's really really cool
features to the big game works features
that were interested in right now our
wave works & Turf while our water and
our terrain are very very beautiful we
want to be able to take those and make
them really pop really make people go
holy cow this is fantastic and game
works is going to be wonderful for that
that the most exciting things that are
yet to come are the boss arenas and the
big boss battles that we have prepared
we have a lot of plans for taking and
making actual boss arenas that provide
their own individual challenges there
two three more bosses on their way that
are going to be just tremendous really
really dynamic really challenging fights
that you're gonna be able to bring an
army of dinosaurs and a bunch of your
friends with and it's just gonna be
dinosaurs vs. giant monsters and it's
gonna be an absolute blast
you
Title: Augmenting Radiology with AI
Publish_date: 2019-01-18
Length: 194
Views: 25426
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/9fAcjfnWyso/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 9fAcjfnWyso

--- Transcript ---

[Music]
as I travel around the world talking to
fellow radiologists I think the
fundamental feeling we get universally
is that we are at our absolute limit
providing care to our patients I think
that we're seeing radiologists facing
increasing workload and associated with
that you're seeing we're seeing burnout
we're seeing more data from patients in
regards to everything from the
electronic health record to their
imaging a CT scan that used to be a
thousand images is now 10,000 images as
datasets become bigger and more complex
and the expectations from our referring
physicians are requiring us to give much
more than just the traditional
qualitative interpretation to more
actionable phenotypic characterization I
barely have time to measure lesions let
alone do this what we call precision
radiology I will never see every cancer
I will always have to ask certain
numbers of my patients to have
additional follow-up or biopsies to find
the answer and I never feel like I have
enough time to talk to my patients so I
see artificial intelligence tools as
really helping me with all of those
challenges so one of the things
happening in radiology is a big demand
to create a more efficient environment
and that challenge
coupled with the need for more
quantitative imaging makes artificial
intelligence applications a very
important tool in our ability to provide
better patient care what we're looking
for is leveraging AI machine learning
all these advanced IT tools to achieve
data-driven or evidence driven machine
human optimized workflow collaboration I
like the term instead of artificial
intelligence augmented intelligence
where we would use these tools to help
enhance the way we practice
whether it's pixel-based trying to
diagnose something or aiding in
diagnosing two things like making our
workflow and ergonomics better you know
hanging protocols and PACs or work lists
or scheduling patients like I would love
to see AI and deep learning be used for
things that take a lot of our time I
think AI has the potential to be a an
excellent contributor to the entire
image interpretation lifecycle beginning
from image acquisition all the way to
the interpretation and communication of
those results I think that we're going
to get to the best vision of the future
if we can collaborate and create an
effective ecosystem using artificial
intelligence tools enabling better care
for our patients
I believe that AI will empower
clinicians to work better more precise
which will downstream lead to better
outcomes for patient care
you
Title: Building Autonomous Rail Networks in NVIDIA Omniverse with Digitale Schiene Deutschland
Publish_date: 2022-09-22
Length: 119
Views: 27359
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/9FHzanjkd9k/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 9FHzanjkd9k

--- Transcript ---

Digitale Schiene Deutschland - or DSD - part of German  
railway network Deutsche Bahn, 
is designing and building a digital twin of its rail  
network to maximize transport efficiency 
and capacity without building new tracks.
Realizing a digital twin of 5,700 stations, and 
33,000 km of railways is a massive undertaking  
requiring a custom 3D pipeline connecting DSD’s HD map  
with CAD datasets and simulation tools 
including those from the Siemens JT ecosystem.
In NVIDIA Omniverse, based on open 3D framework 
Universal Scene Description, the teams are  
able to connect and aggregate critical data 
sources into a single high-fidelity model.
With a single source of truth, DSD can use the 
digital twin to test and validate the entire  
network enabling continuous improvement in 
the deployment of vehicles and railways,  
maximizing operational efficiency and speed while reducing costs.
Beyond simulation, the digital twin 
will provide the environment to train  
the complex AI that manages the fully automated railway system.
Running on NVIDIA OVX, the digital twin can 
be synchronized to the real-world trains  
and rail networks via 5G-enabled 
sensors and 5G edge microcells.
If a train's cameras or LiDAR sensors detect a 
potential hazard, its precise location can be  
quickly relayed to the rest of the trains in the 
network, preventing a collision or congestion.
If the AI-powered computer vision-enabled 
cameras at a station notice an issue,  
an event can be triggered to instantly notify personnel.
The intelligent sensors are trained and optimized 
on a combination of real-world and synthetic data,  
generated by Omniverse Replicator - ensuring the models can perceive,  
plan and act when faced with any scenario
With Omniverse AI-enabled digital twins, Deutsche Bahn 
is working to increase the efficiency 
and capacity of the existing rail networks,  
reducing waste and carbon footprint while 
keeping passengers and goods on time.
Title: AI-Powered Cancer Screening Applications Managed with NVIDIA AI Enterprise
Publish_date: 2021-11-28
Length: 127
Views: 32614
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/9gmeVAmsTW4/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 9gmeVAmsTW4

--- Transcript ---

One in eight women will develop breast cancer in their lifetime.
The key to improving their outcomes is regular screenings.
AI can dramatically speed up this process but integrating AI 
solutions into existing infrastructure is complex.
More than half of AI projects never make it into production.
NVIDIA AI Enterprise helps streamline that integration.
With NVIDIA-Certified Systems and an end-to-end,
cloud-native suite of AI and data analytics software,
it enables the rapid deployment, management, and scaling of AI workloads.
With NVIDIA AI Enterprise, both new and existing 
applications can run side by side  
and be centrally managed in an environment
IT administrators are already familiar with.
Let’s take a look at two applications which leverage AI 
to improve breast cancer detection and treatment.
Vyasa uses a deep learning analytics solution
to quickly search and filter patient records
— including unstructured records — by simply asking a series of questions.
Here we filter by age, time since last visit, and other risk factors.
Now let's look at iCAD's ProFound AI software.
It can act as an extra pair of eyes for radiologists reviewing mammograms.
Here we see a 2017 scan…
.. being compared to a 2018 scan.
iCAD’s application is able to automatically
identify an area of suspected cancer
and generate an accurate short-term breast cancer
risk estimation that is personalized for each woman.
With NVIDIA AI Enterprise, hospitals can put
modern solutions like Vyasa and iCAD into
their caregivers’ hands, without changing existing infrastructure.
To learn more, visit the links in the description.
Title: NVIDIA GTC 2019 Show Highlights
Publish_date: 2019-03-20
Length: 164
Views: 24466
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/9K9We5mZY9w/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 9K9We5mZY9w

--- Transcript ---

[Music]
9,000 of the world's best developers
researchers and business leaders swarm
Silicon Valley this week for the 10th
annual GPU technology conference the
world's premier AI developer event this
year GCC boasts more than 600 sessions
and labs aimed at harnessing the power
of AI across industries to speed data
science rate race realism in real-time
bring AI to the edge make self-driving
cars reality and much more Nvidia CEO
Jenson Wang kicked off the show with a
knockout keynote unleashing an avalanche
of news showing how Nvidia is
accelerating the world's most dynamic
industries this year and our biggest
exhibit hall ever more than 200 partners
showed off the latest of what's possible
with AI in fields from agriculture to VR
invidious booth at the heart of the hall
featured more than 40 demos attendees
got hands-on with Gaughan a powerful app
that uses generative adversarial
networks to create photorealistic images
with just a few strokes and a test of
real or rendered unities real time rate
race BMW coupe was indistinguishable
from the real thing in a crowd pleaser
was a remaster of quake 2 showing off a
classic game in a spectacular new light
attendees got a chance to see a dozen
cars sporting the latest sin
self-driving tech from a massive
semi-truck from - simple - II rides Robo
taxi - Auto X and their autonomous
grocery delivery service popular for
selfies was invidious bb-8 which had the
latest intelligent cockpit experience on
display nearby and the robotic showcase
a host of dexterous robots were in
action and even underfoot Nvidia jet
bought AI kit for makers took to task in
a miniature Lego City while AWS showed
off dinosaur tracking tech in the VR
village attendees immerse themselves in
virtual worlds made possible through
Nvidia the standout was a chance to
customize a Koenigsegg Jesco mega car to
your heart's content inside holodeck we
had a record number of startups at GTC
in the inception showcase eight of the
best presented their work to the crowd
from preventing counterfeit sneakers to
pushing
edge of precision robotics according to
Forbes magazine GTC is among America's
top conferences for women looking to
further their careers an AI with events
tailored for attendees including the new
women's early career accelerator it's
been another record GTC and we're
already looking ahead to the next one
until then find out how you can get your
hands on all the show content in the
weeks ahead and GPU tech conference calm
Title: NVIDIA Quadro K2000, the World's #1 CAD Accelerator, at SolidWorks World 2014
Publish_date: 2014-02-10
Length: 92
Views: 27544
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/9nf7DyJwk5o/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 9nf7DyJwk5o

--- Transcript ---

hi using here with Nvidia at SolidWorks
world and I'm going to show you a
demonstration of the project a 2,000
graphics card running SolidWorks 2014 so
I'm going to show you a demo of the
Redback spider chassis vehicle here
powered by the quadrille k mm we have a
perfectly working model with all these
details and you can see the
interactivity look how fluid it is so
it's totally workable any changes that
you need to make in the model you can
make it a Nia sleeve and this makes her
a very usable environment as a result of
moving to 2014 with the quadruped k2000
we're seeing performance gains of up to
50% which you in any workflow is
significant you can see greater detail
on our model here i just enabled the
real view graphics and you can see the
shadows here now
a beautiful representation and i'm going
to go ahead and enable the shadows and
shaded mode now as well you can see the
shadows all of a sudden appear as well
so in addition to the great performance
that you just saw with the k2000 it's
all it works 2014 you also have to take
into consideration the stability offered
by the quadric k2000 as well as the
reliability of all drivers all the
support that nvidia gives customers and
that's what really makes keep the k2000
of the number one cat accelerator for
more information please go to WWE and
video comm slash Quadra thank you very
much
Title: NVIDIA CES 2016 News Summary
Publish_date: 2016-01-05
Length: 88
Views: 34623
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/9q8MZkQiVsk/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: 9q8MZkQiVsk

--- Transcript ---

hi it's Dani Shapira I'm the senior
director of automotive at Nvidia we're
here at CES in Las Vegas where we just
unveiled the new Nvidia Drive px 2 and
videos
AI platform basically enables automakers
to train a deep neural network in the
cloud and then bring that rapidly
accelerated process of learning into the
vehicle so now as we drive we can sense
everything that's going on around the
car using cameras radar lidar and
ultrasonic sensors were able to process
that massive amount of data in real time
and have a true understanding of
everything that's going on around the
car so if there's unexpected road debris
jaywalking pedestrians our car will be
able to sense understand it and then
take appropriate action to plan a safe
path the complete end-to-end platform
encompasses our Nvidia digits deep
learning solution the drive px 2
platform for use in the vehicle as well
as our drive work software for
developers this is being used by
automakers all over the world including
outtie BMW Ford Mercedes and Volvo who
actually has just announced that the
drive px 2 will be part of their
autonomous fleet that will be a deployed
next year in Sweden Nvidia is helping
automakers to accelerate the race the
autonomous car
you
Title: NVIDIA Studio | NVIDIA RTX Takes Substance Painter to the Next Level
Publish_date: 2019-11-06
Length: 125
Views: 15414
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/a01C2JKLZmU/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: a01C2JKLZmU

--- Transcript ---

so substance painter is our 3d painting
package and so it's designed for the
creative individuals and so if you have
you know an NVIDIA GPU is already
accelerated but when you add to that the
RT cores you get an even faster results
so whether you are a professional
whether you're a hobbyist whether you're
you know just starting into 3d substance
painter is aimed at being the tool that
allows you to just create freely my
daily drive is a game artist in a mobile
company I actually manage it and produce
all the content on a daily level and
painter is just a tool which is like
automatically open through the entire
days or the way I work it's another list
I do both characters and environments
and we like we produce all our assets
using substance tools using painter we
texture we bake everything inside of it
and prepare all the models for for our
projects now one of the things that is
unique about substance painter is we
built it in such a way as to be very
interactive quick and easy to use and so
some of these kind of I guess we'll call
it mundane tasks of like oh I need to
make a set of maps you know you don't
want that to take a lot of time so we
really worked hard to use GP accelerated
bakers and having the arti course just
gives you that much extra room and extra
speed to be able to work as quick and as
possible as you can before I used to
like start the process and go grab a cup
of coffee but these days no coffee
because it's immediate I remember you
know when I first started I mean so many
things were impossible and how we were
working so slowly and how things are
real time so I'm more and more excited
about being able to you know not have to
wait for renders anymore being able to
you know see what I want to create you
know in real time we have real time
ray-tracing now GPUs especially with our
TX and the RT cores I mean the way that
that is bringing such speed and
flexibility to were close it's really
exciting
you
Title: Bringing Avatars to Life with NVIDIA Omniverse Avatar Cloud Engine
Publish_date: 2023-01-03
Length: 70
Views: 24065
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/a05X3rAfYLs/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: a05X3rAfYLs

--- Transcript ---

NVIDIA Omniverse Avatar Cloud Engine,
also known as ACE,
opens a new world of possibilities for
bringing avatars to life.
By combining the realistic tones of the
NVIDIA Riva speech AI
with the life-like expressions and
gestures from animation AI
like Audio2Face.
Creating the next generation of digital interaction
has never been easier.
ACE lets you breathe life into a
wide range of avatars from 2D 
portraits driven by your own video
and 3D characters from
top avatar creation platforms
like Ready Player Me.
ACE is now available for early access development,
connecting the core AI building blocks of
avatar intelligence to virtual 
characters built on any engine
and deployed at scale on 
any public or private cloud.
I can even answer challenging questions
using the power of a large language model.
Together with NVIDIA 
Omniverse Avatar Cloud Engine,
we'll make the metaverse a smarter,
more engaging space for all.
Title: Viktoria Modesta: GTC 2021 AI Art Gallery
Publish_date: 2021-04-14
Length: 126
Views: 10431
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/acu6krK39y8/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: acu6krK39y8

--- Transcript ---

hi my name is victoria modesta and i'm a
bionic performance artist and a creative
director
and i'm here in los angeles at the 3d
live and capture studio
this year gtc is free and available to
everyone
and the program is just about as
interesting to artists as it is to
engineers
and here's why in all of digital arts
the aesthetic and the feel of artists
using deep learning has become pretty
well known over the years
in music sampling has changed music
before about once every decade
but neural networks are bigger more
intelligent samplers
and artists are discovering that they're
whole new instruments
[Music]
so we'll have never before heard sounds
that's a company that creates style
transfer from musical instruments
and will have holly herdmann who pushes
the craft of composing with deep
learning
showing work here
there's also a hackerjewel called
dadabots if you go to the youtube you'll
see they made
infinite streams death metal and math
rock
now takui a playful japanese musician
made a neural beast box buddy
you'll give it a beat and it improvises
with you but in visual arts deep
learning is also becoming famous and
important not just a novelty
we'll have artists like helena sarin and
sophia crespo who have been
using the mastery ai has with organic
forms like vegetation and rocks
and pindavan almond is going to show
paintings his robots draw
haki nagano and kyle mcdonald who both
work with generating human faces will
chat about how ai
can capture and modify our appearance so
if you're an artist who wants to know
more about ai
or an engineer and you're curious what
art to see in machine learning
you can check out the ai art gallery and
get playlists for both technical art and
conceptual talks
on wednesday 80 and pacific holly hernen
hotend man and databots will host a live
panel
where you can ask them questions about
deep learning i'm really excited about
that one
i'll see you there
Title: Transforming Financial Services with VMware and NVIDIA
Publish_date: 2021-05-18
Length: 169
Views: 6834
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/AdzQMzR2ohY/hq720.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AHUBoAC4AOKAgwIABABGC8gVChyMA8=&rs=AOn4CLD5Nf0i2vcF864F-VNpILy6x7Qi6g
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: AdzQMzR2ohY

--- Transcript ---

[Music]
vmware and nvidia are working together
to transform the future of work for
financial services
secure remote capabilities have now
become a business continuity
must have today's financial services
professionals require
access from anywhere without impact to
performance
they use graphics intensive applications
to get fast access to news and analyze
real-time market data work across
multiple monitors
and require the same performance and
security whether working
in the office or remotely the vmware
anywhere workspace combined with
nvidia gpus and software provides an
enhanced user experience
with the consistent level of performance
required for employees to thrive in
today's new distributed and remote work
environments
the bloomberg terminal is used by almost
every major finance and
investment corporation to monitor
markets analyze securities
and place trades the quad 4k resolution
monitor setup was configured on the gpu
accelerated virtual desktop while only
three 4k monitors are supported on the
cpu-only virtual desktop
adding a gpu results in a 33
increase in screen real estate allowing
more information to be presented
a launch pad was built using the top
components downloaded by traders
even when the user is not interacting
with the dashboard
notice the much higher cpu utilization
on the cpu-only desktop
the cpu-only virtual desktop hovers at
around 100
while the gpu-accelerated virtual
desktop hovers around 50
the gpu is able to offload work from the
cpu and reduce overhead
bloomberg tv is often used by traders to
stream real-time market news
notice the smoother video playback on
the gpu accelerated desktop
while the cpu-only virtual desktop drops
frames
notice how much longer the trader needs
to wait when he is downloading data on
securities to excel for further analysis
[Music]
scrolling through the news is smoother
on the gpu accelerated virtual desktop
while the cpu-only virtual desktop is
jittery
you can instantly see values when
hovering your cursor
over points on charts on the gpu
accelerated virtual desktop while the
cpu-only desktop experiences delays
with the new nvidia a10 and a16 gpus
financial services firms can future
proof their vdi
investment solutions for more
information
visit vmware.com or nvidia.com
Title: NVIDIA GTC Spring 2021: Healthcare Special Address
Publish_date: 2021-04-26
Length: 1688
Views: 11722
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/AfC7-Iksl_M/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: AfC7-Iksl_M

--- Transcript ---

[Music]
i
am a healer
bringing clarity to our most important
questions diabetic retinopathy can
happen if blood sugar stays too high
over a long period of time and comfort
in times
of uncertainty what procedure am i
having today
you are having a bronchoscopy
shining a light on a path forward
and finding answers when every second
counts
i am watching over our frontline heroes
creating a safer place to do their
life-saving work
[Music]
guiding them to faster answers
and delivering care wherever it's needed
[Music]
i am opening new worlds of discovery
and mapping new treatments for every
individual
[Music]
creating a community to collaborate
with thousands and sharing
knowledge that can heal millions
i am a i
brought to life by nvidia and brilliant
healers
[Music]
everywhere
[Music]
welcome to gtc 2021 i'm kimberly powell
vp of healthcare nvidia
we are walking into the ai healthcare
era here at gtc
there's so much to see and learn let's
get started
there's a digital biology revolution
underway and it's generating enormous
data
far too complex for human understanding
terabytes of data per day from genomic
sequencers
electron microscopy and high content
screening systems
and petabytes of data per year from just
a single hospital
with algorithms and computations
standing at the ready
we now add the third ingredient data to
truly enter the ai healthcare era
and at a time we need it the most with
healthcare spending shooting past 8
trillion
outpacing gdp thousands of diseases
still without treatment and a deadly
public health crisis
we need ai assistance automation
innovation
to accelerate discovery and improve
health care outcomes
and the world has mobilized exponential
growth in ai research with over
20 000 papers last year that's 50 papers
a day
and the investment in drug discovery has
skyrocketed to 13.8 billion
a private ai investment four and a half
times that of 2019.
we are delighted to be hosting such a
distinguished and diverse healthcare
ecosystem together here at gtc which has
become the ai healthcare conference with
over 175 healthcare sessions from 150
healthcare organizations
we bring physicians biomedical
researchers data scientists computer
scientists
across domains of drug discovery
genomics imaging smart hospitals
together in one unique conference i'd
like to welcome you all to gtc
today we're going to talk about how ai
and healthcare requires domain-specific
computational platforms
and some amazing breakthroughs let's get
into it
nvidia clara is our computational
platform for healthcare
our mission is to bring the entirety of
state-of-the-art computing approaches
spanning
accelerated computing artificial
intelligence data analytics and advanced
visualization
to the healthcare industry addressing
domains of computational genomics
biomedical and clinical natural language
processing
imaging of all kinds intelligent
instruments
and smart hospitals all the way through
to computational drug discovery
each has a set of libraries pre-trained
models application frameworks and
reference applications to accelerate the
development of healthcare applications
nvidia clara has expanded tremendously
over the last year
we have developed over 40 pre-trained
models that jump start ai development
and reduce data needed to adapt these
models for specific use cases
with over 12 libraries and sdks we
provide the development tools and
reference applications so developers can
iterate faster and streamline
applications to production
clara downloads from ngc our software
hub shot up
five times to 125 000 downloads last
year
and we're so excited about the adoption
of monae
dubbed the pie torch of imaging it's
matured to production
in just 12 months with the with the
dedication of over 50 contributors and
eight working
groups from around the world the
adoption has been amazing with over 50
000 downloads our nvidia inception
program
has over 1200 ai healthcare startup
members
it's the largest of all of our
industries and our install base of gpu
instruments
is growing now larger with the ai models
and the multiple models that are being
deployed
today we're going to talk about clara
discovery clear imaging and clara agx
to discover develop and deploy ai in
healthcare
let's start with clara discovery
understanding disease and discovering
therapies is humanity's greatest
challenge
drug discovery is a multi-disciplinary
endeavor that today
takes well over 10 years and fails 90 of
the time
gpu computers for simulation and imaging
digitizing biology
and now ai are all converging to make
computation
an invaluable tool nvidia clara
discovery is our suite of libraries
ai models frameworks and applications to
accelerate the pace
and broaden the scale of drug discovery
let's look at some of the latest
advances using computing
to advance drug discovery
transformer-based neural network
architectures
which have become available only in the
last couple of years
allow researchers to leverage
self-supervised training methods that
avoid the need for large labeled data
sets
a challenge in drug discovery and
healthcare overall
transformers are skilled at learning the
language of many diverse data types
using unsupervised
learning and fine-tuned to a specific
task using supervised
learning in silico medicine has
pioneered
these methods and recently announced a
novel pre-clinical drug using generative
models
google's alpha fold uses transformers to
predict a protein structure from an
amino acid sequence
natural language processing models have
demonstrated human capability
in question and answering summarization
and language generation
and most recently transformers are being
used for image generation analysis
nvidia research recently came up with a
novel transformer architecture for 3d
segmentation called u-netter
nvidia created megatron a framework
purpose built to train
giant transformer models across multiple
gpus
and multiple nodes with fast data
loading and scheduling
it uses every accelerated computing
invention nvidia has
megatron reduces training time so we can
train huge models with equally huge
unlabeled data sets
with biomedical data at scale of
petabytes
and learning at the scale of billions
and soon trillions of parameters
transformers are helping us do and find
the unexpected
today we're announcing megamobart a
pre-trained model that understands the
language of chemistry
co-developed with astrazeneca
pre-trained mobart
with one billion compounds from the zinc
database on
32 dgx a100s that's 256 gpus
astrazeneca is using mega mobart for
countless drug discovery and development
applications
you can use mega mobart for molecular
generation de novo molecular design
or important task specific drug
development applications
like optimizing the molecule for
desirable properties like
pharmacokinetics
absorption distribution metabolism
excretion toxicity
or for synthetic prediction
how makeable is this compound
let's take a look at how these models
can be used in interactive
chemi informatics similarity search
let's explore the results of a virtual
screen that identified hits from the
kimbel database
predicted to bind to three
chymotrypsin-like protease
osiris-cov2 drug target we'll look at
how nvidia's rapid suite of software
libraries
can cluster molecules by similarity in
just a few seconds
the molecules represented here were
first featurized by calculating morgan
fingerprints
which are a binary vector representing
the molecule
next rapids was used to reduce the
dimensionality of the data
with pca and umap and finally k-means
was applied for cluster assignment
the u-map projection of the campbell
compounds is shown in the plot
with colors corresponding to cluster
identity
compounds identified in the virtual
screen are plotted with large diamonds
they are also shown in the table where
columns with molecular properties can be
added
and removed next two hits from the sars
cov2 virtual screen
are selected and mega mobart infers five
compounds between them
in the molecular latent space it has
learned
in this table the compounds selected
from the screen are shown at the top and
bottom
the five generated compounds are in
between these new compounds are novel
ai generated molecules which can be
prioritized for further study by
scientists and researchers
that was how a transformer model can be
used to generate chemistry
now i want to talk to you about
transformer models that understand
doctor's notes
today we announce gatortron jointly
developed with the university of florida
health and nvidia healthcare
hypergator a dgx superpod domain donated
by nvidia co-founder chris malikowski
last year
was immediately put into action using
nvidia megatron
pre-training on the last 10 years of
epic electronic health records
containing over 300 million unstructured
doctor notes
and 2 million patients the results are
amazing
the world's largest clinical model at
3.9 billion parameters
achieving the state-of-the-art on
clinical nlp benchmarks
of named entity recognition and beating
university of florida's
existing patient data anonymization
methods
nvidia dgx and megatron have put this
capability within the reach of every
medical center
democratizing the development of
language models
learning the language of specific
institutions and specializations has
unlimited healthcare applications from
matching patients to life-saving
clinical trials
to predicting patient complications and
giving physicians
decision support i'd like to
congratulate the university of florida
and the nvidia research team
on an amazing achievement and
inspiration to the industry
recursion is a biotech company using
leading edge
computer science to decode biology and
to industrialize drug discovery
leveraging technology for automation and
data science across
biology and chemistry it modernizes drug
discovery and development
recursion produces 10 million human
cells of data each week
that's 80 terabytes using deep learning
on dgx recursion is applying their
operating system
to automate classification of cell
responses to small molecule drugs
and extract insights and identify
relationships from data sets
far too complex for humans recursion
built by ohive one
in just three weeks leveraging nvidia
dgx superpod architecture
it ranks number 58 on the top 500 making
it one of the most powerful
biotech computers in the world
today we are announcing a partnership
with schrodinger to put the world's
highest throughput computational drug
discovery supercomputer
in the hands of every pharmaceutical
company
and thousands of research institutions
today schrodinger's drug discovery
platform
is an ultra high throughput search
screen and simulation platform
using their famous fep plus
state-of-the-art
absolute binding free energy
perturbation calculations
just introduced in 2020 absolute binding
free
fep plus achieves experimental accuracy
optimizing drug candidates for safety
and biological
efficacy but it's five times more
computationally expensive than the
relative binding fep
working together we have optimized
throughput of absolute binding feb plus
on dgxa100 pod using the a100 mig
architecture
we can now simulate over 1 million
drug candidates per year
if you were to do this in the lab it
would cost over 100 million dollars and
take over five years
schrodinger's own drug discovery success
like yesterday's announcement of the
pre-clinical cdc-7
inhibitor and their use of hundreds of
millions of gpu hours
prove advanced computing methods
accelerate discovery
and it can reduce the cost doing more in
silico
to improve the downstream success rates
today we partner to give the industry a
giant boost in drug discovery
productivity with schrodinger drug
discovery platform
and the nvidia dgx super pod
let's hear from pat lorton chief
technology officer of schrodinger
explain why computing is so important to
drug discovery
the chemical space for designing new
medicines is nearly infinite
and yet how drug discovery is
traditionally done is based largely on
trial and error
which significantly limits the scale of
exploration
schrodinger is pioneering the next
generation of drug and materials
discovery
by combining a deep understanding of
physics chemistry
and machine learning to develop
solutions that enable scientists to
rapidly
and accurately assess massive quantities
of virtual molecules
the ultimate goal is to find the right
molecule for the right protein
for the right disease access to massive
computing power is critical
to scale the number of simulations and
accurately predict the desired
properties of many more drugs than would
be possible in the lab alone
this allows us and our partners to bring
better medicines to patients more
quickly
by significantly accelerating the speed
and reducing the cost
of pre-clinical drug discovery combining
our novel platform with nvidia's
world-leading accelerated computing
empowers both schrodinger and the
broader drug discovery industry
to do more ultimately developing drugs
with properties that were only dreamed
of a couple years ago
our schrodinger partnership is one way
we are continuing to build out the clara
discovery ecosystem
for the first time in history biology is
being digitized
and we can apply the power of computing
to tackle humanity's
greatest challenge understanding disease
the pieces are coming together new
powerful ai algorithms like transformers
can apply to domains of genomics
proteomics chemistry and imaging
nvidia clara discovery applies state of
the art
to the domain of computational drug
discovery
today we have four new models as part of
clara discovery
mega mobart for generating molecules
google's alphaphobe 1 to predict 3d
structure
from amino acids university of florida
gatortron
the world's largest clinical language
model
and nvidia attack seek denoising
algorithm for rare and single cell
epigenomics
helping to understand gene expression in
individual cells
clara discovery expands our partner
platforms that are optimized for dgx
with recursion's operating system
schrodinger's drug discovery platform
viasa's layer and in silico's chemistry
42
all running on nvidia dgx
let's move from discovery to the
development of ai in healthcare
the delivery of healthcare is an
extremely complex series of data points
feeding decisions our healthcare staff
often need to make in real time
data flows in from health records
medical imaging instruments
lab tests and surgical procedures and
patient monitors
and no one hospital is the same or no
healthcare practice is the same so we
need an
entire ecosystem approach to developing
algorithms that can predict the future
see the unseen and help healthcare
providers make complex decisions
hospitals are generating 50 petabytes of
data each year
from medical imaging instruments and
health i.t systems
that can be used to develop ai to
improve care
efficiency and clinical decision making
hospitals instrument companies and
startups around the world
are building ai applications on nvidia
nvidia clara is a domain specific
application framework to accelerate the
development of ai applications
clara is an end-to-end training
framework an ai assisted annotation
built on one eye training framework and
easy validation workflows
videoclaire has over 40 pre-trained
models available on
ngc with full model credentials on the
architecture
data used to train it and model
performance
and starting from pre-trained models
developers massively reduce the amount
of data and time
needed to adapt these models for their
particular use case or environment
healthcare environments are continuously
changing and data is spread all over the
world
to build robust ai applications we need
to adapt to the changing environment
and learn from diverse data we need an
ai training framework that can live on
the edge and learn from local data
today we are announcing our next
generation of clara federated learning
with global orchestration of edge
clients and
privacy preserving homomorphic
encryption
from a single user interface federated
learning programs can be deployed
managed and monitored a one-touch
deployment with nvidia fleet command can
securely provision the federated
learning
to a certified edge server removing i t
overhead and our privacy preserving
machine learning has introduced a third
layer of protection
homomorphic encryption this allows for
computing to happen on encrypted data
preserving the privacy of the client
data from any types of model inversions
we're excited to announce that bear
pharmaceutical adopts nvidia clara
federated learning to scale their
science and development
and develop new digital business models
with over 50 internal sites spanning
five continents
federated learning can overcome
challenges of data governance and
privacy
and at the same time and with the same
platform it can be used to deploy
on at on-prem hospitals and leverage
real-world data for ai innovation
king's college london and their ai
center for value-based healthcare
are building the nhs ai platform across
england
the ai center and nvidia are partnering
to roll out their flip
federated learning interoperability
platform built on nvidia clara and mmoni
to support
ai systems across the nhs
nvidia clare is being put into
production across the industry
and clara is being integrated into every
machine learning platform
to make it easy for developers to use
and accelerate time to market
integrated into every public cloud to
easily get started
allegro clear ml helps clara developers
manage and scale training experiments
with their ml ops platform
flywheel and xnet offer domain specific
research platforms for
healthcare i.t environments that build
on nvidia clara as their ai annotation
and training engines
and just launched out of stealth is
rhino health
developing a federated and continuous
learning platform
to validate monitor and maintain ai
models
with regulatory great evidence helping
startups and other algorithm developers
translate their ai model
into a clinical application
for our final chapter let's talk about
ai deployment
medical instruments of all shapes and
sizes are building on nvidia
as sensor technology continues to
innovate so must the computing platforms
that process them
and now with ai instruments can become
smaller
cheaper and guide an inexperienced user
through the acquisition process
this is really exciting to think about
ai driven devices can reach
the two-thirds of the population that
doesn't have access to these diagnostics
today
if they're cheaper and easier to use
nvidia computing platforms serve the
diverse computational pipelines of
instruments
from signal processing to reconstruction
to analysis and visualization
these devices are amazing grundium
digital pathology microscope is just a
bit bigger than a tablet
and requires only 15 minutes of training
active surgical developed a novel
hardware agnostic
endoscopy visualization system that uses
machine learning to guide surgeons
and caption health the only fda approved
ai guided ultrasound
to deliver advanced imaging at the point
of care
every instrument company is innovating
with ai and oxford nanopore
technologies is pioneering the third
generation of sequencing
it's being used for sequencing viral
genomes human genomes cancer genomes and
even plants and animals oxford nanopore
uses dgx for deep learning training
and algorithm development to
continuously improve the accuracy of
their sequencing analysis
with the latest algorithm they have
reached the gold standard for human
sequencing
nvidia computing platforms provide
oxford sequencers the ability to have a
single architecture
from embedded to desktop to benchtop as
well as the ability to continuously
upgrade to their new algorithms
inspired by the amazing instruments and
powerful capabilities ai brings
we built the nvidia clara agx developer
kit and i'm delighted to say it's now in
production
clara agx is a universal architecture
for ai medical instruments
real time or high throughput embedded or
as a connected compute
architected with nvidia jetson agx
xavier
nvidia rtx and nvidia mellanox smartnic
it's the ultimate developer platform for
next generation instruments
the clara agx developer kit comes with
an sdk
makes it easy for developers to get up
and running with real-time system
software
libraries for i o processing and video
pipelining
and reference applications to create ai
models for ultrasound and endoscopy
clara agx is a cloud native platform
enabled with nvidia fleet command
to securely deploy ai applications
remotely
the response has been amazing from the
world leaders like medtronic
siemens and carestream to innovative
startups and academic labs all over the
world
we are partnering with industry leading
and sensor interface companies to make
it easier for developers to connect
and leverage gpu direct rdma enabling a
direct path
to gp processing us4us develops point of
care
ultrasound front ends you can use it to
do advanced image reconstruction and ai
processing research and development
aja builds professional pci express
cards for 4k
and ak video input and output this can
be used for endoscopy and surgical
visualization systems
kaya instrument makes pci express cards
for coax press and camera link inputs to
connect to
lots of different microscopes and
connectx 6dx
can be used for just about everything
along with the melanox streaming
technology
it supports the development of fully
scalable high-throughput instruments
with a ubiquitous ethernet physical
interface and cpu offloading technology
with last year flying by we are well on
our way with the clara agx road map
today clary agx xavier devkit is
generally available
and a perfect proxy for agx oren that is
coming first half
next year we will continue to build out
clear agx developer kits to take
advantage of innovations in oren
and our discrete gpus in line with the
oren availability next year
nvidia healthcare is building computing
platforms for the ai healthcare era
the computational instrument for drug
discovery is nvidia dgx super pod
and clara discovery suite of tools today
we announced nvidia and schrodinger are
partnering to bring an
all-in-one drug discovery supercomputer
to the global pharma industry
we shared breakthrough research we are
doing with astrazeneca using
transformers to generate new drug
candidates
and predict which ones are safe and
effective and can be manufactured
we recognize that healthcare is
continuously changing environment
and healthcare data is distributed so we
need nvidia clara federated learning to
develop
robust algorithms at global scale and
bear is well on their way
finally the instruments that measure
biology
see inside our bodies and perform
surgeries are becoming intelligent
sensors
with ai and computing clara agx
developer kit provides a universal
computing architecture
to create the next generation ai medical
instruments
thank you so much for joining nvidia
healthcare at gtc
2021
Title: GeForce Garage – How to Create a Monster Build
Publish_date: 2014-12-05
Length: 636
Views: 468142
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/AhOSJezjmnc/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLBivRpwlZgcKZgm4IXdHqmdrfqjvQ
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: AhOSJezjmnc

--- Transcript ---

Hi.  I'm Andrew with NVIDIA
and you're watching GeForce Garage.
For today's episode we have two amazing guests
that are combining forces to produce a ferocious build.
Hi.  I'm Larry from UltimateMods.net
Hi.  I'm Whilce Portacio.
Together we will combine both artforms
in the ultimate fusion of powerful hardware and stunning graphics.
We'll create a Monster Build that'll be a beast- both inside and out.
To house this beast of a machine
we'll start of with the Corsair 780T case.
The foundation of our PC starts with the ASUS Rampage V Extreme motherboard
built on the X99 platform,
powered by an Intel i7 5960X Haswell-E CPU, 
accompanied by 64 GB of DDR4 Corsair Dominator RAM clocked at 2666 Mhz
And of course, powered by Maxwell GeForce GTX 980 GPUs
But not just one, we have three of them in SLI configuration.
So let's see what happens when we put all this awesome hardware in a room with two powerful nerds.
Everything's unpacked and I start with the motherboard.
Sometimes people put the motherboard in first,
but I find it much easier to work on the motherboard
with the smaller components first on the outside and then mount it in the case afterwards.
Just makes it easier.
I like to work on the outline of everything 
and take a little time in just making sure it's the way I want it.
that means anatomy and everything
I don't like to plan everything to every detail.
I like to just let the drawing tell me what to do.
The thrill I get as an artist is the seeming chaos of "Is it going to work?"
That is almost like a high for me
to be able work my way through "Oh no."  Teetering here and there...
 and then ending hopefully -
With my career so far of 30 years, 
more times than not - it works out.
So now that I this set up, I'm ready to install the motherboard inside the case.
The CPU and the RAM have been installed on the motherboard.
So now I have to put in the I/O shield.
This here is the power supply, so I'm going to put that in here.
These are copics and this specifically is what's called a broad brush.
It's got a really broad tip.  They dry fairly good.
It seems to adhere to the surface pretty well
but we still have to then coat it and protect it later on.
So I'm going to try to use as much permanent ink as possible.
So now I'm ready to install the hard drives.
They go in these slots here.
Here are the SSDs
Now I'm ready to install the motherboard in the case.
So now I'm ready to remove the top part to install the fans.
So now I'm going install the power supply cables.
I put in the graphics cards last because I don't want to bend or break these cables.
So now I'm installing the SATA cables for the hard drives and the solids state drives.
I'm going to now use the brushes.
Unfortunately I forget my nice Japanese ceramic plate.
So now it becomes the scary part.
I came in here with the plan of doing everything with the brush.
Now I'm finding I can actually take the Sharpie and use it like a pencil
and get some effects I couldn't get on a brush.
So that's the fun things about art, getting into a hole
and then finding a way out
and it can turn out to better than I had anticipated.
I used to love playing console games like Halo with my friends, my roommates. 
 We use to have so much fun, 
but after college everybody moved out.
When Halo 2 came out and it was an online game,
I realized there was a lot of communication issues
becuase it required using a headset and how was I going to do that being deaf.
So then some friends said, "Why don't you use a PC?"
This was back in 2007.
So I got into gaming on the PC and "Oh my God!" I love it.
It's so deaf-friendly, if you will.
We can communicate easily.  We can play the games.  
We can be chatting by typing at the same time.
You can even attach a web cam if you want so you can video communication while you're playing.
It's a huge improvement
The headsets on an XBOX and other consoles are so limiting
 because you don't have that access for a separate video cam
but with PCs you can.
You can chat online over video
You can have two screens going on if you want.  
Two or three or four screens, if you want.
Right now I have a three screen system.
A lot of deaf friends really love it, so this is how I got into building PCs in the first place.
It's really nice.
My first computer in 1986 was an Amiga 2000
because I wanted Sculpt 3D because I wanted to do ray tracing.
Ray tracing which is basically high end 3D full rendering and full shading, full shadows and stuff.
Back in the day, each monitor had like 300 scan lines
and so when you start to render to crunch a scene.
I sat there watching every line forming
for two hours per frame because it was so new and so cool.
That sparked me into the digital media and wanting to do art on a computer.
Back then in the day, it was DPaint, a pixel by pixel art program.
so every little pixel was just a big block and you had to figure out how to make shapes with those blocks.
And so that then led me into the pocket PCs, the tablet PCs, the Newton, everything...
 on my quest for the best, user friendly portable digital tablet art tool.
so anywhere I go - convention, on a plane, hotel, airport, on the beach - I can draw, I can work.
Hi, I'm Whilce Portacio and I've been drawing super hero comics for 30 years now.
Hi, I'm Larry and I love building high end PCs.
and together we make awesome rigs.
Performance and Aesthetics.
Power and Beauty.
Don't forget to check out geforce.com/garage for other guides and helpful information.
You're watching GeForce Garage- The Ultimate Resource for Designing, Building and Customizing Your GeForce PC.
Title: Transforming the Transportation Industry with AI
Publish_date: 2021-06-03
Length: 734
Views: 303825
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/akHAK2YOWnI/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: akHAK2YOWnI

--- Transcript ---

[Music]
hi
everyone it's great to be at this
computex i'm allie canney vice president
and general manager of nvidia's
automotive business
ai is the most powerful technology force
of our time and it's transforming
virtually every industry from healthcare
to finance to manufacturing i'm here
today to talk about autonomous vehicles
it's perhaps the most intense ai
challenge
but also one with the greatest impact
the transportation industry is
incredibly large at 10 trillion dollars
it includes not just passenger vehicles
but also taxis
delivery vans commercial trucks and even
specialized vehicles like tractors
this industry is going through a major
transformation in the past
vehicles were defined by their driving
experience a key capability of a vehicle
used to be its horsepower today i'm
going to speak about how advancements in
deep learning and robotics technologies
are redefining the industry
based on a vehicle's computing
capability eventually this entire
industry will be autonomous
and it will take many many billions of
dollars to develop
the mission of the nvidia drive av team
is to develop the self-driving
technology to help
our partners bring this technology to
market now i want to spend some time
talking about how challenging it is to
develop self-driving cars
first you need a high performance
computer architecture and sensor set in
the car that's capable to deliver a
safe driving experience second
you need to ingest many petabytes of
data a day
safely encode and secure it and then
send it to the cloud to then
curate label and identify every object
road user and traffic sign after all
this data is processed
we need to train ai networks on this
really large data set
these ai models then must also be
validated on many
thousands of hours of data to guarantee
safe performance
and finally to further increase the
range of possible scenarios for testing
and optimization
we rely heavily on simulation simulation
allows us to run the av
technology through rare and difficult
scenarios in a scalable
physically accurate and efficient manner
now all of these reasons is why nvidia
is building
our av platform end-to-end from av
chips and computers sensor architecture
data processing av perception and
mapping software
to the infrastructure where we do the
training and create the simulated
digital twin
to fleet command and the operation
center where we can remotely operate
your fleet
finally to road testing nvidia is
building this entire platform to the
highest
functional safety and cyber security
standards
and we're building this platform in
modules so our partners in the 10
trillion dollar transportation industry
can take what they need
since av is such a grand challenge and
will take years to develop
we believe what differentiates av
excellence more than
anything else is the speed of this
development flow
the companies with the best development
flow will be the most successful av
companies
and we believe this because the av
development process is continuously
improving
as we still have much work to do
ai-powered vehicles will continue to get
smarter
and smarter over time as the software is
trained for new tasks
enhanced tested and validated and then
finally that software will be updated
over the air into your vehicle
now one of the key modules in our av
platform is the perception layer where
the autonomous vehicle sees
everything on the road just like a human
driver would
in total you need more than 20 ai models
to take a vehicle to production
first you need some models that help you
see objects
like vehicles bicycles pedestrians
and even road debris you also need
networks to help you see lanes
and to compute the best path to take
when dealing with merges and splits
third you need models to understand
urban scenarios like
intersections traffic lights you even
need networks to understand the gestures
from people on the street since av
is developed to provide a redundant
coverage for safety
we build networks that not only run on
the camera sensors in your vehicle
but we also have object detection
networks that run on
radar and lidar to provide a redundant
detection to ensure a vehicle does not
contribute to an accident
now once you have great perception
software you can perceive
lanes signs poles and lights
which you can then use to build a high
quality driving map
this generated high definition map is
also kept fresh
because an autonomous vehicle is always
learning so drive av
dynamically can update maps after any
vehicle in an oem fleet
drives the same route again the final
core component
of av is the driving or motion planning
application
once you have high quality perception
and mapping software
the autonomous vehicle needs to decide
how to drive
and navigate to a passenger's target
route
this motion planning layer needs to be
very advanced
this is why nvidia has created the
safety force field
which analyzes and predicts the dynamics
of the surrounding environment by taking
in sensor data and determining a set of
actions
to protect the vehicle and other road
users
the sff framework ensures these actions
will never create
escalate or contribute to an unsafe
situation
and includes actions necessary to
mitigate potential danger
having a robust av validation strategy
is critical to make sure an av is safe
of course we need to test avs on the
road but it's really hard to experience
all the corner case situations that
happen in the real world
this is why we also need to test av in
simulation and why nvidia has built our
own simulation platform based on
omniverse
that creates a physically accurate base
simulator of the real world
it's called drive sim only with a
combination of on-road testing
and it's simulated digital twin can you
really make sure an av
vehicle is ready to safely drive now let
me show you what drive sim software
based on omniverse lets you do
in this video as nvidia announced
last year we are partnering with
mercedes-benz to deliver an
av platform to their fleet of vehicles
starting in 2024.
this video shows you the early results
of our collaboration
which starts with a development flow
that lets you first develop
av software in a physically accurate
simulator
that is essentially identical to a real
car
in this video you can see that we've
modeled a mercedes-benz eqs to be true
to life
it has accurate vehicle shape and
dimensions
true to life vehicle dynamics including
the vehicle's
tires and the acceleration of the car
is physically accurate and of course we
have modeled the accurate sensor models
and are running av
software on an identical computer
architecture
in the cloud
after building this digital twin car
we're able to simulate av
software running on daimler's vehicle
and we can run
different traffic and weather models to
test the robustness of
av really quickly this way we're very
proud
of this video because we can get av
running on a new sensor set
and a new car in just a few months
by having a physically accurate drive
simulator i hope this video shows you
how
you can speed development by letting
developers first test in sim
and then test in the real world
since av is such a large investment
nvidia
offers an architecturally compatible
hardware lineup that provides
attractive price performance across the
entire av stack
this spans from entry eight ask computer
at ten tops and six watts
to a full self-driving solution that
needs more than two thousand tops
we built this end-to-end lineup to allow
oems to leverage
a single architecture across their fleet
of vehicles
we still have some partners that build
their fleet
with different computers and sensor set
across
a few of their models and we're happy to
support them this way
the auto industry is going through a
change similar to the phone industry
there was a time that apple's value was
based on the number of phones it sold
the year
today they sell close to 200 million a
year
but the industry realized a few years
ago that the value of apple
is in its installed base of phones which
is in the billions of phones
and the key is what's the value of
services that they provision
over this installed base apple now
generates 75
billion dollars a year of services on
their installed base
this is why apple puts the highest
performing chips into their phones
so they have room to provide new
services
over the life of that phone tesla has
created the same paradigm
in auto they ship a high performance
computer
and full sensor set in every vehicle
and they are now selling their av
service at ten thousand dollars a car
this is really great and transformative
for the industry
as it enables a fundamentally new
business model
not possible before we are now seeing
startups
build electric vehicles that are
disrupting the auto industry
rather than just being great at auto
manufacturing
they've learned from apple fundamentally
their technology companies
focused on building software-defined
cars
with a computing service business model
that they can amortize
over the life of their vehicles a lot of
the exciting work in this industry is
being led
in the asia-pacific region with really
fast
moving and exciting companies building
disruptive cars
from companies such as neo xbang li auto
and saic there are also
exciting companies that are trying to
turn the taxi industry
into a service now what's really
exciting about this market
is that vehicles with full self-driving
are using computers that include
not just our socs but they also
use our ampere architecture discrete
gpus
while an l2 autonomous vehicle might
need
a few cameras and radar and run on a 200
top computer
a fully self-driving vehicle could need
more than 20 cameras
20 radar and even use 10 lidar
this type of vehicle can need more than
2 000 tops of performance
what i'm most proud of is the size
of nvidia's ecosystem there are hundreds
of companies that are developing
applications on our drive platform today
some are building software that can get
deployed
on oem vehicles others are actually oems
that are building
their full l2 driving application
some are building l4 commercial trucking
products
others are building l5 robo-taxi we
build our platform to be modular
so our customers can build the best
product that fits their needs
i want to thank you very much for
joining me it was a pleasure to get to
talk to you today
hopefully next year i can see you all in
person thank you
you
Title: From Canvas to 3D with NVIDIA RTX Professional Laptops
Publish_date: 2022-03-22
Length: 209
Views: 31259
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/AkQK06JBx-4/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: AkQK06JBx-4

--- Transcript ---

The first step to environment design is exploration
which is accomplished through prototyping in 2D and 3D.
This established workflow is more accessible,
mobile, and flexible thanks to NVIDIA RTX technology.
With additional CUDA Cores, RT Cores, 
Tensor Cores, and large frame buffers, 
new NVIDIA RTX professional laptop GPUs are specifically engineered 
to enhance large-scale design workflows from anywhere.
I am going to show you how my NVIDIA RTX A5000-class laptop 
enhances an existing workflow to design this 3D environment on the go.
So let's get started with the hard part, coming
up with a solid concept from a completely blank canvas.
To do this, I used NVIDIA Canvas, which requires NVIDIA RTX technology.
This amazing tool leverages AI and the extra
Tensor Cores in NVIDIA RTX laptop GPUs 
to generate landscape ideas in real time.
I can build up my scene in layers and change
style filters to adjust the appearance of
the concept before exporting a final composition.
Within a matter of minutes, I have a solid
design loaded into Adobe Photoshop.
It is already separated into layers so I can easily make selections 
so I can block out my scene in Maxon Cinema 4D.
This is done by converting layers from Photoshop
to vector shapes through Image Trace in Adobe Illustrator.
We can then load our Illustrator file directly into Cinema 4D as 3D geometry.
In Cinema 4D, I am using a camera based projection
mapping technique to apply the image generated
from NVIDIA Canvas as a texture in 3D.
This allows me to quickly block out the scene
with some depth and parallaxing.
When I turn on Redshift 3D by Maxon, the scene
instantly takes on a new element of realism
thanks to ray tracing which creates realistic
lighting, shadows, and reflections.
In Redshift, interactive ray tracing in the
viewport is powered by the CUDA and RT Cores
in my RTX laptop GPU which allow me to work
confidently in an accurate, noiseless viewport.
Now I can transform the scene and use lighting
to establish different moods. I can start exploring
my design by adding an HDRI domes, maybe some 
environmental fog, and perhaps a few additional fun details. 
With scene lighting and elements in place,
I can start to target props like these bridges
and cliffs for re-texturing using Adobe Substance
Sampler and Adobe Substance Painter.
Starting with Sampler, I use Image to Material AI
which leverages RT and Tensor Cores in
NVIDIA RTX hardware to convert my 
photos into seamless materials.
Having a laptop makes it really easy for me
to go to different locations, shoot new references,
and generate a comprehensive library of materials
that I can quickly and easily 
send to Substance Painter to apply as custom 
textures onto my objects.
In Substance Painter, I can take advantage
of RTX-accelerated texture baking 
to bring even more detail and realism to my elements.
By leveraging the RT Cores in my GPU, Substance Painter quickly
transforms this mound into a completely majestic mountain!
At this point, I have used 7 apps to prototype this 3D concept.
Each program requires memory in the GPU, so
having a large amount of GPU memory is critical.
The additional frame buffer memory in my NVIDIA
RTX laptop GPUs allows me to move between
apps without worrying about system lag or app crashes.
Being able to work creatively without constraints
really does help me to iterate all my designs and bring my scene to life. 
In an increasingly digital world, being able
to easily create concepts across applications
and share them quickly is important.
NVIDIA RTX professional laptops bring meaningful
increases in RT Cores, Tensor Cores, VRAM,
and unlock AI and RT-enhanced solutions that
artists need to accelerate their workflows,
explore their designs, and share them with anyone from anywhere.
Title: Tracking Objects With Surround Camera Vision - NVIDIA DRIVE Labs Ep. 5
Publish_date: 2019-06-05
Length: 85
Views: 21815
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/aQwqD5cB2ck/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLCYApqxAiLo2rxVvb3Cg9XqI7FSKg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: aQwqD5cB2ck

--- Transcript ---

Back in DRIVE labs, today we're going to talk
about DRIVE mission 27, surround camera object
tracking.
And here we're using our six camera set-up
to see 360 degrees around the car with no
blind spots, and also use that to track objects
as they move around the scene.
Here the top row shows tracking results for
the front left, front center and front right
cameras, and the bottom row shows tracking
results for the rear left, rear center and
rear right cameras.
The tracking results for different types of
objects are visualized by bounding boxes in
different colors with a number at the top,
and this number is the track ID used to identify
each unique real world object.
What we see here is that for all six cameras,
the track IDs are stable over time, and this
track stability is important because it allows
us to obtain accurate velocity information
about the object, which in turn is an important
input into planning and control functions.
We're also observing low track initialization
latency, which is how long it takes to recognize
an object when it first appears on the scene,
and assign it a track ID number.
What we see here is that there are no missed
object tracks, and that this holds true even
for crowded and occluded object scenes.
And that was DRIVE mission 27.
Now you know what we look for in surround
camera object tracking, including initialization
latency, and track stability.
Title: Audi and NVIDIA to Create the World's Most Advanced AI Car
Publish_date: 2017-01-12
Length: 129
Views: 65385
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/aw2s-o-sC8c/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: aw2s-o-sC8c

--- Transcript ---

I think you'd have a hard time to come
across a more warm family light
partnership between two companies and
that's nvidia and audi we've been at
this for a good part of about ten years
and we have really been hand-in-hand
shaping the future of the automotive
industry if you look at the Google Earth
that we introduced together back in 2010
if you look at virtual cockpit and what
we achieve there and of course all of
our companies are very interested in
automated piloted driving this car has a
very very special set of sensors and a
camera and it's utilizing the Nvidia
Hardware inside as well as artificial
intelligence and so it is actually
learning the course as it's going and
with that knowledge it is able to
actually drive autonomously around and
then half way throughout the demo there
is an obstacle that is brought into it
it recognizes there is now an obstacle
and it drives around it in a different
kind of course all of that again is done
by the vehicle itself and it just shows
how powerful this technology could be in
the real world artificial intelligence
is absolutely crucial in achieving the
next generation of autonomous driving
that's for two reasons number one it's
much safer and number two it will allow
us to get there a lot quicker because we
have this data and we have the robust
experience of all these different cars
out there that will enable us to have
this technology a lot quicker than we
would otherwise have we have announced
that we will be doing the ZFS in the
next generation
a8 which will be coming to the global
market this year in 2017 to the US
market it will come in early 2018 and
then of course the next big step for us
is that we will have the artificial
intelligence with level for automation
available in the marketplace by 2020 if
you look at our partnership and what
we've achieved over the years it's been
absolutely exemplary and now I think
it's really really exciting to look at
the new frontier that we are embarking
on together and that is of course
autonomous driving
[Music]
Title: AI and the Future Series, Episode 4: Self Driving Cars with NVIDIA’s Katie Burke
Publish_date: 2018-07-16
Length: 158
Views: 80460
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/AY7i0CuAv24/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLC2AG2jASM7ljlCkQBBwIyiKP88IQ
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: AY7i0CuAv24

--- Transcript ---

self-driving cars have the potential to
really improved the world we live in
today they can provide safer more
affordable and more efficient
transportation they can help us reclaim
time that we lost uck in traffic
commuting to work and they can provide
mobility to people who aren't able to
drive on their own the key to unlocking
all of these benefits lies in artificial
intelligence or AI more than six million
accidents occur in the US every year
costing drivers more than two hundred
and forty billion dollars in damages and
94 percent of those accidents are caused
by human error or an attentiveness
to help solve this problem AI can create
a self-driving car that is always aware
and can monitor a 360-degree view around
the vehicle ai doesn't get drowsy or
distracted and can help drivers maintain
focus on the road for those that commute
to work
the average drive is 26 miles each way
multiply that by five times a week 12
months a year and you quickly get to 200
hours a year spent in the car with AI at
the wheel that time can be spent being
productive working or enjoying
entertainment 60 million people in the
US suffer from hearing or vision
impairment and the aging population is
growing the self-driving cars equipped
with AI assistants can offer people who
can't drive on their own the freedom of
mobility
development of self-driving cars starts
with the training of the AI brain of the
vehicle much like teenagers spend years
preparing to learn how to drive
observing their parents navigate
intersections and spending hours and
driver's ed AI must learn how to drive
by analyzing massive amounts of data by
using supercomputers in a data center
like an Nvidia dgx
hundreds of thousands to millions of
hours of data can be analyzed
rather than engineers explicitly writing
algorithms to teach the car at a drive
the data writes the software deep
learning algorithms are trained to
identify all the objects surrounding it
from lane markings to road signs to
other cars on the road motorcyclists and
pedestrians we can put these deep
learning algorithms to the test in the
virtual world before putting them on
public roads with Nvidia Drive
constellation the AI software and
hardware can run through billions of
miles of driving testing situations that
would take years of driving to encounter
in the real world
simulation enables us to test all
different kinds of driving situations
from time of day to traffic to weather
over and over again we are able to
create a safe AI driver in the fraction
of the time it would take to train a
human currently there are over 370
companies developing on the drive
platform from car companies to truck
makers to mapping companies to Robo taxi
companies to sensor manufacturers to
software startups we are proud to be
developing a transformational technology
that will have a positive effect on so
many people and will certainly help
improve our communities
Title: Research at NVIDIA: The First Interactive AI Rendered Virtual World
Publish_date: 2018-12-03
Length: 92
Views: 1636805
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/ayPqjPekn7g/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: ayPqjPekn7g

--- Transcript ---

[Music]
the friends that you see is not rendered
by the graphics engine is actually
rendered by a technology that we built
[Music]
this is the first time we combine
machine learning and computer graphics
to do image generation using deep
networks
fortune in data are given some driving
sequences of different cities and then
we used another segmentation network to
extract the high-level semantics from
these sequences we have the ue4 engine
to generate this colorized high level
layouts different objects were given
different colors the network converts
this representation to images
[Music]
I made my quarter to dance Condon style
which I don't think he would do by
himself we find some good dancing videos
from another person and then use my
motto to synthesize the dance move that
was created by machines it's not me
[Music]
Title: GAME24 Keynote: NVIDIA CEO Jen-Hsun Huang
Publish_date: 2014-09-23
Length: 2803
Views: 22625
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/b57f0nR58RI/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: b57f0nR58RI

--- Transcript ---

ladies and gentlemen please welcome
jensen juan
hello la
welcome to the first ever 24-hour
celebration of pc gaming what do you
think about that
we're gonna have product announcements
we're gonna show you technology you've
never seen before all right so let's get
started gaming is huge
gaming is huge and we're all about
gaming there are hundreds of millions of
gamers around the world now
people thought it was a fad people
thought it was a waste of time
not us
there are hundreds of millions of gamers
there's a couple of hundred geforce
gamers around the world and it all
starts with great games
game is
adventure game is art
game is puzzle
game is competition
game is social
game is sport
from absolutely nowhere just a few years
ago they're now over a thousand
tournaments each year
stadiums packed full of people
price money over a million dollars
professional gamers professional teams
training living together
playing 24 7. and the reason for that is
because people are intense about games
they love to game there's skill involved
in gaming there's ability in gaming and
let's face it those who are dedicated to
it are just better than everybody else
it's more than a sport
it's a spectator sport
never thought never thought i'd see the
day
where other people watch us game
where other people watch this game
it is so fun it is so fun it's so
amazing that people are tuning in
all over the world watching twitch
55 million viewers in a month crazy talk
absolutely crazy talk
more people watch league of legend
champion
than all of the nba finals
i love lebron but we're bigger than
lebron
gaming is huge gaming is huge and we
love gaming i love what we've built
together it's taken us about 20 years to
get here i love what we've done together
we did it all together we own this
thanks you guys
our promise to you
is number one to build the most advanced
platform in the world
we dedicate ourselves to delivering on
that promise every single day we hire
the world's best engineers engineers
whose lives work
as computer graphics
engineers who dedicate themselves
back by billions of dollars of r d
to build the world's best gpus for you
we want to build the world's most
advanced technology platform but we also
know that in final analysis it's about
great games it's about amazing games
beautiful imagery amazing experience
that's not possible
without just a lot of mathematics
one of the things that you'll see today
is all of the things that we've enjoyed
up to now recognizing that computer
graphics is one of the most
computationally intensive fields in
computing
it's all about mathematics and we're
going to show you a lot more in
mathematics today
we want to make it so that you can play
in more ways it's not just about sitting
in front of your pc
it's not just about that it's absolutely
about that but it's not just about that
we want you to be able to enjoy in 3d we
want you to be enjoying surround
we invented game stream so that you
could stream your pc game to tv if you
want stream it to your mobile device if
you want if you had a shield you'll be
able to stream it all the way out of
your house if you want
we have other ways we would like you to
be able to enjoy your games
one of our newest ways is g-sync
really really excited about the
invention
our innovation is now
extending beyond the gpu beyond system
software all the way out to the pc into
the monitor making it possible for you
to enjoy just silky smooth graphics with
a revolutionary technology called g-sync
we have so much more that we want to
share with you and lastly
it's pc
it's open
it's for everybody everybody gets to
play
we make our experience wonderful
whether you have
a basic pc with a g-force inside
all the way up to a water cooled
quad sli driving multiple 4k monitors
a pc that costs more than most average
cars
we want you to be able to enjoy great
games irrespective of the computer you
have in front of you
to make it possible hundreds of
engineers at nvidia work around the
clock to test every single permutation
of your pc every single game and find
what we call the optimal playable
setting and for you all you have to do
is make it one click
and bam
you're enjoying that game as good as it
possibly can be enjoyed on your pc
everybody gets to play
that is our promise to you today
today
we would like to
continue
extend
raise the bar in that promise to you
ladies and gentlemen today we're going
to talk about maxwell
there are four things i want to talk to
you about there are four things i want
to show you the first
is
the processor
what is called the maxwell sm the
maxwell streaming multiprocessor you
know that we invented the general
purpose processor of modern computer
graphics it's possible to of course
compute all kinds of wonderful pixel
shaders make every single pixel the
scene incredibly beautiful
we use it for physics it's a general
purpose massively parallel processor
thousands of processors are working in
parallel
communicating over these fabrics and
crossbars
working together to generate a result as
fast as possible
the maxwell sm
everything starts from that
making it as computationally efficient
as architecturally efficient is the work
of hundreds of engineers
in the case of maxwell
we combed it up and down and the reason
for that is this
we had to make this architecture
perfectly compatible with mobile
we had to bring it to tegra as well
and as a result energy efficient energy
efficiency was job number one
well turns out today
in all of computing
we are power limited
so your architectural performance for
energy efficiency is directly related to
your performance in the case of maxwell
in the case of maxwell in just one
generation relative to kepler we improve
the architectural efficiency the
performance per watt by a factor of two
second thing of the architecture memory
bandwidth is vital to us memory
bandwidth is air
memory bandwidth is air in our frame
buffer
we store textures
we store lighting information we store
bump information we store all kinds of
including the frame the z buffer
how fast we're able to communicate with
that frame buffer directly determines
our performance
in the case of maxwell we've now
implemented our third generation of
delta compression basically what that
means is this every single color as
we're writing it out to the frame buffer
we compare it to the last value we're
about to write
and we simply write out the delta that
last scene that you just saw everything
that's pink was compressed as a result
of that maxwell's memory interface
was able to reduce the traffic by 30
percent
really really amazing the two things
combined
the sm
architecture efficiency
the memory bandwidth amplification as a
result of that comparing maxwell to
kepler whenever i compare against kepler
i'm going to compare everything to gtx
680 our first
kepler
comparing against kepler across
a large number of very challenging games
the performance
energy efficient performance per watt
of maxwell versus kepler is 2x
unbelievable
so let me just read that off to you
kepler
average frame rate of those games at 25
by 16
25 frames per second consuming 195 watts
in the case of maxwell 41 frames per
second but consuming only
165 watts
but you guys know what it means when
you're energy efficient
it's a technology called
overclocking
that's right overclocking this is going
to be the mother of overclockers
we know it's all about games so what are
you going to do with all of this
horsepower what are you going to do with
all this horsepower and maxwell well
create even more amazing special effects
you know that we have this
team in our company we call the
gameworks lab and we have this engine a
libraries mathematics
algorithms
created by some of the world's best
computer graphics engineers special
effects engineers computational
mathematicians and they solve some
really really amazing problems and they
run those algorithms
on our gpus we call it the gameworks
team
the work that they do is really about
doing for real-time computer graphics
what ilm what industrial life magic does
for film
we know today
that having a great
and fun game is really important still
but production value matters production
value matters
and production value the exquisiteness
of the image that you see the beauty of
the images that you see how wonderfully
it works
we expect it to amaze us generation
after generation we expect it to blow us
away
just like we do when we go to a brand
new movie that's what this team does we
translate
mathematics into amazing imagery now one
of the benefits of the of course is
long-term not only do we get wonderful
games
we know that it's impossible
for artists
to meticulously paint
and create through art the real world it
is just simply not possible
we have to do it from first principles
and the first principles are the
principles of physics
and so let me show you some stuff now
rev let's show them turf
let me just tell you what you're seeing
you know usually
the way that grass is done is through
large large expansive texture maps and
they
prop them up as billboards and the
billboards of these texture maps are
translucent so that you see through it
except for the part that looks like
grass but we know that grass doesn't
work like that we know grass doesn't
look like that and when grass is made
like that unfortunately
it can't interact with nature when the
wind blows
when the sun changes
its place in the sky
when you walk through the grass
when you roll around in it
when you throw boxes
and throw things at it
we would expect the grass
to that's right i know it's craziness
in order to do all this
every single blade of grass
has been converted to geometry in fact
if you look closely every single blade
of grass is a v
shape of grass and it's curved and fully
tessellated as a result of that because
it's just another geometric object it
behaves properly with everything else
inside the scene
when you change the lighting
when you throw things at it
it behaves exactly as we expect
and rev did you change the light
change the light source a little bit
look at that
it all just works turf effects brand new
thank you rev
anytime
physx flex is the world's first
real-time unified physics solver what
that means is this
whether it's particles fluids smoke
water rigid bodies flexible bodies
cloth rope
whatever objects are inside that
environment behaves according to the
first principles of physics
and is one singular unifying solver
well we've been able to do all kinds of
great things we're going to show you in
just a second but there are three brand
new special effects that are just
utterly amazing
but what you're looking at here is this
you're looking at a body of fluid and it
has
objects in it the objects have mass the
objects have buoyancy
when you drag it around it has drag
it bumps into each other when it bumps
into each other it behaves physically as
we expect
you stir the water
it would do exactly what you would
expect
now imagine
if you're in some game call of duty
assassin's creed and we wait you weighed
yourself into a lake
and it did just this
because every single atomic
particle is performing according to the
laws of physics
and so the unified solver irrespective
of what objects we're talking about
would simply behave according to physics
well if you behave according to physics
and if you're liquid
you have the ability to change
phases
phase changing let's take a look at
another demo
this demo is kind of fun
you can kind of imagine
those three rabbits first of all are
kind of like jello rabbits
by simply changing the
interaction
the force between the atomic particles
we can cause the object to change shape
change phase
what else can we do this is viscosity
uh-huh
that looks a little bit like milk
it looks a little bit like milk you're
simply looking at the visualization of
mathematics every one of those
globs of milk or milk shake depending on
the viscosity that that
the red rev has selected it's just a
particle and thus particle is colliding
with other particles those particles
when they collide with other particles
have some amount of surface tension
there's physics between there's field
between forces between those particles
and they behave according to what we
would expect them to behave now if we
were
if it is so we should be able to even go
to the extreme and cause some of these
particles when they collide and touch
each other to not bounce off but
to stick to each other and so maybe rev
why don't we do the next thing
imagine if i were to throw some goo at
this monster
but instead of bouncing off
instead of bouncing off
now this this you guys know this is
going to be a brand new bfg weapon
it's going to be a brent it's going to
be a brand new gun the goo gun
and usually it's going to i believe this
is what you call sliming them to death
okay all of this is made possible
by the physics flex
unified
solver we treat everything like
particles like atomic particles we give
every one of the
those atomic particles
physical behaviors
associate with them
fields force fields and so when they
interact with other particles they do
the appropriate things just like physics
does
and that ladies and gentlemen is what
we're going to do
with the 2x increase
in perf per watt which will translate to
2x increase in perf over time
thank you rev
that was my first thing now i want to
talk about 4k
well as it turns out as you guys know
4k monitors
are starting to come down in price
however while we're trying to increase
the fidelity of every single pixel we're
also trying to make every single pixel
more beautiful when we make every single
pixel more beautiful we add more
geometry to the scene we do more special
effects to it each one of the pixel that
you see has much much more math behind
it
so when 4k comes along we've got to find
a way to deal with
how to drive that monitor on the other
hand there are many games that we play
league of legends dota
dark souls 2.
these games they're basically running
full out at 60 hertz and when you put a
max well in your system you're just
going to run a lot faster in 60 hertz
and so the question is how can we
find a way
for games that are already performance
challenged
all of a sudden need to come to 4k
or for games that has plenty of frame
rate already how do we provide it with
more fidelity well the answer to that
are two technologies that we've invented
one is called dynamic super resolution
the other technology we invented called
multi-frame sampled aaa
so let me start with the first one
dynamic super resolution basically what
it does is this suppose you have a 1080p
monitor the vast majority of us still
have 1080p monitors
when you put a maxwell in your system
you have so much horsepower we will
render your game in 4k
and we'll do an image processing pass
to resolve that 4k back to 1080p
enhancing
the visual quality
pretty substantially why don't we uh go
to the demo okay
so what you're looking at here is
actually a game dark souls 2. it's maxed
out it's got 4xaa on
and it's running you know pegged at 60
frames a second there's really nothing
else to do to improve the fidelity now
what i want you to focus on is the grass
and you'll see just those tips of grass
they're kind of scintillating you know
textures are coming in and out
that's because the textures are actually
not being sampled
detailed enough
using dsr we can dramatically improve
this look of the game so let's switch to
the dsr demo
and now sean is doing a great job with
the exact same setup only now dsr is
enabled and if you look when he's
panning you can actually see the grass
is much more solid there's no more
popping there's no more texture problems
both of these gain
examples are running at 19 by 10. but on
the but on this example we're rendering
it at 4k and then as jensen said we're
sampling it down so let's go ahead and
do the side by side
and now on the left hand side it's 19 by
10 again so i'm going to scroll around a
little bit and again look at the grass
tips
focus on the fact that there's like this
weird shimmer
okay that's that's not good it's not
supposed to be like that
let's look at the right hand side and do
another pan this is dsr
so the point is we can dramatically
improve the experience
with people that have a 19 by 10 monitor
using the power of dsr and maxwell
wow that's beautiful okay good job tap
thanks
okay so that's dsr well here's the
amazing thing about dsr
it's completely automatic
all you have to do
is
pop open gfe it'll show you
this resolution basically the 4k
resolution and with a dsr behind it
and whenever maxwell provides frame
rates that are just utterly fantastic
already we will render it beyond your
native resolution of your monitor
to as high as 4k
and maybe one of these days beyond and
then using image processing
move it back to the native resolution of
your monitor the second technology that
we invented is called mfaa multi-frame
sampled anti-aliasing and that's how
fast it is mfaa the way that mfa works
is this
look at the right forex msaa it's the
workhorse of anti-aliasing today
basically the way it works is this
if you look at the red line which is the
edge of a triangle and in this
particular case the inside of the
triangle is on the bottom
in msaa basically all we do is we
mathematically calculate how many
samples how many of those dots are
inside the polygon
in this particular case
on the right hand side 4x msaa
one quarter of the samples of the lower
quadrant is inside the triangle and
therefore we will blend 25 percent color
of the pixel inside a triangle with 75
of the color outside the triangle in the
case of the next pixel on the lower
right quadrant three samples are inside
the triangle in this case therefore we
have three quarter coverage
and therefore that sample or that pixel
that you see has 75 percent of the color
of inside the triangle and 25 of the
color outside the triangle well here's
the problem with this
in the case of msaa now look at how many
samples we have to generate more per
pixel
each one of those quadrants is basically
one pixel but we have to sample it four
times every time you sample a line you
sample something it takes some
horsepower takes some performance
in the case of multi-frame
sampled aa
the clever thing that we're doing
is now on the upper left-hand
frame n minus one
notice we're only doing two samples but
maxwell is clever he samples two
samples in one frame he changes the
sample
position and samples the other two
samples in the next frame then what we
do is we take multiple frames and using
again an image processing pass
combine those pixels
into the final result
your eyes
see no difference here's the beautiful
thing 4x mfaa
has basically the quality of 4x msaa
but it has the performance of 2x
msaa
that is the ultimate way of getting
something
for nothing
okay as a result of that
we can now deliver
anti-alias images at a much higher
performance let's take a look at that
and so in this particular scene i'm
going to focus in on that part of the
window of course there are sharp edges
everywhere and you know how we hate
crawlies so we're going to zoom into
that little tiny spot
and now look at the three bands the
first band is no anti-aliasing this is
the classic example of the creepy
crawlies the edges are very very sharp
as you pan slowly across it those stair
steps are going to start moving up and
down the screen and you'll notice that
as jaggies
you'll notice that as jaggies so
solution for that is to apply 4x msaa
which is in the middle
we now sample every one of those pixels
now four times
but you know of course it adds a little
bit of blurriness basically a form of
filtering filtering on edges and then on
4xmfaa
mfaa
notice on the right hand side
all of the edges whether it's vertical
all looks basically the same as msa in
fact there are many many areas where mfa
does an even better job but on balance
it basically gives you the quality the
pixel quality of msaa 4x msa at the
performance level of 2x
aaa
now this is what it looks like
the first green bar the dark bar the
first dark bar basically shows you the
performance gains that comes naturally
for maxwell but when you turn on mfaa
the performance
is dramatically higher if you compare
this
generation to generation we're getting
2x the performance boost
what do you think about that
the next thing i want to talk about
is
helping us enjoy games in more ways
one of the coolest things that's
happening right now
the convergence of all kinds of
technologies
is vr
how many of you guys have heard of
oculus well
we have a new
platform new sdk new technology called
vr direct
vr direct it comes in three components
the first component is auto stereo the
second thing is the bane of existence of
vr is latency
it causes you to feel nausea it creates
a great deal of discomfort through all
kinds of techniques we have reduced vr's
latency to a point where you can enjoy
the living daylights out of it
and then number three
a new technology called auto synchronous
warp asynchronous warp that i'm going to
explain just a second that reduces the
latency even further now let me very
quickly explain latency to you guys
there are two basic sensors that tracks
the movement of your head
there's the gyro
that's being sampled about a thousand
hertz
there's a couple of
cameras that's tracking your head
that goes into what is called sensor
fusion i'm not showing that part here
just showing it as a dotted line it's a
few milliseconds after that it goes into
the game the game figures out what to do
next
goes into the operating system
the operating system includes the system
software includes
the api includes the transfer of
commands
from the api out over the pci express
bus into the gpu the gpu renders it
performs all the processing as fast as
it can in this particular case i'm
showing that this person already has a
fabulous gpu so this person is already
enjoying the game at 60 frames per
second after that it gets scanned out to
the display and so how quickly you scan
those pixels up on the display
takes a little bit of time about 13
milliseconds to refresh rate of the
display all of that added together is
about 50 milliseconds
if you were inside a vr
environment and the latency is somewhere
between 60 70 80 milliseconds it's
intolerable utterly intolerable between
30 and 40 milliseconds you're having a
great time so the first thing that we
can do
is simply through great engineering
comb through every line of code and comb
through every data transfer and reduce
the latency every possible place as much
as we can we've done that
job number one is just careful
engineering and with just careful
engineering the obsession to reduce
latency we've taken latency down to
about 40 milliseconds the second thing
that we've done
is we know
that the resolution is already
challenged
and so we want to create the best
possible pixel we possibly can most of
the time you're running with msaa in
this particular case why do that turn on
mfaa you'll get to save a few more
milliseconds
well
now you're down to about 36 milliseconds
through an extraordinary amount of just
careful engineering
clever new technologies and a really
really fast gpu
you've got yourself 36 milliseconds you
can now enjoy vr well we can take it
even further than that and that's this
technology called asynchronous work
the way asynchronous warp works is this
we've already rendered a frame if we
could predict where your head's going to
be at we might be able to take the
previous frame
and warp it
so fast so that we don't have to
recompute it and so part of it has to do
with projecting and predicting where
your head's going to be at but mostly
it's about waiting until the very last
second and recognizing that the fact of
the matter is most of the time your head
doesn't move that fast and the fact that
the latency is already so low
we could predict and at the very last
second warp and put the screen up for
you to enjoy
the fact is vr is now here the
technology is sufficiently cost
effective the resolution is getting
better and better
and most importantly between the gpu
performance system software new clever
techniques we're able to get latency
down really really far to the point
where i believe this is going to be a
really really cool new way to enjoy
games vr direct what do you guys think
okay i saved the best for last
since the beginning of time
generating a photorealistic image
has been a grand challenge for computer
graphics for real-time computer graphics
the problem is on the one hand simple on
the other one incredibly hard to imagine
how to solve and the reason is this well
it turns out
that light
behaves according to laws of physics
and when they bounce off of when they
bounce off of objects and surfaces those
surfaces reflect based on laws of
physics
well let me just take a look at this
example right up here
there's a whole bunch of little lights
there emitting light those little tiny
light bulbs light is also coming in
through the windows this room is
incredibly hard for computer graphics
very few
large sources of very bright lights
most of the lights
are rather subtle
all the shadows are incredibly subtle
all the materials are rather rather
different
and so they reflect light in very
different ways
well this particular image is not a
photograph
this particular image was rendered with
nvidia's ira
irae is nvidia's physically based
otherwise physically
simulated rendering system
and runs on a basically a supercomputer
takes several seconds about three
seconds to generate
and it takes
a hundred and fifty gpus
now what we would like to do of course
do the same thing in real time that is
the grand challenge i'm super excited to
tell you that we although it will take
quite some time
to achieve this level of realism
photorealism
in real time
we have found ways to simplify the
problem to approximate the problem
we've reduced the problem by several
orders of magnitude
over a thousand times and the way that
we solve that problem is instead of
tracing and following pixels
light beams bouncing off of pixels we
simplified the scene into a new data
structure called voxels
voxels are basically cube data
structures
and each scene
would be decomposed reconstructed
it using we call voxelization
reconstructed into these little voxels
imagine minecraft
from there we've simplified the data the
amount of data from billions of pixels
and billions of rays
to
millions and thousands the technology is
called vxgi
vxgi voxel global illumination this is a
giant step forward
in lighting realistic photo realistic
environments it's dynamic it basically
simulates one bounce
and with just one balance we can capture
the vast majority of the amount of light
energy that as it bounces throughout the
room
and because it's a general unified
algorithm
for path tracing we call it cone tracing
and because it's designed this way it
could be integrated into
engines like unreal engine and others
in the near future
now let me show you
how global illumination works now this
particular demo is called the cornell
box
okay there's several things i want you
to look at before i switch the scene
the first thing i'd like you to look at
is this
notice
that although the light is
projecting downward
that the ceiling is lit
the second thing is this notice the
creases on the ground
basically where the
red wall the green wall intersects with
the white floor
notice it's lit wonderfully
all the creases are lit wonderfully
however if you go up to the top
you'll see that in fact there are these
dark shadows there that's called ambient
occlusion the third thing i'd like you
to notice
is look at the size of the balls
in the case of the left hand side
it's bleeded some red paint
on the right hand side is bl some green
paint has bleeded onto the onto the wall
and the reason for that onto those balls
and the reason for that is because when
light hits those walls the red wall and
the green wall reflect the light
and they become
basically a light source i look at those
three simple effects and now um curtis
let's go to state-of-the-art computer
graphics using direct lighting so a
couple of things to notice
notice the ceiling is dark
and the reason for that is because the
light that's at the center of the room
as it bounces off the floor never made
its way back to the ceiling notice that
the ambient occlusion on the top the
corners are really dark look at the
shadows
you would expect some light to go
underneath those balls
to light and create that soft shadow
effect that you saw earlier now here's
what vxgi does after it does the first
direct pass
it will voxelize and simplify the scene
and from that simplification discover
where the light would bounce off of and
where it would light other surfaces and
so what we'll do now is we'll show you
using maxwell's vxgi
capability and this is the really
wonderful part
a curtis let's let's uh move the light
around
oh this is just fun
that's so beautiful
guys
what do you guys think about that
on july 20th 1969
three astronauts
flew three days
to the moon
shortly after buzz aldrin
makes his way out of the limb
and down the stairs
what you're looking at here
is that seminal photograph that was
taken by neil
armstrong well those photographs came
back to earth
and of course to this day still takes my
breath away thinking about it
but at the same time caused many people
to be quite excited about whether
the origin of this photograph
was truly from the moon
there was good reason for them to
question these photographs
first of all when you look at that
image of buzz aldrin and you look at the
shadows on the ground
the dark shadows
he's surely in shadow if he's surely in
shadow because the sun is on the other
side of the limb
how is it possible
that he is that bright
second we know there's no atmosphere on
the moon
and if there's no atmosphere on the moon
we should be able to see the stars
now
with the invention
of vxgi
and through the extraordinary efforts of
our engineers we meticulously model the
lunar limb
it's made out of aluminum
aluminum foil
cloth
and a generous application of tape it
was a flying coke can
we modeled every detail we modeled the
surface of the moon so that we could
start to study and test these mysteries
and come to our own conclusion
through scientific means
whether it is possible whether they did
land on the moon
ladies and gentlemen i'd like to
demonstrate to you guys now
our work
and so let's go through let's test each
one of those conspiracy theories
the first one is this
uh let's let's assume that he is in
shadow and that the only light source is
direct well this is what it would look
like and this is what people expected
him to look like and the reason for that
is because the sun is on the other side
of the lamb
and you can also see the shadows on the
ground are perfectly dark so how is it
possible that he's lit well as it turns
out
we modeled the entire surface of the
moon we modeled the dust particles of
the moon and we learned this
we learned that the dust particles on
the moon
it's fine particles and in fact has
reflective properties now the reflective
properties are not great
it only reflects about 10 12
of the light that
hits it
however
as in many
type of
dust particles and crystals when a light
hits it directly
and you look at it from the same angle
as the light source
the reflected light
is multiple times brighter
than it otherwise would have reflected
it's a phenomenon
called opposition
surge well we simulated
the properties of the
moon dust
and as you can see
right now the sun is behind me the limb
is beside me notice the brightness of
the sun as is reflecting off
the lunar surface
well suppose we were to turn on vxgi and
we turned every single
surface into a light source so that it
reflects the light let's see what
happens
voila
well
that's
mystery number one mystery number two is
this
why is it that while we're looking at
him right now we don't see the stars
well that's a simple answer turns out
it's a simple matter of exposure
the light is so bright in fact that they
had to change the exposure to their
camera to the point where you're not
over blindingly
bright on the surface of the moon while
therefore you're no longer able to see
the stars and so what we did was this in
fact when we look out into
into the sky we've modeled all 84 000 of
the closest
stars around earth that we can see from
earth and if we were to change our
exposure
this is what we would see now curtis
what would happen now if we were to look
back down
now let's change the exposure again
please
look how beautiful that is huh hey guys
what do you think
real-time computer graphics with
real-time
global illumination
well there's another conspiracy
this particular question was kind of
interesting well it turns out
when you look at this other iconic image
of buzz stepping on the moon
it's taken from a camera that plops down
from the lamb
the theory that this this photograph was
in fact taken on earth was because they
some believed that really bright light
source
beyond
buzz was in fact a light that they
forgot or somehow
mistakenly did not take out of the way
when they took this picture
and so if we go back to our simulation
in fact if you were to look at that
well shucks that's an interesting
question
until if you were to figure out where
neil armstrong was standing when he took
the other iconic photograph
you'll discover that lo and behold
there's a white object there
well here's the amazing thing
neil armstrong's spacesuit is made out
of
largely teflon and it has a reflectivity
of about 90 90
and it helped us solve one last
unexplained issue when we were working
on this project the one last thing we
could never figure out is
why is it that when we compared
we've modeled every little detail now
why is it that when we look at it buzz
still seemed too dark
well it wasn't until we modeled neil
armstrong
and we put him in the scene
that we were able to see the difference
how about let's
turn neil on and off
the extraordinary detail
that the engineers put into it is just
really really wonderful to see
and then lastly let's compare
the before and after if you can go to
slides
if you're not sure
the right side
is the photograph
the left side is the simulation
what do you guys think
now one last mystery i didn't even know
i didn't even know about this particular
mystery and and uh it turns out uh we
weren't we weren't the first ones there
that before
neil armstrong and buzz aldrin landed on
the moon
there was somebody else there and
we are not alone
so
ladies and gentlemen maxwell there were
four things about maxwell that are
utterly groundbreaking the first is two
times the energy efficiency two times
the architectural efficiency
and because of the performance made
possible by mfaa
and also the the chip itself two times
the performance which enables us to
enjoy 4k
this is the first gpu that we have
that connects directly to oculus and
other vr glasses in the future with vr
direct
enable you to enjoy vr
with your pc
all of that is going to make
be programmed and profiled and set up
using gfe and then lastly
the very first of its kind
a giant step forward
for real-time computer graphics
technology
that makes possible real-time global
elimination
vxgi
ladies and gentlemen the first flagship
of maxwell
you want me to throw it
gtx 980
the first maxwell flagship gpu
we're so incredibly proud of this this
is really really a giant step forward
for us
gtx 980 your first question is how much
does it cost
2048 cores the fastest gpu in the world
even without
considering mfaa with that consideration
boosted even further four gigabytes of
memory
549. the fastest gpu in the world
we do have one other and this one is
insane
the gtx 970 320
dollars
you
Title: Research at NVIDIA: Researchers Help Robots Work Alongside Humans
Publish_date: 2018-05-20
Length: 198
Views: 33157
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/B7ZT5oSnRys/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: B7ZT5oSnRys

--- Transcript ---

[Music]
in this work we present a system to
infer and execute a human-readable
program from a real-world demonstration
during the demonstration our perception
system will detect objects in their
poses ultimately predicting a program
for the robot to reproduce the
demonstration the execution system
operates in a closed loop with the
perception system to run the program and
replicate the demonstrated arrangement
should anything be amiss in the
predicted program the human demonstrator
can repeat the demonstration or edit the
program directly our system consists of
a series of neural networks to observe
the demonstration we use a pipeline of
three networks the object detection
network perceives the poses of objects
built from a convolutional neural
network it predicts locations of object
vertices in image space after detecting
object vertices the second Network
infers object relationships the program
generation Network consumes observed
object relationships and outputs a plan
to reproduce the observed arrangement
finally the execution Network guides the
robot to reproduce the demonstration
following the program this network
operates in a closed loop with the
perception networks to select the
correct pick and place operations using
a structure inspired by convolutional
posed machines the network resolves
posed ambiguities by multiple stages of
processing perception networks are
trained via domain randomization using
only synthetic images to increase
applicability to new visual settings we
predict an image space rather than a
fixed world space despite never
observing a real image during training
the perception network reliably detects
the bounding cuboids of objects in real
images even under severe occlusions
together the set of object detection and
relationship networks define the
vocabulary of the human readable
programs in our implementation we
trained object detectors for several
blocks and a toy car we trained the
relationship network for two physical
arrangements
and left of we test the system in the
real world on pick-and-place stacking
demonstrations using a Baxter robot in
this clip the operator shows a pair of
short cube stacks the system predicts an
appropriate program during execution
after correctly placing the blue cube
the robot fails to place the yellow cube
on the green cube due to an error in
depth estimation however because the
execution Network integrates perception
of the current state the system
automatically recovers the command is
issued again and this time it succeeds
in this final clip we demonstrate
stacking a non cube object by placing
the toy car on the yellow block
despite being trained to recognize only
a blocky cartoon of a toy car the
perception and execution systems perform
the stack correctly going forward we
continue to study the use of synthetic
training data for robotic manipulations
thanks for watching
Title: Pro-Quality Streaming with NVIDIA RTX and GeForce-Optimized OBS
Publish_date: 2019-01-24
Length: 167
Views: 13407
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/b9KAUltDLnU/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: b9KAUltDLnU

--- Transcript ---

hey guys we're here at CES and we're
showing you the new r-tx 2016 coding
capabilities and how it will bring your
professional level quality for your
stream and give you top fps what we're
showing here is what the professional
broadcasting sutton looks like here on
the left we have a dual PC configuration
is what a professional broadcaster would
use nowadays to broadcast to twitch or
YouTube and on the right what they do is
they have a gaming PC that exclusively
runs the game and on the left you have
an encoding machine that takes the
output of the gaming machine when it's
the mi cable captures it with a capture
card and then encodes it to twitch and
they need to do this because the single
machine the gaming machine is just not
able to run the game and input at the
same time using CPU encoding and what
this allows you to do is that you can
play your game with top fps and you can
stream at the maximum political twitch
at x264 maybe but the problem is that
this is a very expensive and complicated
setup you need two key words to Mai's
you need to wire the audio for which you
may need an audio mixer which is a bit
of a mess and it's just it's hard not
everyone can get access to this however
here on the right we're showing the new
york TX 2016 this single machine is able
to output the same level of quality as
the dual PC because it has a brand new
encoder in guitar TX 2016 it has been
improved I can match that quality we can
run the gaming screen at the same time
because it's using MV Inc it's a
dedicated area in the GPU that takes
care of the encoding that way you upload
the encode of the CPU and it doesn't
touch the GPU rendering at all and
thanks to the collaboration that we've
made with OBS we're able to get top FPS
on a single machine while we're
streaming to twitch a top quality
let me show you here on the right we can
see the twitch outputs on the left is
the dual PC and on the right is the our
TX 2060 and we've had out of trash
coming through this street they've been
comparing it some of them were experts
in reviewing monitors and they've been
very impressed the quality is very very
similar but what they've told us is that
the colors and the text was a lot
sharper on the right and textures for
example in the HUD were a lot easier to
see very in mind that this is Call of
Duty is right now the hardest game to
encode because of the CPU toll it
takes on your system at CES we also
introduced the new max-q laptops in this
case using an msi with an RT X 2018 and
it's playing battlefield 5 with ray
tracing on and at the same time it's
broadcasting to twitch and because it
has an RT X card it has a brand new
encoder so it can output the best
quality out there essentially a mobile
broadcasting station where you can be
mobile and you can play with top fps and
still broadcast the very best quality to
twitch we're hoping that with the new RT
x 2060 a lot more people will be able to
get into streaming
Title: NVIDIA RTX Extreme: Real-Time Automotive Rendering with Dual NVIDIA RTX A6000 and NVLink
Publish_date: 2023-02-07
Length: 239
Views: 35208
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/BBhj9wVx5kc/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: BBhj9wVx5kc

--- Transcript ---

foreign
hi everyone and welcome back to part two
of the video my name is David Bayless
and I work as an automotive artist
specialized in real-time rendering the
main software is that I use are Autodesk
3ds Max for the modeling Unreal Engine
for the rendering parts and then
Lightroom for the final touches
so in part one of the videos we used a
single RTX a6000 using Unreal Engine in
pass Tracer mode
the scene that was used contained over
50 million triangles which was no
problem for the RTX a6000 we were able
to Output 12K resolution renders without
hitting the vram limits my RTX 86000 is
paired with an Intel I9 12900k and 32
gigabytes of ddr5 RAM which is a great
combination my daily programs though
rely much more on GPU computation such
as Cuda accelerated Hardware or RT cores
for real-time Ray tracing where the
a6000 shines
now this is the NV link and this is
going to be used to bridge my two gpus
together in order to scale memory and
performance in the selected applications
this means programs such as Nvidia
Omniverse or Unreal Engine can benefit
from multi-gpus to render frames faster
nvlink will also combine your GPU memory
which in this case gives me 96 gigabytes
of vram in total
so here we are back in Unreal Engine 5
with our Porsche GT3 this time in a
warehouse environment the environment is
this time custom modeled with added scan
geometry mostly for the background
here you can see the mesh density as
known as The nanite View mode in the
Unreal Engine 5 showing how dense my
scene is but still running perfectly
fine on the a6000
now here's the advantage of using dual
GPU it takes almost one minute to render
past Trace that 500 samples on a single
a6000 now let's take a look at this
using nvlink with dual a6000 we are
seeing a 50 speed increase using two
gpus which will save us considerable
amount of time for animation
[Music]
[Music]
thank you
[Music]
okay so now to recap with one single GPU
I was able to achieve pretty substantial
results from using high density polygon
in my scene running at real time 60
frames per second up to outputting 12K
resolution without hitting the vram
limits now adding a second GPU to the
mix using the NV link I was able to
double my vram allowing me to add some
heavy particle simulation in my scene
without compromising the quality but
what I could do also is cut my render
Times by 50
allowing me to be more efficient in my
everyday work I hope you enjoyed this
part 2 of the RTX extreme series video
and hope to see you very soon
Title: Optimizing Light Source Perception with Software-Defined AI - NVIDIA DRIVE Labs Ep. 22
Publish_date: 2020-09-16
Length: 118
Views: 18299
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/bBn_Ba7rW9k/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: bBn_Ba7rW9k

--- Transcript ---

Today in DRIVE Labs, we are talking about
using software-defined AI techniques
to optimize our light source perception, deep neural network
which we first introduced to you in DRIVE Labs ep 13
initially trained to generate
control outputs for the vehicle’s high beam light system
Initially, our light source perception deep
neural network was designed to just perceive
vehicles with active lighting (that is, cars
with head and tail lights on, as shown in green).
However, by using software-defined AI techniques,
such as DNN training dataset re-shaping
and DNN loss function re-design
we were able to significantly increase the functionality
of the network in just a few weeks.
The DNN is now able to detect and classify various light sources,
including street lights, shown in red, and other illumination
such as from traffic lights and even skyscrapers, shown in blue.
This additional light source information can help expand the functionality of autonomous vehicles
for example, detecting illumination from urban buildings
can help inform the self-driving car
that it is entering a city environment
in which high beams should not be used.
And here we see the DNN correctly differentiation between cars that have their lights on,
and parked cars with lights off.
In this clip, we see DNN-based light source perception on a rural highway in Germany,
with the DNN detecting headlights from groups of oncoming cars,
as well as taillights of a leading car.
We also measured the DNN’s detection range for far-away headlights to be over 800 meters.
To measure this, we captured highly-accurate differential GPS data from two cars,
time-synchronized the data recordings
and then used this to compute the distance between the two cars.
This software-defined approach allows AI-enabled vehicle functions to improve continuously over time
rather than remain fixed.
In a production system,
these advances in safety and performance could
be quickly deployed to a consumer vehicle
through an over-the-air software update.
Title: The Making of Quadro RTX: Professional Graphics Reinvented
Publish_date: 2018-11-07
Length: 216
Views: 204474
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/bd1L3h9kLq8/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLC5nkUObVrN745Mu920Y88mzI35sA
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: bd1L3h9kLq8

--- Transcript ---

[Music]
Quadro has been the standard for
professionals whether they're making a
movie designing a car designing a
building like this campus here for last
20 years
Quadra r-tx builds upon this legacy of
Quadro the r-tx is the epitome of 15 20
years of work to bring real time ray
tracing into a single GPU Quadro r-tx is
the next generation of our Quattro
graphics products which are GPUs and
graphics products specifically designed
for the professional user and the
high-end professional applications be
that animation or CAD or video editing
those kinds of things and the r-tx
component of that is the Quadra product
built off the latest technology which is
the Turing GPU architecture touring
itself is an architecture that frankly
many people in graphics and particulate
Nvidia have kind of fantasized about for
well more than a decade because it
really brings together all the things
that were known for but it adds two new
key pieces of technology which really
advance the state of the art quadrille
r-tx includes what we call an RT core
which is specialized hardware to
accelerate ray tracing which the same
kind of rendering that the film industry
is used for years and then the other
component of that is AI so we've
integrated tense records for AI
processing and in particular we're
excited about bringing AI graphics or
neural graphics to the the mainstream
market into the professional market and
we can use artificial intelligence to
produce a much higher quality image much
faster than they could before
I think one of the biggest reasons why
people choose quad rope is the fact that
we do make the card ourselves and really
starts with the components
it's almost cruel what we do to the
cards thousands of hours of testing go
into the evaluation of Quadro as the
hardware platform we've got teams of
people that spend day and night putting
this through shock testing thermal
testing EMC testing drop testing we have
literally dozens and dozens of people
that do nothing but try to break the
cart
[Music]
for the regression testing for the QA
and the quality testing we have racks
and racks of servers and data centers
that do nothing but make sure that the
Quadro RDX card runs across multiple
alessa's across multiple apps
it's a certified solution that
certification is the additional steps
that the hardware the software vendors
take to make sure that the hardware and
the software are 100% compatible that
level of involvement with the ISPs
ensures that the apps keep up with the
capability of the cards with that
becomes a requirement that everything
still works we expect these products to
be used extensively in production
environments 24 hours a day seven days a
week for as long as it takes to do the
job and so when we build the products
that we look at the components of what
we need to go into that product that's
what we're desiring towards Quadro r-tx
is the most trusted platform of the
world it is the de facto standard for
every artist every designer every
scientist every researcher everybody
involved in a dozen industries that are
just as relentless as us in the pursuit
of perfection or in the pursuit of
advancing mankind or advancing their
industry they push us we push them and
it's a fun partnership to see what we
can get done and what the future holds
you
[Music]
Title: GeForce Garage: VR Series, Video 1 - How To Build A VR Ready PC
Publish_date: 2016-04-08
Length: 270
Views: 25258
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/BeFMfXxDPNo/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLDNiGmMu6a4v6Hl0ozyla6CpfJhfA
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: BeFMfXxDPNo

--- Transcript ---

this is Eve Valkyrie multiplayer combat
with a 360-degree view of deep space for
your cockpit it's unlike any other video
game you've ever played and today we're
gonna show you how you can bring
experiences like this into your own home
for a lot less money than you think
what's up everybody my name is Dwight
and welcome back to g-force garage today
we're gonna be working on an affordable
V already rigged with VR headsets
costing as much as they do these days
building a gaming rig capable of VR
without breaking the bank
sometimes can be a challenge that's
exactly what we're gonna show you today
but first
Reza what makes a great VR experience VR
technology today is a lot more immersive
and comfortable than it's been in the
past while your headset has the
capability to implement head tracking
and positional tracking your PC needs to
be capable of rendering to display feeds
one for each eye so that you experience
minimum latency and none of that jitter
and motion blur that can make you
nauseous yeah no I get motion sick so I
know that first hand
you won't with this rig okay I'm excited
so let's take a quick look at what VR
headset developers are recommending for
a minimum spec for a consistently good
VR experience quite likely that the rig
you're using today already exceeds the
minimum spec in fact a lot of folks may
only need to upgrade their video cards
to a geforce 970 or above but if you do
choose to build a PC from the ground up
we've hand-picked a bunch of components
for their affordability and performance
so the min spec for VR headsets is a 45
to 90 so that's all we have here and I'm
gonna put in the motherboard with the
stock cooler that comes with it
now keep in mind that this specific
motherboard is equipped with USB 3.0
ports but if you're not building your
own system from scratch and you do not
have USB 3.0 ports you can find USB 3.0
add-on cards cheaply online here's a ram
but we're going to use eight gigabytes
of Corsair Vengeance ddr3 you probably
already have more than this running in
your rig already alright and for the
case we're gonna be using the Corsair
Air 240 a feature about this case that
we like is that it comes with two front
USB 3.0 ports when Reza is done
installing the motherboard I'm going to
install the EBG a 500 watt power supply
which is the minimum spec for the
geforce gtx 970 for this build we're
using a wesen digital one terabyte drive
although we'd like to be using an SSD
this one is much cheaper and it will
hold more games alright
this is your GTX 970 graphics card or
all the heavy lifting for VR is done if
you're gonna upgrade one thing in your
computer getting a better graphics card
is going to have the most impact for VR
gaming all right now that that's done
are we gonna play some eval curry what's
good
so there you have it not only enough
performance for an $800 rig but enough
Headroom for even the most insane
dogfights and that right Reza's busy now
that you got a good idea of what steps
you need to take to get into virtual
reality head on over to g-force comm /
vr to learn more and if you want to see
more g-force Quran go ahead and click
right here the video I'm in one of us
Title: NVIDIA GTC May 2020 Keynote Pt2: NVIDIA RTX - A New Era for Computer Graphics
Publish_date: 2020-05-14
Length: 799
Views: 379104
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/BeScfkCm3b4/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: BeScfkCm3b4

--- Transcript ---

Computer graphics is the driving force of
NVIDIA.
It is one of the world's most computationally
intensive applications.
It has been for decades and it will continue
to be for decades to come.
40 years ago, one of NVIDIA's researchers
wrote a seminal paper on a technique to simulate
light, we call ray tracing.
You trace a light beam through an environment
bouncing off surfaces, reflecting, refracting,
or illuminating that surface, ultimately generate
what is a photorealistic image.
Two years ago, at 2018 SIGGRAPH in Vancouver,
BC, we announced one of our most ambitious
endeavors, we call it the NVIDIA RTX.
NVIDIA RTX fuses two groundbreaking technologies.
The first is accelerated ray tracing.
And the second breakthrough is deep learning.
Ray tracing is so computationally intensive,
even with the amazing accelerator that we'd
created, it just simply wasn't fast enough.
And then the breakthrough of artificial intelligence
happened, and over the last three years we've
been piling onto this technology to solve
the last missing piece of the puzzle.
We use ray tracing and our programmable shaders
and the fastest possible GPUs we could make
to generate a relatively low-resolution image,
and in this particular case, 540p.
Not even anti-aliased.
It also generates along with it a motion vector,
where the pixel is and where it's traveling.
That goes into an artificial intelligence
network, which tries to synthesize a higher
resolution image.
We teach this artificial intelligence network,
what extremely high-resolution and high-quality
images look like.
In this particular case, we used a supercomputer
to render 16K, anti-aliased resolution images.
We then compare what comes out of the neural
network with this ground truth.
The difference propagates back into the network
through a supercomputer, and it corrects the
weights of the neurons as to improve its ability
to guess the next time.
We go through this trillions of times.
Eventually, this neural network could take
just a few pixels, 540p in this case, and
synthesize what otherwise would be a beautiful
image.
Incredible.
Then we take this neural network, we download
it into your GeForce computers, particularly
the ones with Turing, are now ready to receive
this neural network and process it on the
Tensor Core processor in the Turing GPU.
We call this technology DLSS: deep learning
super sampling.
What you're looking at here is the image generated
by the supercomputer.
It's 16K resolution, it's completely anti-aliased.
This is a scene from an Unreal Engine demo
that Epic did called Infiltrator.
It's really a beautiful demo.
They did this several years ago, and here
what I'm showing you is 16K ground truth.
The next scene is rendered and it's 720p.
Notice how blurry it is.
Let me just go back one more time so you can
see it.
This is ground truth.
16K.
Look at the small lights.
Be like, look at the leaves on the trees,
the clouds from afar.
It is so crisp.
The detail's incredible.
This is rendered at 720p.
And here what I'm showing you is our first
try.
And we call it DLSS 1.0.
Notice it improves the resolution that appears,
but only by a little bit.
And most people felt that the artificial intelligence
technology was not going to work, but we believed
in it and we didn't give up.
This is, ladies and gentlemen, DLSS 2.0, scaling
from 720p, generating the pixels necessary
to create a 1080p ant-aliased image.
Look at that.
First-generation, a little blurry.
Second generation, look at all the lights.
It's much more than sharpening.
Look at all the lights that all of a sudden
appear that didn't appear before.
How do you create content where content did
not exist?
Well, partly because the neural network has
learned what the image should look like.
And secondarily because we have motion vectors
and the pixels, by observing across a few
scenes, the neural network could predict what
each scene should look like.
Now, if you were to render this with native
1080p, using the GPU to render each and every
pixel anti-aliased, this is what it looks
like.
This is native 1080p.
And look, artificial intelligence actually
does a better job.
Going back to DLSS 2.0, look at that, AI does
a better job than 1080p native.
That is a complete breakthrough.
Suppose we started from 540p and because there
are so few pixels, most of the pixels are
blurry when we scale it up to this image.
Now, imagine if we were to take this 540p
image and put it into an artificial intelligence
network, DLSS 2.0, and this neural network
had learned from beautiful images that were
generated by a supercomputer, and it's now
asked to recreate that image.
Look at that.
This is the input, 540p, and this is the output,
DLSS 2.0, 540p to 1080p.
What an amazing breakthrough.
Let's take a look at the combination of RTX
and DLSS on the most popular game in the world,
Minecraft.
Because each one of the worlds are created
by the gamer, it is not possible to pre-bake
a lot of the shadows and lighting effects
that you see in very big blockbuster games.
This is created by the users themselves.
And so the lighting effects can't be cheated,
and it has to be generated by the program,
which is the reason why we chose to work with
the team at Minecraft to bring RTX to it.
Now with this particular scene, you can see
that when we render Minecraft without DLSS
with just ray tracing, the frame rate was
only 35 frames per second.
With DLSS, we can render this beautiful image,
and then use DLSS to scale that low-resolution
image and still maintain the speed.
So now you get a beautiful image with ray
tracing, high resolution, and high speed all
at the same time.
And that is the requirement for modern computer
graphics.
Let me show you now a video that we just made
of Minecraft.
The reception has been incredible all over
the world.
You're going to love this video.
Oh my goodness, this looks insane.
Look at the shiny floor.
I feel like I can just slip and fall down
and hurt myself.
Look at the way the light comes through the
wall.
It really adds just this level of depth.
Is that mirrors?
It's mirrors!
Ladies and gentlemen, RTX ON, ray tracing,
DLSS.
We've made possible real-time ray tracing
10 years earlier than anybody thought was possible.
When we launched, the people were skeptical,
but now it is very, very clear that ray tracing
is here and it's the next big thing.
Creating 3D content is hard.
It takes so many different types of disciplines,
from artists to designers to software programmers.
It uses all kinds of different tools, from
Maya, to 3D Studio Max to Photoshop.
And they're creating these worlds that take
enormous databases.
That's one of the reasons why it's so expensive
and so hard to create world-class 3D content.
Well, we have a solution for that.
We call it the NVIDIA Omniverse.
And it leverages all of NVIDIA's technology over the last 10 years.
On the foundation, is our RTX server, our
latest generation GPUs.
Then it's built on top of a virtual application
server.
Each one of the GPUs could be shared by many
different designers using virtual Quadro,
or many GPUs could gang up to accelerate one
application.
The networking is accelerated and offloaded
by Mellanox NICs, the smart NICs that we were
talking about earlier.
And then one of the virtual machines is the
Omniverse Nucleus.
This Nucleus has created a shared space, a
shared world, and this shared world has portals.
The output of that portal is visualized and
streamed to any device you like.
Multiple designers could work on one design
at the same time, and reviewers could ask
for changes in real time.
The ultimate design collaboration platform.
Let me show you a demo that's created by NVIDIA
engineers.
What I'm about to show you is really amazing.
This was done over the course of the last
couple of months.
Artists, designers from different locations.
and it's never been seen before.
It is completely ray traced.
None of the lights are baked, none of the
shadows are baked, everything is completely
lit and shadowed in real time.
And one of the most important things is everything
obeys the laws of physics.
Let's roll it.
Isn't that amazing?
Real time ray tracing, physically based materials,
obeys the laws of physics.
It was created by just a few designers and
engineers on top of Omniverse, working remotely
from different states.
Incredible achievement. Just so beautiful.
I love it.
And so, ladies and gentlemen, this is the
NVIDIA Omniverse.
It starts with a server with a whole bunch
of RTX 8000s.
These are the most powerful ray tracing GPUs
in the world.
Tensor Core processing to do AI so that we
can both have beautiful images and high resolution
and high performance at the same time.
The servers are available from BOXX, Dell,
HP, and Supermicro.
And it's been preconfigured with all the hypervisors
necessary, the networking stack, and the virtual
Quadros so that you can remotely run applications,
so that you could create portals into the
shared space.
It's really, really an amazing thing.
In today's world where we have to work remotely
and share and collaborate with large numbers
of people, this couldn't have come at a better
time.
The NVIDIA Omniverse.
Title: NVIDIA GTC May 2020 Keynote Pt1: CEO Jensen Huang Introduces Data-Center-Scale Accelerated Computing
Publish_date: 2020-05-14
Length: 776
Views: 326246
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/bOf2S7OzFEg/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: bOf2S7OzFEg

--- Transcript ---

[Music]
hi welcome to Nvidia GT C 2020 our first
kitchen keynote I'm coming to you from
my home in California and I hope all of
your well and sheltering safely at home
before we start I want to take the
opportunity to thank all the brave men
and women who are finding in the front
lines against kovat 19 the nurses the
doctors the truck drivers the retail
clerks the warehouse workers all of the
people who are keeping the world going
while we're sheltering at home we're
also doing our part to fight kovat 19
scientists and researchers around the
world are racing to find a vaccine for
kovat 19 we're working with them across
the entire spectrum from containment
mitigation treatment to eventually
tracking and monitoring Oxford nanopore
for example was able to sequence the
virus genome in just seven hours using
our technology and working with plotly
we can do now real-time infection rate
analysis and with Oak Ridge and Scripps
we were able to screen a billion drug
compounds in a day versus a year the
team is for Tara NIH and UT of Austin
use the cryo spark to reconstruct the 3d
structure of the virus by protein NIH
and Nvidia built an AI model to classify
kovat 19 key we bought build a robot to
deliver medical supply autonomously and
whiteboard coordinator build an AI
system to automatically measure in
screening the elevated body temperature
researchers and scientists applying
Nvidia's accelerated computing to save
lives is the perfect example of our
company's purpose we build computers to
solve problems that normal computers
cannot we address applications from
computer graphics scientific computing
artificial intelligence to robotics and
our computing platforms are incorporated
into PCs supercomputers cloud computers
as well as autonomous machines we work
with scientists and researchers all over
the world and it's a great privilege for
our company to partner with them to
advance and discover the future our
amazing creative team may
video to celebrate the great works of
these researchers and scientists let me
show it to you
[Music]
I am an explorer
[Music]
searching for the origins of our
universe
and charting the safer path
world
[Music]
I am a helper
moving us forward with one step at a
time
when giving voice to every emotion
[Music]
and a human
modeling the future of
[Music]
in a haystack
[Music]
second counts
I am a visionary
uncovering masterpieces lost to the ages
[Music]
and finding new adventures in a galaxy
far far away
[Music]
driving perfection and
we creamed
[Music]
and even the narrator of this
the story you are watching
and a composer of the music
it's greatest challenge
[Applause]
to take it home
[Music]
[Music]
it is truly inspiring to see what is now
possible because of accelerated
computing accelerated computing starts
with a specialized processor we call the
GPU that offloads the computationally
intensive tasks from the CPU it also
includes an acceleration stack the
software is a vitally important part of
accelerated computing the acceleration
libraries the algorithms the system
software and the optimizations that we
develop together with the application
developer the system architecture is
also important it is important for us to
be able to optimize the system whether
it's for high-performance computers
cloud data centers pcs or autonomous
machines but ultimately the most
important part of accelerated computing
is developers developers optimize their
applications which increases the
performance and the value of the
platform which attracts customers and
increases the install base which
attracts other developers the positive
feedback system grows and it is now very
clear that Nvidia's accelerated
computing platform is at its tipping
point over the last several years two
fundamentally new dynamics has happened
the pickax is already computing to the
next level the first is the emergence of
this new type of algorithm called
data-driven where machine learning
algorithms data processing and the
movement of data around the data center
is more important than ever the second
is the applications that we're
processing now are so large it doesn't
fit in any computer no server no matter
how powerful are able to possibly
process the type of application
workloads that we're now looking at in
fact the server is no longer the
computing unit the data center is the
new computing unit with software-defined
data centers an application developer is
able to write applications that run in
the entire data center it is important
now for us to think about optimizing
across the entire end-to-end of a data
center from networking and storage to
computing for us to optimize the entire
stack top to bottom to be able to
optimize that the data centers scale
is invidious new approach and I believe
that in the next decade data centers
scale computing will be the norm and
data centers will be the fundamental
computing unit the importance of
high-speed networking and data
processing in a data center is exactly
the reason why we bought Mellanox
Mellanox is the world leader in
high-performance networking and high
speed networking and a high speed
computing go hand-in-hand let me show
you what they make I have one here that
I've been cooking for a while this is a
state-of-the-art Mellanox spectrum 4000
series Ethernet switch each one of the
port's can scale up to 400 gigabits per
second the entire switch has an
astounding 25.4 terabits per second of
bandwidth and what makes this particular
switch special beyond the fact that it's
high performance and Mellanox is
world-famous low latency are three
characteristics first this data
buffering architecture that makes it
possible to meter the bandwidth across
the entire switch so that every port
gets a fair and predictable bandwidth
second incredible virtualization
technology so that you can virtualize VX
LAN
routing across your hyperscale data
center and third you could find out
exactly what's happening with this new
technology called w JH what just
happened on the other side of the switch
is this programmable smart NIC the
Nvidia Mellanox Bluefield to the world's
most advanced programmable smart NIC
Bluefield to accelerates security and
packet processing at line speeds in this
particular case up to 200 gigabits per
second the networking stack the storage
stack the security stack is now
completely offloaded and runs on one of
these programmable smart NICs and what
will eventually become essentially a
data processing unit this is going to
represent one of the three major pillars
of computing going forward the CPU for
general-purpose computing the
GPU for accelerated computing and the
dpu which moves data around the data
center and does data processing no one
knows more about networking storage and
security in the data center than
Mellanox and they are so vital to
today's high performance data centers
scale computing I'm so happy that we're
officially one company I want to welcome
all of our families at Mellanox
to Nvidia GT C is all about developers
and developers are all about SDKs this
year we announced and ship 50 new SDKs
and videos SDK is formed in a stack that
is represented here and it's basically
three layers the first part of course is
our CUDA architecture it is
architectural II compatible against the
entire installed base of a midea and
this year we announced the 11th
generation of CUDA
that tells you something about our
commitment to this architecture 2 for
compatibility and backwards
compatibility I could a developer who
develops on CUDA will know that the
entire installed base of Emidio GPUs
will run that application and run it
wonderfully the next layer is CUDA X our
acceleration library our linear algebra
library signal processing library graph
analytics libraries and this year we
have several new ones kudi nn8 version
10 ser RT 7.1 which is our deep Learning
Network compiler and optimizer and then
on top of it we have our market or
domain-specific libraries from our TX
switches for ray tracing HPC for high
performance computing rapids for data
analytics AI for AI Clara for healthcare
and life sciences metropolis our video
analytics and our streaming signal AI
platform drive autonomous vehicles Issac
for robotics and aerial 5g one of our
latest for 5g virtual Ram processing the
number of developers on Nvidia
accelerated computing has continued to
grow and is growing what appears to be
an accelerated rate and this year we
have reached 1.8 million developers I
want to thank all of you for your
support we're committed to continue to
advance this platform and to continue
our
and backwards-compatibility our
architecture so that your install base
continues to grow this year although we
didn't have a live GTC 46,000 developers
signed on to watch GTC five and a half
million developers downloaded CUDA and
we now have over 700 applications that
are accelerated I have a lot to announce
this year we have four new applications
that are really important that I'm super
excited to share with you we have a new
chip that we're announcing and we have
four new systems so let's get started
Title: PAX West 2016 with NGN
Publish_date: 2016-09-02
Length: 28
Views: 7185
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/bQ7EXMKDkTs/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: bQ7EXMKDkTs

--- Transcript ---

hey nvidia fans on Julian from ngn and
we are here in Seattle Washington if you
aren't here that means you're not
attending pax West 2016 but we are you
want all the coverage talking to some
developers looking at big games little
games and some Nvidia tech be sure to
head on over to ngn subscribe and
comment for your chance to win some
sweet Nvidia stuff and our game ready
contest and we'll see over there
Title: NVIDIA Clara Federated Learning
Publish_date: 2020-06-18
Length: 184
Views: 12956
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/bVU-Ea6hc0k/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: bVU-Ea6hc0k

--- Transcript ---

The NVIDIA Clara Train SDK features 
privacy preserving Federated Learning. 
This lets multiple healthcare and 
research institutions collaborate  
on powerful deep learning models 
to improve patient outcomes.
An AI model is only as good as the data quality it's been trained on.
Clara Federated Learning 
dramatically improves model training by  
increasing the volume and quality of its data.
Let's take a look at a deep learning segmentation model.
In this medical imaging viewer we see an image of a 
brain volume with two brain tumor overlays. 
The red overlay is a brain tumor segmentation from a model 
that's been trained on a data set from a single institution.   
You can see the contours of the tumor 
are imperfect and there are even missing pixels.  
This can lead to false positives or false 
negatives. The green overlay is the tumor  
segmentation from a collaboratively trained 
model using Clara Federated Learning.   
In this model diverse data sets have been pooled together 
via shared parameters and aggregated in a central model
The segmentation is clearly improved with 
tighter tumor contours and better pixel accuracy  
Improved model performance is just one 
of the benefits of Federated Learning.  
With Clara Federated Learning data is not 
shared and the server client authentication  
and communication are securely established.  
The collaboration and model training can occur  
without violating security or data privacy.  
The participating institutions start with  
generically pre-trained models which can be 
fine-tuned for specific patient populations.  
Each institution then iteratively shares 
a fraction of their model parameters  
not the data itself with a centralized model. 
This creates a consensus model that's later shared back  
with the institutions. Clara Federated Learning 
truncates the values of model parameters and  
adds noise thresholding to maintain data privacy. 
Let's go back to our brain tumor example and take  
a look at how we can create a consensus model with 
Federated Learning. The Clara Train SDK featuring  
Federated Learning is available for download 
on NGC, NVIDIA's hub of GPU optimized software.  
Also included are pre-trained models and an easy 
to use Jupyter Notebook to help you get started.  
Once the server is configured to securely 
communicate with the participating clients  
each one downloads the model and a training 
round can begin. Here we're seeing the training  
progress in TensorBoard for a single client. 
Each client can monitor their own training progress.  
As clients complete their training rounds 
the model parameters are sent back to the  
server for aggregation and validation. 
This process iteratively continues until  
the model accuracy converges. And that's how 
easy it is to collaboratively build robust AI models.   
To get started with Federated 
Learning download the NVIDIA Clara train SDK today. 
Title: A Bountiful Harvest - Agrobot | Season 1 Episode 9 | I AM AI Docuseries
Publish_date: 2018-10-10
Length: 320
Views: 47420
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/bXQg_M7_6_E/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLBaGnWv-hDz0rhfBRKjbE71VrPv8w
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: bXQg_M7_6_E

--- Transcript ---

are you tired of a basket of fresh
strawberries costing a fortune what if a
I could bring that cost down and help
roars harvest their crops far more
efficiently one company is trying to do
just that I'm Arjun dot with NVIDIA and
this is Imai this is a grow bot an AI
powered autonomous harvester that's
poised to revolutionize the agriculture
industry it's fast sophisticated and can
operate without human guidance which
promises to deliver a huge boost in
harvesting efficiencies the AG robot
team is starting out focusing on one
crop in particular strawberries all the
reasons for going for strawberries is
the planck's is mold so we don't need to
make long movements second is like wait
I'm sorry
and it's really intensive in labor you
have to pick every three days and only
the ripe strawberry but you have to pick
one by weighing in a delicate way so we
try to make a selective system delicate
system that can harvest only what we
want to harvest but this being such a
delicate and precise process how can a
machine hope to replicate where humans
have mastered over generations first we
are scanning with a 3d camera for the
ribosome where we have artificial
intelligence that is trying to indicate
where the ripe strawberry square is this
thing of the servery because to be in
vertical to be on Gough we select the
cutting point and then we move the
robotic arm to the cutting point we have
a gripper we have this R away by the
stem really important we don't touch the
story it's not a question about bruising
it because we can make fingers that are
as delegated by humans it's a question
of spray out diseases so as less counter
we have with the plants with a field
higher quality product we have in the
lab to get pick a strawberry easily with
some elemental algorithm to detect
object then you have to realize that in
the field is totally different
our goal tiempo que llamo 200 Campo
movie to free shakuntala for my
procedure is a quadratic I see
love coming up in a heavily connected so
an infinite that every single variety
depending on the weather
on the environment they grow more they
grow less they have longest and shortest
time so naturally I wondered how could a
robot possibly keep up with all of the
variation in their target crop the
answer is a I elaborate rock-a-doodle
attractively in my community nolley well
Oh muy emocionante remotely certain
Acosta pursue a mentor dr. AHA oh he had
some artists memento
the estamos technical division TV
theologian alleys the polymerase ooh
momentum together muy difícil en el
barrio cuando gambia most elusive a
general a convolutional a las Luciana
primeira a lament at a lock-up in Viera
apart in the hemlock
everybody system uses semantic
segmentation which relies on a
convolutional neural network to classify
each pixel of the incoming image as
either belonging to the strawberry class
or not
however if strawberries overlap in the
image then semantic segmentation is
inadequate Agra bot is looking at
extending their system to perform
instant segmentation where each
strawberries understood as unique a
bounding box is generated for each
strawberry by a neural network that
performs object detection the system
uses the unique bounding boxes to
generate roughly instant segmentation
the areas where bounding boxes overlap
can be further refined by weighing every
pixel against a likelihood of belonging
to each of the overlapping instances the
result allows the system to more
accurately assess individual
strawberries for harvesting psyllium
possibly I said look at the ASIMO
synovium of tenido LEvolution the like
apatheia computer hyung yellow sheet
Amano - simply multi-shadow happy with
the media Porky's alien possibly a
sentiment element or element inaudible
Kunis hola es una de Guiche dos tenemos
la patria equivalent a supercomputer
door gasps a keto and Daniels another
way and now immigrating from text want
to taik to you and we have now one
processing unit
every body cam that we use the potential
use of of shivir replaced the individual
processing units for one two hundred
several robotic um even one machine
these are all fascinating and incredible
breakthroughs but I still wondered about
the impact this would have on people
doing these jobs today and what does
this mean for the future of the industry
because Dhirubhai belushin Ava can be
are poor compared to love to see Mohana
significantly less porosity de como el
apartamento a la corte while cereal
pretty idea vo e de la fruta una lechuga
sino Villa communica doctora carolallan
Campo is essential to start thinking on
the using of robot to assist actually in
the u.s. we get wasted fruit because
it's gettin picked the forecast says
that five ten years they're gonna be an
amazing shortage of labor force if you
know what they want do this kind of jobs
who is gonna make it who's gonna harvest
our food so that is why we need
solutions for harvesting we are sailing
thinking on new ways new approaches to
diagnose to run out fruit so all the
system will evolve over the time and
particularly in the case of AI
[Music]
Title: NVIDIA Special Address at ISC22: Accelerating AI Innovation and Scientific Discovery
Publish_date: 2022-05-31
Length: 2162
Views: 1807655
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/By0tHlNEdFE/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: By0tHlNEdFE

--- Transcript ---

[Music]
well thank you everyone for joining me
this evening um i'm here to give a uh i
guess the last talk of the day so i'm
between you and dinner i'll be as
efficient as i can
um i wanted to take a minute to describe
a little bit what
uh we're seeing in hbc and super
computing and share some vision share
some directions and where of course
nvidia is investing
in the backdrop of hpc uh it's always
important to think about and what
motivates a lot of the industry
are the grand challenges that we're all
working on whether it be a climate and
hpc helping us to predict the impacts of
melting ice maps or how greenhouse gases
will affect you know the world's extreme
weather events
in renewable energy
where hpc researchers can actually
simulate future wind farms to optimize
clean energy production
applying hpc to the global pandemic and
certainly this last few years where hpc
has played a pivotal part in research
for covid19
and beyond that expanding to to drug
discovery as a whole we're reaching the
point where computational methods are
becoming a foundation for future
disease prevention and vaccines and
disease treatments like the mrna
vaccines and then finally of course
understanding the universe new
instruments like ligo and understanding
gravitational waves reaching farther
into the universe to observe what's
happening and also simulate it from
first principle physics to understand uh
the nature of what's going on in space
and in astrophysics and fundamental
physics these are some of the ground
grand challenges that motivate our work
and fund our work that justify the
creation of of the world's super
computers as a result we're also seeing
how the modern super computer is
changing and what it's being asked to do
and really there's five main things that
are the workloads of the modern
supercomputer today and moving forward
the first being of course simulation
simulation will always be the foundation
of hpc and and super computing uh and
certainly it has been for the last
decades and we'll be into the future and
we'll talk about that the new workload
of hpc plus ai bringing ai techniques to
science
is everywhere you know five years ago
there was probably only about a hundred
papers published
uh
across the five thousand papers in in in
in data science actually we're deploying
ai methods for scientific problems today
uh approximately one third of all the
papers being published in ai
machine learning and deep learning are
being applied to scientific problems
this is clearly a new workload for the
modern supercomputer and an important
one at that
going beyond just simulation and
augmenting with ai is edge computing
connecting superior computers to the new
instruments of science and making them
part of the scientific instrument
themselves
we see as an opportunity to in advance
what modern experimental science looks
like one that's coupled deeply with the
computing infrastructure
in a real-time scientific experiment and
then you combine those with the
opportunities of digital twins digital
twins as you saw from this morning
keynote bringing together simulation ai
applied techniques and even experimental
science into a digital twin and having a
super computer that could be a proxy for
the real world to experiment both move
forward in time and in the past and
explore different scenarios
and then the fifth workload that we're
seeing emerge is quantum computing we're
still decades away from a quantum
computer at least one that's practical
enough to apply to large scale science
problems but the super computers of
today can be a critical tool in
evaluating and exploring and redefining
you know what does computer science mean
when you're dealing with schrodinger's
equations and being able to apply
quantum computing techniques and
rebuilding the foundations of
mathematics of computer science to to
some of these large scale problems super
computers today can be that
can proxy and simulate the quantum
computers of the future so let's go
through each of those five things first
of course is simulation and with
simulation it's always a question of
scale and we're seeing two areas where
this is always being challenged one is
climate science today's supercomputers
are being built to study the convection
at a global scale to simulate the the
transfer of heat the movement of heat
around the globe to do this we need to
actually model the entire earth down to
about a one kilometer resolution
and can be processed basically at minute
kind of time scales in order to
accurately simulate the global
convection of the earth but it doesn't
stop there the intergovernmental panel
on climate change issued a report you
know step one is
understanding convection at one
kilometer their second projection was
the need to get to a hundred meter
resolution this is really necessary to
predict some of the extreme weather
impacts reliably the the complexity of
hurricanes and other such events require
a resolution that's much down at the 100
meter resolution and of course as we
reduce the resolution or refiner we also
have to reduce the time step which
compounds the computational problem and
if we go a step further we want to
actually simulate accurately low-level
cloud formation to do the low-level
cloud formation we got to go a step
further beyond just 100 meter down to a
one meter resolution and now potentially
even sub-second time scales that is a
100 billion x computational need
to model
for for large-scale simulation of the
earth and will drive a lot of the
investment in large-scale super
supercomputing uh the same can be
applied at the microscale the biological
scale understanding the covid spike
and the the global pandemic spurred of
course a lot of research into
understanding the physics the actual
quantum physics the quantum chemistry of
the kovitz bike this was a simulation
done on the pearl motor supercomputer
with 4400 i think 8100 gpus where they
actually did it in a software package
called cp2k which you guys are familiar
with actually executed and simulated the
entire kovitz bike inside all of its
water of course water is a big part of
the simulation um
it's not uh operating in a vacuum
and that act this simulation actually
exceeded one extra flop of scientific
calculation leveraging the mixed
precision capabilities over modern gpus
in order to to accurately predict the
motion and behavior of the covet spike
to to come up with drugs that can
intercept its motion its in its activity
so the scale is happening at for
both these problems at the global
earth scale but also at the microbiology
scale driving simulation up as well as
the size of our supercomputers of course
there's a new tool for a for
computational science and that's ai just
like before in the beginnings when as
moore's law was tailing off on the blue
curve there was the introduction of
using leveraging accelerators leveraging
gpus to offload the compute rich parts
of the computation
we saw a jump in continuing an
exponential scale of growth of computing
as companies integrators and providers
here you've seen the show floor have
been investing in scale up and scale out
technologies we continue to make bigger
bigger supercomputers that can actually
solve these problems as one ai is
offering yet another leap in technology
and the scale of what we can do in our
modern super computers a couple of
examples i've highlighted here forecast
net was developed in collaboration with
nvidia with lbnl university michigan
rice purdue caltech and this was
actually developing a neural network
model that
modeled atmospheric rivers that could
predict atmospheric rivers
based being trained on actual data to
understand extreme weather events across
the pacific orb net was partnering with
a with entos to build machine learning
potentials to do interactive to actually
build a machine learning model that
could predict
and use as a proxy for inter atomic
potential energy energies
they actually got a 1000 x speed up
compared to using first principle
physics methods by using an ai
approximator that learned from the real
physics
sgtc which was done for a fusion code
working with bill tang at princeton and
argonne national labs successfully built
an ai surrogate model
for modeling the plasma physics inside
of a fusion reactor and we're even
seeing this being applied with
industrial hpc
siemens energy in collaboration with aws
built a
ai model that accurately predicted the
efficiency and breakdown of a
steam-based generating plant which
siemens produces they're using this
model to predict when they have to take
down the steam generator perform
predictive maintenance on it both ahead
of a catastrophic failure which of
course is a big interruption and cost
millions of dollars
and actually produces a service
interruption but also to minimize the
downtime to decide when to actually need
to do the predictive maintenance to keep
the efficiency and the cost under
control one of the areas in videos
investing in this is their nvidia
modulus platform so you know these
workflows uh developing an ai that can
accurately predict uh are based upon
physics-based methods is an entire
workflow we're contributing that with an
open source project called nvidia
modulus it's providing kind of a high
level abstraction for domain experts it
offers scalable performance so it can
run out actually on multiple gpus across
a cluster
and can be and has some of the latest
recently published model support in it
it has support for fnos afnos pinos
basically you use a the actual physics
equations as the reinforcement function
for the the neural net itself
so the neural net can learn
multiple ways that the partial
differential equations want to be solved
and their operators as proxies
and of course run much faster the third
use case is the rise of hpc at the edge
that we see and here there's a bunch of
scientific experiments
that are coming online right now the aps
the advanced photonic source which has
the ultra high ultra bright high energy
x-ray beam generator at argonne national
labs of course the ska and scott project
observatory which is the world's largest
telescope radio telescope the diamond
oxford which is the uk's national
synchrotron light source
etl the extremely large optical
telescope which is the european southern
observatory ligo which i mentioned
earlier is the gravitational wave
inferometer
which is developed by caltech and mit
and dlr which is the stratospheric
observatory for infrared astronomy in
partnership with nasa each of these
instruments produces obviously a vast
amount of data and they want to
collect that data of course it's too
large to store it's not can't be
operated as a batch and often they're
constantly running looking for the
needle in the haystack events that
they're designed to discover to
understand either in space or in science
or nature
ai has been and compute real-time stream
processing is being employed to whittle
that data down you know hundred or
thousand fold and then produce only the
data that needs to actually be processed
and connected directly with the data
center or the super computer to inspect
and learn and often these workflows has
been mentioned at this conference have
an active learning component so as
they're studying the data they're
refining their model which is being
pumped back into
the edge device which is looking to to
optimize and extract the data
efficiently
this is really hard
obviously you're dealing with massive
amounts of data so minimizing copies and
keeping the data flowing seamlessly
throughout the system throughout the
networking and to the actual computing
device is paramount it has to scale so
as the experiment gets more complicated
often they want to scale dynamically
apply more resources to it
or leverage an existing super computer
which maybe have other use cases
often we're combining multiple streams
of data so a lot of these instruments
are multimodal they have multiple
different sensors that want to come
together to to solve a solution but
let's also not forget the developer a
lot of these scientific experiments are
you know designed uh as experimental
data but not necessarily programmed the
programming of the instrument is and
deploying and developing the ai models
is as big as a part of the actual
engineering of the physical instrument
itself so developers uh users for these
instruments need
easy to use
uh
software platforms to code up in python
and apply modern ai programming
techniques to become part of the
instrument itself another example of
this actually is uh in lithography and
using ai-based methods to build to go
beyond the natural optics that are
naturally allowed to this is work being
done at argonne national labs they
developed a neural network called tycho
net tycho n which is actually an ai
model and what it does is actually takes
the raw in
a diffraction data from the microscope
itself and that diffraction data itself
is basically
a highly blurry image and actually tries
to reconstruct what kind of diffraction
co what kind of uh
material could have produced that
distraction pattern so now we're looking
beyond the actual wavelengths of light
to look at the diffraction different
interferences of light to reconstruct
what would normally we see in a typical
and what we would expect from a typical
microscope image
to do this and you can do this certainly
offline but it requires a
fairly extensive uh sort of first
principles calculation not not
dissimilar perhaps from
uh seismic imaging but now down to a
microscope a microscope scale at a batch
level but by deploying ai they develop a
neural network that actually minimizes
the computation
using a technique
built from auto encoders using ai's for
compression they actually can reduce the
number of layers down to a smaller
resolution to find what effectively is
the eigen vector and the i matrix of the
problem and expand it back out to
reconstruct the image this dramatically
reduces the computation by about 300 x
and and is basically an approximation a
quite good one of actually what is this
this microscope actually seeing based on
using an ai technique instead of first
principle math and physics and applying
computation turning this into a
real-time kind of instrument
and psychography is a very hot area
right now and is one of the really
interesting applications of ai this
isn't just for for silicon and
semiconductor technology but it also
could be applied to biological methods
in fact work being done at
lawrence berkeley and berkeley labs is a
building a new kind of microscope one
which actually uses ai to reconstruct
what biology can be doing
as it's happening so this is a we'll
show a video of this and
and you can see this is actually a new
light field camera that's being used to
actually reconstruct biology it's
happening in real time so please go
ahead and roll the video
[Music]
now with clara holoscan and nvidia index
we can visualize the entire large volume
of living cells in real time as the data
is being recorded directly from the
microscope
watching these living cancer cells move
about we can see normal healthy biology
and malignant processes at the same time
the fluorescent marker rendered in blue
marks nuclei which we see splitting to
form two cells from one cell a hallmark
of cancer is cell division occurring
more frequently and with less error
checking than normal healthy cells
using berkeley's lattice light sheet
microscope the ultimate high resolution
allows scientists to see what is hidden
to normal light optics not seen using
traditional microscopes as we zoom in
watch cancer cells display what is
thought to be a rare event even for
cancer cell lines see one cell split
into three cells this phenomenon has
only been reported anecdotally in a
couple of scientific publications
scientists do not yet know what we will
see but this technique enabled with
real-time processing and visualization
now allows the scientific community to
discover new unseen events like this
let's see what the future has in store
so this technology actually took the
light the light sheet microscope and
actually reconstructed that image
far beyond what the traditional optics
would do this was not a live
image from the researcher they can
actually move around and explore the
space as the biology was actually
happening normally this would be done as
an off
basically an offline process you would
scan place something like this an
electron microscope which of course
would destroy the sample in the biology
and you'd only get a static image now uh
we're applying ai to the uh to
tychography we can actually reconstruct
and build these kinds of cameras and
actually witness biology at resolution
way finer than we could before uh the
researchers that did this use the uh
holoscan technology this is an sdk it's
available from nvidia basically it's a
streaming reactive framework so you can
build your own pipeline for processing
the raw data from a sensor applying ais
and then producing whatever output you
need in the case of them it was a an
image it's optimized for high
performance low latency service oriented
and multi-model it's just minimizer
copies either working with our agx
platform which is the embedded version
of our gpus or can actually be deployed
on actual servers or even clusters if
necessary so if you're interested in
this work please check out the video the
holoscan sdk the fourth workload i
wanted to talk about was digital twin
and as this morning i think rev did a
great job explaining that that building
digital twins either at the molecular
the plant or factory level the city
level or even the planetary level makes
supercomputers another instrument of
science to be able to simulate all these
things and understand them in in a
virtual world in ways you can't really
in a physical world and and further by
applying uh modern scale-up simulations
as well as these new real-time ai
assisted simulation techniques we can
build these these systems these these
digital twins that actually are
interactive in real time environments so
another video i wanted to show which is
actually the atmospheric this is the
application of the forecast net
technology being applied to a digital
tune on earth so if you could roll this
video please
climate change is making storms both
stronger and less predictable leading to
more fires floods mudslides heat waves
and droughts atmospheric rivers are
enormous rivers of water vapor in the
sky
each carrying more water than the amazon
they provide a key source of
precipitation for the western u.s
but these large powerful storms can also
cause catastrophic flooding and massive
snowfalls
nvidia has created a physics ml model
that emulates the dynamics of global
weather patterns and predicts extreme
weather events like atmospheric rivers
with unprecedented speed and accuracy
powered by the fourier neural operator
this gpu accelerated ai enabled digital
twin called forecastnet is trained on 10
terabytes of earth system data using
this data together with nvidia modulus
and omniverse we are able to forecast
the precise path of catastrophic
atmospheric rivers a full week in
advance forecast net takes only a
fraction of a second on a single nvidia
gpu
that's a staggering five orders of
magnitude faster than traditional
weather forecasting models
with such enormous speed we can generate
thousands of simulations to explore all
possible outcomes allowing us to
quantify the risk of catastrophic
flooding with greater confidence than
was ever possible before
including rare but dangerous extremes
that may be missed by traditional
numerical simulations
with a combination of ai plus the
digital twin it really be the earth
becomes a digital experiment
and we can interact with this with this
digital earth by changing initial
conditions changing weather conditions
and changing of course
describe a future earth which may have
different climate conditions
and a different environment
okay the last use case of course is
quantum computing
and i don't need to remind this
community as a very hot topic in
computing in general however mapping the
schrodinger equations to computer
science algorithms is still in its
infancy and super computers have a very
important role to play to actually
simulate and emulate these future
computing technologies
there are lots of players in this space
lots of different frameworks that are
building quantum frameworks that can be
used to emulate or give computer
scientists the tools they can to figure
out how these future computing platforms
are going to be used
our job of course is to basically
provide some of the core mathematics so
we've we have a library called quantum
which actually does state vector and
tensor network uh primitives math
premises successfully is a math library
that's been integrated in all these
different quantum frameworks and the
results are pretty significant so today
uh from pascal aws anna dao and medicine
who's actually using quant trying to
develop a quantum algorithms to be
applied for drug discovery
gpus offer a huge performance benefit
over traditional
cpu-based algorithms and again these are
all integrations and we're using our
quantum library i think supercomputers
in general are the
platform for the next wave of computer
science one that's going to be powered
by quantum computing
in the next decades
okay so as a result we're super excited
about the super computers being built
for the modern workloads of ai and these
are just some of them certainly the perl
motor system at nurse who's just already
been built and been used for some of the
demos i just showed you here and for the
applied science of 3.8 extra flops of ai
performance nvidia is building uh eos
this is our own super computer that we
use for developing our products as well
as for our self-driving car and then
video research that'll have 18 extra
flops of ai performance built on our
hopper gpus
cscs which will be built next year we'll
have 20 extra flops of ai
using our nvidia cpus and gpus and the
chaneka system leonardo with 10x efflux
of ai also leveraging our quantum the
new system which is being announced here
at isc is the venato system which is be
hosted at the los alamos national
library built with our partners from
hewlett-packard venato will be the first
us-based arm system with grace and our
grace hopper superchips it'll have 10
extra flops of ai performance
and actually quite a bit of memory with
over 7 700 terabytes of aggregate memory
bandwidth in the system
the goals for the system is to do
fundamental open science
materials material science leveraging
vasp
for dft calculations and developing some
of those ai accelerated methods and
potentials for doing the lithography
work i talked about prior
also be used for energy research
particularly emulating and simulating
power grids as a digital twin
and for fusion energy research we're
also super excited they're using this to
actually develop
unmanned vehicles which are 5g
accelerated drones effectively that can
help fight detect and fight fires
i mentioned fusion energy fusion energy
is also an excellent candidate for
building digital twins so bringing
together the omniverse technology that's
talked about this morning
native simulation as well as ai assisted
simulation
is
digital twins are an important part of
defining the future of fusion energy so
in one last video i wanted to show you
which is the work being done at icf
at the uk atomic energy
to actually develop and design their
next fusion reactor both physically as
they're building it and a digital twin
in parallel so with one last figure
let's show what's going on there
united kingdom's atomic energy authority
and university of manchester are
partnering to produce carbon-free clean
power they're testing nvidia omniverse
to accelerate the design and development
of a full-scale fusion reactor a small
star held inside a magnetic bottle
building a digital twin that accurately
represents all reactor components the
plasma and the control and maintenance
systems is a massive challenge one that
can benefit greatly from ai and exascale
gpu computing the design of the physical
reactor itself involves numerous experts
from a variety of domains with so many
moving parts it's critical to keep
scientific engineering and design
experts and their data in sync at all
times
with omniverse teams can connect
disparate 3d design engineering and
simulation tools so they can collaborate
on a single source of truth simulation
of the fusion plasma itself is also a
grand challenge
scientists developed omniverse
extensions to ingest data from two
simulation software programs
the monte carlo neutronics code gm4
enables them to simulate neutron
transport in the reactor core which is
what carries the energy out of the
reactor and the joric plasma simulation
code simulates visible light emissions
giving insight into the plasma state in
the future scientists will use nvidia
modulus an ai physics framework to
further accelerate their simulations
with a physically accurate digital twin
of both the reactor and the internal
reactions researchers can work to create
ai-based systems to help control and
sustain the burning plasma maintenance
will be performed by robots trained and
re-optimized in simulation with large
amounts of synthetic data generated in
omniverse this helps them to perform
tasks accurately before being deployed
into the real world in the future the
omniverse digital twin will be
synchronized to the physical reactor
with real-time streams of sensor data
researchers will be able to explore
hypotheses by first testing in the
digital twin before deploying changes to
the physical reactor boosting
performance improving predictive
maintenance and reducing downtime
super cool and also a great example of
how super computers are going to be
used moving forward
so that's the software and
and we see both simulation hpc plus ai
uh edge hpc edge
digital twin and of course quantum
computing being an important part of the
modern supercomputer
nvidia is investing in all those areas
with our different sdks so please do
check them out
of course that's the software it
wouldn't be a video presentation without
providing some insight on the hardware
as well of what's going into these
modern supercomputers and what people
are using today recently we announced
our hopper gpu this is the gpu that's
going to be deployed moving forward this
is one of our most advanced chips ever
it's 80 billion transistors is
specifically designed both for hpc and
ai it's having supporting all the mixed
precisions from fp64 all the way down to
fp8 to do both the simulation and the ai
workloads most interestingly transformer
based models transformers which are the
majority of ai that's being done today i
think 70 of the ai papers published
today is based on transformers and we
specifically designed this gpu with a
custom transformer engine to apply
models like gpt3 developed by open eye
for conversational large language models
to be deployed and trained for science
use cases with this gpu it has our
fourth generation me link for connecting
them all together supports confidential
computing and multi-instance gpu as well
as new instructions specifically for
dynamic programming our dpx instructions
were designed in collaboration with the
genomic sequencing community of how can
we advance or improve the instruction
set of our gpus
for a better use case for genomic
sequencing optimization planning and
anywhere you can apply dynamic
programming techniques
its performance compared to our previous
gpu is obviously
a step up from anywhere from 2x to 3x in
some cases as for the genomics smith
waterman applications 7x improvement
prior to to the prior a100 gpu
but we didn't stop there you know in in
we've been working on building gpus for
uh 15 years now
for for hbc and and the world has
largely adopted accelerated computing as
as a path forward for optimizing
computation
that work has been done obviously
focused on the most important
applications in hpc the top 50 apps
which typically represent maybe 60 of
the workload in an open science
supercomputer
but what about the long tail the rest of
the 40 of the reigning applications
which are legacy applications perhaps
written in fortran which was of course
developed in 1957. you know it's
estimated there's over 1 billion lines
of fortran in hpc today and not all
that's going to be excel necessarily
accelerated or canopy the workload may
be to spread thin spread out
or the application may be certified and
it's difficult to move and be recoded
and move to an accelerated platform
for that use case
we developed our new
cpu called grace this is our grace
hopper module
on the left hand side is our new gray
cpu this is an arm based leveraging a
standard arm architecture v-class core
and combined directly with our hopper
gpu on the right and you can see it's
designed to have the densest possible
nvidia accelerated solution
actually supports a 900 gigabyte a
second interconnect between those two
superchips basically radically limited
dies to provide the chip coherence in
addition that coherence link can also be
used to connect two graces together to
build a grey superchip this is a 144
core cpu it has one terabyte of memory
as you can see along the side and since
everything's soldered down put close to
it it's a one terabyte a second of
memory bandwidth
we're also leveraging the lpddr
technology which is
designed for embedded in socs and but
applying it to data center we worked
with one of our memory vendors to build
an ecc and data center ready memory
technology as a result this processor
actually runs has double 2x the perf per
watt for cpu workloads and there's a lot
of interest in deploying the gray
superchip for legacy cpu workloads
especially in this era where energy
efficiency is paramount
some of the performance obviously by by
bringing the gpu and cpu closer together
we're seeing 2 to 5x
performance improvement in hpc
application performance and even with
traditional applications like wharf or
open phone where a large library of cpu
applications they can take advantage of
grace's improved performance its sve and
of course its energy efficiency
we don't stop there adjusting the chips
of course we want to connect and build
entire supercomputers and data centers
and our investment here is basically
bringing some of the
ideas that were first formulated in the
cloud to supercomputing itself to build
native nvidia cloud native super
computing where we have in-network
computing we have zero trust models with
multi-tenancy comp doing computational
storage and enhanced telemetry all based
upon our networking products
ranging from the quantum to infiniband
switch to connect x7 to bluefield smart
smartnix to the skyway gateway and of
course the software using for managing
all that the ufm one of the exciting
parts of cloud native supercomputing is
the idea of performance isolation
instead of where your your use of your
supercomputer does not affect the usage
of others and vice versa
but which is obviously very important to
the cloud where multi-tenancy is
paramount we can bring that technology
to supercomputing this is a chart for
love lamps and that basically a
histogram of the response times for the
the compare and exchange operations that
are happening inside of lamps as you can
see if you're a single tenant you're
getting great throughput uh but as you
have multiple users on the system your
performance can do great and the
performance of your collective
operations can can smear out and you're
affected by others by applying cloud
native techniques and performance
isolation we can actually build a
software-defined network that ensures a
certain amount of performance and gives
you even in a multi-tenant situation
predictable performance and peak
performance and keeps other other other
users out of your way of your
calculation
this platform it has both support for
doing different levels of security
networking isolation so and also is
integrated with some of our storage
providers we call this platform doca and
i also encourage you to check it out as
well we have multiple engagements going
on with research clusters to help
develop and work and partner as we
develop it at cambridge university our
own super pods but also at durham
university and the thor system of the
hbc advisory council
here at isc we announced two more
collaborations on doka
the lone star system attack
which is looking at smartnix using our
bluefield technology basically arm
course at the nic level and then also at
los alamos national laboratory where we
have a goal of providing a 30x
performance speedup by doing in network
computing some of the calculations that
we do in in hpc actually make sense to
perform be performed in the network by
doing so we can reduce the network uh
traffic altogether by performing those
operations in the network and
collaborating with los alamos to explore
that space
both how to program and actually how to
take advantage of in-network computing
as as a primitive for hpc
and the last announcement that has been
made today was that nvidia has now
partnered with cypril to help advance
the arm ecosystem here in europe and
around the world we're very excited to
be
doing the things that arm enables
the uh hpc ecosystem to do to build new
interesting more power efficient more
creative architecture system
architectures for optimizing hpc for
optimizing ai for getting better energy
efficiency and and customizing our
solutions for what hpc and their
supercomputing world needs
our partnership will start with
integrating our platform and sharing it
with sci perl also by building dev
platforms by helping the community take
advantage of arm technologies like sve
and exploring as proxies
future arm integrated gpu solutions
for better programmability and of course
increased performance
and that's it you know accelerating the
new wave of innovation of scientific
discovery consists of the five things
that our new modern super crew is going
to sbs to do ever increasing in larger
scale simulation that will always exist
and continue to exist in order to solve
the grand challenges but increasingly
building modern super crews involved
making them great for hpc plus ai
workloads for integrating them as part
of science as the instrument itself as i
showed you today
also being a digital twin to some of
these large experiments like
developing the next generation fusion
reactors and of course being the
development platform the dev kit if you
will for the future of quantum computing
though those systems are coming together
and being built now and nvidia is
building not just that helping with the
software and open sourcing and providing
contributions and software for this
ecosystem but also of course providing
the hardware components which we believe
are a great fit for these five workloads
and of course going beyond we're a big
partner with the cloud and the
hyperscalers and what they are doing and
bringing some of that technology and
bringing cloud native super computing to
super computing itself and with that i
want to thank you for uh listening to me
today i'm gonna end with one more fun
video uh which
i i think you'll enjoy so thank you very
much
ready
once upon a time it was all fun and
games
[Music]
nvidia started off by making chips for
video game machines
graphics became serious business when
people started using nvidia for
blockbuster movies
medical imaging devices
and the world's most powerful super
computers
and then one day researchers discovered
that our technology was perfect for ai
today nvidia is the engine of ai
[Music]
engineering the most advanced chips
and systems
and software that makes them sync
so robots can lend us a hand
cars can drive themselves
and even the earth can have a digital
twin
we live to tackle the world's biggest
challenges
and don't worry
we still love our fun and games
[Music]
you
Title: NVIDIA® G-SYNC™.  See Gaming Differently
Publish_date: 2013-10-24
Length: 135
Views: 84111
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/BZS8Bbyf1to/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: BZS8Bbyf1to

--- Transcript ---

[Music]
hi this is Tom Peterson with Nvidia and
today I'm up here in Montreal I'm really
excited we just got done launching a new
technology called g-sync what is it it's
super simple g-sync
is GPU sink and it synchronizes your
monitor to your GPU to eliminate
artifacts like tearing stutter or long
latency so what we did is we invited
three of gaming's founding fathers up
here to Montreal to check it out and
hear their impressions there's a ton of
PC games that look absolutely brilliant
when you look at a screenshot but when
you're actually playing them they run
between this this kind of no-man's land
between 30 and 60 frames per second and
all you get to choose between is either
an ugly tear line or a stuttery
framerate and that is exactly what jisuk
can absolutely fix a real connoisseur of
you know having game graphic to resemble
the real world the artifact free and
realistic and predictable and gsync
primary benefit is eliminating these
visual artifacts associated with
framerate synchronization enable you to
go to a game that scale is in frame rate
from time to time and on different
levels of computer hardware so that the
gamer is always getting the smoothest
experience it's possible
[Music]
biggest impact for gamers with g-sync
will be sexually actually easier to play
your games the hand-eye coordination
becomes quite significant easier when
you see this stable set of pictures that
your mind interprets as a single hole so
there's that feeling you get when you
get a much faster graphics card and the
experience is that you're used to or all
of a sudden smoother and silkier and
more exciting and you just got a big
grin on your face g-sync does that for
the same graphics card by making
something that's already great that you
like much smoother more responsive and a
better overall experience when you look
at a game running with g-sync it is just
buttery smooth it's smooth in the way
that a blu-ray movie is smooth and just
makes the play experience that much more
immersive and impressive
[Music]
Title: Delivering 3D graphics for VDI
Publish_date: 2014-01-21
Length: 209
Views: 11093
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/C1_zhCAvBdM/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLCP0HM4B3A_bG_INADil0YS-v7C1w
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: C1_zhCAvBdM

--- Transcript ---

hi I'm Sean from Nvidia today I want to
talk to you about how Nvidia vmware and
cisco are solving the challenges of
delivering rich virtualized desktop
experiences to the end-user the
popularity of desktop virtualization is
exploding as companies look for new ways
to enable employee mobility and data
security however up until now VDI
solutions have faced their own set of
challenges as more and more applications
utilize a graphics processor traditional
VDI is struggled to provide rich
interactive experiences nvidia grid
changes all that grid solutions install
into VDI servers to provide dedicated
graphics hardware for virtual desktops
as you can see in this side-by-side
video example the instance of adobe
photoshop on the left is running in a
typical VDI environment while on the
right that same application is running
on cisco's UCSC 240 rack server using
vmware horizon view equipped with nvidia
grid GPUs as a longtime provider of
professional graphic solutions and video
works closely with hundreds of software
providers to certify their applications
this allows grid VDI implementations to
offer the same performance users have
come to expect from Nvidia depending on
user needs organizations can implement
shared or dedicated GPU platforms on
servers including the Cisco UCS c240 m3
with shared GPU technology commonly
referred to as vs GA on VMware horizon
view 5.2 the hypervisor translates
nvidia driver commands allowing
applications running DirectX 9 or opengl
2.1 to utilize the GPU for full
acceleration because vs GA virtualizes
the grid GPU each grid board can
accelerate multiple users on a server so
now knowledge workers up to light 3d
users can share GPUs across their
virtual machines for faster more
responsive experiences and applications
such as windows Aero microsoft office
and even autocad pass through technology
in the form of
on VMware horizon view 5.3 allows the
hypervisor to provide virtual machines
with direct access to a dedicated nvidia
grid GPU organisations can use vdg aged
rate virtual workstations that give
power users all the benefits of a
dedicated processor at their desk users
like architects designers or engineers
can finally have the freedom to take
advantage of graphic intensive
applications remotely that have exactly
the same functionality as traditional
desktop GPUs full API supports such as
opengl 4.3 microsoft directx 9 10 11 and
nvidia cuda 5 are all supported in V DGA
so whether you're renting medical
imagery or designing an Autodesk sweets
or ptc creo your applications will
perform just as they would on a desktop
workstation with nvidia grid is an
integral part of your cisco desktop
virtualization solution with vmware
horizon view user's benefit from greater
mobility and productivity organizations
benefit from centralized security and
management while maintaining high
performance for workstation applications
to find out how to implement this in
your current VDI environment visit w WC
scoff IM where VDI calm / nvidia
you
Title: SHIELD Controls for Parrot AR.Drone 2.0 - New app, new controls
Publish_date: 2013-06-13
Length: 114
Views: 33350
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/CGdukO6KsQI/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: CGdukO6KsQI

--- Transcript ---

hey guys will hear at e3 at the end of
the day but we want to show you one more
cool demo before we leave the booth we
have a Nicholas here from parrot and a
parrot ar.drone down here you wanted to
show off the final shield optimized
version of the app that controls this
drone it's called free flight I think
the best way to show it off would be to
go for a test flight right so Nicholas
if you fly a fire that up
the right choice to control stuff like
rotation and altitude the left joystick
control stuff like tilt so you can go
forward backward left and right with the
drone and the d-pad controls flips right
so you can do all four flips forward
backward flips and left and right barrel
roll the right trigger will actually
take a photo the left trigger will start
recording video and one last point and
it's a big one is that shield connects
to the drone over Wi-Fi direct and
shield happens to be the only device
that can do a range over 300 meters it's
the longest range of any device that can
connect to the drone at that distance
and still work so that was a quick demo
of the new parrot ar.drone
app called tree flight with shield
optimizations controlling this drone
right here and you have this available
on Google Play for download now so as
soon as you get your shield and grab it
drone you're good to go
Title: NVIDIA Holoscan: AI Computing for Medical Devices
Publish_date: 2021-11-10
Length: 70
Views: 12191
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/cGuh5XAdowg/hq720.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AHUBoAC4AOKAgwIABABGH8gEygtMA8=&rs=AOn4CLBS1mCEnA8m2_p0yEoTMLemjVcc3A
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: cGuh5XAdowg

--- Transcript ---

[Music]
from medical imaging to surgical
assistance artificial intelligence is
helping to deliver a new standard of
health care
doctors use medical devices to learn
everything they can about their patients
now nvidia clara holoscan can be used
with medical devices to add real-time ai
applications
like what radiation therapy company
research is doing here to help
healthcare providers at every stage of
treatment
here's an example of how clara holoscan
reconstructs automatically segments and
renders patient anatomy
clara huddlescan can help to navigate
treatment plans with voice
giving doctors the insight they need
when they need it
zoom in zooming in the image rotate left
rotating image left remove ribs removing
the ribs
ai has also entered the surgical suite
and clara holoscan can be used as an
extra set of eyes detecting anatomy and
tracking tools
together medical devices and the clara
holoskin computing platform bring
real-time decision support to healthcare
Title: Celebrating the 50th Anniversary of Apollo 11's Moon Landing, with Commentary from Buzz Aldrin
Publish_date: 2019-07-19
Length: 161
Views: 209316
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/chuQkZZyfyM/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: chuQkZZyfyM

--- Transcript ---

today we celebrate the 50-year
anniversary of Apollo 11 and those who
take on the world's greatest challenges
very few know more about this spirit of
exploration than Buzz Aldrin the second
man to walk on the moon you know what's
special about that picture
three words location location
it just look so totally lifeless that
you couldn't describe it in any way but
a word that comes close to desolate you
can see that I think clearer and what
you had some time ago and Vidya had an
opportunity to show buzz an interactive
real-time recreation using the advanced
graphics capabilities today powerful new
and Vidya r-tx technology is bringing
these details to life like never before
we can render the lunar lander and its
surroundings with stunning levels of
realism and simulate how the sun's rays
react to every surface in real-time this
ability to play with light and shadow
gives us incredible new perspectives on
the moon landing each of us we're gonna
test the gravity and our strength in our
ability to get back in again that's
pretty good jump when I jump back up I
underestimated me and overestimated the
gravity you're doing a technical
analysis of this which shows a new
interpretation and it catches my
attention
[Music]
fifty years later Apollo 11s historic
achievement still inspires new
generations moonshots I got a good photo
that you could work on next it's called
the first selfie in space
[Music]
you
Title: NVIDIA Special Address at CES 2023
Publish_date: 2023-01-03
Length: 2841
Views: 5634761
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/CJWZuZsD0zQ/hq720.jpg?v=63af5924
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: CJWZuZsD0zQ

--- Transcript ---

Hi everyone.
Happy New Year, and welcome to CES 2023!
AI will define the future of computing, 
and this has influenced much of what we’re
covering today.
I will kick it off with gaming.
Stephanie Johnson will cover creating,
and then Deepu Talla will give
an update on robotics.
Finally, Ali Kani will talk about new
partners and tools in Automotive.
So let’s get started.
Last year proved challenging on many fronts,
yet gaming left 2022 stronger than ever.
We added 20M new PC gamers to our ranks,
and 100M since 2019.
Steam peak daily users reached a
new high of 32M, 
up 16% from 2021.
Esports viewing hours increased
40% over the prior year,
 the 2022 League of Legends Champions Finals 
broke a new audience record.
And no one missed the Call of Duty Modern Warfare 2 release,
which exceeded $800M in the opening weekend,
bigger than the combined openings of  ‘Top Gun: Maverick,’
‘Doctor Strange in the Multiverse of Madness’.
Last fall, we introduced a
quantum leap in PC gaming, 
our Ada Lovelace architecture.
Ada features next generation
shader cores, RT cores and Tensor cores, 
and breakthroughs in graphics like 
Shader Execution Reordering.
Ada also delivered a stunning
advance in neural graphics.
AI will change gaming forever.
Every RTX GPU comes bundled with a Supercomputer, 
training AI networks to generate 
high resolution frames 
from a lower resolution base.
The AI inference is then done in real time 
on RTX Tensor Cores.
This is DLSS.
With Ada, we introduced the next
breakthrough in AI-powered graphics:
DLSS 3.
DLSS 3 uses AI to generate
entirely new frames, 
outside of the graphics pipeline, 
boosting game performance up to 4X
over legacy rendering.
For CPU-limited games, 
like Microsoft Flight Simulator,
DLSS 3 frame generation doubles the frame rate.
And the supercomputer is always running,
as we continue to enhance the network
 to better understand different classes 
of game content.
Look for our next DLSS 3 update later this quarter.
DLSS 3 momentum is in high gear.
There are over 250 released games and creative apps 
that have turned to AI-powered DLSS
to boost performance.
DLSS 3 is being adopted by developers
faster than any prior NVIDIA tech,
 with 50 released or soon-to-be released titles.
Let’s take a look at DLSS 3 in action
 in some of today’s biggest games.
And there is more to come…
Witchfire is an upcoming 
dark fantasy first-person shooter
from The Astronauts,
a BAFTA-winning studio and the team behind
 Painkiller and Bulletstorm.
I’m excited to announce that
Witchfire will feature the
AI-performance boost of DLSS 3
when it launches in early 2023.
Let’s take a look at exclusive new gameplay of Witchfire.
The Day Before is an open-world MMO survival game 
set in a deadly, post-pandemic America
overrun by flesh-hungry infected.
It is one of the top most wishlisted games on Steam.
Today we are announcing The Day Before
will launch March 1st
featuring Ray Tracing and DLSS 3.
Here’s an exclusive look at
The Day Before with RTX ON.
Warhaven is a medieval-fantasy sword fighting experience 
by Nexon.
 It will pit teams of 16 players against one another 
in brutal battles with blades, bows, and magic.
Today we are announcing that
Warhaven will be accelerated by NVIDIA DLSS 3 
when it launches later this year.
Here is an exclusive new look at Warhaven.
THRONE AND LIBERTY is a brand-new IP from NCSoft,
famous for universally acclaimed MMORPGs
including Guild Wars 2, Lineage II and Blade & Soul.
Coming in 2023, 
RTX gamers will jump into boss raids
with the performance multiplier of DLSS 3.
Here is an exclusive reveal of THRONE AND LIBERTY.
Atomic Heart is one of the most anticipated games of 2023,
 where you take part in explosive encounters
in a mad and sublime utopian world.
Releasing February 21st, RTX gamers will explore 
this twisted sci–fi world with DLSS 3.
Let’s take a look at exclusive new gameplay of Atomic Heart.
We built GeForce NOW to make
high-performance gaming accessible
to billions of gamers.
It is now serving more games to more countries
than any other gaming service.
Subscribing to GFN puts a
GeForce RTX GPU in any client,
including Chromebooks, phones,
low-end laptops, even Macbooks.
25M members have enjoyed playing
over 1500 games including
Fortnite, Genshin Impact,
Far Cry, and Apex Legends.
With more games added each week on GFN Thursday.
GFN continues to expand globally
through our network of partners
that now include Rain Communications in South Africa, 
YTL Communications in Malaysia, 
and GFN.AM in Armenia.
I am excited to announce that
the Ada Lovelace architecture is
coming to GeForce NOW.
The new RTX 4080 SuperPODs 
will deliver an amazing 64 TFLOPS of
graphics goodness to each gamer.
That is 5X the performance of an Xbox Series X,
and will include full ray tracing and DLSS 3.
For competitive gamers, 
we are also bringing NVIDIA Reflex to GFN.
The RTX 4080 SuperPODs 
can render and stream at 240 FPS.
When combined with our Reflex technology, 
it achieves click-to-pixel latency below 40ms—
 a first in cloud gaming.
That is about half the latency
most console gamers see.
Take a look for yourself!
The RTX 4080 will be available 
in our new Ultimate Membership,
which is replacing our highest tier,
 the RTX 3080 Membership.
Only Ultimate members will have exclusive access 
to our highest performance GPUs, 
and can stream at up to 4K and 240 FPS.
RTX 4080 availability will start
in select data centers in late January 
with added coverage across North America
 and Western Europe throughout Q1.
All RTX 3080 members will receive the Ultimate upgrade
and be among the first to become Ultimate members.
And more exciting, the price to
existing 3080 members will
remain the same, just $19.99 a month.
The GeForce RTX 4090 and RTX 4080 launched last fall,
delivering our biggest leap ever
in performance and power efficiency.
The RTX 40 Series extended our leadership in graphics,
including ray tracing, AI-powered DLSS, 
and Reflex for low latency competitive gaming.
Today I am excited to announce
our next GPU in the family,
the GeForce RTX 4070 Ti.
The RTX 4070 Ti delivers up to
three times the performance of
our previous flagship GPU, the 3090Ti.
It will max out your 1440p gaming monitor, 
delivering over 120fps on modern games like
A Plague Tale: Requiem, Warhammer 40K: Darktide, 
F1 2022, and Spiderman: Miles Morales.
With the power of DLSS 3, the RTX 4070 Ti delivers
 1.8x performance across a range of titles…
…while consuming almost half the power.
The RTX 4070 Ti is packed with
40TF of Ada Shader cores,
93TF of 3rd generation RT cores
and 641TF of 4th generation Tensor cores.
It features dual AV1 encoders to
double the performance of video export.
The RTX 4070 Ti will be available
on January 5th with a starting price of $799.
The RTX 40-series is the
ultimate GPU for serious gamers
and creators.
The RTX 30-series continues to be
the best GPU for mainstream
gamers, starting at just $329.
Gamers and Creators are turning
to RTX laptops to drive the most
demanding games and apps.
The need for performance and portability 
has fueled RTX laptop growth well beyond the overall PC market.
Now over one third of GeForce users are creating.
And there is more to come,
 with 60% of the GeForce laptop installed base 
not yet upgraded to RTX.
Power and thermal constraints are the ultimate challenges 
in delivering high performance
in thin, highly portable laptops.
With the Ada architecture, 
we set out to transform laptops
through process, design and technology.
The result is a massive 2/3rd reduction in power
 at the same performance.
Put another way, 
Ada beats our current flagship GPUs 
with one third of the power.
Today, we are announcing the
GeForce RTX 40 Series laptops.
They are three times more power efficient 
and bring the Ada architecture, DLSS 3, and new
Max-Q technologies to the next generation of laptops.
Across gaming and creating,
 RTX 40 series laptops deliver up to
4 times the performance in the
most demanding apps, 
blowing away the previous generation in
creative apps like Blender and Arnold, 
and top AAA games like Cyberpunk 2077.
For years, we have worked with
system OEMs and CPU manufacturers
to look beyond the GPU
to a full system design approach
to performance and portability.
We call it Max-Q.
We optimize every aspect of the laptop: 
The GPU, CPU, memory, thermals,
software, display, and more…
to deliver powerful devices that are
thin and quiet with great battery life.
And with every generation, 
we have featured new technologies
that take laptops even further.
Today, we are introducing four new Max-Q innovations.
DLSS 3 has been optimized for laptops,
built into all of our Max-Q technologies
including Whisper Mode and Battery Boost, 
improving the performance, noise, and battery life by up to 2x.
We’ve partnered with memory manufacturers 
to offer the lowest voltage GDDR6 ever shipped.
And tri-speed memory control
that enables the GPU to switch to newer,
low power memory states dynamically.
Finally, Ada’s on chip memory
has been optimized for Max-Q,
doubling the bandwidth,
increasing the size 16x, 
and improving clock gating.
Our focus on laptop design, and
Max-Q, has led to a massive 22x
increase in efficiency over the
past 6 years.
Highly portable 14” laptops 
are becoming the fastest growing form factor.
Over 13 million were sold last year,
 twice as many as in 2020.
But 95% of these laptops are
only capable of basic productivity tasks.
Thanks to the RTX 40 Series, 
14” laptops are transformed 
to take on tasks never before possible,
 like ultra-fast 3D rendering in Blender,
or cutting-edge AI tools in popular apps 
like Adobe Photoshop or Premiere Pro.
They are also gaming powerhouses.
Up to twice as fast as a PlayStation 5
 but one sixth the size.
You can play the latest AAA games
like Cyberpunk 2077, with
with ray tracing and DLSS.
Connect them to an external monitor
 and your 14” ultra-portable 
becomes a desktop-class gaming rig or creator studio.
Today we are introducing the new
RTX 4070, 4060, and 4050 laptops.
Faster than our last generation flagship laptops
 at one third the GPU power.
Delivering 80 FPS 1440p ultra gaming.
And transforms creating, 
like rendering scenes in Blender that
used to take 2.5 hours, to just 10 minutes on RTX.
40-Series laptops start at just
$999 and will be available on
February 22nd.
The Ada architecture has also enabled 
a new class of enthusiast laptops.
I am excited to introduce today
the new RTX 4090 and 4080 flagship laptops.
They start at $1999 and will be
available on February 8th.
These are the world’s fastest.
Gamers can play on up to three 4k
monitors for surround gaming at
60FPS, enough to power a
professional-grade driving simulator.
Creators can use NVIDIA
Omniverse at 4K for 3D Design
with fully simulated physics, lighting, and materials.
Livestream your games at 4K
60FPS with AV1 to Discord.
And Ada’s new dual channel encoders 
cut video export time in half.
The RTX 40 series is our biggest launch ever.
There are laptops coming from every major OEM.
With a huge variety of devices,
there is an RTX laptop for every gamer and creator.
Now, I’d like to introduce Stephanie Johnson, 
our global marketing VP for Studio and Cloud Gaming.
Thanks, Jeff.
Gamers demand the absolute best performance,
but they are not alone in those needs.
With over 110M professional and hobbyist PC creators, 
the market continues to grow, fueled by powerful technology.
2D artists are moving to 3D.
From game modders to graphic designers,
 the growth is staggering at 42% year-over-year.
AI eliminates tedious tasks and
brings new assistance to the
creative process.
And video workloads are moving to real time, 
from livestreams to short form videos, like TikTok.
NVIDIA Studio is our platform
for this new breed of content creators—
super-charging workflows with RTX GPUs.
Over 110 RTX accelerated apps,
dozens of SDKs and Studio Drivers
 to deliver continual performance updates.
The heartbeat of the Studio platform 
is found in NVIDIA Omniverse,
 where creators can connect those accelerated apps
and collaborate in real-time.
3D creators use many specialized
design and content creation
tools to complete projects.
Those tools don’t easily interact with one another,
making it hard for artists to iterate quickly
 or bring their projects and assets from tool to tool.
NVIDIA Omniverse is a collaboration platform
 enabling artists to connect their favorite tools
 from Adobe, Autodesk, Unreal Engine, SideFX, and more.
Creators see the aggregated
scene instantly come together
without lengthy import and export cycles.
Changes happen in real-time
across the connected apps –
whether a single artist working in multiple apps simultaneously,
or a team contributing their
individual elements from around the globe.
Omniverse is based on Universal Scene Description, 
which provides a common language
between popular 3D design tools.
Powered by NVIDIA RTX, 
Omniverse supercharges creativity with
advanced materials, AI tools,
PhysX for easy visual effects,
and real time 4K ray tracing powered by DLSS 3.
Let’s take a look at 6 freelance creators
 using their favorite 3D tools, 
RTX Studio laptops, 
new GeForce RTX Ada GPUs, 
and Omniverse Nucleus Cloud to build a scene together.
- Hey everybody, thanks for jumping into today’s collab.
Where is everybody coming from?
- I’m in London.
- Finland.
- Finland.
- I’m in Florida.
- Vegas.
- Guangzhou city.
- Guangzhou city.
- Toronto.
- Wow. Cool, alright, well
I was thinking we could do an experimental 
short film in Omniverse
 using some assets we have on hand. 
But first things first, we need to get some
characters into the scene.
- I’ve got a couple of characters,
here’s ThreeDee and Figgy. 
What do we all think?
- Amazing.
- Amazing.
- That’s so cool.
- What did you build those in Rafi?
-I made ThreeDee is ShapesXR and Figgy
in Maya, but they are all
essentially drag and drop now
because they’re in USD.
- Man, amazing. Yeah I love ThreeDee.
 Have you ever thought about putting any
additional textures on her?
- Yeah, I was just working in Substance Painter.
I’ve got some knitted textures I can apply real quick.
Has that updated for you?
- Beautiful.
- Perfect.
- Awesome.
- Raf, you want some MoCap on ThreeDee?
- Oh. yes please! That’ll be fun.
- Oh Jae, what are you using?
- I’m using Xsens right now.
- Wow.
- So cool.
- Nice.
- That’s awesome!
- Hey, I have this background I made in Blender,
what do you guys think of this?
- Nice.
- Love it!
- Oh that one’s brilliant, yeah.
- Yeah it rendered in really fast.
- Yeah, RTX is insane.
Pekka, what do you think of the lighting now?
- Looking good, looking good, but
maybe it could be a little more epic?
Let’s bring some storm.
- Oh, super dramatic.
- I have this Houdini rain sim, here I’ll add it.
- Wow, that’s so cool.
- Excellent, excellent!
- That’s just brilliant.
- What if they escape the storm,
what do you guys think of that?
- Ok, but how are they gonna do that?
- Um, maybe that balloon over there?
- Yeah, the balloon!
- Where did that come from?
- Ok, we should probably get out of here.
- Uh, I think we just took Toy Jensen’s balloon.
- So, they’re taking off in his balloon, 
but where are they going?
- Well we could take them higher with some steam thrusters.
 I can add it in Omnigraph because I have the 4090
 so it renders particles really well.
- Fantastic.
- How about we take them even higher then,
like into space or something
- Love it.
- Ok, so now they’re space adventurers, that’s it.
- Oh perfect. Actually Shangyu has a satellite, 
we should totally add it.
- Sure, please use it.
- Sweet.
- Sweet.
- Oh, cool.
- And they could go all the way to the
eye of Jupiter to seek more storms.
- Beautiful.
- Beautiful.
- Yeah, let’s do it.
- Yeah.
- This is amazing!
New GeForce RTX 40-Series GPUs
power Omniverse creative workflows, 
with Omniverse Nucleus Cloud 
for cloud 3D collaboration—
in free early access now.
And Omniverse is just getting started.
With the alpha Blender release
available from the Omniverse
Launcher, users can easily
generate realistic facial
expressions from an audio file
with Audio2Face 
and optimize their Blender scene for
multi-app collaboration with Omniverse.
We are also excited to introduce
a new suite of AI tools and
experimental plugins using the
power of AI as the ultimate
creative assistant.
Audio2Face and Audio2Gesture
both generate animations from an audio file. 
The AI Toy Box by
NVIDIA Research lets you
generate 3D meshes from 2D inputs. 
And Move.AI’s Omniverse plugin enables video-to-animation.
When we announced the Ada Lovelace architecture, 
we also introduced the world to NVIDIA
RTX Remix built on Omniverse,
the easiest way to mod classic games —
where AI, software, and
hardware come together to enable
fans to channel their creativity
and reimagine the games we all grew up loving.
The modding community can’t wait
to get their hands on Remix.
We've already enabled two amazing
modders to bring RTX to Portal's
most famous mod, Portal Prelude,
preserving the timeless gameplay
while re-lighting it with full ray tracing.
RTX Remix early access is coming soon.
And with NVIDIA Omniverse ACE
early access, developers will be
able to more easily create
intelligent, interactive avatars
with the combined power of
NVIDIA’s many AI tools.
Let’s take a look.
NVIDIA Omniverse Avatar Cloud Engine,
 also known as ACE, 
opens a new world of possibilities for
bringing avatars to life.
By combining the realistic tones of the
NVIDIA Riva speech AI 
with the life-like expressions and
gestures from animation AI
like Audio2Face.
Creating the next generation of digital interaction
has never been easier.
ACE lets you breathe life into a
wide range of avatars from 2D portraits driven by your own video
and 3D characters from
top avatar creation platforms
like Ready Player Me.
ACE is now available for early access development, 
connecting the core AI building blocks of
avatar intelligence to virtual characters built on any engine
and deployed at scale on any public or private cloud.
I can even answer challenging questions
 using the power of a large language model.
Together with NVIDIA Omniverse Avatar Cloud Engine, 
we'll make the metaverse a smarter, more
engaging space for all.
AI assisted creator tools are also expanding
 throughout our NVIDIA Studio apps.
NVIDIA Canvas allows creators to
paint with materials using
simple brushstrokes and AI to
quickly conceptualize a beautiful image.
With the new Canvas 360 feature,
artists can create panoramic scenes 
and export them into any 3D app or platform 
as an environment map.
These can be created and
iterated upon quickly to change
background and lighting in a 3D scene.
The Canvas 360 beta will be
available to download for RTX
users later this quarter.
NVIDIA Broadcast enhances your mic and webcam.
With the latest update, we are adding eye contact 
to the feature list.
Come take a closer look!
Eye contact will change the
position of the speaker’s eyes
to appear as if you are looking at the camera,
 allowing for better audience engagement.
In this case, allowing me to
read the script from a screen
next to the camera, 
while I still engage with you.
The new NVIDIA Broadcast update
will be available to all RTX users
 later this month.
Watching video is one of the most popular activities
 on PCs today.
90% of online video runs at 1080p or lower, 
while nearly half of RTX users have
higher-resolution monitors.
This mismatch can leave a blurry
or soft visual experience.
Today, we're announcing RTX Video Super Resolution—
using AI to improve the quality of
internet video streaming in two ways:
First by removing blocky compression artifacts, 
and second by upscaling video resolution.
This AI-accelerated combination
improves video sharpness and
clarity on high-resolution displays.
RTX Video Super Resolution will
enhance videos streamed in
Chrome and Edge browsers
starting this February on RTX 30
and 40 series GPUs.
Now let me hand it back to Jeff.
Thanks Stephanie.
Gaming has grown beyond
entertainment into the social
fabric of generations of gamers.
Creating has grown beyond photos
and videos to virtual worlds
rendered with 3D cinematic
graphics and true to life physics.
And the RTX platform is powering this growth.
Today we covered several topics…
RTX momentum continues to build.
Ray Tracing and AI are defining
the next generation of content.
And one of our greatest
inventions, DLSS 3, has become
our fastest adopted.
NVIDIA Studio, with Omniverse at its heart,
 powers RTX Remix
and our new Avatar platform ACE.
The 40-series performance and
power efficiency is enabling a
new class of laptops, including
14” gaming and creating powerhouses.
And the new RTX 4070 Ti for desktop
brings the Ada architecture down to $799.
Finally, the RTX 4080 is coming to GFN.
Millions more gamers
will have access to the Ada architecture 
with our Ultimate Membership.
Now here’s Deepu Talla to share
with you our latest innovations
in robotics.
Thanks, Jeff.
Shortage of labor and effects of
the pandemic have accelerated
the deployment of robots in various industries 
such as manufacturing, retail,
agriculture, 
logistics & warehouses, 
delivery, 
and healthcare.
We have been working on a
robotics platform named NVIDIA Isaac
 for over 5 years, building on top of
NVIDIA AI and NVIDIA Omniverse.
Broadly speaking, the robotics
challenge can be broken down
into two categories.
The first category is the
continuous development and
operation of the robot 
or fleet of robots.
The second category is the
run-time on the physical robot.
Whilst building the run-time of
a physical robot is very complex, 
the first category of
dev-ops of robots is many more
times the effort and investment.
We have been methodically adding
tools to simplify this.
The biggest part of it is simulation
 and much of this presentation 
will focus on Isaac Sim 
and how innovators in the robotics industry are
successfully using Isaac Sim to
accelerate their development.
Physically accurate robotics
simulations require numerous 3D
assets to build the proper environments
 to develop and test robots.
But all too often, 3D assets are
built for visualization and not
physically accurate simulation.
Isaac Sim Ready Assets address this problem.
Replicator is the tool for
synthetic data generation.
Augmenting real-world labeled data 
with auto-labeled synthetic data 
vastly improves AI model accuracy 
and reduces the time to create the AI model.
Pre-trained models and the TAO toolkit
 provide a great starting point
 to rapidly train, adapt, and optimize AI models.
Operating a fleet of hundreds
if not thousands of robots
 requires accurate, dynamic route planning.
NVIDIA CuOpt is used for layout
planning and real-time route optimization.
Finally, for the second category
of run-time on physical robots,
we are working with the ROS community 
to accelerate Isaac ROS.
The robotics ecosystem is large
and complex and spans across industries. 
As we expand the breadth and depth of 
the NVIDIA Isaac platform, 
adoption continues to grow.
Over a million developers and
more than a thousand companies
are leveraging one or many parts
of the platform.
You can see several of our
ecosystem partners at CES this year.
John Deere, Skydio, Agrist,
Gluxkind, Cepton and Seoul Robotics 
are a few of our partners who have 
also received innovation awards.
Recently we enabled Isaac Sim in the cloud. 
Any cloud, your choice.
This has made Isaac Sim more
accessible, scalable, and collaborative.
A developer is no longer limited
by the capability of their PC or workstation.
They can access all
the features of Isaac Sim with a
single-click to the cloud from a browser.
The ability to simulate tens,
hundreds, if not thousands of robots in parallel
 is one of the unequivocal benefits of simulation.
Engineers can spawn multiple
instances of a virtual robot in the cloud.
In addition to addressing this
scalability challenge, soon
Isaac Sim will also address
simulation of multiple robots in
a single instance.
Developing robots is a
multidisciplinary endeavor.
Mechanical engineers, electrical engineers, 
computer scientists, and AI engineers 
come together to build the robot.
With Isaac Sim in the cloud,
these teams can be located across the globe 
and still collaborate efficiently.
Simulation is playing an
important role throughout the
life cycle of a robotics project.
Our partners are using Isaac sim
to create a digital twin.
Isaac Sim is quickly becoming
the must-have robotics development tool 
for all the benefits we talked about.
Many leading robotics companies
are leveraging Isaac Sim and
deploying physical robots that
were developed and tested virtually first.
Telexistence has deployed
beverage re-stocking robots
across 300 convenience stores in Japan.
Deutsche Bahn is training AI models
to handle very important
but unexpected corner cases that
happen rarely in the real-world.
Sarcos is developing pick and place robots
 for solar farm installation.
Ready Robotics is teaching
programming of industrial robots with Isaac Sim.
Festo uses Isaac Cortex, 
a tool in Isaac Sim, 
to simplify programming for Cobot skills 
and transfer from sim-to-real.
Flexiv is using Replicator for
synthetic data generation 
to train AI models.
Universal Robots is using
Isaac Sim for workforce development 
to train end-operators from the cloud.
Fraunhofer is developing
advanced AMRs using the
physically accurate and full fidelity 
visualization features of Isaac Sim.
Today we are announcing the next
release of Isaac Sim.
The new features include:
improved camera and lidar support 
to more accurately model real-world performance,
a new conveyor building tool,
a new utility to add people to
the simulation environment,
a collection of new sim-ready warehouse assets,
and a host of new popular robots that come pre-integrated.
For ROS developers, this release
upgrades support for ROS 2 Humble and Windows.
For robotics researchers, we’re
introducing a new tool, called
Orbit, which provides operating
environments for manipulator robots.
We’ve also improved Isaac GYM
for reinforcement learning,
And updated Cortex for
collaborative robot programming.
There are many more new features, 
and the list is rather long, 
but suffice it to say
there is something for everyone
in the new Isaac Sim.
We are committed to advancing robotics 
and arguably investing
more than anyone else in the world.
We are well on the way to having
a thousand to million times more
virtual robots for every
physical robot deployed.
NVIDIA Isaac platform for
robotics continues to grow
rapidly in its capabilities and
as a result in adoption.
We’re looking forward to
updating you with more
groundbreaking things in the near future.
Now here’s Ali to talk about autonomous vehicles.
Thanks Deepu.
Developing self-driving cars is
one of the most complex AI
challenges of our time.
It requires two computers:
an AI factory in the data center
that is used for Software Development & testing,
and there is an AI computer in the car.
The bigger computer is the AI data factory in the cloud.
OEMs need to process massive amounts of data
 collected from their fleet, 
and then curate, label, and train their AI
self-driving software models.
Using NVIDIA’s Drive Sim, 
you can then test and validate this
self-driving software in the
digital twin of these cars on
millions of scenarios every day.
All this software runs on NVIDIA DGX
and OVX servers in the cloud.
Now in the vehicle computer,
NVIDIA DRIVE provides a suite of
full-stack self-driving and
cockpit application software.
This includes the operating system, 
middleware, parking,
self-driving, 
and various in-vehicle cockpit applications.
NVIDIA is unique in that we are
the only company helping the
industry end to end—
from the complete AI data factory in the cloud 
to the full software stack running in the car.
The traditional vehicle architecture is very distributed
with nearly a hundred
different computers in a car.
There are computers for your
power seats & windows, 
your cockpit displays, 
driver monitoring system, 
as well as your parking 
and self-driving applications.
The NVIDIA DRIVE platform is
designed to simplify and
centralize the architecture for
software defined vehicles,
enabling a leap in performance
and capability while reducing
energy consumption and cost.
We recently announced our DRIVE Thor SoC.
It’s the first advanced robotics central computer
 that NVIDIA has built with the safety and security
required to allow multiple
vehicle applications to run in parallel.
With Thor, OEMs can re-design
their vehicles to have an
architecturally compatible
high-performance computer
and sensor suite across their entire fleet.
The automotive industry is
undergoing two technology
disruptions at the same time.
Electrification is giving OEMs
the opportunity to re-architect
their vehicles from the ground up
to be energy efficient.
Plus advancements in Autonomous Driving 
and Advanced AI cockpit
require a major investment in AI
software development
 and data factory operations.
These two disruptions are just starting.
Today, it represents just about
10% of cars or 10M a year.
In the next decade, 
this opportunity should grow nearly 10 times—
essentially all cars will be electric 
and support self driving.
There are two contrasting strategies
 and business models in Automotive.
Traditionally, Auto OEMs have
been in the business of selling vehicles.
With this type of mindset, they build vehicles
 that are focused on being at their best 
for their production date.
These OEMs specify basic computers 
computers and sensor sets for their entry cars,
 average computers for their mid-segment cars, 
and high-end computers with a rich sensor set
 in their premium cars.
Key metrics tracked for these OEMs
 are number of cars sold a year
and gross profit for a car 
on the year it was sold.
We are now seeing the automotive industry 
go through a transformation towards fleet operation.
This requires a contrasting mindset and know-how 
and it enables a fundamentally new business model.
These OEMs will focus on the
installed base of cars they have
on the road and how they can
best improve the features and
functions over the life of these cars.
This business model unlocks the
potential for more than ten
thousand dollars of value to the
customer as well as additional
software revenue to the OEM.
The NVIDIA DRIVE platform is developed 
to be open and easy to program.
We are proud that hundreds of
diverse partners in the
automotive ecosystem are
developing SW on our platform.
This includes 20 of the top 30 OEMs
 building New Energy Vehicles,
 many of the industry’s top Tier 1s and ISVs,
 8 of the largest 10 trucking 
and RoboTaxi companies.
Today, I’m excited to announce
our partnership with Foxconn,
the world’s largest technology
manufacturer and service provider.
Foxconn will build Electric Vehicles
 based on the NVIDIA DRIVE Hyperion platform
with leading electric range 
as well as state-of-the-art AV technology
 while reducing time to market.
It’s clear that as the next
generation of cars become
autonomous and electric, 
the interior is transformed into a mobile living space, 
complete with the same entertainment
available at home.
We already talked about GeForce NOW
and how NVIDIA’s revolutionary streaming service
is transforming how you can play games.
Today we’re announcing that GFN
is coming to the screens in your car.
Powered by gaming supercomputers in the cloud, 
GeForce NOW connects to the world's biggest
digital game stores, 
so users can stream across any device—
including internet-enabled cars—
no special equipment required.
Stream and play the biggest
games while you charge,
wait to pick up a family member,
or entertain kids in the backseat.
With over 1500 games ready to play, 
passengers can enjoy top game titles
 with millions of other PC gamers online.
By combining NVIDIA's heritage
in gaming and infotainment,
we're reimagining an in-vehicle
experience for passengers to
relax -- and have fun.
Today, I am excited to announce
that several DRIVE partners are
integrating GeForce NOW.
BYD, Hyundai Motor Group,
 and Polestar are all working with
NVIDIA to enable the vast catalog of game titles
 to be enjoyed in their cars.
In the past we’ve shown how
important the metaverse is in
the development and testing of
autonomous vehicles.
DRIVE Sim, built on Omniverse,
enables us to create a digital twin
 of our roadways to ensure
that the AI car can safely
navigate through complex scenarios.
Today, I’m announcing that
Mercedes-Benz is using this same
digital twin technology to plan
and build more efficient
production facilities.
Let’s take a look.
NVIDIA Omniverse is a scalable,
end-to-end platform enabling all
industries to build and operate
digital twins for scientific research,
infrastructure,
product design,
architecture, 
and more.
Now Mercedes-Benz is using
Omniverse to optimize new
production and assembly facilities.
Building a car requires
thousands of parts and workers,
all moving in harmony.
Using digital twins created in
Omniverse, an assembly line for
a new model can be reconfigured
in simulation without
interrupting current production.
Production planners can
synchronize plants around the world, 
enabling over-the-air software updates 
to manufacturing equipment,
streamlining operations,
 while improving quality and efficiency.
Mercedes-Benz is preparing to
manufacture its new EV platform
at its plant in Rastatt, Germany.
Operations experts are simulating 
new production processes in Omniverse 
which can be used alongside 
existing vehicle production.
This virtual workflow also
allows the automaker to quickly
react to supply chain disruptions, 
reconfiguring the assembly line as needed.
Using NVIDIA AI and Omniverse,
Mercedes-Benz is building
intelligent, sustainable factories that
 improve efficiency, 
reduce waste, 
and continually enhance vehicle quality.
The applications for Omniverse
in the automotive market are staggering.
The same simulation technology
that is accelerating AV development
 and used in manufacturing is also
revolutionizing the way the
interior of the vehicle is
designed, tested, and even sold.
Using NVIDIA DRIVE Sim,
automakers can design their
vehicles and retail experiences
entirely in the virtual world,
streamlining a traditionally
lengthy process.
Designers and engineers anywhere in the world 
can use the DRIVE Sim platform to work 
side by side developing the cockpit experience, 
and test potential physical design concepts 
before the vehicle goes to production.
- How do these look?
Here’s option 1, 2, and 3.
- I like the third one, but it
partially occludes the speed panel.
 Needs to be fixed.
- Got it. Will do.
DRIVE Sim can also be used to
validate that the vehicle design
meets local safety standards.
- Hey, the occupant monitoring camera,
let’s adjust it so we can 
better detect distracted driving.
- Sure.
- That’s perfect.
This experience is fully immersive, 
enabling engineers
and product managers to
integrate the driving experience
in Virtual Reality.
- Okay, let’s start Drive.
On the highway, 
daytime with clear weather.
- Okay.
- Please activate Drive Chauffer.
- Please change the confidence
output to the center screen.
- Alright. How does that look?
- I actually like it there.
- Looks great.
- Stop driving.
Let’s try out a different scene.
Let’s change it from day to night.
- Sure thing. How’s this?
- The night mode UI looks great.
I think that’s it.
Thanks team.
The ability to test features and
various configurations in
simulation ultimately helps
avoid costly mistakes.
And DRIVE Sim isn't just for developers.
Potential car buyers can also
benefit from the simulation platform, 
configuring and experiencing the car 
from the comfort of their homes.
- Check out the music player.
- That’s pretty cool.
Oh, and there’s backseat screens, 
the kids are going to love those.
- Yeah.
From concept to customer experience, 
DRIVE Sim is easing pain points 
and opening up new possibilities
 for intelligent vehicles.
Thank you for joining us today.
From the cloud to the car,
NVIDIA is developing AI and
Metaverse technologies for the transportation industry,
enabling safer vehicles and more
enjoyable experiences on the road.
Today we have also shown you the
latest examples of how AI is
transforming Gaming,
 Creative Design, and Robotics.
Have a great CES!
Title: Hottest Announcements at NVIDIA GTC 2020 October
Publish_date: 2020-10-14
Length: 156
Views: 41554
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/CKnipnFsuFo/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: CKnipnFsuFo

--- Transcript ---

ai will revolutionize computing
everywhere
from scientific computing cloud
enterprise data centers
edge to autonomous machines we've
created the ai system to write the
software
and the robotics computer to run it and
with nvidia omniverse
we have a physically based simulation
engine to create the worlds to train the
robots
omniverse is nvidia's platform for
simulation
and collaboration the nvidia clara
discovery
a state-of-the-art suite of tools for
scientists to discover life-saving drugs
where no tools exist we develop them
like the nvidia parabrakes
clara imaging biomegatron biobert
and nvidia rapids we built a ready-made
ai supercomputer
called the nvidia dgx superpod
scalable from 20 to 140 dgx systems
the bluefield dpu is a data center
infrastructure on a chip
bluefield 2 has armed cpus and a whole
host of state-of-the-art accelerators
and hardware engines
today we're announcing doka a
programmable data center infrastructure
processor architecture
nvidia ai inference compute is growing
10 times every couple of years
and this year surpassed total cpu
compute in the cloud
we've reached a tipping point of
accelerated ai inference
every ai service and application can now
be gpu accelerated
nvidia jarvis our state-of-the-art
conversational ai application
is now in open beta today we're
announcing nvidia magazine
a cloud native streaming video ai
platform for applications like video
calls
we're announcing a major initiative to
advance the arm platform
today we are thrilled to announce a
major partnership with vmware
together we're porting vmware onto
bluefield
our partnership will redefine the data
center and we announced that cloudera is
accelerating with nvidia ai
the final piece the enabling technology
of iot
is ai this is the nvidia egx
edge ai platform nvidia egx is designed
to make it easy for the world's
enterprises to quickly stand up a
state-of-the-art
edge ai server we're announcing that the
jetson nano 2 gigabyte
will be 59 anyone can now build a robot
the age of ai has begun and nvidia is in
full throttle to advance this new form
of computing
Title: NVIDIA Omniverse: Exploring DeepSearch with Activision
Publish_date: 2021-04-16
Length: 53
Views: 27815
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/cls3ewI8YII/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: cls3ewI8YII

--- Transcript ---

Game development is an extremely complex process with 
hundreds of thousands of assets  made across dozens of teams.
 3D asset libraries are becoming more difficult to search through.  
Together with Activision, we are exploring the ways 
Omniverse can help game developers build games faster. 
With Omniverse DeepSearch, game developers 
can now search intuitively through enormous  
3D libraries using natural language or images. 
The developer can simply place the 3D asset from  
DeepSearch into a fully interactive path traced scene. 
Using Omniverse's advanced simulation and  
physics capabilities, designers can configure, 
adjust, and place physically accurate objects  
with just a few clicks -- doing in seconds what 
was traditionally a time-consuming process.
Title: Surround Camera Radar Fusion Eliminates Blind Spots for Self-Driving Cars - NVIDIA DRIVE Labs Ep. 15
Publish_date: 2019-11-13
Length: 104
Views: 23075
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/cMlGyIJH5L8/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: cMlGyIJH5L8

--- Transcript ---

In DRIVE Labs episode five we showed you surround
camera object tracking.
Today we're going to show you surround radar
obstacle detection and tracking, and how we
combine the camera and radar perception results
using sensor fusion.
Here we're looking at a three-dimensional
top-down view of results generated by an eight
radar surround perception setup.
The ego car is shown in white at the center
and the concentric rings around it denote
10-meter distance intervals from the ego car's
rear axle.
Diamonds denote objects tracked by the surround
radar with moving objects shown in green and
stationary ones in purple.
We see that the track IDs are stable even
as objects move around the ego car which enables
accurate velocity measurements.
And here we see the surround camera view of
the same scene that we just saw in 3D radar
view but with camera radar fusion also enabled.
The small white squares denote the radar tracks
with track histories visualized as orange
traces.
When the camera object detections shown by
the bounding boxes are fused with the radar
tracks the small squares are visualized in
green.
With camera radar fusion the object position,
velocity, and acceleration measurements become
a more precise and reliable input into planning
and control functions.
In this clip, we see camera radar fusion in
side by side views.
In the radar view on the left, camera radar
fusion is shown by the association between
yellow cubes, camera objects, and green diamonds,
radar objects.
While in the camera view on the right, it
is shown by the association between green
bounding boxes and small green squares.
Camera radar sensor fusion brings improved
accuracy, diversity and redundancy to overall
surround obstacle perception, and is available
in the NVIDIA DRIVE Software 10.0 release.
Title: Driving AI Innovation for the Long Haul - PACCAR | Season 1 Episode 2 | I AM AI Docuseries
Publish_date: 2017-12-18
Length: 283
Views: 25287
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/CNgi_E6vj34/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: CNgi_E6vj34

--- Transcript ---

as a frequent one-click shopper and free
two-day shipping enthusiast I often take
for granted all the amazing advancements
that it takes to bring these goods to
our doors
today's truck drivers are running more
miles and hauling more Freight than ever
before
that's why Chuck manufacturer of pack R
is turning its sights towards AI I'm
Arjun duck with NVIDIA and this is Imai
trucks really are the lifeblood of the
US economy 70% of the Freight tonnage
moved in the United States every year
goes on trucks so that means over 10
billion tons of freight being hauled
every year what we want to do is to make
the truck driver as safe comfortable and
productive as possible at the pack our
Technical Center in Washington State
the potential of autonomy is becoming a
reality so in order to create an
autonomous vehicle we need to be able to
perceive our environment this requires a
lot of sensors so what we've done with
this truck is we've added by dark which
is a laser imaging system it gives us
range detection we've added cameras we
have a camera in the
we have a radar unit in the bumper we
have additional cameras in the mirror on
the a-pillars and we also have cameras
in the side mirrors all of that
information from those cameras the radar
the light are all feed into the drive px
2 where we can do image processing and
then we can take all of that fused image
data and run it through the neural
network and decide on what it is over
scene so I'm just gonna start driving
the truck manually here get a little bit
of speed up
engage the autonomous system with the
switch
we're moving the autonomous mode right
now seeing 40 tons of AI trucking down
the road is an incredible sight but I
found myself wondering is a truck truly
driving itself or just following a
pre-programmed path
the truck is building a picture of the
environment from the sensors so a
combination of the cameras radar and
lidar it's building a map of the
environment immediately surrounding the
truck and then based on its position
within that environment it makes
decisions on where it's going to drive
one of the most important pieces of this
equipment we've had to add to just truck
to make it fully autonomous is to drive
px 2 which is providing the compute
power that we need to process all of
this data one of the amazing things
about this project was we were able to
take neural networks that have been
trained by the Vidya in a completely
different environment and apply them
directly in our vehicles what Chris is
talking about is called transfer
learning the neural network that's
capable of driving anywhere needs to be
trained on all types of roads and
conditions but currently there's not
enough data being collected with trucks
instead passenger cars can be used to
collect a much wider variety of data
more efficiently we can use this data to
build a neural network giving the
vehicle and awareness of the world
around it however there are some major
differences between driving a car in a
large truck things like trailer
articulation car addresses this by using
transfer learning adding some software
layers that adapt the path planning for
the truck including proper trailer
traffic
the nvidia dry platform lets the neural
network interpret the incoming data in
real-time and send the proper commands
to braking steering and throttle to
control the vehicle safely so that idea
of being able to transfer that learning
into our environment that's a lot of
power it really can save a lot of time
what we have here is a demo of the level
4 system which is a high automation
highly automated system we are working
now to improve that system to handle
more challenging conditions such as
poorly marked roads or difficult weather
but this headstart PACCAR is well on its
way to creating trucks that can drive
autonomously on public roads does this
mean that there isn't a future for truck
drivers there's a lot of concerns out
there about what autonomy will do to the
trucking profession and what we're
looking to do is absolutely not to
replace the driver but to make a tough
job easier as far as autonomous driving
goes not having to focus for 10 to 14
hours a day driving a long-haul or 500
miles it would be good benefits for both
the industry itself safety wise and in
the driver we just look at the aviation
industry we've had auto pilots for a
very long time
and we still got two pilots in the
cockpit I think it's going to be a
similar story with trucking
[Music]
you
Title: AI with the Heart of a Composer - Aiva | Season 1 Episode 1 | I AM AI Docuseries
Publish_date: 2017-12-02
Length: 333
Views: 64438
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/CPh0bKcXgLo/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: CPh0bKcXgLo

--- Transcript ---

any film buff knows that music adds a
whole other dimension to the drama on
the screen as a huge moving art to me
it's one of the most important elements
finding or composing the right music is
no simple task but one company is trying
to change that using the power of AI I'm
Arjun dot with NVIDIA and this is Imai
as a composer writer's block is probably
the most dreadful thing so what we came
up with is this algorithm that can
compose a lot of music very fast and
provide them as extremely fast to
clients IVA is an artificial
intelligence that composes music for
films TV shows games any kind of
entertainment content this is quite
different from what's out there what
we're trying to do is create deep
learning algorithm that creates music at
the same level of top composers so music
full of emotions it was exactly this
kind of emotional resonance we wanted
when we were searching for an original
composition for Nvidia's annual
conference about AI the Nvidia GT C
project started when Nvidia reached out
to us started talking about how IVA
works and eventually they told us about
their project to tell the story from the
perspective of an artificial
intelligence in their keynote
introduction so on behalf IVA we were
very happy to agree to make the music
for the video
having AI help speed up the creative
process is interesting but I wanted to
know how Ava knows what to compose we
sit down with the client they tell us
what they need is it an emotional song
is it an uplifting song and then they
might tell us okay I need Ava to learn
in the style of this specific composer
and maybe they need music in the south
Mozart or Beethoven every project is
different in Nvidia's case and we
deliver some sample tracks and we
decided a subset of a database was
similar to those tracks so we retrain
either so how did Ava learn to be a
classical composer it starts with a huge
database of classical music that's
stored in MIDI format this means that
the computer knows every note in court
that's played including the pacing and
rhythm using a recurrent neural network
it looks for patterns in the selected
tracks to understand the basic style of
the music ava practices by predicting
what notes will come next in the track
once it gets good at these predictions
it creates a set of rules for that style
of music it's now ready to create an
original score
that sounds incredible but what is
stopping Ava from simply copying bits
from different songs and calling the
result an original composition using GPU
computing who have created what we call
a plagiarism checker which is able to
understand whether a created track is
partly or fully plagiarized from the
database the AAI learn from and once the
client is satisfied with the music were
able to have humans orchestrate the the
piece which means converting it from
piano music to orchestral music adding
different instruments then once the
client is happy that we're able to
record it with musicians
I was surprised at how emotional the
music was but that may be because
there's still a lot of humans involved
can a I really take it to the next level
to a judge music implies to be outside
and to look from out outside and when
I'm playing when I'm conducting whenever
I'm conducting the music I must be in
the very heart of the music but I think
it's a big chance for creators and
musician to use technology in order to
ask it to push us to excellence so in
order to continue developing the
capabilities of the artificial
intelligence we have three features in
mind the first one is creating a musical
ear so Ava will be able to understand if
music is good or not because so far it's
been and how it work we've been going
behind the AI to choose whether a piece
is fitting a client requirement or not
the second feature is allowing Ava to
compose full full orchestra directly and
the third feature is allowing Ava to
read a script of a film or video game
extract the Mattocks and emotions and
then directly map that into music if
there was any doubt that Ava is truly a
composer that question has been answered
ava has been recognized by sa CEM a
French organization that protects the
rights of composers having an artificial
intelligence recognizing an author's
right Society means that all of her
composition are protected
copyright
I am the tutor of this artificial
intelligence until she becomes more
independent and she meant space herself
and has even more rights to the eyes of
law
[Music]
Title: GeForce Garage– A CS:GO Weapons Case Mod
Publish_date: 2016-10-06
Length: 197
Views: 22750
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/cW8BWzpdVw0/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLDWhwwrm8wKrDbn8zu1aa2h65NMcg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: cW8BWzpdVw0

--- Transcript ---

what's up everybody and welcome back to
g-force garage my name is Dwight and we
just got back from the ESL one New York
where we gave away two 1080s and this
amazing PC built inside of a weapon
crate from CS Cal it was built by my
good buddy John hands from nurdrage Pro
let's not wait and open this thing up
alright let's crack it open
so what are the specs in this thing well
we got a ASUS Maximus 8 formula
motherboard we have 16 gigs of a victor
ram force of GTX 1080 and h100 a IO or
the for all the cooling ok well what can
you tell me about the actual enclosure
that's in here because I don't think
that you just put a case in here right
because this is actually built inside
the case yeah pretty much
everything in here had to be hand-built
the challenge was to make it to where it
was portable and to where it was both
true yeah we took some aluminum angle
and riveted it together to a solid frame
got it to the general shape you wanted
and started adding pieces to that the
design is to have air flow go in one
side and air flow go out the other the
power supply I had to design it into
here where it didn't vent back into the
system cut out the window notch it on
both sides and hinged it so it could
open so you could route all your cables
through the inside out the back that way
you don't have any cables just coming
out of this thing we backed it with some
basically some pieces of plastic in here
that have some vinyl over the top to
just put some back color behind this it
just adds a little bit of three
dimension so the angle you look at it is
how dark it is then we go to the back
and we got a piece of plexiglass across
here with some black vinyl and we inlaid
some green vinyl in it for the Nvidia
claw and backlit at all with LEDs and we
ran it right off that's RGB header on
the Asus board yeah so that's the LEDs
just go straight from the motherboard to
behind here that's right I know external
box or not yep so I noticed that there's
a cubby here can you actually put stuff
here yeah it wouldn't be a computer
without a keyboard and mouse so because
I'd put a keyboard and a mouse on the
other side there so that way you can
take it with you alright well we've seen
what's in the inside don't you tell us
about
the paint job on the outside so we
started with a black Pelican case and
started by Adams of primer and lots and
lots of layers then we added some custom
site details of triangles on the side
that you'll see on some csgo crates then
we went to town with distressing making
it look weathered making it look like an
actual crate well you did a great job
awesome man thanks well I gotta say I'm
pretty jealous of whoever gets to take
this home because this is an amazing
amazing mod it's all packed up and just
gonna take it and go anyways if you guys
want to see more from BSL one New York
there's gonna be a couple of videos from
Indian down below so check them out
thanks again John's coming bye thanks
for having me it's a pleasure yeah we'll
see you next time on G 4th garage
Title: NextLimit Maxwell Accelerates Design Simulation with GPU Rendering
Publish_date: 2018-09-10
Length: 136
Views: 29977
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/cX7Mcdh6uFk/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: cX7Mcdh6uFk

--- Transcript ---

[Music]
we are very interested in simulating
reality we start the simulating fluids
study single given physics we wanted to
simulate light light is the one which is
the most important thing Maxwell render
is a simulation software which is known
as light transport simulation problem
for 3d scenes and it is a sober that has
been around for more than 15 years or is
a quite advanced piece of technology at
least 80 percent of our user base can be
considered from the pearl design
industry as well as architecture
industry and there is we consider from
the film production industry since we
only had the CPU version of module we
wanted to make something faster and
optimized for users as well is a very
powerful algorithm needs a lot of power
and as well we found that videos
collaborative as the booster planning
before we decided that it was like a
master to go with Keira for NVIDIA GPUs
because Frazee wars are quite flexible
language then it was really well
supported by Nvidia
I remember the day that we've got the
first GP 100 at the office having the GP
100 and ambling working together we test
our new engine and benchmark we code
were completely amazing even without
doing any kind of atomization for this
specific piece of hardware we can add a
very complex code on multiple GPUs since
our every year we have new hard work and
a positive result so that's a real
exciting situation where every year
which is encouragement I think that
anyone that can take a look at the
evolution of the last four or five years
for NVIDIA GPUs can clearly see that
evolution is quite amazing we are going
to continue looking to the new
technology because our GPU atom is going
to be like probably the most important
feature for much more under team
you
Title: NVIDIA Quadro vDWS Demo on Solidworks
Publish_date: 2020-04-02
Length: 94
Views: 21269
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/CXVUnB-BTdg/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: CXVUnB-BTdg

--- Transcript ---

[Music]
we're executing SolidWorks within a
virtual machine or VM NVIDIA Quadro vws
software opens new functionalities in
SolidWorks that showcase the benefit of
mechanical visualization with AI
denoising the Nvidia t4 tensor quart GPU
delivers excellent performance for
graphics and compute you are ready to
run your design and engineering software
from a VM in this example we launched
SolidWorks visualized Pro using a 16 Q
profile with Quadro vws as you rotate
the model notice that it resolves much
faster due to GPU acceleration now we'll
turn the AI dee' noiser on the image
resolves much more quickly enabling the
user to work more efficiently
let's turn the AIT noise er off again
as we drag and drop new textures or
paints onto the car the image quality is
poor when rotating the car when you turn
the AI Dino's are back on you can rotate
and move the car smoothly with much
higher quality
[Music]
[Music]
Title: Postmates Serve - World's First Socially-Aware Delivery Robot
Publish_date: 2019-10-23
Length: 86
Views: 31781
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/d7eRafWqqRg/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: d7eRafWqqRg

--- Transcript ---

post beads is a on-demand delivery
platform and we connect the best of
cities with customers whether it's a
meal or something you really need when
you need it when you want it so many of
our deliveries happen in dense urban
areas and really short distances and
we've got bikes walking in cars but at
the end of the day we realized it was
somewhat silly to have a two-ton car
bring you a two pound burrito sir our
newest post mate is an autonomous robot
socially aware in tune with sidewalks
and people around it serve harnesses the
power of the egx platform to solve
complex problems like what is the best
route to get to a merchants location how
to avoid an obstacle as well as how to
interpret and react to people around
serve and ultimately interacting with
merchants and the cost
the great thing about the edx platform
is it enables us to scale and deploy a
fleet not just empower one robot but
many robots out on delivery
it's mission-critical for the robot to
be autonomous to have real-time
decision-making egx makes it possible
for us to compute best routes react to
incidents as they happen and be on time
on delivery
Title: GeForce ShadowPlay - Share Every Win
Publish_date: 2014-04-07
Length: 95
Views: 47928
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/D9I3KbarTJQ/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: D9I3KbarTJQ

--- Transcript ---

[Music]
you play in your favorite game and you
just had one of those great gaming
moments and want to share it with your
friends well now you can with shadow
play shadow play is the easiest way to
capture game footage or broadcast your
game on Twitch it runs in the background
so you never miss those moments and
because it's hardware-accelerated
game performance stays great to enable
shadow play open GeForce experience and
click on the shadow play button flip on
the switch and you're ready to go when
shadow play is running you'll see this
icon displayed in the corner of the
screen shadow play is capable of
capturing everything from your Windows
desktop all the way to a full screen
game in shadow mode when something cool
happens in the game capture it by
hitting alt f10 shadow play will capture
up to the last twenty minutes of your
gameplay in addition to shadow mode you
can use manual mode to start and stop
recording at any length use alt f9 to
start and stop the recording all your
videos will be saved in a high-quality
h.264 format shadow play also makes it
super easy to broadcast your game on
Twitch select twitch in the drop down
menu to log into your account once
in-game press alt f8 to start and stop
broadcasting
once you enable shadowplay you'll never
miss your greatest gaming moments
you
Title: Gameworks and a New Generation of Blockbusters at Gamescom 2015
Publish_date: 2015-08-06
Length: 346
Views: 32874
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/DA_k2NA80vM/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: DA_k2NA80vM

--- Transcript ---

hello and welcome sir gamescom 2015 I'm
Dan ma reporting for Nvidia from the
heart of the coal Messer behind me is
the NVIDIA green room where we'll be
chatting to the talent behind some of
the year's most anticipated games and
hardware
over the next four days we'll bring you
highlights from the show including the
latest on the likes of Paul of duty
black ops 3 Mirror's Edge catalyst and
Rainbow six siege and an in-depth look
at bleeding edge virtual reality
technology first up though we spoke to
Nvidia zone mattress in eeeh to find out
how Nvidia game works is benefiting a
new generation of blockbusters and video
game works is a set of technologies that
we're working on and an entire platform
to make the real-time rendering and
simulation of video games today a lot
more visceral a lot more beautiful and a
lot more realistic generally we have
discussions with developers very early
in the production stage of a game and we
try and understand what they're trying
to achieve and we have a lot of research
and development that goes on internally
and sometimes those things just match up
perfectly so there might be they won't
want to do a really nice technology
around Roy wave rendering and all of a
sudden we have that or we have it in rnd
so it's it's always what fits the game
best and generally when two people get
excited about it like we know we've got
a winner so in the witcher 3 we had a
few pieces of technology probably the
most prominent one was hair works and
with hair works you can see that we have
hair for Geralt the main character and
lots of the monsters in the game and fur
for animals we also use HB ao+
which is a nice technology for ambient
occlusion so it makes the geometry a
little deeper and gives us a lot more
definition with the shadows and then the
underlying physics engine also was
physics which means that the developer
has access to our clothing and
destruction technology specifically with
hair works and The Witcher 3 the
conversation started very early in the
life of the technology and because we
worked very closely with the artists and
the creative director of the game they
had a lot of requirements that we didn't
have in the technology because not every
game has Griffin's or bears or wolves or
Geralt so what happens is all of the
work that we do we do it in a general
sense so we want to add clumpiness to
the hair or we want to make the hair a
lot stiffer in certain areas different
from the root to the tip that goes into
the general game works library and then
every other developer that gets it gets
all of those features as a result
right now one of the hot technologies
that we've got working on is flex which
is a unified physics solver now what
that means is that generally speaking in
the world you might have a big box and
you can shoot that box but that box then
won't float or the pieces of the box
won't float later flex means that the
whole world simulates in one big
simulation so I can have a balloon
filled with milk and inside that milk is
a rubber bunny and there's a cereal bowl
and the rubber bunny falls into the
cereal bowl the milk splashes everywhere
and it all effects itself another big
one is VX GI which is our real-time
global illumination what that means is
that normally lighting is very difficult
to calculate in real time and because of
the advances that we have in the
processing power of the GPU were able to
start to calculate how light bounces so
if I have a red ball in a white wall and
I shine a flashlight on the red ball
I should see some red light on that wall
and all of that adds together generally
developers do this they bake it so to
speak and they do it before the game
launches we're gonna enable them to do
it in real time giving these really
dynamic environments I can't speak too
much about all the specific titles we
have but we do integrate all of our
technologies into modern engines so
Unreal Engine 4 has an integration with
all of our game works technology and
that's really available to developers to
download and play with now one of the
many publishers who work closely with
Nvidia over the years is you just saw
two as ever is here against Tom in a big
way
the latest he regains a benefit in this
continue
in partnership is Tom Clancy's Rainbow
six siege which gamers are getting
hands-on spy with right here at Nvidia's
booth because he was soft Scott Mitchell
with the latest on this tactical
spectacular I think Rainbow six siege is
going to be set apart from the other
shooters and there's all because it's
definitely very tight intense everything
is close quarters combat most shooters
happen in large outdoor arenas where
we're exclusively indoors we also have
the procedural destruction which means
that the maps are forever changing it's
very dynamic the map that you start with
will never be the same map that you
finish with as well we've got the
concept of one life which means that
there's no regen there's no respawn so
once you're out you're depending on
which operators you choose to play with
or who you're playing against it means
that you're working with or against
different gadgets that are going to
affect the environment a little bit
differently again everything blows up
anything that's not part of the
supporting structure inside these
buildings in people in a way with
precision we've really been able to
achieve this level of realism thanks to
all the new hardware it's really giving
us an advantage as too close to what we
had before specifically with the
destruction and the super intelligent AI
we need that kind of power
and that's all we've got from the show
floor today but we've got low as more
coming to you over the rest of the week
in the meantime be sure to visit G for
stop codon UK for even more Gamescom
coverage until then we'll see you
tomorrow
Title: NVIDIA SHIELD April Updates
Publish_date: 2014-04-07
Length: 97
Views: 69211
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/DBe5tAr8NJQ/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: DBe5tAr8NJQ

--- Transcript ---

hi i'm brian with nvidia and we have a
number of exciting new software updates
that will make your nvidia shield gaming
experiences better than ever before
first our exclusive game stream
technology now lets you play over 100 of
your favorite pc games you can already
stream from your geforce gtx powered pc
to your shield within your home network
and you can even hook it up to your big
screen tv for a console like experience
now you can stream your games outside of
your home network as long as you have a
fast wi-fi connection you'll be able to
stream your favorite pc titles to shield
on the go
and speaking of console mode we've just
expanded shield's gaming library to add
support for bluetooth-enabled mouse and
keyboard you can now enjoy games like
civilization 5 world of warcraft dota 2
and league of legends on your big screen
tv from the comfort of your couch
we've also enhanced gamepad mapper with
new features and user interface making
it easier to map your favorite
touch-based games to shields console
grade controller you can even share or
download thousands of community profiles
already available on the cloud integra
zone has a whole new user interface
allowing you to personalize your library
of games
faster search results for the best
shield games and stay up to date with
the latest shield news
finally shield comes with full support
for the android kitkat operating system
so you're fully equipped with the latest
and greatest from google now more than
ever shield provides a richer experience
with more game content all at a great
value
Title: GeForce Garage: Cross Desk Series, Video 2 – How to Sleeve Your Cables
Publish_date: 2014-08-26
Length: 566
Views: 330363
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/dcsP2S0l9Ps/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: dcsP2S0l9Ps

--- Transcript ---

hi I'm Andrew with Nvidia and you're
watching GeForce garage last episode we
improved airflow by cutting a custom fan
port and our red Harbinger cross desk
now we've added all the awesome hardware
and we're going to make it really stand
out with some custom braided cables
needs today in-house we have Mike land
and Berger from neutral customs and
they're an awesome resource for cables
leavings plots but today he's going to
show us how to take this ugly
monstrosity and turn it into a beautiful
braided custom cape hello everybody my
name is mike landon burger with Lutra
customs the modding movement has really
been going towards color coding systems
everything down to your components
Hideaway the cables the way they're
supposed to and show off just the part
that you want and that helps out with
heat flow there's two different
materials that are widely used
there's paracord and then there's PE tee
plastic paracord is a nylon
multifilament as great coverage comes in
a lot of colors but it's really kind of
hard to work with because you have to
stretch it so much now my favorite is PE
qi we have our own line we draw customs
teleo sleeping really dense weave it
makes it really easy to stretch tightly
which is a main component of cable
management that allows you to have a
perfect Bend and it'll hold its shape on
you there's a few different methods for
sleeping cables there's heat shrink
lifts and then there's heat shrink style
heat shrink lists is basically not
having any heat shrink on the end and
the wire goes directly into the
connector now heat shrink style uses
little pieces of heat shrink that go
into the connector and they actually
hold everything together now I'm not
going to be going over the heat shrink
style but you can check that out on my
own YouTube channel so I'm going to
start going through some of these tools
that we're going to be using for
sleeping the first up is I have our wire
stripper I really do trust the NIP
expand really straight really perfect
and right on strips the second one is
the LC crimping tool it's custom milled
it's made for perfect crimps third is a
Bic lighter
I like the Vic because it allows me to
control how I melt the heat shrink next
up
have wire cutters I use this for
sleeping and I use it for wire cutting
and last but not least is a ruler and
that's pretty much all that we have for
sleeping tools we won't be using all of
them today but I wanted to show you the
basics so I'm going to jump into the PEP
tutorial you're always going to start
out with either an extension or an OEM
cable it is most important you want to
make your own pin out diagram that way
you are a hundred percent certain that
your cables are right the first step
that I'll grab my 16 gauge wire and I'll
use a ruler you're going to want to make
sure that it is absolutely straight and
right on we want to go ahead and strip
off the ends of these cables you're
going to want them to at least a two
millimeter now that you have the cable
strip both of the sides I do things a
little bit different this is the LC
crimping tool I will pre crimp it on
your female ATX pin here you have a big
set of wings what you're going to want
to do is set that on the inside you'll
see that there's a little shelf here now
we want to get those wings perfectly
centered and right in the Shelf you'll
hear one click two clicks three clicks
this is what we want it to look like so
the next step in this process is I will
be taking my crimper opening up and I'll
hold it upside down putting it in I'm
using that same shelf on the other side
and putting the big wings in the same
spot and then I'm simply closing it all
the way if you look at this you'll
realize that it bit perfectly into the
wire when you go to sleeve you don't
want your cable to twist so what I do is
there is wings that are facing up I will
put my thumb facing up and I will drag
it all the way across the wire so I know
that that's my reference point and then
I'll take the crimp I'll do the same
thing now we have a wire that's crimped
absolutely perfectly so the next thing
that we want to do is well actually take
our sleeving now
and we'll measure it out on the sleeve
you'll see again where the crimps are I
will match this right or the second set
of the wings meet and then I'll hold it
without stretching the sleeving if you
stretch this leading when you go to put
it on what it'll do is it'll be too
short where I measured this as I
measured it right up to the bottom of
the wings now with PT some people will
melt the end basically you're taking a
bit lighter and you are barely just
barely hitting the top and then you're
feather it down and what I did is I
sealed the end here and that makes
sleeving way way easier if you do that
too much it'll bulge up and it'll make
it really hard to for you to sleep with
and this parts one of the easiest you're
simply putting it on sometimes you have
to inch it but if you have everything
straight it should go right on one of
the important things with heat shrink
lists is that I want to make sure that
where the actual back end of the crimps
the stress relief and these front little
prongs I want to make this right in the
middle of it because if I went too far
up it won't click into the socket if I
go too far down then you're actually
going to be able to see some of it this
is heat shrink that I cut it's 1/4 inch
heat shrink
three-two-one shrink ratio and it has a
thin wall one of the key things about
this is that it handles heat very well
so it's able to soak up the heat without
making a big gummy mess on you I'm going
to be taking a piece of heat shrink and
then put the end of the heat shrink all
the way up to the end of the sleeving
I'll take my lighter
I try to use the blue part the best I
can and then I will shrink it up just a
little bit until it's just a little bit
smokey and I'll take my fingers and then
immediately I will take my flush cutters
put a slit in it and cut it off what it
did is it actually pinched down and it
forced it into a cone this will never
come off of that end now I'm going to do
the same thing to the other side this is
our sleeve cable you'll notice that it's
tight it's straight and it's exactly the
way we wanted it what I'm going to do is
I'm going to take our finished connector
again you have to make a pin out just so
you don't blow up your system I can't
stress that enough so I'm going to pull
it through the connector the prongs on
this side the ones that face up they
have to be facing in the direction of
the top side of the connector that has
the clasp it doesn't work any other way
now when I put this in you will actually
hear a little bit of a click that little
click means that your two prongs on the
side actually are gripping on to where
they're supposed to be and that's what's
going to make a tight connection what I
always do is I pull on it a little bit
and then I know it's in the right spot
now I can also look on the opposite side
and I will see that all of the pins on
the inside where they're actually going
to terminate our actually level and
that's another really good way to see if
they're going to work right I'm just
going to pull this cable through now
again
prongs need be forward facing towards
the clip people ask me a lot of times
Lutrell how do you make this look good I
will hold it up in a vertical direction
like this and I will make little turns
like this but just by doing this it
helps out a lot we have these awesome
cable combs here we're just going to put
them in good places and that is how we
do a PT heat-shrink with style
in my opinion probably the most
permanent style out there
so here's a completed 8 pin that we did
for the desk it's completely themed out
to match everything awesome that looks
great well let's go ahead and use the
seam and wire up the whole desk
absolutely wow this looks super clean I
really like your cable style man I
really appreciate that I really love how
custom cables sleeving really ties the
team together even more with all the
rest of the components looks amazing so
thanks so much Mike for coming in
absolutely thanks for having me of
course don't forget to check out our
next episode we're going to show you how
to lay some copper pipe to water cool
your GeForce GTX PC thanks for watching
GeForce garage the ultimate resource
center for designing building and
customizing your GTX PC
you
you
you
Title: Assassin's Creed: Unity gameplay, overview and interview at E3 2014
Publish_date: 2014-06-12
Length: 127
Views: 12831
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/DdxNHA1lU4A/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: DdxNHA1lU4A

--- Transcript ---

this year we had the chance to check out
Assassin's Creed unity the latest
edition in the franchise and my god fit
look gorgeous not only have they rebuilt
the engine from the ground up but they
brought an all new feature four player
co-op
so it's a backdrop of the French
Revolution it's not the French
Revolution is the story of our know an
hour no is on a Redemption quest you see
a charismatic character take shape and
grow and basically become an assassin hi
I'm James ma I'm technical director on a
CEO unity so basically I'm responsible
for the technology for the project we
refocus on the gameplay pillars the core
gameplay to dillards navigation files
and stealth and we repackage it in a
really next-gen deal there's a whole set
of missions where you can play with up
to four friends so it's the same
narrative missions and you can have all
the same navigations stealth and faction
and everything so everything is there it
wasn't me I'm not the man you huh where
is he we had great graphics before but
we wanted to make sure that we had
richness everywhere and so we basically
redid the whole rendering engine so
we're now physically based we have like
much richer material totally different
lighting engine you can see the quality
of the image is just nothing like you've
seen before and a nice side benefit is
we get interiors also so seamless
interiors and exteriors landmarks can go
inside it has a lot of layer and depth
to the city and we also reworked
everything that we did on characters so
everything from skin shading hair
animation performance capture I mean we
rethought everything to make sure that
we could deliver as much quality and as
much quantity also as we could
you
Title: NVIDIA SHIELD Showcase - How-to Set Up Your SHIELD (Part 1)
Publish_date: 2013-07-30
Length: 195
Views: 43128
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/DHiDDtrdQ0Y/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: DHiDDtrdQ0Y

--- Transcript ---

[Music]
hey guys i'm will and this is your
shield setup video this is actually the
first of four videos where we're going
to show you how to set up and use your
shield all right so i got my shield here
and it's official shield carrying case
it's actually a rugged reinforced nylon
case that comes with a wrist strap
an integrated charging access port on
the back and of course the zipper that
goes all the way around
like so
open it up boom we have our shield so
let's take a look at the hardware here
we have our five inch hd touch screen of
course all the shield controls that
includes a d-pad two joysticks which
actually have buttons under them so
they're clickable
the navigation buttons with the center
mounted shield multi-function button the
abxy buttons here the base reflex stereo
speakers right here
and on the back we have our bumpers our
triggers
and ports
so you can actually swap out your tags
like so i happen to have a custom tag
here it's the carbon fiber version i'm
going to swap them on they're magnetized
so they swap on real easy like so
so let's take a look at how to set up
your shield i mentioned before you have
your micro usb port back here that's
your charging port so you're going to
want to
press this shield multi function button
it's going to light up to let you know
it's powered on and let's wait for it to
boot up here we go now you're at your
shield startup screen you're just going
to want to flick through that or you can
press a to continue through the setup
screen when you're at the end you're
going to want to press y to let us know
that you agree and when you do that
you're going to get on the android
startup screen this is where we add our
google account information that's the
same login information that you use for
gmail so let's get started
all right so we're on the setup screen
here we're just going to hit the start
button
and we're going to find
an internet connection also known as
your wi-fi network we're just going to
enter the password here
like so
and click connect and wait for it to
connect to your wi-fi network
all right so now that we're connected
we're going to enter our google account
information again this is the same
information that you would use to log
into gmail so let's enter that now i'm
going to add my username
then i'm going to enter my password
we're going to click done
all right now take a look at the
checkboxes make sure you got what you
need click next again
make sure you have your location
settings the way you want them and hit
the arrow
and in just a second we'll be done and
finished now you're at your home screen
just click ok
and you're ready to go
this is how you access your apps
you can long press them to put them on
the home screen and when you want to go
back home you press the home button like
so so that's pretty much all you need to
know to get up and running with shield
if you want to revisit the setup process
you can restart this video or find the
shield help app on your device give it a
click and you're going to want to hit
the y button for the user's guide that
does it for this tutorial video in the
next video we're going to talk about how
to navigate the android user interface
and the tigerzone app using shield
controls you can find that video by
clicking right here
[Music]
you
Title: GTC 2023 Keynote with NVIDIA CEO Jensen Huang
Publish_date: 2023-03-21
Length: 4671
Views: 22461320
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/DiGB5uAYKAg/hq720.jpg?v=6419e695
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: DiGB5uAYKAg

--- Transcript ---

For nearly four decades
Moore’s Law has been the governing dynamics
of the computer industry
which in turn has impacted every industry.
The exponential performance increase
at constant cost and power has slowed.
Yet, computing advance has gone to lightspeed.
The warp drive engine is accelerated computing
and the energy source is AI.
The arrival of accelerated computing and AI is timely
as industries tackle powerful dynamics
sustainability
generative AI
and digitalization.
Without Moore’s Law, as computing surges,
data center power is skyrocketing
and companies struggle to achieve Net Zero.
The impressive capabilities of Generative AI
created a sense of urgency for companies to reimagine their products and business models.
Industrial companies are racing to digitalize and reinvent into software-driven tech companies
to be the disruptor
and not the disrupted.
Today, we will discuss how accelerated computing and AI are powerful tools for tackling these challenges
and engaging the enormous opportunities ahead.
We will share new advances in NVIDIA’s full-stack, datacenter-scale, accelerated computing platform.
We will reveal new chips and systems,
acceleration libraries, cloud and AI services
and partnerships that open new markets.
Welcome to GTC!
GTC is our conference for developers.
The global NVIDIA ecosystem spans 4 million developers,
40,000 companies
and 14,000 startups.
Thank you to our Diamond sponsors for supporting us
and making GTC 2023 a huge success.
We’re so excited to welcome more than 250,000
of you to our conference.
GTC has grown incredibly.
Only four years ago, our in-person GTC
conference had 8,000 attendees.
At GTC 2023, we’ll learn from leaders
like Demis Hassabis of DeepMind
Valeri Taylor of Argonne Labs
Scott Belsky of Adobe
Paul Debevec of Netflix
Thomas Schulthess of ETH Zurich
and a special fireside chat I’m having with Ilya Sutskever
co-founder of OpenAI, the creator of ChatGPT.
We have 650 amazing talks from the brightest minds in academia and the world’s largest industries:
There are more than 70 talks on Generative AI alone.
Other great talks, like pre-trained multi-task models for robotics…
sessions on synthetic data generation, an important method for advancing AI
including one on using Isaac Sim to generate physically
based lidar point clouds
a bunch of talks on digital twins, from using AI
to populate virtual factories of the future
to restoring lost Roman mosaics of the past
cool talks on computational instruments, including a giant optical telescope and a photon-counting CT
materials science for carbon capture and solar cells,
to climate science, including our work on Earth-2
important works by NVIDIA Research
 on trustworthy AI and AV safety
From computational lithography for micro-chips,
to make the smallest machines
to AI at the Large Hadron Collider to explain the universe.
The world’s most important companies are here
from auto and transportation
healthcare, manufacturing, financial services,
retail, apparel, media and entertainment, telco
and of course, the world’s leading AI companies.
The purpose of GTC is to inspire the world on the art-of-the-possible of accelerating computing
and to celebrate the achievements of the scientists
and researchers that use it.
I am a translator.
Transforming text into creative discovery,
movement into animation,
and direction into action.
I am a healer.
Exploring the building blocks that make us unique
modeling new threats before they happen
and searching for the cures to keep them at bay.
I am a visionary.
Generating new medical miracles
and giving us a new perspective on our sun
to keep us safe here on earth.
I am a navigator.
Discovering a unique moment in a sea of content
we’re announcing the next generation
and the perfect setting for any story.
I am a creator.
Building 3D experiences from snapshots
and adding new levels of reality to our virtual selves.
I am a helper.
Bringing brainstorms to life
sharing the wisdom of a million programmers
and turning ideas into virtual worlds.
Build northern forest.
I even helped write this script
breathed life into the words
and composed the melody.
I am AI.
Brought to life by NVIDIA, deep learning,
and brilliant minds everywhere.
NVIDIA invented accelerated computing to solve problems
that normal computers can’t.
Accelerated computing is not easy
it requires full-stack invention from chips, systems, networking,
acceleration libraries, to refactoring the applications.
Each optimized stack accelerates an application domain
from graphics, imaging, particle or fluid dynamics
quantum physics, to data processing and machine learning.
Once accelerated, the application can enjoy incredible speed-up, as well as scale-up across many computers.
The combination of speed-up and scale-up
has enabled us to achieve a million-X
for many applications over the past decade
helping solve problems previously impossible.
Though there are many examples,
the most famous is deep learning.
In 2012, Alex Kerchevsky, Ilya Suskever, and Geoff Hinton
needed an insanely fast computer
to train the AlexNet computer vision model.
The researchers trained AlexNet with
14 million images on GeForce GTX 580
 processing 262 quadrillion floating-point operations,
and the trained model won the ImageNet challenge by a wide margin, and ignited the Big Bang of AI.
A decade later, the transformer model was invented.
And Ilya, now at OpenAI, trained the GPT-3
large language model to predict the next word.
323 sextillion floating-point operations were required to train GPT-3.
One million times more floating-point operations
than to train AlexNet.
The result this time – ChatGPT, the AI heard around the world.
A new computing platform has been invented.
The iPhone moment of AI has started.
Accelerated computing and AI have arrived.
Acceleration libraries are at the core of accelerated computing.
These libraries connect to applications which connect to the world’s industries, forming a network of networks.
Three decades in the making, several thousand applications
are now NVIDIA accelerated
with libraries in almost every domain of science and industry.
All NVIDIA GPUs are CUDA-compatible, providing a large install base and significant reach for developers.
A wealth of accelerated applications attract end users, which creates a large market for cloud service providers
and computer makers to serve.
A large market affords billions in R&D to fuel its growth.
NVIDIA has established the accelerated computing virtuous cycle.
Of the 300 acceleration libraries and 400 AI models
that span ray tracing and neural rendering
physical, earth, and life sciences, quantum physics
and chemistry, computer vision
data processing, machine learning and AI, we updated 100
we updated 100 this year that increase performance
and features for our entire installed base.
Let me highlight some acceleration libraries that solve new challenges and open new markets.
The auto and aerospace industries use CFD for turbulence
and aerodynamics simulation.
The electronics industry uses CFD for thermal management design.
This is Cadence’s slide of their new CFD solver
accelerated by CUDA.
At equivalent system cost, NVIDIA A100 is 9X
the throughput of CPU servers.
Or at equivalent simulation throughput, NVIDIA is 9X lower cost
or 17X less energy consumed.
Ansys, Siemens, Cadence, and other leading CFD solvers
are now CUDA-accelerated.
Worldwide, industrial CAE uses nearly
100 billion CPU core hours yearly.
Acceleration is the best way to reclaim power and achieve sustainability and Net Zero.
NVIDIA is partnering with the global quantum
computing research community.
The NVIDIA Quantum platform consists of libraries and systems for researchers to advance quantum programming models,
system architectures, and algorithms.
cuQuantum is an acceleration library
for quantum circuit simulations.
IBM Qiskit, Google Cirq, Baidu Quantum Leaf, QMWare, QuEra, Xanadu Pennylane, Agnostiq, and AWS Bracket
have integrated cuQuantum into their simulation frameworks.
Open Quantum CUDA is our hybrid GPU-Quantum
programming model.
IonQ, ORCA Computing, Atom, QuEra, Oxford Quantum Circuits, IQM, Pasqal, Quantum Brilliance, Quantinuum, Rigetti,
Xanadu, and Anyon have integrated Open Quantum CUDA.
Error correction on a large number of qubits is necessary to recover data from quantum noise and decoherence.
Today, we are announcing a quantum control link, developed in partnership with Quantum Machines
that connects NVIDIA GPUs to a quantum computer to
do error correction at extremely high speeds.
Though commercial quantum computers are still a decade or two away, we are delighted to support this large and vibrant
research community with NVIDIA Quantum.
Enterprises worldwide use Apache Spark to
process data lakes and warehouses
SQL queries, graph analytics, and recommender systems.
Spark-RAPIDS is NVIDIA’s accelerated Apache Spark
data processing engine.
Data processing is the leading workload
of the world’s $500B cloud computing spend.
Spark-RAPIDS now accelerates major cloud data processing platforms, including GCP Dataproc
Amazon EMR, Databricks, and Cloudera.
Recommender systems use vector databases to store, index, search, and retrieve massive datasets of unstructured data.
A new important use-case of vector databases is large language models to retrieve domain-specific or proprietary facts
that can be queried during text generation.
We are introducing a new library, RAFT,
to accelerate indexing, loading the data
and retrieving a batch of neighbors for a single query.
We are bringing the acceleration of RAFT to Meta’s open-source FAISS AI Similarity Search, Milvus open-source vector DB
used by over 1,000 organizations,
and Redis with over 4B docker pulls.
Vector databases will be essential for organizations building proprietary large language models.
Twenty-two years ago, operations research scientists Li and Lim posted a series of challenging pickup and delivery problems.
PDP shows up in manufacturing, transportation,
retail and logistics, and even disaster relief.
PDP is a generalization of the Traveling Salesperson Problem
and is NP-hard
meaning there is no efficient algorithm to find an exact solution.
The solution time grows factorially as the problem size increases.
Using an evolution algorithm and accelerated computing
to analyze 30 billion moves per second
 NVIDIA cuOpt has broken the world record and discovered
the best solution for Li&Lim’s challenge.
AT&T routinely dispatches 30,000 technicians to service 13 million customers across 700 geographic zones.
Today, running on CPUs, AT&T’s dispatch
optimization takes overnight.
AT&T wants to find a dispatch solution in real time that continuously optimizes for urgent customer needs
and overall customer satisfaction, while adjusting
for delays and new incidents that arise.
With cuOpt, AT&T can find a solution 100X faster
and update their dispatch in real time.
AT&T has adopted a full suite of NVIDIA AI libraries.
In addition to Spark-RAPIDS and cuOPT, they’re using Riva for conversational AI and Omniverse for digital avatars.
AT&T is tapping into NVIDIA accelerated computing and AI
for sustainability, cost savings, and new services.
cuOpt can also optimize logistic services. 400 billion parcels
are delivered to 377 billion stops each year.
Deloitte, Capgemini, Softserve, Accenture, and Quantiphi are using NVIDIA cuOpt to help customers optimize operations.
NVIDIA’s inference platform consists of three software SDKs.
NVIDIA TensorRT is our inference runtime
that optimizes for the target GPU.
NVIDIA Triton is a multi-framework data center inference serving software supporting GPUs and CPUs.
Microsoft Office and Teams, Amazon, American Express,
and the U.S. Postal Service
are among the 40,000 customers using TensorRT and Triton.
Uber uses Triton to serve hundreds of thousands
of ETA predictions per second.
With over 60 million daily users, Roblox uses Triton to serve models for game recommendations
build avatars, and moderate content and marketplace ads.
We are releasing some great new features – model analyzer support for model ensembles, multiple concurrent model serving,
and multi-GPU, multi-node inference
for GPT-3 large language models.
NVIDIA Triton Management Service is our new software that automates the scaling and orchestration
 of Triton inference instances across a data center.
Triton Management Service will help you improve the throughput and cost efficiency of deploying your models.
50-80% of cloud video pipelines are processed on CPUs
consuming power and cost and adding latency.
CV-CUDA for computer vision, and VPF for video processing, are new cloud-scale acceleration libraries.
CV-CUDA includes 30 computer vision operators for detection, segmentation, and classification.
VPF is a python video encode and decode acceleration library.
Tencent uses CV-CUDA and VPF
to process 300,000 videos per day.
Microsoft uses CV-CUDA and VPF to process visual search.
Runway is a super cool company that uses
CV-CUDA and VPF to process video
for their cloud Generative AI video editing service.
Already, 80% of internet traffic is video.
User-generated video content is driving significant growth and consuming massive amounts of power.
We should accelerate all video processing and reclaim the power.
CV-CUDA and VPF are in early access.
NVIDIA accelerated computing helped
achieve a genomics milestone
now doctors can draw blood and sequence
a patient’s DNA in the same visit.
In another milestone, NVIDIA-powered instruments reduced the cost of whole genome sequencing to just $100.
Genomics is a critical tool in synthetic biology
with applications ranging from drug discovery
and agriculture to energy production.
NVIDIA Parabricks is a suite of AI-accelerated libraries for end-to-end genomics analysis in the cloud or in-instrument.
NVIDIA Parabricks is available in every public cloud and genomics platforms like Terra, DNAnexus, and FormBio.
Today, we’re announcing Parabricks 4.1 and will run on NVIDIA-accelerated genomics instruments
from PacBio, Oxford Nanopore, Ultima,
Singular, BioNano, and Nanostring.
The world’s $250B medical instruments
market is being transformed.
Medical instruments will be software-defined and AI powered.
NVIDIA Holoscan is a software library
for real-time sensor processing systems.
Over 75 companies are developing
medical instruments on Holoscan.
Today, we are announcing Medtronic, the world leader in medical instruments, and NVIDIA are building their AI platform
for software-defined medical devices.
This partnership will create a common platform for Medtronic systems, ranging from surgical navigation
to robotic-assisted surgery.
Today, Medtronic announced that its next-generation GI Genius system, with AI for early detection of colon cancer
is built on NVIDIA Holoscan and
will ship around the end of this year.
The chip industry is the foundation of nearly every industry.
Chip manufacturing demands extreme precision, producing features 1,000 times smaller than a bacterium
and on the order of a single gold atom or a strand of human DNA.
Lithography, the process of creating patterns on a wafer, is the beginning of the chip manufacturing process
and consists of two stages – photomask making
and pattern projection.
It is fundamentally an imaging problem at the limits of physics.
The photomask is like a stencil of a chip. Light is blocked or passed through the mask
to the wafer to create the pattern.
The light is produced by the ASML EUV
extreme ultraviolet lithography system.
Each system is more than a quarter-of-a-billion dollars.
ASML EUV uses a radical way to create light.
Laser pulses firing 50,000 times a second at a drop of tin, vaporizing it, creating a plasma that emits 13.5nm EUV light
nearly X-ray.
Multilayer mirrors guide the light to the mask.
The multilayer reflectors in the mask reticle take advantage of interference patterns of the 13.5nm light
to create finer features down to 3nm.
Magic.
The wafer is positioned within a quarter of a nanometer and aligned 20,000 times a second to adjust for any vibration.
The step before lithography is equally miraculous.
Computational lithography applies inverse physics algorithms
to predict the patterns on the mask
 that will produce the final patterns on the wafer.
In fact, the patterns on the mask
do not resemble the final features at all.
Computational lithography simulates Maxwell’s equations
of the behavior of light passing through optics
and interacting with photoresists.
Computational lithography is the largest computation
workload in chip design and manufacturing
consuming tens of billions of CPU hours annually.
Massive data centers run 24/7 to create reticles
used in lithography systems.
These data centers are part of the nearly $200 billion annual CAPEX invested by chip manufacturers.
Computational lithography is growing fast
as algorithm complexity increases
enabling the industry to go to 2nm and beyond.
NVIDIA today is announcing cuLitho, a library
for computational lithography.
cuLitho, a massive body of work that has taken nearly four years, and with close collaborations with TSMC,
ASML, and Synopsys, accelerates computational
lithography by over 40X.
There are 89 reticles for the NVIDIA H100.
Running on CPUs, a single reticle currently
takes two weeks to process.
cuLitho, running on GPUs, can process
a reticle in a single 8-hour shift.
TSMC can reduce their 40,000 CPU servers used for computational lithography by accelerating with cuLitho
on just 500 DGX H100 systems, reducing power
from 35MW to just 5MW.
With cuLitho, TSMC can reduce prototype cycle time,
increase throughput
and reduce the carbon footprint of their manufacturing,
and prepare for 2nm and beyond.
TSMC will be qualifying cuLitho for production starting in June.
Every industry needs to accelerate every workload, so that we can reclaim power and do more with less.
Over the past ten years, cloud computing has grown 20% annually into a massive $1T industry.
Some 30 million CPU servers do the majority of the processing.
There are challenges on the horizon.
As Moore’s Law ends, increasing CPU performance comes with increased power.
And the mandate to decrease carbon emissions is fundamentally at odds with the need to increase data centers.
Cloud computing growth is power-limited.
First and foremost, data centers must accelerate every workload.
Acceleration will reclaim power.
The energy saved can fuel new growth.
Whatever is not accelerated will be processed on CPUs.
The CPU design point for accelerated cloud datacenters
differs fundamentally from the past.
In AI and cloud services, accelerated computing offloads parallelizable workloads, and CPUs process other workloads,
like web RPC and database queries.
We designed the Grace CPU for an AI and cloud-first world,
where AI workloads are GPU-accelerated
and Grace excels at single-threaded execution
and memory processing.
It’s not just about the CPU chip. Datacenter operators optimize for throughput and total cost of ownership of the entire datacenter.
We designed Grace for high energy-efficiency
at cloud datacenter scale.
Grace comprises 72 Arm cores connected by a super high-speed on-chip scalable coherent fabric that delivers 3.2 TB/sec
of cross-sectional bandwidth.
Grace Superchip connects 144 cores between two CPU dies over a 900 GB/sec low-power chip-to-chip coherent interface.
The memory system is LPDDR low-power memory, like used in cellphones, that we specially enhanced for use in datacenters.
It delivers 1 TB/s, 2.5x the bandwidth of today’s systems
at 1/8th the power.
The entire 144-core Grace Superchip module
with 1TB of memory is only 5x8 inches.
It is so low power it can be air cooled.
This is the computing module with passive cooling.
Two Grace Superchip computers can fit
in a single 1U air-cooled server.
Grace’s performance and power efficiency are excellent for cloud and scientific computing applications.
We tested Grace on a popular Google benchmark, which tests how quickly cloud microservices communicate
and the Hi-Bench suite that tests Apache Spark
memory-intensive data processing.
These kinds of workloads are foundational for cloud datacenters.
At microservices, Grace is 1.3X faster than the average
of the newest generation x86 CPUs
and 1.2X faster at data processing
And that higher performance is achieved using only 60% of the power measured at the full server node.
CSPs can outfit a power-limited data center with 1.7X more Grace servers, each delivering 25% higher throughput.
At iso-power, Grace gives CSPs 2X the growth opportunity.
Grace is sampling.
And Asus, Atos, Gigabyte, HPE, QCT,
Supermicro, Wistron, and ZT are building systems now.
In a modern software-defined data center, the operating system doing virtualization, network, storage, and security can
consume nearly half of the datacenter’s CPU cores
and associated power.
Datacenters must accelerate every workload to reclaim power and free CPUs for revenue-generating workloads.
NVIDIA BlueField offloads and accelerates the datacenter operating system and infrastructure software.
Over two dozen ecosystem partners, including Check Point,
Cisco, DDN, Dell EMC
Juniper, Palo Alto Networks, Red Hat, and VMWare,
 use BlueField’s datacenter acceleration technology to run their software platforms more efficiently.
BlueField-3 is in production and adopted by leading cloud service providers, Baidu, CoreWeave, JD.com, Microsoft Azure,
Oracle OCI, and Tencent Games, to accelerate their clouds.
NVIDIA accelerated computing starts with DGX
the world’s AI supercomputer
the engine behind the large language model breakthrough.
I hand-delivered the world’s first DGX to OpenAI.
Since then, half of the Fortune 100 companies
have installed DGX AI supercomputers.
DGX has become the essential instrument of AI.
The GPU of DGX is eight H100 modules.
H100 has a Transformer Engine designed to process models
like the amazing ChatGPT,
which stands for Generative Pre-trained Transformers.
The eight H100 modules are NVLINK’d to each other across NVLINK switches to allow fully non-blocking transactions.
The eight H100s work as one giant GPU.
The computing fabric is one of the most vital systems
of the AI supercomputer.
400 Gbps ultra-low latency NVIDIA Quantum InfiniBand
with in-network processing
connects hundreds and thousands of DGX nodes
into an AI supercomputer.
NVIDIA DGX H100 is the blueprint for
customers building AI infrastructure worldwide.
It is now in full production.
I am thrilled that Microsoft announced Azure is opening private previews to their H100 AI supercomputer.
Other systems and cloud services will soon come from Atos, AWS, Cirrascale, CoreWeave, Dell, Gigabyte, Google, HPE,
Lambda Labs, Lenovo, Oracle, Quanta, and SuperMicro.
The market for DGX AI supercomputers has grown significantly.
Originally used as an AI research instrument, DGX AI supercomputers are expanding into operation
running 24/7 to refine data and process AI.
DGX supercomputers are modern AI factories.
We are at the iPhone moment of AI.
Start-ups are racing to build disruptive products and business models, while incumbents are looking to respond.
Generative AI has triggered a sense of urgency in enterprises worldwide to develop AI strategies.
Customers need to access NVIDIA AI easier and faster.
We are announcing NVIDIA DGX Cloud through partnerships with Microsoft Azure, Google GCP, and Oracle OCI
to bring NVIDIA DGX AI supercomputers to every company, instantly, from a browser.
DGX Cloud is optimized to run NVIDIA AI Enterprise, the world’s leading acceleration library suite
for end-to-end development and deployment of AI.
DGX Cloud offers customers the best of NVIDIA AI and the best of the world’s leading cloud service providers.
This partnership brings NVIDIA’s ecosystem to the CSPs,
while amplifying NVIDIA’s scale and reach.
This win-win partnership gives customers racing to engage Generative AI instant access to NVIDIA in global-scale clouds.
We’re excited by the speed, scale, and reach of this cloud extension of our business model.
Oracle Cloud Infrastructure, OCI,
will be the first NVIDIA DGX Cloud.
OCI has excellent performance. They have a two-tier
computing fabric and management network.
NVIDIA’s CX-7, with the industry’s best RDMA,
is the computing fabric.
And BlueField-3 will be the infrastructure processor
for the management network.
The combination is a state-of-the-art DGX AI supercomputer that can be offered as a multi-tenant cloud service.
We have 50 early access enterprise customers, spanning consumer internet and software, healthcare
media and entertainment, and financial services.
ChatGPT, Stable Diffusion, DALL-E, and Midjourney have awakened the world to Generative AI.
These applications’ ease-of-use and impressive capabilities attracted over a hundred million users in just a few months
- ChatGPT is the fastest-growing application in history.
No training is necessary. Just ask these models to do something.
The prompts can be precise or ambiguous. If not clear,
through conversation, ChatGPT learns your intentions.
The generated text is beyond impressive.
ChatGPT can compose memos and poems, paraphrase a research paper, solve math problems,
highlight key points of a contract,
and even code software programs.
ChatGPT is a computer that not only
runs software but writes software.
Many breakthroughs led to Generative AI.
Transformers learn context and meaning from the relationships and dependencies of data, in parallel and at large scale.
This led to large language models that learn from so much data
they can perform downstream tasks without explicit training.
And diffusion models, inspired by physics, learn without
supervision to generate images.
In just over a decade, we went from trying to recognize
cats to generating realistic images of a cat
in a space suit
walking on the moon.
Generative AI is a new kind of computer — one that we program in human language.
This ability has profound implications. Everyone can direct
a computer to solve problems.
This was a domain only for computer programmers.
Now everyone is a programmer.
Generative AI is a new computing platform like PC,
internet, mobile, and cloud.
And like in previous computing eras,
first-movers are creating new applications
and founding new companies to capitalize on
Generative AI’s ability to automate and co-create.
Debuild lets users design and deploy web applications
just by explaining what they want.
Grammarly is a writing assistant that considers context.
Tabnine helps developers write code.
Omnekey generates customized ads and copy.
Kore.ai is a virtual customer service agent.
Jasper generates marketing material. Jasper has
written nearly 5 billion words,
reducing time to generate the first draft by 80%.
Insilico uses AI to accelerate drug design.
Absci is using AI to predict therapeutic antibodies.
Generative AI will reinvent nearly every industry.
Many companies can use one of the excellent
Generative AI APIs coming to market.
Some companies need to build custom models, with their proprietary data, that are experts in their domain.
They need to set up usage guardrails
and refine their models to align
with their company’s safety, privacy, and security requirements.
The industry needs a foundry, a TSMC,
for custom large language models.
Today, we announce the NVIDIA AI Foundations
a cloud service for customers needing to build, refine, and operate
custom LLMlarge language models and Generative AI
trained with their proprietary data
and for their domain-specific tasks.
NVIDIA AI Foundations comprises Language,
Visual, and Biology model-making services.
NVIDIA Nemo is for building custom language text-to-text
generative models.
Customers can bring their model or start with the Nemo pre-trained language models, ranging from GPT-8, GPT-43
and GPT-530 billion parameters.
Throughout the entire process, NVIDIA AI experts will work with you, from creating your proprietary model to operations.
Let’s take a look.
Generative models, like NVIDIA’s 43B foundational model, learn by training on billions of sentences
and trillions of words.
As the model converges, it begins to understand the relationships between words and their underlying concepts
captured in the weights in the embedding space of the model.
Transformer models use a technique called self attention: a mechanism designed to learn dependencies and relationships
within a sequence of words.
The result is a model that provides the foundation for a ChatGPT-like experience.
These generative models require expansive amounts of data
deep AI expertise for data processing and distributed training
 and large scale compute to train, deploy
and maintain at the pace of innovation.
Enterprises can fast-track their generative AI adoption
with NVIDIA NeMo service running on NVIDIA DGX Cloud.
The quickest path is starting with one of NVIDIA’s state-of-the-art
pre-trained foundation models.
With the NeMo service, organizations can easily customize a model
with p-tuning to teach it specialized skills
like summarizing financial documents
creating brand-specific content
and composing emails with personalized writing styles.
Connecting the model to a proprietary knowledge base
ensures that responses are accurate, current
and cited for their business.
Next, they can provide guardrails by adding logic
and monitoring inputs, outputs, toxicity, and bias thresholds
so it operates within a specified domain
and prevents undesired responses.
After putting the model to work, it can continuously improve
with reinforcement learning based on user interactions.
And NeMo’s playground is available for rapid prototyping
before moving to the cloud API
for larger-scale evaluation and application integration.
Sign up for the NVIDIA NeMo service today
to codify your enterprise’s knowledge into a personalized
AI model that you control.
Picasso is a visual language model-making service for customers who want to build custom models
trained with licensed or proprietary content.
Let’s take a look.
Generative AI is transforming how visual content is created.
But to realize its full potential, enterprises need massiveamounts of copyright-cleared data, AI experts, and an AI supercomputer.
NVIDIA Picasso is a cloud service for building and deploying
generative AI-powered image, video, and 3D applications.
With it, enterprises, ISVs, and service providers
can deploy their own models.
We're working with premier partners to bring
generative AI capabilities to every industry
Organizations can also start with NVIDIA Edify models
 and train them on their data to create a product or service.
These models generate images, videos, and 3D assets.
To access generative AI models
applications send an API call with text prompts
and metadata to Picasso.
Picasso uses the appropriate model running on NVIDIA DGX Cloud
to send back the generated asset to the application.
This can be a photorealistic image, a high-resolution video,
or a detailed 3D geometry.
Generated assets can be imported into editing tools or into NVIDIA Omniverse to build photorealistic virtual worlds,
metaverse applications, and digital twin simulations.
With NVIDIA Picasso services running on NVIDIA DGX Cloud
you can streamline training, optimization, and inference
needed to build custom generative AI applications.
See how NVIDIA Picasso can bring transformative generative AI capabilities into your applications.
We are delighted that Getty Images will use the Picasso service to build Edify-image and Edify-video generative models
trained on their rich library of responsibly licensed
professional images and video assets.
Enterprises will be able to create custom images
and video with simple text or image prompts.
Shutterstock is developing an Edify-3D generative model
trained on their professional image, 3D, and video assets library.
Shutterstock will help simplify the creation of 3D assets for creative production, digital twins and virtual collaboration,
making these workflows faster
and easier for enterprises to implement.
And I’m thrilled to announce a significant expansion
of our long-time partnership with Adobe
to build a set of next-generation AI capabilities
for the future of creativity
integrating generative AI into the everyday workflows
of marketers and creative professionals.
The new Generative AI models will be optimized
for image creation, video, 3D, and animation.
To protect artists’ rights, Adobe is developing with a focus on commercial viability and proper content attribution
powered by Adobe’s Content Authenticity Initiative.
Our third language domain is biology.
Drug discovery is a nearly $2T industry
with $250B dedicated to R&D.
NVIDIA’s Clara is a healthcare application framework for imaging
instruments, genomics, and drug discovery.
The industry is now jumping onto generative AI
to discover disease targets
design novel molecules or protein-based drugs, and predict the behavior of the medicines in the body.
Insilico Medicine, Exscientia, Absci, and Evozyme, are among hundreds of new AI drug discovery start-ups.
Several have discovered novel targets or drug candidates and have started human clinical trials.
BioNeMo helps researchers create
fine-tune, and serve custom models with their proprietary data.
Let’s take a look.
There are 3 key stages to drug discovery
discovering the biology that causes disease
designing new molecules - whether those are small-molecules, proteins or antibodies
and finally screening how those molecules interact with each other.
Today, Generative AI is transforming every step of the drug discovery process.
NVIDIA BioNeMo Service provides state-of-the-art
generative AI models for drug discovery.
It’s available as a cloud service, providing instant and easy access to accelerated drug discovery workflows.
BioNeMo includes models like AlphaFold, ESMFold and OpenFold
for 3D protein structure prediction.
ProtGPT for protein generation,
ESM1 and ESM2 for protein property prediction
MegaMolBART and MoFlow and for molecule generation
and DiffDock for molecular docking.
Drug discovery teams can use the
models through BioNeMo’s web interface
or cloud APIs.
Here is an example of using NVIDIA BioNeMo
for drug discovery virtual screening.
Generative models can now read a proteins amino acid sequence
and in seconds, accurately predict the structure of a target protein.
They can also generate molecules with desirable ADME properties that optimize how a drug behaves in the body.
Generative models can even predict the 3D interactions
of a protein and molecule
accelerating the discovery of optimal drug candidates.
With NVIDIA DGX Cloud BioNeMo also provides on-demand super computing infrastructure to further optimize and train models,
saving teams valuable time and money so they can focus on discovering life saving medicines.
The new AI drug discovery pipelines are here.
Sign up for access for NVIDIA BioNeMo Service.
We will continue to work with the industry
to include models into BioNemo
that encompass the end-to-end workflow of
drug discovery and virtual screening.
Amgen, AstraZeneca, Insilico Medicine, Evozyne, Innophore, and Alchemab Therapeutics are early access users of BioNeMo.
NVIDIA AI Foundations, a cloud service, a foundry, for building custom language models and Generative AI.
Since AlexNet a decade ago, deep learning has opened giant new markets — automated driving, robotics, smart speakers,
and reinvented how we shop, consume news, and enjoy music.
That’s just the tip of the iceberg.
AI is at an inflection point as Generative AI has started a new wave of opportunities, driving a step-function increase
in inference workloads.
AI can now generate diverse data, spanning voice, text, images, video, and 3D graphics to proteins and chemicals.
Designing a cloud data center to
process Generative AI is a great challenge.
On the one hand, a single type of accelerator is ideal,
because it allows the datacenter to be elastic
and handle the unpredictable peaks and valleys of traffic.
On the other hand, no one accelerator can optimally process the diversity of algorithms, models, data types, and sizes.
NVIDIA's One Architecture platform
offers both acceleration and elasticity.
Today, we are announcing our new inference platform - four configurations - one architecture - one software stack.
Each configuration is optimized for a class of workloads.
For AI video workloads, we have L4 optimized for video decoding and transcoding, video content moderation,
and video call features like background replacement,
relighting, making eye contact,
transcription, and real-time language translation.
Most cloud videos today are processed on CPUs.
One 8-GPU L4 server will replace over a hundred dual-socket CPU servers for processing AI Video.
Snap is a leading user of NVIDIA AI for computer vision
and recommender systems.
Snap will use L4 for AV1 video processing,
generative AI, and augmented reality.
Snapchat users upload hundreds of millions of videos every day.
Google announced today NVIDIA L4 on GCP.
NVIDIA and Google Cloud are working
to deploy major workloads on L4.
Let me highlight five.
First, we’re accelerating inference for generative AI models for cloud services like Wombo and Descript.
Second, we’re integrating Triton Inference Server with Google Kubernetes Engine and VertexAI.
Third, we’re accelerating Google Dataproc
with NVIDIA Spark-RAPIDS.
Fourth, we’re accelerating AlphaFold,
and UL2 and T5 large language models.
And fifth, we are accelerating Google Cloud’s Immersive Stream that renders 3D and AR experiences.
With this collaboration, Google GCP is a premiere NVIDIA AI cloud.
We look forward to telling you even more
about our collaboration very soon.
For Omniverse, graphics rendering and generative AI like text-to-image and text-to-video, we are announcing L40.
L40 is up to 10 times the performance of NVIDIA’s T4,
the most popular cloud inference GPU.
Runway is a pioneer in Generative AI.
Their research team was a key creator of Stable Diffusion
and its predecessor, Latent Diffusion.
Runway is inventing generative AI models
for creating and editing content.
With over 30 AI Magic Tools, their service is revolutionizing the creative process, all from the cloud.
Let's take a look.
Runway is making amazing AI-powered video editing and image creation tools accessible to everyone.
Powered by the latest generation of NVIDIA GPUs running locally or in the cloud, Runway makes it possible
 to remove an object from a video with just a few brush strokes.
Or apply different styles to video using just an input image.
Or change the background or the foreground of a video.
What used to take hours using conventional tools can now be completed with professional broadcast quality results
in just a few minutes.
Runway does this by utilizing CV-CUDA, an open-source project that enables developers to build highly efficient
GPU-accelerated pre- and post-processing pipelines for computer vision workloads and scale them into the cloud.
With NVIDIA technology, Runway is able to make impossible things to give the best experience to content creators.
What previously limited pros can now be done by you.
In fact, Runway is used in Oscar-nominated Hollywood films and we are placing this technology
in the hands of the world's creators.
Large language models like ChatGPT
are a significant new inference workload.
GPT models are memory and computationally intensive.
Furthermore, inference is a high-volume, scale-out workload and requires standard commodity servers.
For large language model inference, like ChatGPT, we are announcing a new Hopper GPU — the PCIE H100
with dual-GPU NVLINK.
The new H100 has 94GB of HBM3 memory.
H100 can process the 175-billion-parameter GPT-3
and supporting commodity PCIE servers make it easy to scale out.
The only GPU in the cloud today that can practically process ChatGPT is HGX A100.
Compared to HGX A100 for GPT-3 processing, a standard server with four pairs of H100 with dual-GPU NVLINK
 is up to 10X faster.
H100 can reduce large language model processing costs
by an order of magnitude.
Grace Hopper is our new superchip that connects Grace CPU and Hopper GPU over a high-speed 900 GB/sec
coherent chip-to-chip interface.
Grace Hopper is ideal for processing giant data sets like AI databases for recommender systems
and large language models.
Today, CPUs, with large memory, store and query giant embedding tables then transfer results to GPUs for inference.
With Grace-Hopper, Grace queries the embedding tables and transfers the results directly to Hopper
across the high-speed interface – 7 times faster than PCIE.
Customers want to build AI databases
several orders of magnitude larger.
Grace-Hopper is the ideal engine.
This is NVIDIA's inference platform –
one architecture for diverse AI workloads,
 and maximum datacenter acceleration and elasticity.
The world’s largest industries make physical things,
but they want to build them digitally.
Omniverse is a platform for industrial digitalization
that bridges digital and physical.
It lets industries design, build, operate, and optimize physical products and factories digitally,
before making a physical replica.
Digitalization boosts efficiency and speed and saves money.
One use of Omniverse is the virtual bring-up of a factory, where all of its machinery is integrated digitally
before the real factory is built.
This reduces last-minute surprises, change orders,
and plant opening delays.
Virtual factory integration can save billions for the world’s factories.
The semiconductor industry is investing half a trillion dollars
to build a record 84 new fabs.
By 2030, auto manufacturers will build 300 factories
to make 200 million electric vehicles.
And battery makers are building 100 more mega factories.
Digitalization is also transforming logistics, moving goods through billions of square feet of warehouses worldwide.
Let’s look at how Amazon uses Omniverse to automate, optimize, and plan its autonomous warehouses.
Amazon Robotics has manufactured and deployed the largest fleet of mobile industrial robots in the world.
The newest member of this robotic fleet is Proteus, Amazon's first fully autonomous warehouse robot.
Proteus is built to move through our facilities using advanced safety, perception, and navigation technology.
Let's see how NVIDIA Isaac Sim, built on Omniverse is creating physically accurate, photoreal simulations
to help accelerate Proteus deployments.
Proteus features multiple sensors that include cameras,
lidars, and ultrasonic sensors
to power it’s autonomy software systems.
The Proteus team needed to improve the performance of a neural network that read fiducial markers and helped the robot
determine its location on the map.
It takes lots of data—and the right kind—to train the ML models that are driven by the robot sensor input.
With Omniverse Replicator in Isaac Sim, Amazon Robotics was able to generate large photoreal synthetic datasets that improved
the marker detection success rate from 88.6% to 98%.
The use of the synthetic data generated by Omniverse Replicator also sped up development times, from months to days,
as we were able to iteratively test and train our models
much faster than when only using real data.
To enable new autonomous capabilities for the expanding fleet of Proteus robots, Amazon Robotics is working towards
closing the gap from simulation to reality, building large scale multi-sensor, multi-robot simulations.
With Omniverse, Amazon Robotics will optimize operations with full fidelity warehouse digital twins.
Whether we're generating synthetic data or developing new levels of autonomy, Isaac Sim on Omniverse
 helps the Amazon Robotics team save time and money as we deploy Proteus across our facilities.
Omniverse has unique technologies for digitalization.
And Omniverse is the premier development platform for USD, which serves as a common language that lets teams collaborate
to create virtual worlds and digital twins.
Omniverse is physically based, mirroring the laws of physics.
It can connect to robotic systems and operate with hardware-in-the-loop.
It features Generative AI to accelerate the creation of virtual worlds.
And Omniverse can manage data sets of enormous scale.
We've made significant updates to Omniverse in every area.
Let’s take a look.
Nearly 300,000 creators and designers
have downloaded Omniverse.
Omniverse is not a tool, but a USD network and shared database,
a fabric connecting to design tools used across industries.
It connects, composes, and simulates the assets
created by industry-leading tools.
We are delighted to see the growth of Omniverse connections.
Each connection links the ecosystem of one platform to the ecosystems of all the others.
Omniverse’s network of networks is growing exponentially.
Bentley Systems LumenRT is now connected.
So are Siemens Teamcenter, NX, and Process Simulate, Rockwell Automation Emulate 3D, Cesium, Unity, and many more.
Let’s look at the digitalization of the $3T auto industry
and see how car companies are
evaluating Omniverse in their workflows.
Volvo Cars and GM use Omniverse USD Composer
to connect and unify their asset pipelines.
GM connects designers, sculptors, and artists using Alias, Siemens NX, Unreal, Maya, 3ds Max,
and virtually assembles the components
into a digital twin of the car.
 In engineering and simulation, they visualize the power flow aerodynamics in Omniverse.
For next-generation Mercedes-Benz and Jaguar Land Rover vehicles, engineers use Drive Sim in Omniverse to generate
synthetic data to train AI models, validate the active-safety system against a virtual NCAP driving test,
and simulate real driving scenarios.
Omniverse’s generative AI reconstructs
previously driven routes into 3D
 so past experiences can be reenacted or modified.
Working with Idealworks, BMW uses Isaac Sim
in Omniverse to generate synthetic data
and scenarios to train factory robots.
Lotus is using Omniverse to virtually assemble welding stations.
Toyota is using Omniverse to build digital twins of their plants.
Mercedes-Benz uses Omniverse to build, optimize, and plan assembly lines for new models.
Rimac and Lucid Motors use Omniverse to build digital stores from actual design data that faithfully represent their cars.
BMW is using Omniverse to plan operations across nearly three dozen factories worldwide.
And they are building a new EV factory, completely in Omniverse, two years before the physical plant opens.
Let's visit.
The world’s industries are accelerating digitalization with over $3.4 trillion being invested in the next three years.
We at BMW strive to be leading edge in automotive digitalization.
With NVIDIA Omniverse and AI we set up new factories faster and produce more efficiently than ever.
This results in significant savings for us.
It all starts with planning – a complex process
in which we need to connect many tools,
datasets and specialists around the world.
Traditionally, we are limited, since data is managed separately in a variety of systems and tools.
Today, we’ve changed all that.
We are developing custom Omniverse applications to connect our existing tools, know-how and teams
all in a unified view.
Omniverse is cloud-native and cloud-agnostic enabling teams to collaborate across our virtual factories from everywhere.
I’m about to join a virtual planning session for Debrecen in Hungary – our new EV factory – opening in 2025.
Letʼs jump in.
Planner 1: Ah, Milan is joining.
Milan: Hello, everyone!
Planner 1:Hi Milan – great to see you, we’re in the middle of an optimization loop for our body shop.
Would you like to see?
Milan: Thanks – I’m highly interested. And I’d like to invite a friend.
Planner 1: Sure.
Jensen: Hey Milan! Good to see you.
Milan: Jensen, welcome to our virtual planning session.
Jensen: Its great to be here. What are we looking at?
Milan: This is our global planning team who are working on a robot cell in Debrecen’s digital twin.
Matthias, tell us what’s happening …
Matthias: So, we just learned the
production concept requires some changes.
We’re now reconfiguring the layout to add a new robot into the cell.
Planner 2: Ok, but if we add a new robot, on the logistics side, we’ll need to move our storage container.
Planner 3: Alright, let's get this new robot in.
Matthias: That’s perfect. But let’s double-check -
can we run the cell?
Excellent.
Jensen: Milan, this is just incredible!
Virtual factory integration is essential for every industry.
I’m so proud to see what our teams did together. Congratulations!
Milan: We are working globally to optimize locally.
After planning, operations is king, and we’ve already started!
To celebrate the launch of our virtual plant, I’d like to invite you to open the first digital factory with me.
Jensen: I’d be honored. Let’s do it!
Car companies employ nearly 14 million people.
Digitalization will enhance the industry's
efficiency, productivity, and speed.
Omniverse is the digital-to-physical operating system
to realize industrial digitalization.
Today we are announcing three
systems designed to run Omniverse.
First, we’re launching a new generation of workstations powered by NVIDIA Ada RTX GPUs and Intel's newest CPUs.
The new workstations are ideal for doing ray tracing, physics simulation, neural graphics, and generative AI.
They will be available from Boxx, Dell, HP,
and Lenovo starting in March.
Second, new NVIDIA OVX servers optimized for Omniverse.
OVX consists of L40 Ada RTX server
GPUs and our new BlueField-3.
OVX servers will be available from Dell, HPE, Quanta, Gigabyte, Lenovo, and Supermicro.
Each layer of the Omniverse stack, including the chips, systems, networking, and software are new inventions.
Building and operating the Omniverse computer
requires a sophisticated IT team.
We’re going to make Omniverse
fast and easy to scale and engage.
Let’s take a look.
The world’s largest industries are racing
to digitalize their physical processes.
Today, that’s a complex undertaking.
NVIDIA Omniverse Cloud is a platform-as-a-service that provides instant, secure access to managed Omniverse Cloud APIs,
workflows, and customizable applications running on NVIDIA OVX.
Enterprise teams access the suite of managed services through the web browser Omniverse Launcher
or via a custom-built integration.
Once in Omniverse Cloud, enterprise teams can instantly access, extend, and publish foundation applications
 and workflows - to assemble and compose virtual worlds -
generate data to train perception AIs -
test and validate autonomous vehicles -
or simulate autonomous robots…
…accessing and publishing shared data to Omniverse Nucleus.
Designers and engineers working in their favorite
3rd party design tools on RTX workstations,
publish edits to Nucleus in parallel.
Then when ready to iterate or view
their integrated model in Omniverse,
can simply open a web browser and log in.
As projects and teams scale, Omniverse Cloud helps optimize cost
by provisioning compute resources and licenses as needed.
And new services and upgrades are automatically
provided with real time updates.
With Omniverse Cloud, enterprises can fast-track unified digitalization and collaboration
across major industrial workflows, increasing efficiency,
reducing costs and waste,
 and accelerating the path to innovation.
See you in Omniverse!
Today, we announce the NVIDIA Omniverse Cloud,
a fully managed cloud service.
We’re partnering with Microsoft to bring Omniverse Cloud
to the world’s industries.
We will host it in Azure, benefiting from Microsoft’s rich storage, security, applications, and services portfolio.
We are connecting Omniverse Cloud to Microsoft 365 productivity suite, including Teams, OneDrive, SharePoint,
 and the Azure IoT Digital Twins services.
Microsoft and NVIDIA are bringing Omniverse to hundreds of millions of Microsoft 365 and Azure users.
Accelerated computing and AI have arrived.
Developers use NVIDIA to speed-up and scale-up to solve problems previously impossible.
A daunting challenge is Net Zero. Every company must accelerate every workload to reclaim power.
Accelerated computing is a full-stack,
datacenter-scale computing challenge.
Grace, Grace-Hopper, and BlueField-3 are new chips for super energy-efficient accelerated data centers.
Acceleration libraries solve new challenges and open new markets.
We updated 100 acceleration libraries, including cuQuantum for quantum computing, cuOpt for combinatorial optimization,
and cuLitho for computational lithography.
We are thrilled to partner with TSMC, ASML,
and Synopsys to go to 2nm and beyond.
NVIDIA DGX AI Supercomputer is the engine behind the generative large language model breakthrough.
The DGX H100 AI Supercomputer
is in production and available soon
from an expanding network of OEM and cloud partners worldwide.
The DGX supercomputer is going beyond
research and becoming a modern AI factory.
Every company will manufacture intelligence.
We are extending our business model with NVIDIA DGX Cloud by partnering with Microsoft Azure, Google GCP, and Oracle OCI
to instantly bring NVIDIA AI to every company, from a browser.
DGX Cloud offers customers the best of NVIDIA
and the best of the world’s leading CSPs.
We are at the iPhone moment for AI.
Generative AI inference workloads have gone into overdrive.
We launched our new inference platform -
four configurations - one architecture.
L4 for AI video.
L40 for Omniverse and graphics rendering.
H100 PCIE for scaling out large language model inference.
Grace-Hopper for recommender systems and vector databases.
NVIDIA’s inference platform enables maximum
data center acceleration and elasticity.
NVIDIA and Google Cloud are working together to deploy a broad range of inference workloads.
With this collaboration, Google GCP is a premiere NVIDIA AI cloud.
NVIDIA AI Foundations is a cloud service, a foundry, for building custom language models and Generative AI.
NVIDIA AI Foundations comprises language,
visual, and biology model-making services.
Getty Images and Shutterstock are
building custom visual language models.
And we're partnering with Adobe to build a set of next-generation AI capabilities for the future of creativity.
Omniverse is the digital-to-physical operating system
to realize industrial digitalization.
Omniverse can unify the end-to-end workflow and digitalize the $3T, 14 million-employee automotive industry.
Omniverse is leaping to the cloud.
Hosted in Azure, we partner with Microsoft to bring Omniverse Cloud to the world’s industries.
I thank our systems, cloud, and software partners,
researchers, scientists,
and especially our amazing employees
for building the NVIDIA accelerated computing ecosystem.
Together, we are helping the world do the impossible.
Have a great GTC!
Title: How AI Reads the Writing on the Road - NVIDIA DRIVE Labs Ep. 16
Publish_date: 2019-12-04
Length: 83
Views: 26512
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/dl8MI4vZmUY/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: dl8MI4vZmUY

--- Transcript ---

In DRIVE Labs ep. 7, we showed you high precision
lane line detection using our LaneNets deep
neural network, and today we're going to show
you how this DNN has evolved to detect not
just lane lines but also other important road
markings and visual landmarks.
It is now called MapNet.
In this clip, we are seeing results from a
MapNet DNN model that performs complete end
to end detection of road boundaries shown
in white, dash lane lines in blue, solid lane
lines in red, as well as vertical landmarks
such as polls in yellow.
MapNet also detects intersection entry and
exit lines in orange, road markings such as
HOV lane signs in cyan, and cross-traffic
intersection lines in magenta.
Here MapNet is detecting road markings and
landmarks on a Japanese highway including
arrows, road texts on the left lane denoting
an upcoming exit, and road texts on the right
lane that indicates to keep straight to stay
on the highway.
And here, MapNet is detecting road texts on
the right lane that denotes an upcoming merge
as well as detecting vertical guideposts in
yellow.
Where these guideposts end, we also observe
a highly accurate transition from solid to
dash lane line detections.
MapNet now also enables camera-based high-quality
map creation and localization using commercially
available sensors for consumer vehicles.
Title: Optimizing an Ultrarapid DNA Sequencing Technique for Critical Care Patients
Publish_date: 2022-03-22
Length: 161
Views: 169630
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/dnKJTaYoAh8/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: dnKJTaYoAh8

--- Transcript ---

Patients with rare or undiagnosed diseases
often undergo a diagnostic odyssey—spending
time, money, and numerous hospital visits
to uncover the cause of their disease.
Rapid whole genome sequencing can help 
identify the genetic cause of a disease 
and match patients to the right therapies quickly.
But sequencing a patient's whole genome can
take weeks and isn't a part of standard care.
So a team from Stanford, Oxford Nanopore Technologies,
NVIDIA, the University of California Santa Cruz and Google
 set out to accelerate a whole genome analysis workflow.
They developed a world-record, ultra-rapid
DNA sequencing technique from sample to diagnosis
in just 7 hours and 18 minutes.
Their results were published in January in
the New England Journal of Medicine.
It was a phenomenal collaboration between all the parties. 
So, we're talking about putting something on at breakfast 
and having actionable data at lunchtime.
Oxford Nanopore's PromethION 48 DNA sequencing
instrument was critical to their achievement.
The PromethION 48 uses flow cells, which contain
an array of tiny holes — protein nanopores
— embedded in an electro-resistant membrane.
As DNA passes through the nanopore, an electric
signal is generated, which is classified into
base pairs using a deep learning model powered by NVIDIA.
This is called basecalling.
The team used 64 GPUs in the cloud for 
basecalling, alignment, and variant calling.
Genetic variants were identified by a pipeline
using NVIDIA Clara Parabricks.
Curation and final diagnosis happened after that.
This ultra-rapid nanopore DNA sequencing technique
helped a three-month-old infant who was suffering from seizures.
By identifying the correct genetic variant
in just hours, a definitive diagnosis was made quickly.
A standard epilepsy gene panel that was ordered
at the same time took two weeks to return results
— and it didn’t even include the gene affected.
This is an exciting moment as this has the
potential to fundamentally change critical care.
NVIDIA and Oxford Nanopore have further 
optimized this nanopore workflow.
Using the PromethION 48, a single NVIDIA DGX A100,
 and an UltraRapid Whole Genome Sequencing
pipeline container — available for download
from the NVIDIA NGC catalog — they simplified
a complex pipeline, achieved state-of-the-art performance 
and reduced compute costs by more than 50 percent.
To learn more, and to try it for yourself,
visit the links in this video's description.
Title: NVIDIA RTX in High Demand with Graphics Software Companies
Publish_date: 2018-08-24
Length: 91
Views: 30906
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/DSeAI_sl7eU/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: DSeAI_sl7eU

--- Transcript ---

so I've been the graphics programmer for
20 years I am the new engineer head of
R&D F CEO and co founders and in those
early days we have never dreamed the
real-time ray-tracing would ever be
possible
I lost my mind because I knew the
vacation partner was the future it's
been used in film and television
productions he opening a Westworld but I
am slack w get Siva I am at mr. walls
and later now so when we got the r-tx
vault like the autistic thousand that's
running the demo behind me we were able
to take advantage of it in a few hours
of development this is the first car
that's been rate raised in real time and
what you see is a combination of
specular effects with NI so traffic
taint area light shadows and all the
effects that you would expect to see in
reality you can see the huge island
navigate since it's in a manipulate
object with an instant feedback
obviously you want this because of speed
because you can interact faster so on
Pascal octane bench was getting about
400 million raised a second and on
touring it's about 3.2 billion so that's
the 8x improvement in speed we have been
able to reach 50 x increases and that's
really huge for visualization product
design and so to see this to become
possible in real time it really shows
that graphics technology has increased
in performance a million fold since
those early days a million Ford in 20
years just imagine that
[Music]
Title: ISC 2018: Making AI Real with HPE and NVIDIA
Publish_date: 2018-06-25
Length: 67
Views: 6848
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/DswzU78O1OY/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLBgxVlb8mswjbYK4dw1xMgEZwW7lw
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: DswzU78O1OY

--- Transcript ---

[Music]
he'll Packer enterprise truly believes
in the potential of AI we really believe
that AI will transform the industries in
a major way and it will solve the most
stuff and the most complex problems for
the human civilization we are committed
to help our customers adopt AI through
the journey of exploration
experimentation and expansion HP and
NVIDIA are bringing the best technology
together to offer and to help our
customers through this journey we
partner with Nvidia to help our
customers learn deep learning through
the deep learning Institute and then
finally from we bring in the right
infrastructure such as Apollo 6500 Gen
10 which is at eight GPU dense server
bringing in three to five X acceleration
for deep learning workloads so HP n in
media want to help you and we are
committed to help you towards getting
transform through AI
Title: NVIDIA's Culture
Publish_date: 2019-12-18
Length: 76
Views: 26796
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/DVvEzFHty1A/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: DVvEzFHty1A

--- Transcript ---

[Music]
what I really value about building a
career at Nvidia is that there are
people here who will take the time to
notice your interests and your skills
and your talents and help you find a
place to channel them in a way that has
impact for you and for the company I
don't need to necessarily stick to
exactly what my projects are and so the
freedom to just kind of like explore and
go around it's been really nice and it's
usually me learning something completely
new and I never expected to learn we're
lucky to have that here in a lot of
other places don't have that I really
very much appreciate how flexible Nvidia
is and in particular work on really hard
problems and not be afraid to fail
it's things don't work out and I think
that that's helpful in general for
moving technology forward and also
design technology that's more inclusive
[Music]
in addition to being world-class in the
technical work that we do we care about
the way in which we do our work the way
that we treat people we care for each
other we respect each other and that
comes straight from the top and so that
is something that I really value about
working here
Title: GPU Technology Conference (GTC) Keynote Oct 2020 | Part 1: "The Coming Age of AI"
Publish_date: 2020-10-05
Length: 439
Views: 231627
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/Dw4oet5f0dI/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: Dw4oet5f0dI

--- Transcript ---

I am an explorer.
Searching for the origins of our universe.
And charting a safer path to other worlds.
I am a helper.
Moving us forward, one step at a time.
And giving voice to every emotion.
I love you.
I love you.
I am a healer.
Modeling the future of medicine.
And finding the needle in a haystack when every second counts.
I am a visionary.
Uncovering masterpieces lost to the ages.
And finding new adventures in a galaxy far far away.
I am a builder.
Driving perfection in everything we create.
I am even the narrator of the story you are watching.
And the composer of the music.
And when the world faces its greatest challenge.
I give us the power to take it on together.
I AM AI
Welcome to GTC!
This is one of the biggest ones ever.
Over 1000 sessions, 400 more than last GTC.
This is the first GTC that will run across the world's time zones.
With sessions in Chinese, Korean, Japanese and Hebrew, all in local times.
I would like to thank all the speakers and sponsoring partners for helping us make this a great GTC.
This is an amazing time for the computer industry and the world.
As mobile cloud matures, the age of AI is beginning.
Why is AI so important?
Why is it the most powerful technology force of our time?
Computing is the technology of automation. Software algorithms automate.
Automation drives productivity and growth for industries.
For example, farming automated food production and revolutionized society.
Automation lowers cost for consumers and make life more convenient.
Automatic electricity and water improved our quality of life.
Automation, at large scales lead to breakthroughs.
Just as search has automated getting information and getting answers.
There are many things we'd like to automate.
We are limited by our ability to write the
software.
Finally, after decades of research, deep learning, the abundance of data, the powerful computation
of GPUs came together in a big bang of modern AI.
Now software can write software.
So, AI is the automation of automation.
Incredible implications.
Let's explore some of it today.
The software written by AI is very different than that written by a human.
It is vastly more parallel, and thousands-to-millions-of-times more compute intensive.
The method of developing the software is different, the computing infrastructure is different,
the tools are different, the software runs differently, and even the method of deployment is different.
AI requires a whole reinvention of computing, full-stack rethinking, from chips,
to systems, algorithms, tools, the ecosystem.
NVIDIA is a full stack computing company.
We love working on extremely hard computing problems that have great impact on the world.
This is right in our wheelhouse.
We're all-in to advance and democratize this new form of computing for the age of AI.
NVIDIA's dedicated to advancing accelerated computing.
Enabling developer's success on our platform is core to everything we do.
GTC is all about developers.
Developers adopt a new platform only if it solves a problem 10 times better and that
has a large and growing install base.
And developers need great tools.
Domain after domain, graphics, high-performance computing, data science and analytics,
artificial intelligence, healthcare, self-driving cars, robotics, to the 5G Edge.
NVIDIA created platform SDKs to make it easier for developers to tap into the power of NVIDIA computing
and make it easy to deploy your software on NVIDIA computers in every cloud and any data center and edge.
This GTC we're releasing 80 new and updated SDKs.
There are now a total of 110 SDKs.
Developers need an architecturally compatible and significant install base.
NVIDIA has shipped over 1 billion CUDA compatible GPUs.
CUDA SDK has been downloaded 20 million times, and 6 million times this year alone.
It is clearly accelerating.
1,800 applications are now CUDA-accelerated.
There are 6,500 startups building on NVIDIA.
The top developer SDKs included NVIDIA AI, NVIDIA RTX, NVIDIA RAPIDS for data science,
and NVIDIA ISAAC and DRIVE for robotics and autonomous machines.
There are over 2 million NVIDIA developers now.
600,000 joined this last year.
To all, a big welcome!
Title: NVIDIA RTX Brings Lunar Landing Experience to SIGGRAPH
Publish_date: 2019-08-01
Length: 87
Views: 12955
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/dWjsK-D4-Bc/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: dWjsK-D4-Bc

--- Transcript ---

hi i'm greg estes from nvidia and we're
here at SIGGRAPH 2019 SIGGRAPH is the
most important computer graphics
conference in the world and our
researchers brought one of our coolest
and best demos ever here to the
attendees at the show what we're showing
is an amazing compilation of a I pose
estimation and real-time ray tracing let
me tell you how all of this works the
first thing that we're doing is we're
aiming a camera at the attendees and
we're using new research from Nvidia
that does artificial intelligence based
pose estimation the second thing that
we're doing in real time is to take that
information and we put it in a virtual
scene then we're rendering that scene
now with the attendee as an astronaut on
the moon in the Apollo 11 lunar landing
scenario
[Music]
when we're doing this we're combining AI
in real time ray tracing in a way that
would let a Hollywood producer for
instance understand with just an iPad
and somebody who is in street clothes
immediately see in real time what that's
going to look like in a real time ray
traced environment this is one of over
40 different ray traced applications
that we have here all running on NVIDIA
RT X at SIGGRAPH 2019
Title: SHIELD Showcase: Oddworld Stranger’s Wrath Developer Interview
Publish_date: 2014-12-11
Length: 176
Views: 12981
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/dwtGPuf73HM/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: dwtGPuf73HM

--- Transcript ---

hi I'm Lauren laning I'm co-founder
president and creative director of
oddworld inhabitants
so underworld strangers wrath had an
interest in beginning it was originally
backed by Microsoft and that was in the
heyday of the Xbox and our job was make
the Xbox look as great as possible and
then I was approached by a group called
square one who believed that they could
bring stranger to the mobile platform
and in particular Android we didn't
think it could happen and they did they
did it and they made it run I think
would lead our team to focus on the
Nvidia shield tablet with the k1 he
really came down to a no-brainer
decision was about the most powerful if
not the most powerful GPU on the Android
environment and it actually wound up
running better on the Tegra k1 than we
had seen running on the Xbox in the day
which was very surprising we also saw a
significant advantage of the shield
controller because this gave us a full
console controller experience so one of
the nice things there is that we not
only solve the solution of dealing with
a two-finger interface for a touchscreen
but at the same time it enabled us to
retain the original controls to bring
that to a tablet experience in this game
I've replayed twice on handheld devices
because I just it felt like a new game
to me and that was pretty special
working with Nvidia is always exciting
this is a company deeply committed to
next-generation graphics and always
trying to push that with it and really
on that level they provide a developer
support that's second to none
and in that we see that there's large
future opportunities to start optimizing
and taking advantage of technologies
that haven't been exploited yet that we
really see some exciting opportunities
with you might want to take a peek at it
what's and feel there's three primary
reasons why gamers should play odd world
strangers wrath on shield tablet for the
first time we're able to deliver the
game at 1080p number two is it's really
connecting the highest tech of mobile
platform with the highest tech game
development and I think the third most
important reason is that the existing
fans of the game when they've seen it
playing on the tablet they have said
that they want to play it again
Title: GTC 2018 NVIDIA News Recap
Publish_date: 2018-03-28
Length: 171
Views: 156816
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/e-Zkx1AarxA/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLBG6Z6I764x8Gt3nNp_Ag5cSxyKFg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: e-Zkx1AarxA

--- Transcript ---

hi I'm here at GTCC 2018 with 8,500
developers researchers data scientists
and decision makers from all kinds of
industries we just came off of a great
keynote by Nvidia CEO Jensen Wong we
started talking about graphics which is
of course Nvidia's heritage the first
thing that we showed was amazing
real-time ray tracing doing it
interactively changing shadows looking
at reflections Nvidia r-tx technology
with the brand-new quad road GV 100 32
gigabytes just amazing work who's ever
in charge of this place should be
transferred to a hub we made some great
new announcements around what we're
doing in artificial intelligence our new
DG x2 16 of our Tesla V 100 32 gigabytes
all in one system this is going to be
the most powerful system ever built to
petaflop s-- for doing AI training and
research and high-performance computing
we talked about tensor RT for now the
thing about tensor RT is that makes
inference go faster and it's been
integrated now with Google's tensor flow
when you put the two of them together
you can get up to 1/8 times performance
speed-up versus using tensor flow alone
the next thing that we talked about was
kubernetes on NVIDIA GPUs now kubernetes
is used to scale performance out in the
cloud or on premises so our users can
have their GPUs and have this incredible
performance on any cloud or on systems
in their own data center
we also had a lot of fun with what we're
doing with robotics we showed our new
robot SDK to allow our developers to
build their own applications
[Music]
we had our brand-new drive SIM software
and constellation Hardware working
together they allow us to be able to
simulate billions of miles in the real
world this is going to enable us to
build a neural network that is even more
robust and therefore safer for
self-driving cars there's lots more that
we don't have time to talk about now but
let me tell you about just one more
thing we put one of our engineers in
virtual reality here on stage was
driving an autonomous car in VR that was
hooked to a real car out in the parking
lot that drove around a truck and parked
the car with no driver in it
autonomously it was amazing and I wish
you were here to see it but check out
many more great things like this at the
next GTC
Title: NVIDIA and SONiC
Publish_date: 2021-05-19
Length: 158
Views: 13861
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/e0aUrrEFQuw/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: e0aUrrEFQuw

--- Transcript ---

for decades proprietary network
architecture and dependencies on a
single vendor
have inhibited network innovation and
created a gap in the data center
several of the largest web scalers have
developed their own network stacks to
take back control of innovation
but that innovation gap is still there
what's the next step
the answer is of course open source just
like it was for servers in the past
but this time it's all about the network
enter the era of open source networking
with sonic
software for open networking in the
cloud
initiated by microsoft in 2016 and added
to the open compute project a year later
sonic continues to gain strong momentum
with hyperscalers and tier 2 enterprises
nvidia solutions support pure sonic
which means you aren't limited by a
distribution choice
with sonic you get vendor independence
unlike a vendor-specific distribution
pure sonic is open and standards based
eliminating reliance upon a single
vendor for road map additions bug fixes
and security patches
choice of network operating system mix
and match your leaf spine architecture
with sonic
nvidia cumulus linux or others the
choice is yours
partner with thought leaders nvidia is
the leading contributor to free range
routing the primary routing stack in
sonic
support from asic to protocol
choice of hardware sonic's switch
abstraction interface
psy removes the associated hardware
complexity
and makes it possible to choose the best
networking hardware
a 100 open source experience sonic is
community based
enabling complete flexibility
scalability for hybrid and multi-cloud
networks
sonic's modular extensible
container-based design
accelerates innovation unified
management
existing management tools can be used
across the data center
cost savings the open standards-based
nature of sonic helps save on costs
associated with opex and licensing
prime time readiness sonic is hardened
in the data centers of some of the
largest cloud service providers
these benefits are key to bringing
innovation back to your data center
and with nvidia you get the expertise
experience
training documentation professional
services
and support that can best guarantee your
success
as cloud hyperscalers service providers
of all kinds and enterprises seek faster
more efficient and automated networking
adoption of sonic will continue to
expand
and with it open networking choose
open and close the innovation gap
Title: Batman: Arkham Origins Geforce GTX Technology Trailer
Publish_date: 2013-10-04
Length: 144
Views: 137937
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/E0GnVSyYDjQ/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: E0GnVSyYDjQ

--- Transcript ---

hi this is andrew from nvidia and today
we're going to take the first look at
Batman Arkham origins batman arkham
origins uses some really cool new
graphics technologies like Nvidia PhysX
including particle fluid and cloth
simulation as well as apex turbulence
and NVIDIA TxAA anti-alias let's go take
a look
[Music]
in this scene we see Batman
business-as-usual taking down one of
Gotham's thugs from behind what we're
missing is Nvidia PhysX technology with
Nvidia PhysX volumetric particle
simulation the smoke interacts with
Batman as he sneaks up on the thug to
take him down
[Music]
in this next scene you can see as Batman
jump still allege with Nvidia PhysX
enabled the character interacts with the
volumetric smoke particles as he jumps
off the ledge to grapnel to the next
area
in this final scene we see Batman take
on a group of thugs as he combos all
over their faces you see the characters
interact dynamically with the volumetric
smoke particles this is all possible
using Nvidia PhysX technology
[Music]
it's Christmas Eve in Gotham City and
using Nvidia PhysX technology we can
simulate snow and wind conditions using
physical particle simulation and apex
turbulence without Nvidia PhysX it looks
like just another day in Gotham City
[Music]
Invidia TxAA is a new anti-aliasing
technology that uses a temporal filter
to cut down on swimming and crawling in
motion on objects
play Batman Arkham origins on GeForce
GTX the way it's meant to be played
Title: Get Started with Enterprise AI on NVIDIA LaunchPad
Publish_date: 2022-02-14
Length: 123
Views: 18221
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/E20l5k2wmQE/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: E20l5k2wmQE

--- Transcript ---

with nvidia launchpad enterprises can
kickstart their ai journey with
immediate short-term access to nvidia ai
software running on accelerated compute
infrastructure
one of nvidia products which you can
trial today is nvidia ai enterprise
software suite
it includes key ai and data analytics
tools and frameworks as well as gpu
virtualization software
when a new user requests access to
launchpad
they choose one of two roles either it
administrator or ai practitioner and the
type of curated lab they want to
experience
it admins connect to a live vcenter
server to create their first nvidia ai
enterprise virtual machine with detailed
instructions they add a virtual gpu
device install nvidia ai enterprise
guest drivers and license the virtual
machine
they also installed docker nvidia
container tool kit and required nvidia
enterprise ai containers such as
tensorflow and nvidia triton inference
server these steps will make deploying
and supporting ai within their
environment quick and easy
ai practitioners interact with the
virtual machine through a jupiter
notebook and are able to run ai
workflows in the virtual machine with
the same performance as a local
workstation
within their launchpad lab they get
immediate access to ai frameworks for
data processing and training in addition
to state-of-the-art gpu resources
once their model is trained it's
deployed for inference with nvidia
triton inference server a framework that
simplifies the rapid reliable deployment
of ai models into large-scale production
the lab experience is just the beginning
of the ai journey with launchpad it can
make confident software and
infrastructure decisions
and ai practitioners can get access to
an end-to-end gpu-accelerated ai
software stack
learn how to scale and manage your
entire ai life cycle from development to
deployment with ease
Title: NVIDIA mobile tech at Mobile World Congress 2014
Publish_date: 2014-02-25
Length: 132
Views: 10862
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/E8dyMm3-C1s/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: E8dyMm3-C1s

--- Transcript ---

hi i'm doug from nvidia we're here in
barcelona in the nvidia booth at mobile
world congress 2014.
we have a variety of exciting things to
share with you this this year in
barcelona let's start with our new tegra
k1 our new mobile processor the tegra k1
brings the kepler architecture to mobile
for the first time with 192 core fully
programmable gpu this opens up a whole
host of new opportunities for gaming
graphics automotive and other vertical
markets k1 comes in two varieties a
32-bit version and a 64-bit version the
64-bit version features nvidia's own
denver architecture which is the highest
performance arm cpu available featuring
a 7-way superscaler we're showing a
variety of demos that we showcased
earlier this year at the ces show in las
vegas
showing the graphics capability in our
reference tablet one i'd like to draw
your attention to is unreal engine 4
running in mobile for the first time
unreal engine 4 is a watershed moment in
mobile gaming and brings an entirely new
level of experience to mobile gamers
since we are at mobile world congress
it's important to share with you the
latest advancements in our lte
technology we're also showing a few new
devices the first one is from a company
called wiko it's a rapidly growing
french phone oem that's shown that
launching the first
tegra 4i lte phone in europe
the tegra 4i is nvidia's first
integrated lte mobile processor and in
the wyco wax smartphone delivers 2x the
performance of other phones in its class
i'm also very excited to welcome a new
member to the tiger note 7 family the
tegra note 7 lte
the tegrer note 7 lte features nvidia's
own i500 modem which brings lte and hspa
plus connectivity to operators around
the world the other exciting news about
the techno family is it's now going into
new markets for example it'll be
shipping into western europe and brazil
and q2 and that's the big taker news at
mobile world congress 2014. thank you
very much and see you next year
you
Title: GTC Spring 2021 Keynote with NVIDIA CEO Jensen Huang
Publish_date: 2021-04-12
Length: 6504
Views: 2259162
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/eAn_oiZwUXA/hq720.jpg?v=60747ed1
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: eAn_oiZwUXA

--- Transcript ---

I am a creator
Blending art and technology
To immerse our senses
I am a healer
Helping us take the next step
And see what's possible
I am a pioneer
Finding life-saving answers
And pushing the edge to the outer limits.
I am a guardian
Defending our oceans
Magnificent creatures that call them home
I am a protector
Helping the earth breathe easier
And watching over it for generations to come
I am a storyteller
Giving emotion to words
And bringing them to life.
I am even the composer of the music.
I am AI
Brought to life by NVIDIA, deep learning, and brilliant minds everywhere.
There are powerful forces shaping the world's industries.
Accelerated computing that we pioneered has supercharged scientific discovery, while providing the computer industry a path forward.
Artificial intelligence particularly, has seen incredible advances.
With NVIDIA GPUs computers learn, and software writes software  no human can.
The AI software is delivered as a service from the cloud, performing automation at the speed-of-light.
Software is now composed of microservices that scale across the entire data center - treating the data center as a single-unit of computing.
AI and 5G are the ingredients to kick start the 4th industrial revolution
where automation and robotics can be deployed to the far edges of the world.
There is one more miracle we need, the metaverse,  a virtual world that is a digital twin of ours.
Welcome to GTC 2021 - we are going to talk about these dynamics and more.
Let me give you the architecture of my talk.
It's organized in four stacks - this is how we work - as a full-stack computing platform company.
The flow also reflects the waves of AI and how we're expanding the reach of our platform to solve new problems, and to enter new markets.
First is Omniverse - built from the ground-up on NVIDIA's body of work. It is a platform to create and simulate virtual worlds.
We'll feature many applications of Omniverse, like design collaboration, simulation, and future robotic factories.
The second stack is DGX and high-performance data centers.
I'll feature BlueField, new DGXs, new chips, and the new work we're doing in AI, drug discovery, and quantum computing.
Here, I'll also talk about Arm and new Arm partnerships.
The third stack is one of our most important new platforms - NVIDIA EGX with Aerial 5G.
Now, enterprises and industries can do AI and deploy AI-on-5G.
We'll talk about NVIDIA AI and Pre-Trained Models, like Jarvis Conversational AI.
And finally, our work with the auto industry to revolutionize the future of transportation - NVIDIA Drive.
We'll talk about new chips, new platforms and software, and lots of new customers.
Let's get started.
Scientists, researchers, developers, and creators are using NVIDIA to do amazing things.
Your work gets global reach with the installed base of over a billion CUDA GPUs shipped and 250 ExaFLOPS of GPU computing power in the cloud.
Two and a half million developers and 7,500 startups are creating thousands of applications for accelerated computing.
We are thrilled by the growth of the ecosystem we are building together and will continue to put our heart and soul into advancing it.
Building tools for the Da Vincis of our time is our purpose. And in doing so, we also help create the future.
Democratizing high-performance computers is one of NVIDIA's greatest contributions to science.
With just a GeForce, every student can have a supercomputer.
This is how Alex Krizhevsky, Ilya, and Hinton trained AlexNet that caught the world's attention on deep learning.
And with GPUs in supercomputers, we gave scientists a time machine.
A scientist once told me that because of NVIDIA's work, he can do his life's work in his lifetime.
I can't think of a greater purpose.
Let me highlight a few achievements from last year.
NVIDIA is continually optimizing the full stack.
With the chips you have, your software runs faster every year and even faster if you upgrade.
On our gold suite of important science codes, we increased performance 13-fold in the last 5 years, and for some, performance doubled every year.
NAMD molecular dynamics simulator, for example, was re-architected and can now run across multiple GPUs.
Researchers led by Dr. Rommie Amaro at UC San Diego, used this multi-GPU NAMD, running on Oak Ridge Summit supercomputer's 20,000 NVIDIA GPUs,
 to do the largest atomic simulation ever - 305 million atoms.
This work was critical to a better understanding of the COVID-19 virus and accelerated the making of the vaccine.
Dr. Amaro and her collaborators won the Gordon Bell Award for this important work.
I'm very proud to welcome Dr. Amaro and more than 100,000 of you to this year's GTC - our largest-ever by double.
We have some of the greatest computer scientists and researchers of our time speaking here.
3 Turing award winners, 12 Gordon Bell award winners,  9 Kaggle Grand Masters - and even 10 Oscar winners.
We're also delighted to have the brightest minds from industry sharing their discoveries.
Leaders from every field - healthcare, auto, finance, retail, energy, internet services, every major enterprise IT company.
They're bringing you their latest work in COVID research, data science, cybersecurity, new approaches to computer graphics
and the most recent advances in AI and robotics.
In total, 1,600 talks about the most important technologies of our time, from the leaders in the field that are shaping our world.
Welcome to GTC.
Let's start where NVIDIA started...computer graphics.
Computer graphics is the driving force of our technology. Hundreds of millions of gamers and creators each year seek out the best NVIDIA has to offer.
At its core, computer graphics is about simulations - using mathematics and computer science to simulate the interactions of light 
and material, the physics of objects, particles, and waves; and now simulating intelligence and animation.
The science, engineering, and artistry that we dedicate in pursuit of achieving mother nature's physics has led to incredible advances.
And allowed our technology to contribute to advancing the basic sciences, the arts, and the industries.
This last year, we introduced the 2nd generation of RTX - a new rendering approach that fuses rasterization and programmable shading with
hardware-accelerated ray tracing and artificial intelligence. This is the culmination of ten years of research.
RTX has reset computer graphics, giving developers a powerful new tool just as rasterization plateaus.
Let me show you some amazing footage from games in development. The technology and artistry is amazing.
We’re giving the world’s billion gamers an incredible reason to upgrade.
RTX is a reset of computer graphics.
It has enabled us to build Omniverse - a platform for connecting 3D worlds into a shared virtual world.
Ones not unlike the science fiction metaverse first described by Neal Stephenson in his early 1990s novel, Snow Crash, where the metaverse
would be collectives of shared 3D spaces and virtually-enhanced physical spaces that are extensions of the internet.
Pieces of the early-metaverse vision are already here - massive online social games like Fortnite or user-created virtual worlds like Minecraft.
Let me tell you about Omniverse from the perspective of two applications - design collaboration and digital twins.
There are several major parts of the platform.
First, the Omniverse Nucleus, a database engine that connects users
and enables the interchange of 3D assets and scene descriptions.
Once connected, designers doing modeling, layout, shading, animation, lighting, special effects or rendering can collaborate to create a scene.
The Omniverse Nucleus is described with the open standard USD, Universal Scene Description, a fabulous interchange framework invented by Pixar.
Multiple users can connect to Nucleus, transmitting and receiving changes to their world as USD snippets.
The 2nd part of Omniverse is the composition, rendering, and animation engine - the simulation of the virtual world.
Omniverse is a platform built from the ground up to be physically-based. It is fully path-traced. Physics is simulated with NVIDIA PhysX,
materials are simulated with NVIDIA MDL and Omniverse is fully integrated with NVIDIA AI.
Omniverse is cloud-native, multi-GPU scalable and runs on any RTX platform and streams remotely to any device.
The third part is NVIDIA CloudXR, a stargate if you will.
You can teleport into Omniverse with VR and AIs can teleport out of Omniverse with AR.
Omniverse was released to open beta in December.
Let me show you what talented creators are doing.
Creators are doing amazing things with Omniverse.
At Foster and Partners, designers in 17 locations around the world, are designing buildings together in their Omniverse shared virtual space.
ILM is testing Omniverse to bring together internal and external tool pipelines from multiple studios.
Omniverse lets them collaborate, render final shots in real time and create massive virtual sets like holodecks.
Ericsson is using Omniverse to do real-time 5G wave propagation simulation, with many multi-path interferences.
Twin Earth is creating a digital twin of Earth that will run on 20,000 NVIDIA GPUs.
And Activision is using Omniverse to organize their more than 100,000 3D assets into a shared and searchable world.
Bentley is the world's leading infrastructure engineering software company.
Everything that's constructed - roads and bridges, rail and transit systems, airports and seaports -
about 3% of the world's GDP or three-and-a-half trillion dollars a year.
Bentley's software is used to design, model, and simulate the largest infrastructure projects in the world.
90% of the world's top 250 engineering firms use Bentley.
They have a new platform called iTwin - an exciting strategy to use the 3D model, after construction, to monitor and optimize the performance throughout its life.
We are super excited to partner with Bentley to create infrastructure digital twins in Omniverse.
Bentley is the first 3rd-party company to be developing a suite of applications on the Omniverse platform.
This is just an awesome use of Omniverse, a great example of digital twins and Bentley is the perfect partner.
And here's Perry Nightingale from WPP, the largest ad agency in the world, to tell you what they're doing.
WPP is the largest marketing services organization on the planet, and because of that, we're also one of the largest production companies in the world 
That is a major carbon hotspot for us.
We've partnered with NVIDIA, to capture locations virtually and bring them to life with studios in Omniverse.
Over 10 billion points have turned into a giant mesh in Omniverse.
For the first time, we can shoot locations virtually that are as real as the actual places themselves.
Omniverse also changes the way we make work.
A collaborative platform, that means multiple artists, at multiple points in the pipeline, in multiple parts of the world can collaborate on a single scene.
Real time CGI in sustainable studios. Collaboration with Omniverse is the future of film at WPP.
One of the most important features of Omniverse is that it obeys the laws of physics.
Omniverse can simulate particles, fluids, materials, springs and cables.
This is a fundamental capability for robotics.
Once trained, the AI and software can be downloaded from Omniverse.
In this video, you'll see Omniverse's physics simulation with rigid and soft bodies, fluids, and finite element modeling.
And a lot more - enjoy!
Omniverse is a physically-based virtual world where robots can learn to be robots.
They'll come in all sizes and shapes - box movers, pick and place arms, forklifts, cars, trucks.
In the future, a factory will be a robot, orchestrating many robots inside, building cars that are robots themselves.
We can use Omniverse to create a virtual factory, train and simulate the factory and its robotic workers inside.
The AI and software that run the virtual factory are exactly the same as what will run the actual one.
The virtual and physical factories and their robots will operate in a loop.
They are digital twins.
Connecting to ERP systems, simulating the throughput of the factory,
simulating new plant layouts, and becoming the dashboard of the operator - even uplinking into a robot to teleoperate it.
BMW may very well be the world's largest custom-manufacturing company.
BMW produces over 2 million cars a year. In their most advanced factory, a car a minute. Every car is different.
We are working with BMW to create a future factory. Designed completely in digital, simulated from beginning to end in Omniverse, creating a
digital twin, and operating a factory where robots and humans work together.
Let's take a look at the BMW factory.
Welcome to BMW Production, Jensen. I am pleased to show you why BMW sets the standards for innovation and flexibility. Our collaboration
with NVIDIA Omniverse and NVIDIA AI leads into a new era of digitalization of automobile production.
Fantastic to be with you, Milan. I am excited to do this virtual factory visit with you.
We are inside the digital twin of BMW's assembly system, powered by Omniverse. For the first time, we are able to have our entire factory in
simulation. Global teams can collaborate using different software packages like Revit, CATIA, or point clouds to design and plan the factory
in real time 3D. The capability to operate in a perfect simulation revolutionizes BMW's planning processes.
BMW regularly reconfigures its factories to accommodate new vehicle launches. Here we see 2 planning experts located in different parts of
the world, testing a new line design in Omniverse. One of them wormholes into an assembly simulation with a motion capture suit, records
task movements, while the other expert adjusts the line design - in real time.
They work together to optimize the line as well as worker ergonomics and safety.
“Can you tell how far I have to bend down there?”
“First, I’ll get you a taller one”
“Yeah, it’s perfect”
We would like to be able to do this at scale in simulation.
That's exactly why NVIDIA has Digital Human for simulation.
Digital Humans are trained with data from real associates.
You can then use Digital Humans in simulation to test new workflows for worker ergonomics and efficiency.
Now, your factories employ 57,000 people that share workspace with many robots designed to make their jobs easier.
Let's talk about them.
You are right, Jensen.
Robots are crucial for a modern production system. With NVIDIA Isaac robotics platform, BMW is deploying a fleet of
intelligent robots for logistics to improve the material flow in our production.
This agility is necessary since we produce 2.5 million vehicles per year, 99% of them are custom.
Synthetic data generation and domain randomization available in Isaac are key to bootstrapping machine learning.
Isaac Sim generates millions of synthetic images and vary the environment to teach the robots.
Domain randomization can generate an infinite permutation of photorealistic objects, textures, orientations and lighting conditions.
It is ideal for generating ground truth, whether for detection, segmentation, or depth perception.
Let me show you an example of how we can combine it all to operate your factory.
With NVIDIA's Fleet Command, your associates can securely orchestrate robots and other devices in the factory from Mission Control.
They could monitor in real time complex manufacturing cells, update software over the air, launch robot missions and teleoperate.
When a robot needs a helping hand, an alert can be sent to Mission Control and one of your associates can take control to help the robot.
We're in the digital twin of one of your factories, but you have 30 others spread across 15 countries.
The scale of BMW production is impressive, Milan.
Indeed, Jensen, the scale and complexity of our production network requires BMW to constantly innovate.
I am happy about the tight collaboration between our two companies.
NVIDIA Omniverse and NVIDIA AI give us the chance to simulate all 31 factories in our production network.
These new innovations will reduce the planning times, improve flexibility and precision and at the end, produce 30% more efficient planning processes.
Milan, I could not be more proud of the innovations that our collaboration is bringing to the Factories of the Future.
I appreciate you hosting me for a virtual visit of the digital twin of your BMW Production.
It is a work of art!
The ecosystem is really excited about Omniverse.
This open platform, with USD universal 3D interchange, connects them into a large network of users.
We have 12 connectors to major design tools already with another 40 in flight.
Omniverse Connector SDK is available for download now.
You can see that the most important design tools are already signed-up.
Our lighthouse partners are from some of the world's largest industries - Media and Entertainment, Gaming, Architecture, Engineering, and
Construction; Manufacturing, Telecommunications, Infrastructure, and Automotive.
Computer makers worldwide are building NVIDIA-Certified workstations, notebooks, and servers optimized for Omniverse.
And starting this summer, Omniverse will be available for enterprise license.
Omniverse - NVIDIA's platform for creating and simulating shared virtual worlds.
Data center is the new unit of computing.
Cloud computing and AI are driving fundamental changes in the architecture of data centers.
Traditionally, enterprise data centers ran monolithic software packages.
Virtualization started the trend toward software-defined data centers - allowing applications to move about and letting IT manage from a "single-pane of glass".
With virtualization, the compute, networking, storage, and security functions are emulated in software running on the CPU.
Though easier to manage, the added CPU load reduced the data center's capacity to run applications, which is its primary purpose.
This illustration shows the added CPU load in the gold-colored part of the stack.
Cloud computing re-architected data centers again, now to provision services for billions of consumers.
Monolithic applications were disaggregated into smaller microservices that can take advantage of any idle resource.
Equally important, multiple engineering teams can work concurrently using CI/CD methods.
Data center networks became swamped by east-west traffic generated by disaggregated microservices.
CSPs tackled this with Mellanox's high-speed low-latency networking.
Then, deep learning emerged. Magical internet services were rolled out, attracting more customers, and better engagement than ever.
Deep learning is compute-intensive which drove adoption of GPUs.
Nearly overnight, consumer AI services became the biggest users of GPU supercomputing technologies.
Now, adding Zero-Trust security initiatives makes infrastructure software processing one of the largest workloads in the data center.
The answer is a new type of chip for Data Center Infrastructure Processing like NVIDIA's Bluefield DPU.
Let me illustrate this with our own cloud gaming service, GeForce Now, as an example.
GeForce Now is NVIDIA's GeForce-in-the-cloud service.
GeForce Now serves 10 million members in 70 countries. Incredible growth.
GeForce Now is a seriously hard consumer service to deliver.
Everything matters - speed of light, visual quality, frame rate, response, smoothness, start-up time, server cost, and most important of all, security.
We're transitioning GeForce Now to BlueField.
With Bluefield, we can isolate the infrastructure from the game instances, and offload and accelerate the networking, storage, and security.
The GeForce Now infrastructure is costly. With BlueField, we will improve our quality of service and concurrent users at the same time - the
ROI of BlueField is excellent.
I'm thrilled to announce our first data center infrastructure SDK - DOCA 1.0 is available today!
DOCA is our SDK to program BlueField.
There's all kinds of great technology inside.
Deep packet inspection, secure boot, TLS crypto offload, RegEX acceleration, and a very exciting capability - a hardware-based,
real-time clock that can be used for synchronous data centers, 5G, and video broadcast.
We have great partners working with us to optimize leading platforms on BlueField:
Infrastructure software providers, edge and CDN providers, cybersecurity solutions, and storage providers - basically the world's leading
companies in data center infrastructure.
Though we're just getting started with BlueField 2, today we're announcing BlueField 3.
22 billion transistors.
The first 400 Gbps networking chip.
16 Arm CPUs to run the entire virtualization software stack - for instance, running VMware ESX.
BlueField 3 takes security to a whole new level, fully offloading and accelerating IPSEC and TLS cryptography, secret key management, and
regular expression processing.
We are on a pace to introduce a new Bluefield generation every 18 months.
BlueField 3 will do 400 Gbps and be 10x the processing capability of BlueField 2.
And BlueField 4 will do 800 Gbps and add NVIDIA's AI computing technologies to get another 10x boost.
100x in 3 years -- and all of it will be needed.
A simple way to think about this is that 1/3rd of the roughly 30 million data center servers shipped each year are consumed running the
software-defined data center stack.
This workload is increasing much faster than Moore's law.
So, unless we offload and accelerate this workload, data centers will have fewer and fewer CPUs to run applications.
The time for BlueField has come.
At the beginning of the big bang of modern AI, we recognized the need to create a new kind of computer for a new way of developing software.
Software will be written by software running on AI computers.
This new type of computer will need new chips, new system architecture, new ways to network, new software, and new methodologies and tools.
We've invested billions into this intuition, and it has proven helpful to the industry.
It all comes together as DGX - a computer for AI.
We offer DGX as a fully integrated system, as well as offer the components to the industry to create differentiated options.
I am pleased to see so much AI research advancing because of DGX -
top universities, research hospitals, telcos, banks, consumer products companies, car makers, and aerospace companies.
DGX help their AI researchers - whose expertise is rare, scarce, and their work strategic.
It is imperative to make sure they have the right instrument.
Simply, if software is to be written by computers, then companies with the best software engineers will also need the best computers.
We offer several configurations - all software compatible.
The DGX A100 is a building block that contains 5 petaFLOPS of computing and superfast storage and networking to feed it.
DGX Station is an AI data center in-a-box designed for workgroups - plugs into a normal outlet.
And DGX SuperPOD is a fully integrated, fully network-optimized, AI-data-center-as-a-product. SuperPOD is for intensive AI research and development.
NVIDIA's own new supercomputer, called Selene, is 4 SuperPODs.
It is the 5th fastest supercomputer and the fastest industrial supercomputer in the world's Top 500.
We have a new DGX Station 320G.
DGX Station can train large models - 320 gigabytes of super-fast HBM2e connected to 4 A100 GPUs over 8 terabytes per second of memory bandwidth.
8 terabytes transferred in one second.
It would take 40 CPU servers to achieve this memory bandwidth.
DGX Station plugs into a normal wall outlet, like a big gaming rig, consumes just 1,500 watts, and is liquid-cooled to a silent 37 db.
Take a look at the cinematic that our engineers and creative team did.
A CPU cluster of this performance would cost about a million dollars today.
DGX Station is $149,000 - the ideal AI programming companion for every AI researcher.
Today we are also announcing a new DGX SuperPOD.
Three major upgrades:
The new 80 gigabyte A100 which brings the SuperPOD to 90 terabytes of HBM2 memory, with aggregate bandwidth of 2.2 exabytes per second.
It would take 11,000 CPU servers to achieve this bandwidth - about a 250-rack data center - 15 times bigger than the SuperPOD.
Second, SuperPOD has been upgraded with NVIDIA BlueField-2.
SuperPOD is now the world's first cloud-native supercomputer, multi-tenant shareable, with full isolation and bare-metal performance.
And third, we're offering Base Command, the DGX management and orchestration tool used within NVIDIA.
We use Base Command to support thousands of engineers, over two hundred teams, consuming a million-plus GPU-hours a week.
DGX SuperPOD starts at seven million dollars and scales to sixty million dollars for a full system.
Let me highlight 3 great uses of DGX.
Transformers have led to dramatic breakthroughs in Natural Language Processing.
Like RNN and LSTM, Transformers are designed to inference on sequential data.
However, Transformers, more than meets the eyes, are not trained sequentially, but use a mechanism called attention such that Transformers
can be trained in parallel.
This breakthrough reduced training time, which more importantly enabled the training of huge models with a correspondingly enormous amount of data.
Unsupervised learning can now achieve excellent results, but the models are huge.
Google's Transformer was 65 million parameters.
OpenAI's GPT-3 is 175 billion parameters.
That's 3000x times larger in just 3 years.
The applications for GPT-3 are really incredible.
Generate document summaries.
Email phrase completion.
GPT-3 can even generate Javascript and HTML from plain English - essentially telling an AI to write code based on what you want it to do.
Model sizes are growing exponentially - on a pace of doubling every two and half months.
We expect to see multi-trillion-parameter models by next year, and 100 trillion+ parameter models by 2023.
As a very loose comparison, the human brain has roughly 125 trillion synapses.
 So these transformer models are getting quite large. 
Training models of this scale is incredible computer science.
Today, we are announcing NVIDIA Megatron - for training Transformers.
Megatron trains giant Transformer models - it partitions and distributes the model for optimal multi-GPU and multi-node parallelism.
Megatron does fast data loading, micro batching, scheduling and syncing, kernel fusing. It pushes the limits of every NVIDIA invention -
NCCL, NVLink, Infiniband, Tensor Cores.
Even with Megatron, a trillion-parameter model will take about 3-4 months to train on Selene.
So, lots of DGX SuperPODS will be needed around the world
Inferencing giant Transformer models is also a great computer science challenge.
GPT-3 is so big, with so many floating-point operations, that it would take a dual-CPU server over a minute to respond to a single 128-word query.
And GPT-3 is so large that it doesn't fit in GPU memory - so it will have to be distributed.
multi-GPU multi-node inference has never been done.
Today, we're announcing the Megatron Triton Inference Server.
A DGX with Megatron Triton will respond within a second! Not a minute - a second! And for 16 queries at the same time.
DGX is 1000 times faster and opens up many new use-cases, like call-center support, where a one-minute response is effectively unusable.
Naver is Korea's #1 search engine.
They installed a DGX SuperPOD and are running their AI platform CLOVA to train language models for Korean.
I expect many leading service providers around the world to do the same
Use DGX to develop and operate region-specific and industry-specific language services.
NVIDIA Clara Discovery is our suite of acceleration libraries created for computational drug discovery - from imaging, to quantum chemistry,
to gene variant-calling, to using NLP to understand genetics, and using AI to generate new drug compounds.
Today we're announcing four new models available in Clara Discovery:
MegaMolBART is a model for generating biomolecular compounds.
This method has seen recent success with Insilico Medicine using AI to find a new drug in less than two years.
NVIDIA ATAC-seq denoising algorithm for rare and single cell epi-genomics is helping to understand gene expression for individual cells.
AlphaFold1 is a model that can predict the 3D structure of a protein from the amino acid sequence.
GatorTron is the world's largest clinical language model that can read and understand doctors' notes.
GatorTron was developed at UoF, using Megatron, and trained on the DGX SuperPOD gifted
to his alma mater by Chris Malachowsky, who founded NVIDIA with Curtis and me.
Oxford Nanopore is the 3rd generation genomics sequencing technology capable of ultra high throughput in digitizing biology -
1/5 of the SARS-CoV-2 virus genomes in the global database were generated on Oxford Nanopore.
Last year, Oxford Nanopore developed a diagnostic test for COVID-19 called LamPORE, which is used by NHS.
Oxford Nanopore is GPU-accelerated throughout.
DNA samples pass through nanopores and the current signal is fed into an AI model, like speech recognition, but trained to recognize genetic code. 
Another model called Medaka reads the code and detects genetic variants.
Both models were trained on DGX SuperPOD.
These new deep learning algorithms achieve 99.9% detection accuracy of single nucleotide variants - this is the gold standard of human sequencing.
Pharma is a 1.3 trillion-dollar industry where a new drug can take 10+ years and fails 90% of the time.
Schrodinger is the leading physics-based and machine learning computational platform for drug discovery and material science.
Schrodinger is already a heavy user of NVIDIA GPUs, recently entering into an agreement to use hundreds of millions of NVIDIA GPU hours on the Google cloud.
Some customers can't use the cloud, so today we are announcing a partnership to accelerate Schrodinger's drug discovery workflow with NVIDIA
Clara Discovery libraries and NVIDIA DGX.
The world's top 20 pharmas use Schrodinger today. Their researchers are going to see a giant boost in productivity.
Recursion is a biotech company using leading-edge computer science to decode biology to industrialize drug discovery.
The Recursion Operating System is built on NVIDIA DGX SuperPOD for generating, analyzing and gaining insight from massive biological and chemical datasets.
They call their SuperPOD the BioHive-1 - it's the most powerful computer at any pharma today.
Using deep learning on DGX, Recursion is classifying cell responses after exposure to small molecule drugs.
Quantum computing is a field of physics that studies the use of natural quantum behavior - superposition, entanglement, and interference - to build a computer.
The computation is performed using quantum circuits that operate on quantum bits - called qubits.
Qubits can be 0 or 1, like a classical computing bit, but also in superposition - meaning they exist simultaneously in both states.
The qubits can be entangled where the behavior of one can affect or control the behavior of others.
Adding and entangling more qubits lets quantum computers calculate exponentially more information.
There is a large community around the world doing research in quantum computers and algorithms.
Well over 50 teams in industry, academia, and national labs are researching the field.
We're working with many of them.
Quantum computing can solve Exponential Order Complexity problems, like factoring large numbers for cryptography, simulating atoms and
molecules for drug discovery, finding shortest path optimizations, like the traveling salesman problem,
The limiter in quantum computing is decoherence, falling out of quantum states, caused by the tiniest of background noise
So error correction is essential.
It is estimated that to solve meaningful problems, several million physical qubits will be required to sufficiently error correct.
The research community is making fast progress, doubling physical Qubits each year, so likely achieving the milestone by 2035 to 2040.
Well within my career horizon.
In the meantime, our mission is to help the community research the computer of tomorrow with the fastest computer of today.
Today, we're announcing cuQuantum - an acceleration library designed for simulating quantum circuits
for both Tensor Network Solvers and State Vector Solvers.
It is optimized to scale to large GPU memories, multiple GPUs, and multiple DGX nodes.
The speed-up of cuQuantum on DGX is excellent. Running the cuQuantum Benchmark, state vector simulation takes 10 days on a dual-CPU server
but only 2 hours on a DGX A100. cuQuantum on DGX can productively simulate 10's of qubits.
And Caltech, using Contengra/Quimb, simulated the Sycamore quantum circuit at depth 20 in record time using cuQuantum on NVIDIA's Selene supercomputer.
 What would have taken years on CPUs can now run in a few days on cuQuantum and DGX.
cuQuantum will accelerate quantum circuit simulators so researchers can design better quantum computers and verify their results, architect
hybrid quantum-classical systems -and discover more Quantum-Optimal algorithms like Shor's and Grover's.
cuQuantum on DGX is going to give the quantum community a huge boost.
I'm hoping cuQuantum will do for quantum computing what cuDNN did for deep learning.
Modern data centers host diverse applications that require varying system architectures.
Enterprise servers are optimized for a balance of strong single-threaded performance and a nominal number of cores.
Hyperscale servers, optimized for microservice containers, are designed for a high number of cores, low cost, and great energy-efficiency.
Storage servers are optimized for large number of cores and high IO throughput.
Deep learning training servers are built like supercomputers - with the largest number of fast CPU cores, the fastest memory, 
the fastest IO, and high-speed links to connect the GPUs.
Deep learning inference servers are optimized for energy-efficiency and best ability to process a large number of models concurrently.
The genius of the x86 server architecture is the ability to do a good job using varying configurations of the CPU, memory, PCI express, and
peripherals to serve all of these applications.
Yet processing large amounts of data remains a challenge for computer systems today - this is particularly true for AI models like
transformers and recommender systems.
Let me illustrate the bottleneck with half of a DGX.
Each Ampere GPU is connected to 80GB of super fast memory running at 2 TB/sec.
Together, the 4 Amperes process 320 GB at 8 Terabytes per second.
Contrast that with CPU memory, which is 1TB large, but only 0.2 Terabytes per second.
The CPU memory is 3 times larger but 40 times slower than the GPU.
We would love to utilize the full 1,320 GB of memory of this node to train AI models.
So, why not something like this?
Make faster CPU memories, connect 4 channels to the CPU, a dedicated channel to feed each GPU.
Even if a package can be made, PCIe is now the bottleneck.
We can surely use NVLINK. NVLINK is fast enough. But no x86 CPU has NVLINK, not to mention 4 NVLINKS.
Today, we're announcing our first data center CPU, Project Grace, named after Grace Hopper, a computer scientist and U.S. Navy rear Admiral,
who in the '50s pioneered computer programming.
Grace is Arm-based and purpose-built for accelerated computing applications of large amounts of data - such as AI.
Grace highlights the beauty of Arm.
Their IP model allowed us to create the optimal CPU for this application which achieves x-factors speed-up.
The Arm core in Grace is a next generation off-the-shelf IP for servers. Each CPU will deliver over 300 SpecInt with a total of over 2,400
SPECint_rate CPU performance for an 8-GPU DGX.
For comparison, todays DGX, the highest performance computer in the world today is 450 SPECint_rate.
2400 SPECint_rate with Grace versus 450 SPECint_rate today.
So look at this again - Before, After, Before, After.
Amazing increase in system and memory bandwidth.
Today, we're introducing a new kind of computer.
The basic building block of the modern data center.
Here it is.
What I'm about to show you brings together the latest GPU accelerated computing, Mellanox high performance networking, and something brand new.
The final piece of the puzzle.
The world's first CPU designed for terabyte-scale accelerated computing... her secret codename - GRACE.
This powerful, Arm-based CPU gives us the third foundational technology for computing, and the ability to rearchitect every aspect of the data center for AI.
We're thrilled to announce the Swiss National Supercomputing Center will build a supercomputer powered by Grace and our next generation GPU.
This new supercomputer, called Alps, will be 20 exaflops for AI, 10 times faster than the world's fastest supercomputer today.
Alps will be used to do whole-earth-scale weather and climate simulation, quantum chemistry and quantum physics for the Large Hadron Collider.
Alps will be built by HPE and is come on-line in 2023.
We're thrilled by the enthusiasm of the supercomputing community, welcoming us to make Arm a top-notch scientific computing platform.
Our data center roadmap is now a rhythm consisting of 3-chips: CPU, GPU, and DPU.
Each chip architecture has a two-year rhythm with likely a kicker in between.
One year will focus on x86 platforms.
One year will focus on Arm platforms.
Every year will see new exciting products from us.
The NVIDIA architecture and platforms will support x86 and Arm - whatever customers and markets prefer.
Three chips. Yearly Leaps. One Architecture.
Arm is the most popular CPU in the world.
For good reason - its super energy-efficient. Its open licensing model inspires a world of innovators to create products around it.
Arm is used broadly in mobile and embedded today.
For other markets like the cloud, enterprise and edge data centers, supercomputing and PCs.
Arm is just starting and has great growth opportunities.
Each market has different applications and has unique systems, software, peripherals, and ecosystems.
For the markets we serve, we can accelerate Arm's adoption.
Let's start with the big one - Cloud.
One of the earliest designers of Arm CPUs for data centers is AWS - its Graviton CPUs are extremely impressive.
Today, we're announcing NVIDIA and AWS are partnering to bring Graviton2 and NVIDIA GPUs together.
This partnership brings Arm into the most demanding cloud workloads - AI and cloud gaming.
Mobile gaming is growing fast and is the primary form of gaming in some markets.
With AWS-designed Graviton2, users can stream Arm-based applications and Android games straight from AWS.
It's expected later this year.
We are announcing a partnership with Ampere Computing to create a scientific and cloud computing SDK and reference system.
Ampere Computing's Altra CPU is excellent - 80 cores, 285 SPECint17, right up there with the highest performance x86.
We are seeing excellent reception at supercomputing centers around the world and at Android cloud gaming services.
We are also announcing a partnership with Marvell to create an edge and enterprise computing SDK and reference system.
Marvell Octeon excels at IO, storage and 5G processing. This system is ideal for hyperconverged edge servers.
We're announcing a partnership with Mediatek to create a reference system and SDK for Chrome OS and Linux PC's.
Mediatek is the world's largest SOC maker.
Combining NVIDIA GPUs and Mediatek SOCs will make excellent PCs and notebooks.
AI, computers automating intelligence, is the most powerful technology force of our time.
We see AI in four waves.
The first wave was to reinvent computing for this new way of doing software - we're all in and have been driving this for nearly 10 years.
The first adopters of AI were the internet companies - they have excellent computer scientists, large computing infrastructures, and the
ability to collect a lot of training data.
We are now at the beginning of the next wave.
The next wave is enterprise and the industrial edge, where AI can revolutionize the world's largest industries. From manufacturing,
logistics, agriculture, healthcare, financial services, and transportation.
There are many challenges to overcome, one of which is connectivity, which 5G will solve.
And then autonomous systems. Self-driving cars are an excellent example. But everything that moves will eventually be autonomous.
The industrial edge and autonomous systems are the most challenging, but also the largest opportunities for AI to make an impact.
Trillion dollar industries can soon apply AI to improve productivity, and invent new products, services and business models.
We have to make AI easier to use - turn AI from computer science to computer products.
We're building the new computing platform for this fundamentally new software approach - the computer for the age of AI.
AI is not just about an algorithm - building and operating AI is a fundamental change in every aspect of software - Andrej Karpathy rightly called it Software 2.0.
Machine learning, at the highest level, is a continuous learning system that starts with data scientists developing data strategies 
and engineering predictive features - this data is the digital life experience of a company.
Training involves inventing or adapting an AI model that learns to make the desired predictions.
Simulation and validation test the AI application for accuracy, generalization, and potential bias.
And finally, orchestrating a fleet of computers, whether in your data center or at the edge in the warehouse, farms, or wireless base stations.
NVIDIA created the chips, systems, and libraries needed for end-to-end machine learning - for example, technologies like Tensor Core GPUs,
NVLINK, DGX, cuDNN, RAPIDS, NCCL, GPU Direct, DOCA, and so much more.
We call the platform NVIDIA AI.
NVIDIA AI libraries accelerate every step, from data processing to fleet orchestration.
NVIDIA AI is integrated into all of the industry's popular tools and workflows.
NVIDIA AI is in every cloud, used by the world's largest companies, and by over 7,500 AI startups around the world.
And NVIDIA AI runs on any system that includes NVIDIA GPUs, from PCs and laptops, to workstations, to supercomputers, in any cloud, to our
$99 Jetson robot computer.
One segment of computing we've not served is enterprise computing.
70% of the world's enterprises run VMware, as we do at NVIDIA.
VMware was created to run many applications on one virtualized machine.
AI, on the other hand, runs a single job, bare-metal, on multiple GPUs and often multiple nodes.
All of the NVIDIA optimizations for compute and data transfer are now plumbed through the VMware stack so AI workloads can be distributed to
multiple systems and achieve bare-metal performance.
The VMware stack is also offloaded and accelerated on NVIDIA BlueField.
NVIDIA AI now runs in its full glory on VMware, which means everything that has been accelerated by NVIDIA AI now runs great on VMware.
AI applications can be deployed and orchestrated with Kubernetes running on VMware Tanzu.
We call this platform NVIDIA EGX for Enterprise.
The enterprise IT ecosystem is thrilled - finally the 300,000 VMware enterprise customers can easily build an AI computing infrastructure
that seamlessly integrates into their existing environment.
In total, over 50 servers from the world's top server makers will be certified for NVIDIA EGX Enterprise.
BlueField 2 offloads and accelerates the VMware stack and does the networking for distributed computing.
Enterprise can choose big or small GPUs for heavy-compute or heavy-graphics workloads like Omniverse, or mix and match.
All run NVIDIA AI.
Enterprise companies make up the world's largest industries and they operate at the edge - in hospitals, factories, plants, warehouses,
stores, farms, cities and roads - far from data centers.
The missing link is 5G.
Consumer 5G is great, but Private 5G is revolutionary.
Today, we're announcing the Aerial A100 - bringing together 5G and AI into a new type of computing platform designed for the edge.
Aerial A100 integrates the Ampere GPU and BlueField DPU into one card - this is the most advanced PCI express card ever created.
So, it's not a surprise that Aerial A100 in an EGX system will be a complete 5G base station.
Aerial A100 delivers up to full 20 Gbps and can process up to 9 100Mhz massive MIMO for 64T64R - or 64 transmit and 64 receive antenna arrays
- state of the art capabilities.
Aerial A100 is software-defined, with accelerated features like PHY, Virtual Network Functions, network acceleration, packet pacing, and line-rate cryptography.
Our partners ERICSSON, Fujitsu, Mavenir, Altran, and Radisys will build their total 5G solutions on top of the Aerial library.
NVIDIA EGX server with Aerial A100 is the first 5G base-station that is also a cloud-native, secure, AI edge data center.
We have brought the power of the cloud to the 5G edge.
Aerial also extends the power of 5G into the cloud.
Today, we are excited to announce that Google will support NVIDIA Aerial in the GCP cloud.
I have an important new platform to tell you about.
The rise of microservice-based applications and hybrid-cloud has exposed billions of connections in a data center to potential attack.
Modern Zero-Trust security models assume the intruder is already inside and all container-to-container communications should be inspected, even within a node. 
This is not possible today.
The CPU load of monitoring every piece of traffic is simply too great.
Today, we are announcing NVIDIA Morpheus - a data center security platform for real-time all-packet inspection.
Morpheus is built on NVIDIA AI, NVIDIA BlueField, Net-Q network telemetry software, and EGX.
We're working to create solutions with industry leaders in data center security - Fortinet, Red Hat, Cloudflare, Splunk, F5, and Aria
Cybersecurity. And early customers - Booz Allen Hamilton, Best Buy, and of course, our own team at NVIDIA.
Let me show you how we're using Morpheus at NVIDIA.
It starts with a network.
Here we see a representation of a network, where dots are servers and lines (the edges) are packets flowing between those servers.
Except in this network, Morpheus is deployed. This enables AI inferencing across your entire network, including east/west traffic. The
particular model being used here has been trained to identify sensitive information - AWS credentials, GitHub credentials, private keys,
passwords. If observed in the packet, these would appear as red lines, and we don't see any of that.
Uh oh, what happened.
An updated configuration was deployed to a critical business app on this server.
This update accidentally removed encryption, and now everything that communicates with that app
sends and receives sensitive credentials in the clear.
This can quickly impact additional servers. This translates to continuing exposure on the network. The AI model in Morpheus is searching
through every packet for any of these credentials, continually flagging when it encounters such data. And rather than using pattern
matching, this is done with a deep neural network - trained to generalize and identify patterns beyond static rule sets.
Notice all of the individual lines. It's easy to see how quickly a human could be overwhelmed by the vast amount of data coming in.
Scrolling through the raw data gives a sense of the massive scale and complexity that is involved.
With Morpheus, we immediately see the lines that represent leaked sensitive information.
By hovering over one of those red lines, we show complete info about the credential, making it easy to triage and remediate.
But what happens when this remediation is necessary? Morpheus enables cyber applications to integrate and collect information for automated
incident management and action prioritization.
Originating servers, destination servers, actual exposed credentials, and even the raw data is available. This speeds recovery and informs
which keys were compromised and need be rotated.
With Morpheus, the chaos becomes manageable.
The IT ecosystem has been hungry for an AI computing platform that is enterprise and edge ready.
NVIDIA AI on EGX with Aerial 5G is the foundation of what the IT ecosystem has been waiting for.
We are supported by leaders from all across the IT industry - from systems, infrastructure software, storage and security, data analytics,
industrial edge solutions, manufacturing design and automation, to 5G infrastructure.
To complete our enterprise offering, we now have NVIDIA AI Enterprise software so businesses can get direct-line support from NVIDIA.
NVIDIA AI Enterprise is optimized and certified for VMware, and offers services and support needed by mission-critical enterprises.
Deep learning has unquestionably revolutionized computing.
Researchers continue to innovate at lightspeed with new models and new variants.
We're creating new systems to expand AI into new segments - like Arm, enterprise, or 5G edge.
We're also using these systems to do basic research in AI and building new AI products and services.
Let me show you some of our work in AI, basic and applied research.
DLSS: Deep Learning Super Sampling.
StyleGAN: AI high resolution image generator.
GANcraft: a neural rendering engine, turning Minecraft into realistic 3D.
GANverse3D: turns photographs into animatable 3D models.
Face Vid2Vid: a talking-head rendering engine that can reduce streaming bandwidth by 10x, while re-posing the head and eyes.
Sim2Real: a quadruped trained in Omniverse, and the AI can run in a real robot - digital twins.
SimNet: a Physics Informed Neural Network solver that can simulate large-scale multi-physics.
BioMegatron: the largest biomedical language model ever trained.
3DGT: Omniverse synthetic data generation.
and OrbNet: a machine learning quantum solver for quantum chemistry.
This is just a small sampling of the AI work we're doing at NVIDIA.
We're building AIs to use in our products and platforms.
We're also packaging up the models to be easily integrated into your applications.
These are essentially no-coding open-source applications that you can modify.
Now, we're offering NGC pre-trained models, that you can plug into these applications, or ones you develop.
These pre-trained models are production quality, trained by experts, and will continue to benefit from refinement.
There are new credentials to tell you about the models' development, testing, and use.
And each comes with a reference application sample code.
NVIDIA pre-trained models are state-of-the-art and meticulously trained, but there is infinite diversity of application domains, environments, and specializations.
No one has all the data - sometimes it's rare, sometimes they're trade-secrets.
So we created technology for you to fine-tune and adapt NVIDIA pre-trained models for your applications.
TAO applies transfer learning on your data to fine-tune our models with your data.
TAO has an excellent federated learning system to let multiple parties collectively train a shared model while protecting data privacy.
NVIDIA federated learning is a big deal - researchers at different hospitals can collaborate on one AI model while keeping their data
separate to protect patient privacy.
TAO uses NVIDIA's great TensorRT to optimize the model for the target GPU system.
With NVIDIA pre-trained models and TAO, something previously impossible for many, can now be done in hours.
Fleet Command is a cloud-native platform for securely operating and orchestrating AI across a distributed fleet of computers - it was purpose-built for operating AI at the edge.
Fleet Command running on Certified EGX systems will feature secure boot, attestation, uplink and downlink, to a confidential enclave.
From any cloud or on-prem, you can monitor the health of your fleet.
Let's take a look at how one customer is using NGC pre-trained models and TAO to fine-tune models
and run in our Metropolis smart city application, orchestrated by Fleet Command.
No two industrial environments are the same. And conditions routinely change
Adapting, managing, and operationalizing AI-enabled applications at the edge for unique, specific sites can be incredibly challenging -
requiring a lot of data and time for model training.
Here, we're building applications for a factory with multiple problems to solve
Understanding how a factory floor space is used over time
Ensuring worker safety, with constantly evolving machinery and risk factors
Inspecting products on the factory line, where operations change routinely
We start with a Metropolis application running 3 pretrained models from NGC. We are up and running in minutes.
But since the environment is visually quite different from the data used to train this model, the video analytics accuracy at this specific
site isn't great. People are not being accurately recognized and tracked and defects are being missed.
Now let's use NVIDIA TAO to solve for this.
With this simple UI, we re-train and adapt our pre-trained models with labelled data from the specific environment we're deploying in.
We select our datasets.
Each is a few 100 images, as opposed to millions of labelled images required if we were to train this from scratch.
With NVIDIA Tao we go from 65% to over 90% accuracy.
And through pruning & quantization, compute complexity is reduced by 2x - with no reduction in accuracy
and real-time performance is maintained.
In this example, the result is 3 models specifically trained for our factory - all in just minutes
With one-click, we update and deploy these optimized models onto NVIDIA Certified servers with Fleet Command, 
seamlessly and securely from the cloud.
From secure boot to our confidential AI Enclave in GPUs, application data and critical intellectual property remains safe.
AI accuracy, system performance and health can be monitored remotely.
This establishes a feedback loop for continuous application enhancements.
The factory now has an end-to-end framework to adapt to changing conditions - often and easily.
We make it easy to adapt and optimize NGC pretrained models with NVIDIA TAO and deploy and orchestrate applications with Fleet Command.
We have all kinds of computer vision, speech, language and robotics models, and more coming all the time.
Some are the work of our genetics and medical imaging team.
For example, this model that predicts supplemental oxygen needs from xX-ray and electronic health records.
This is a collaboration of 20 hospitals across 8 countries and 5 continents for COVID-19.
Federated learning and NVIDIA TAO was essential to make this possible.
The world needs a state-of-the-art conversational AI that can be customized and processed anywhere.
Today, we're announcing the availability of NVIDIA Jarvis - a state-of-the-art deep learning AI for speech recognition, language understanding, translations, and speech.
End-to-end GPU-accelerated, Jarvis interacts in about 100 milliseconds - listening, understanding, and responding faster than the blink of a human eye. 
We trained Jarvis for several million GPU-hours, on over 1 billion pages of text,
and sixty thousand hours of speech in different languages, accents, environments and lingos.
Out of the box, Jarvis achieves a world-class 90% recognition accuracy.
You can get even better for your application by refining with your own data using NVIDIA TAO.
Jarvis supports 5 languages today - English, Japanese, Spanish, German, French, and Russian.
Jarvis does excellent translation, in the Bilingual Evaluation Understudy benchmark, Jarvis scores 40 points for English-to-Japanese, and 50 points for English-to-Spanish.
40 is high quality and state-of-the-art. 50 is considered fluent translation.
Jarvis can be customized for domain jargon. We've trained Jarvis for technical and healthcare scenarios. It's easy with TAO.
And Jarvis now speaks with expression and emotion that you can control - no more mechanical talk.
And lastly, this is a big one - Jarvis can be deployed in the cloud, on EGX in your data center, at the edge in a shop, warehouse, or
factory running on EGX Aerial, or inside a delivery robot running on a Jetson computer.
Jarvis early access program began last May and our Conversational AI software has been downloaded 45,000 times.
Among early users is T-Mobile, the U.S. telecom giant. They're using Jarvis to offer exceptional customer service that demands high-quality
and low-latency needed in real-time speech recognition.
We're announcing a partnership with Mozilla Common Voice, one of the world's largest multi-language voice datasets, and its openly available to all 
NVIDIA will use our DGXs to process and train Jarvis with the dataset of 150,000 speakers in 65 languages and offer Jarvis back to the community for free.
So go to Mozilla Common Voice and make some recordings!
Let's make universal translation possible and help people around the world understand each other. 
Now let me show you Jarvis.
The first part of Jarvis is speech recognition.
Jarvis is over 90% accurate out-of-the-box - that's world-class.
And you can still use TAO to make it even better for your application, like customizing for healthcare jargon.
Chest x-ray shows left retrocardiac opacity, and this may be due to atelectasis, aspiration, or early pneumonia.
I have no idea what I said but Jarvis recognized it perfectly.
Jarvis translation now supports five languages.
Let's do Japanese.
"Excuse me, I'm looking for the famous Jangara Ramen shop. It should be nearby, but I don't see it on my map. Can you show me the way? I'm very hungry."
That's great. Excellent accuracy. I think. Instantaneous response.
You can do German, French, and Russian--with more languages on the way.
Jarvis also speaks with feelings.
Let's try this - "The more you buy, the more you save!"
"The more you buy, the more you save"
I think we are going to need more enthusiasm.
"The more you buy, the more you save!"
NVIDIA Jarvis - state-of-the-art deep learning conversational AI, interactive response, 5 languages, customize with TAO,
and deploy from cloud, to edge, to autonomous systems.
Recommender systems are the most important machine learning pipeline in the world today.
It is the engine for search, ads, on-line shopping, music, books, movies, user-generated content, news.
Recommender systems predict your needs and preferences from past interactions with you, your explicit preferences
and learned preferences using methods called collaborative and content filtering.
Trillions of items to be recommended to billions of people - the problem space is quite large.
We would like to productize a state-of-the-art recommender system so that all companies can benefit from the transformative capabilities of this AI. 
We built an open-source recommender system framework, called Merlin, which simplifies an end-to-end workflow from ETL, to training, to validation, to inference.
It is architected to scale as your dataset grows.
And if you already have a ton of data, it is the fastest recommender system ever built.
In our benchmarks, we achieved speed-ups of 10-50x for ETL, 2-10x for training and 3-100 times for inference depending on the exact setup.
Merlin is now available on NGC.
Our vision for Maxine is to eventually be your avatar in virtual worlds created in Omniverse - to enable virtual presence.
The technologies of virtual presence can benefit video conferencing today.
Alex is going to tell you all about it.
Hi everyone, I'm Alex and I'm the Product Manager for NVIDIA Maxine.
These days, we're spending so much time doing video conferencing. Don't you want a much better video communication experience?
Today, I am so thrilled today to share with you the NVIDIA Maxine, a suite of AI technologies that can improve the video conferencing experience for everyone.
When combined with NVIDIA Jarvis, Maxine offers the most accurate speech to text .
See, It's now transcribing everything I'm saying, all in real time.
And in addition, with the help of Jarvis, Maxine can also translate what I am saying into multiple languages. 
That would make international meetings so much easier.
Another great feature I'd love to share with you is Maxine's Eye Contact feature.
Often times when I am presenting, I am not looking at the camera
When I turn on Maxine's eye contact feature, it corrects the position of my eyes so that I'm looking back into the camera again. 
Now, I can make eye contact with everyone else.
The meeting experience just gets so much more engaging!
Last but definitely not least, Maxine can even improve the video quality when bandwidth isn't sufficient.
Now let's take a look at my video quality when bandwidth drops to as low as 50 kbps.
Definitely not great.
Maxine's AI face codec can improve the quality of my video even when bandwidth is as low as 50 kbps.
The most powerful part of Maxine is that all of these amazing AI features can run simultaneously together and all in real time. 
Or you can just pick and choose a few. It's just that flexible!
Now before I go let me turn off all the Maxine features so you can see what I looked like without Maxine's help. Not ideal.
Now let's turn back on all the Maxine features.
See - so much better thanks to NVIDIA Maxine!
NGC has pre-trained models.
TAO lets you fine-tune and adapt models to your applications.
Fleet Command deploys and orchestrates your models.
The final piece is the inference server - to infer insight from the continuous streams of data coming into your EGX servers or your cloud instance. 
NVIDIA Triton is our inference server.
Triton is a model scheduling and dispatch engine that can handle just about anything you throw at it:
Any AI model that runs on cuDNN, so basically every AI model.
From any framework - TensorFlow, Pytorch, ONNX, OpenVINO, TensorRT, or custom C++/python backends.
Triton schedules on the multiple generations of NVIDIA GPUs and x86 CPUs.
Triton maximizes the CPU and GPU utilization.
Triton scales with Kubernetes and handles live updates.
Triton is fantastic for everything from image to speech recognition, from recommenders to language understanding.
But let me show you something really hard - generating biomolecules for drug discovery.
In drug discovery, it all comes down to finding the right molecule, the right shape, the right interactions with a protein - the right pharmaco-kinetic properties.
Working with scientists from Astra-Zeneca, NVIDIA researchers trained a language model to understand SMILES - the language of chemical structures. 
We developed the model with Megatron and trained it on a SuperPOD.
Then created a generative model that reads the structure of successful chemical drug-compounds to then generate potentially effective novel-compounds. 
Now we can use AI to generate candidate compounds that can then be further refined with physics-based simulations, like docking or Schrödinger's FEP+.
Generating with a CPU is slow - it takes almost 8 seconds to generate one molecule.
On a single A100 with Triton, it takes about 0.3 seconds - 32X faster!
Using Triton Inference Server, we can scale this up to a SuperPOD and generate thousands of molecules per second.
AI and simulation is going to revolutionize drug discovery.
We provide these AI capabilities to our ecosystem of millions of developers around the world.
Thousands of companies are building their most important services on NVIDIA AI.
BestBuy is using Morpheus as the foundation of AI-based anomaly detection in their network.
GE Healthcare has built an echocardiogram that uses TensorRT to quickly identify different views of the wall motion of the heart and select the best ones for analysis.
Spotify has over 4 billion playlists. They're using RAPIDS to analyze models more efficiently, which lets them refresh personalized playlists more often.
This keeps their content fresh with the latest trends. 
WeChat is using TensorRT to achieve 8msmillisecond latency even when using the state-of-the-art natural language understanding models like BERT.
This gives them more accuracy and relevant results when maintaining real-time response.
It's great to see NVIDIA AI used to build these amazing AI services.
NVIDIA AI on EGX with Aerial 5G the AI computing platform for the enterprise and Edge - the next wave of AI.
DRIVE AV is our AV platform and service.
AV is the most intense machine learning and robotics challenge - one of the hardest, but also the greatest impact.
We're building an AV platform end-to-end - from the AV chips and computers, sensor architecture, data processing, mapping, developing the
driving software, creating the simulator and digital twin, fleet command operations, to road testing.
All of it to the highest functional safety and cybersecurity standards. safety and cybersecurity standards.
None of these things are openly available, so we build them.
We build in the modules so our customers and partners in the $10 trillion transportation industry can leverage the parts they need.
Meanwhile, we build our end-to-end AV service in partnership with Mercedes.
AV computing demand is skyrocketing.
Orin is a giant leap over Xavier.
The more developers learn about AV, the more advanced the algorithms become, and the greater the computing demand.
More computation capacity gives teams faster iteration and speed to market - leading some to call TOPS the new horsepower.
And many are realizing that the reserved computing capacity today is an opportunity to offer new services tomorrow.
NVIDIA DRIVE is an open programmable platform that AI engineers all over the world are familiar with.
New AI technology is being invented on NVIDIA AI constantly - these inventions will be tomorrow's new services.
Orin is an amazing AV computer.
Today, we are announcing that Orin was also designed to be the central computer of the car.
Orin will process in one central computer the cluster, infotainment, passenger interaction AI, and very importantly, the confidence view 
or the perception world model.
The confidence view is what the car actually perceives around it and construct it into a 3D surround model - this is what is in the mind of the autopilot AI.
It is important that the car shows us that its surround perception is accurate, so that we have confidence in its driving.
In time, rear view mirrors will be replaced by the surround 3D perception.
The future is one central computer - 4 domains, virtualized and isolated, architected for functional safety and security, software-defined
and upgradeable for the life of the car - and super smart AI and beautiful graphics.
Many car companies have already adopted Orin and so it would help them tremendously to also leverage our full AV development platform.
Today, we are announcing NVIDIA’s 8th generation Hyperion car platform - including reference sensors, AV and central computers, the 3D ground
truth data recorder, networking, and all of the essential software.
Hyperion 8 is compatible with the NVIDIA DRIVE AV stack, so easy to adopt and integrate elements of our stack.
Hyperion 8 is a fully functional, going-to-production, open AV platform for the multi-trillion-dollar transportation ecosystem.
Orin will be in production in 2022.
Meanwhile, our next generation is already in full gear and will be yet another giant leap.
Today, we are announcing NVIDIA DRIVE Atlan.
DRIVE Atlan will be 1,000 TOPS on one chip - more than the total compute in most Level 5 robotaxis today.
To achieve higher autonomy in more conditions, sensor resolutions will continue to increase. There will be more of them. AI models will get
more sophisticated. There will be more redundancy and safety functionality. We're going to need all of the computing we can get.
Atlan is a technical marvel - fusing all of NVIDIA's technologies in AI, auto, robotics, safety, and Bluefield secure data center technologies.
AV and software must be planned as multi-generational investments. The software you invested billions in today must carry over to the entire fleet and to future generations.
The DRIVE Atlan - The Next Level - One Architecture.
The car industry has become a technology industry.
Future cars are going to be completely programmable computers and business models are going to be software driven.
Car companies will offer software services for the life of the car.
The new tech mindset sees the car not just as a product to sell, but as an installed base of 10's or 100's of millions to build upon,
creating billions in services and opportunities.
The world's big brands have giant opportunities.
Step one was going electric.
Now the big brands are making their new fleets autonomous and programmable.
Orin will be powering many next generation EVs.
And with Mercedes, we are building the end-to-end system.
The world moves 10 trillion miles a year.
If only a fraction of these miles were served by robotaxis, the opportunity is giant.
We expect robotaxis to start ramping in the next couple years.
These services will also be platforms for all kinds of new services to be built upon - like last mile delivery.
The Internet moves electrons, trucks move atoms.
The rise of e-commerce is putting intense pressure on this system - one click and a new TV shows up at your house, another click and a
cheeseburger shows up.
This trend will only go up.
The U.S. will be short by 100K truckers by 2023.
The EU is short 150K truckers already.
China is short 4 million drivers.
So ideas like driverless trucks from hub-to-hub or driverless trucks inside a port or warehouse campus are excellent near-term ways to augment with automation.
We started my talk with Omniverse, we'll close with Omniverse.
You can see how important it is to our work, to robotics, to anyone building AIs that interact with the physical world, to have a physically based simulator, or digital twin.
In the case of Isaac, the digital twin is the factory in Omniverse.
In the case of DRIVE, the digital twin is the collective memories of the fleet captured in Omniverse.
The DRIVE Digital Twin is used throughout the development - it's used for HD map reconstruction, synthetic data generation so we can
bootstrap training new models, new scenario simulations, hardware-in-the-loop simulations, release validation, for replaying unfamiliar
scenarios experienced by a car, or a teleoperator up-linking into a car to remotely pilot.
The DRIVE Digital Twin in Omniverse is a virtual world that every car in the fleet is connected to.
Today, we're announcing that DRIVE Sim, the engine of DRIVE Digital Twin, will be available for the community this summer.
Let me show you what DRIVE AV and DRIVE Sim can do - this is a mountain of technology.
Enjoy.
What a GTC! It's incredible to think how much was done in this last year.
I spoke about several things:
NVIDIA is a full-stack computing platform.
We're building virtual worlds with NVIDIA Omniverse - a miraculous platform that will help build the next wave of AI for robotics and self-driving cars, 
We announced new DGX systems and new software. Megatron for giant Transformers, Clara for drug discovery, and cuQuantum for quantum computing. 
NVIDIA is now a 3-chip company. With the addition of Grace CPU, designed for a specialized segment of computing focused on giant-scale AI and HPC. 
And for data center infrastructure processing, we announced BlueField-3 and DOCA 1.0.
NVIDIA is expanding the reach of AI.
We announced an important new platform, NVIDIA EGX with Aerial 5G, to make AI accessible to all companies and industries.
We're joined by the leaders of IT - VMware, computer makers, infrastructure software, storage, and security providers.
And to lower the bar to the highest levels of AI, we offer Pre-Trained Models - like
Jarvis conversational AI,
Merlin recommender system,
Maxine virtual conferencing,
and Morpheus AI security.
All of it is optimized on NVIDIA AI Enterprise.
And with new tools like NVIDIA TAO, Fleet Command, and Triton, it's easy for you to customize and deploy on EGX.
And Drive - our end-to-end platform for the $10 trillion dollar transportation industry, which is becoming one of the world's largest technology industries.
technology industries.
From Orin and Atlan, the first 1000 TOPS SOC, to Hyperion 8, a fully-operational reference AV-car platform, to Drive Sim, to Drive AV -
we're working with the industry at every layer.
20 years ago, all of this was science fiction. 10 years ago, it was a dream. Today, we're living it.
I want to thank all of you - developers and partners, and a very special thanks to all the NVIDIA employees.
Our purpose is to advance our craft, so that you, the Da Vincis of our time, can advance yours.
Ultimately, NVIDIA is an instrument, an instrument for you to do your life's work.
Have a great GTC.
Title: 3D Artists Experience their Full Creative Potential with NVIDIA GPUs and AI
Publish_date: 2018-11-20
Length: 69
Views: 11749
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/Ecfcm9D6IwQ/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: Ecfcm9D6IwQ

--- Transcript ---

[Music]
my name is Val milk beverage
I'm 3d artist I work as an art director
and I'm part of a double dimension
development team so I was showing a
breakdown of my creative process using
one of my artworks as a case study
specifically showing how you can create
materials in substance painter and how
scene is being assembled using a double
dimension which AI enabled tools and
artists is now given more room to be
efficient and focus on creativity
instead of doing labor work like trying
to match image and do you know custom
lighting for instance the feature I just
mentioned allows you to kind of compute
the light based on background image so
you don't have to do that on on your own
a machine is gonna do it for you I'm
really excited
I mean excited times are in front of us
for sure especially with the new during
technology every time I update my
systems with a new GPU I feel like a
race driver who's gone from driving the
family sedan to formula 1 car
Title: SHIELD Explosion Teardown
Publish_date: 2013-09-05
Length: 111
Views: 426886
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/ecRkkVrKvQA/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLARr-_jW_V8ZZOzLgj1CV4O43m3ZA
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: ecRkkVrKvQA

--- Transcript ---

so today we got our handsome new Nvidia
shield and even though I love this thing
we've always wanted to do one of those
tear down videos so they probably don't
like all the rage right now such a days
of scientific calculation and figuring
out how many fireworks we have left
after the fourth of July Baby Genius
Andy over here and an excellent idea
we're gonna blow it up and shoot it with
a slow-mo camera set helmet doctor
ordered man the blue one is so shoes an
Android based gaming device and as you
can see by its shape it's meant to feel
like you're playing on a console great
controller some super-sweet speakers
bluetooth did we get into what Andy's
doing over here Andy's trillion Aspen
evolved into it and filling it with
explosive how are we doing Andy doing
well about to blow this thing there's
the magnetic faceplate there's the five
inch touchscreen in about a million
pieces I think that's a Tegra 4 chip
right there there's a button thumb stick
triggers there's the heat sink right in
the middle there goes the battery
there's a green plastic bottom thing
there's the memory module where we blew
this to hell did you know the shell has
16 gigabytes of flash memory do you know
my heart weighs 9 pounds
well I guess that counts as a teardown
video right it's all that down pretty
well I would say so yeah I'm poor
shields yes I gotta go buy a new one
damn it
Title: YouTube Voice Search on NVIDIA SHIELD Android TV
Publish_date: 2016-05-19
Length: 85
Views: 16453
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/EDVhygZldOw/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: EDVhygZldOw

--- Transcript ---

Say it.
See it.
SHIELD delivers the best YouTube experience on your TV.
We all love YouTube, but using it on your TV has never been easy.
Until now.
With support for both Universal Voice Search
and 4K 60FPS video
the NVIDIA  SHIELD Android TV is, hands down,
the best device for fast, easy access  to everything you already love about YouTube.
SHIELD's native voice search lets you find YouTube content right away
so there's no more pecking around on a virtual keyboard.
"Cat videos."
You just say what you want into the SHIELD controller or remote
and then you get immediate access to all sorts of great video content.
All without typing a single letter.
"Hip hop videos."
"Movie trailers."
You won't find this feature on Apple TV, Roku, or Amazon Fire.
SHIELD's Universal Search is also a really great feature letting you search from the homepage.
So I just say, "Search for 4K Videos!"
And then watch what happens.
And the quality is as amazing as the simplicity.
In fact, SHIELD is the ONLY media streaming platform capable of delivering 4K video at 60FPS.
Incredibly intuitive,  Universal Voice Search , with the highest quality video you can find.
It's easy to see why SHIELD delivers the best YouTube experiences. Everytime.
Title: AI-Powered Video Conferencing with NVIDIA Maxine
Publish_date: 2020-10-05
Length: 86
Views: 964208
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/eFK7Iy8enqM/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: eFK7Iy8enqM

--- Transcript ---

[Music]
a self-driving car needs to correctly
anticipate or predict
the future actions of road users in
order for it to drive safely
so today in drive labs we're talking
about our prediction net deep neural
network
wow that is a lot of noise that no one
else needs to hear
[Music]
okay that's better
that's so much better yes there was
recently a sign of extraterrestrial life
on venus what else do you want to know
what do we think we're looking at for
wednesday
in terms of layout so this is
hey everyone this is victoria i hope
you're all doing well
so today we're going to be catching up
on the latest from
nvidia research ai is revolutionizing
many sectors of the industry right now
and nvidia
is at the forefront of this revolution
ai is revolutionizing many sectors of
the industry right now
and nvidia is at the forefront of this
revolution
Title: NVIDIA SHIELD Showcase - An Android Story
Publish_date: 2013-06-05
Length: 129
Views: 42668
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/EgzQLeDfy3Y/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: EgzQLeDfy3Y

--- Transcript ---

hey guys I'm will and this is chilled
showcase where we show you everything
you need to know about you we've been
talking a lot about Android and PC
gaming recently but shield runs the
latest version of the Android jellybean
operating system which means you can use
it as a fully functioning Android device
for example I'm at a cafe here I'm
totally untethered but I'm gonna use our
Wi-Fi to hop on the web browser and
maybe check some news and get my flight
status for my flight later I can
actually use the 5 inch HD touchscreen
here to pan around and get my headlines
or I can use the left joystick to scroll
and pan through the page and the right
joystick actually brings up a cool
little cursor that I can use to home in
on the smaller links that might be a
little hard to tap with the edge of your
finger next maybe I'll go to Google Play
and download a book so I can read and
relax on the plane later and I can do
all this while listening to my favorite
podcast now let's say you're in transit
on the way to the airport you can fire
up Google Maps to get the shortest
distance to your terminal and while
you're at it check traffic conditions or
maybe you're at the airport waiting for
your flight to board just flip open your
shield get some work done maybe fire up
an Excel spreadsheet with your office
app use your cursor to find a cell or
just fire up Google Local to find a
restaurant in your destination city and
scroll through the list there you good
to go
or maybe just kick back and relax and
get your gaming on with Tegra enhanced
games alright so I'm in the plane I've
got my seat and we're about to take off
but I got a little bit of time left and
while I still have Wi-Fi I want to hit
up some Gmail so I'm just gonna swipe on
over check out my Gmail widget make sure
my emails are good they look fine so I'm
gonna spend the rest of the time
relaxing and that means swipe on over
fire up some tunes
maybe start up my book or watch that TV
show I started so that does it for this
chilled showcase make sure to visit us
at shield and video.com
for more information and sign up for
those exclusive email updates find us on
social media on Twitter Facebook and
Google+
you
Title: NVIDIA at Mobile World Congress: 5G Meets AI
Publish_date: 2019-10-21
Length: 5008
Views: 95588
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/ENBUD7gXiyI/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: ENBUD7gXiyI

--- Transcript ---

I am AI I am a creator freeing our
imaginations
and breathing life into our wildest
dreams
[Music]
I am a guardian keeping us safe on our
way home
and wherever a curiosity takes us
I am a visionary anticipating the needs
of others
and simplifying our busy lives
[Music]
I am a protector keeping our most
magnificent creatures out of harm's way
[Music]
and helping our heroes make it home
safely I am a healer
decoding the secrets from within
and providing precision when every
second counts
[Music]
I am an innovator
finding smarter answers to complex tasks
working in harmony to lighten the load
and driving perfection and everything
you create
I am even the composer of the music you
are hearing
[Music]
a
brought to life by Nvidia deep learning
and brilliant minds everywhere
[Music]
ladies and gentlemen please welcome
nvidia founder and CEO jensen huang
[Music]
[Applause]
[Music]
greetings Mobile World Congress
this is our first Mobile World Congress
we have a lot to tell you amazing things
happen when breakthroughs in the
communications industry meets
breakthroughs in the computer industry
in 1995 Windows 95
56k modem and an Internet Explorer
brought to us the Internet 12 years
later in 2007 extremely low-power SOC s
cloud computing backed digital wireless
network came together to bring us the
smartphone revolution the iPhone wasn't
the first smartphone in fact there were
modems that were declared their work
follows our declared smart phones
including smart cameras GPS as I am use
connected to a modem but what made I
phone distinguished was that it was
software-defined because it was
software-defined it was possible for
application developers all over the
world to upload and develop their own
applications and host an entire service
that ran the applications on a phone and
connected to a service in the cloud the
fact that it had the ability to be
software-defined allowed developers of
all kinds to create services and
applications we never dreamed of it
disrupted every single industry books
music videos the way we order food the
way we travel if you just think about
the number of mobile devices that are in
the world and if we just estimated about
a thousand dollars we spent on each
mobile device it's a 500 trillion dollar
industry extraordinary size all because
it was software-defined wasn't a first
smartphone but it was the first
smartphone that put it all together
became software-defined the application
developers contributed two and a half
million applications
two and a half million applications
unheard of number of imagination
imaginative products created if you take
a look at this list not only did it
create new applications and change the
way that we worked disrupted entire
industries products we used to use
completely disappeared industries that
we used to see completely disappeared
created an enormous industry on top of
this platform and is now 12 years later
12 years after Windows 95 was the iPhone
12 years after the iPhone a new era is
beginning in fact the smartphone
revolution is the first of what largely
people will realize someday as the IOT
revolution where everything is
intelligent where everything is smart
this industry people have been talking
about for some time Internet of Things
where everything will become smart where
everything will become connected to the
cloud where everything will have a
platform on top of it with imaginative
services and where every company every
industry have an opportunity to have
their iPhone moment 12 years later we're
standing at the beginning of this
revolution if you take a look at the
things that we could do we could make
our lives easier of course we could
squeeze a little bit more energy out of
the power grid we could extract a little
bit more efficiency out of our
transportation system we could squeeze a
bit more spectral efficiency out of our
wireless networks each one of these
industries are so gigantic that if we
were able to squeeze another two three
four five six seven percent out of it it
is measured in trillions and trillions
of dollars each year the impact
of bringing intelligence out to the edge
is extraordinary
the fundamental difference this time
however is unlike the smartphone there's
no intelligent person sitting next to
most of these things most of these
things are going to be autonomous they
have to sense perceive infer and act by
itself they simply won't have the
benefit of humans next to all of these
things and there's so many things in
fact one of the our beliefs is that
eventually everything will be smart
everything that moves would be
autonomous and if that's the case that
will be trillions and trillions of
things connected to the Internet the
fundamental problems are extraordinary
the vision is clear that everything will
have the benefit of what is effectively
an iPhone however getting there is hard
since we're in Hollywood I thought we
would use this metaphor the Infinity
stones and it turns out we need exactly
six I don't know how they realize that
but we need exactly six miracles and if
we could find these six miracles and put
them together in an effective way
we will be able to realize this amazing
future of smart everything this is our
first Mobile World Congress so maybe
I'll take a second and introduce
ourselves we are a computing platform
company we pioneered a form a computing
called accelerated computing and we
build computers to solve problems that
normal computers cannot solve we
innovate and work in an area at the
intersection of computer graphics high
performance computing and artificial
intelligence in the case of computer
graphics were simulating virtual reality
in the case of high performance
computing we're simulating physics
inside a computer in the case of
artificial intelligence we're trying to
simulate human cognition in each of
these cases and video as a simulation
company
the processor we created is called the
GPU it is recognized as the processor
that could extend the computational
capabilities of some very important
problems as we see now at the end of the
moore's law this is the intersection we
play it and we think we can make a real
contribution to helping discover some of
these gems the first gem is something
that we've been working on over the last
close to a decade now and we dedicated
an enormous amount of our resource to do
this is artificial intelligence modern
AI started about eight nine years ago
where researchers discovered that a new
type of algorithm called deep neural
Nets if fed with an enormous amount of
data it could hierarchically and
systematically discover important subtle
features inside a large amount of
patterns discover the patterns through a
great deal of computation and those
patterns it could apply to predict to
classify to recognize the future deep
neural networks has become so effective
that almost every field of computer
science has been touched almost every
field of science has been touched we're
working with companies all over the
world this is unquestionably the most
important computer science event that's
happened in the last decade and the most
powerful technology force in our
generation in just a few years after its
popularity its discovery this
combination of using NVIDIA GPUs
processing large amounts of data to
discover patterns inside the data using
deep neural Nets amazing breakthroughs
have happened in 2015 deep neural Nets
achieved superhuman levels of image
recognition and shortly after that from
language translation to speech
recognition to speech generation and
image generation
- now superhuman levels of reading
comprehension unbelievable progress in
just a short period of time when you
combine all of these progresses the
applications that we can imagine are
really quite daunting
let me show you some of the crazy stuff
we've been able to do with AI they're
just incredibly fun
this first one example is called Gauguin
and it's basically somebody who could
doodle on the one side and the
artificial intelligence network will
turn it into a photorealistic image here
a camera a single camera is able to look
at an image and discover the
three-dimensional pose of that person
inside and then project that
three-dimensional pose into a
three-dimensional world because as you
could see it's in 3d we were able to
teach a neural network how to make
robots stand move and in fact do
somersaults and then we create a virtual
reality environment where a robot could
learn to be a robot and when it that
success we moved the software into a
physical robot with a real computer and
amazing things happened as well the
robot does the same thing in the
physical world that it did in the
virtual world we taught a neural network
how to synthesize and generate
three-dimensional information
three-dimensional environments by
looking at two-dimensional information
and so looking at a scene is able to
generate a 3d scene so that it's easier
for us to label and learn from this 3d
scene we could go the other way and
learn from a 2d environment all of the
segmentation the meaning of every single
pixel purple is road and blue his car we
could teach a neural network how to take
CT scan and infer from it the different
organs from a dust of blue points and
then of course we could put it all
together to create a self-driving car
which is essentially a robot that senses
in first and takes action and in this
particular case in the application of
self-driving cars this is our
self-driving car it's our first robot
we call it bb-8 it's able to recognize
objects lanes infer where best to drive
recognize its environment using computer
vision as well as radar as well as lidar
and using all of these different sensors
infer from it with as much redundancy
and diversity as possible so that this
robot can be as safe as possible so AI
is making amazing progress so the first
two gems our first two gems that we
discovered the first two Infinity stones
artificial intelligence and
high-performance computing that's made
possible by our GPUs if you took a look
at what's happening the next a Mayer's
amazing breakthrough is clearly 5g
you're experiencing right here the thing
that's really interesting is that if we
look at 5g the incredible bandwidth that
is achieved with millimeter waves the
spectral efficiency that we get from
massive MIMO it's going to of course
make everybody's cell phone experience
so much better the interactivity on the
web is going to be fantastic we're gonna
just enjoy using everything that we have
a little bit better however that only
scratches the surface the fact of the
matter is if we made all of our cell
phone experiences better it would be a
wonderful achievement but it surely
wouldn't be revolutionary 5g and the
architects of 5g the inventors of 5g was
so brilliant in thinking about all of
the other new applications that we could
create some of them are made possible
because of this idea called ultra
reliable low latency connections it is
possible now for the network to be able
to control and put a computer extremely
close to the machine it has to operate
because it's able to achieve that level
of connection with extreme reliability
as well as low latency the other new
capability that's made possible by 5g is
called network slicing network slicing
makes it possible for all of these
services different modes of services to
be provisioned
to the public in different size
different slices and make commitments of
its service to people who are using the
network in different ways the potential
of course is all of a sudden we can
connect these things to the network and
all these things are going to inspire
new applications the fundamental problem
of course is that what used to be a
phone the cloud and the telcos being
essentially a pipe has to change in this
future the edge can no longer be just a
pipe in this future this edge has to be
a computing platform and the reason for
that is very very clear most of the
applications that we will provision and
the new applications that we will invent
requires extremely low latency
computation it is not possible to go all
the way to the cloud and back in several
hundred milliseconds and provide for a
safe operating environment if it happens
to be a machine it could be a
self-driving car it could be a shuttle
it could be a robotic arm that is
working close to somebody that
cooperation capability is only possible
if the latency of processing is
extremely low the second fundamental
problem is the amount of sensor data
that is going to be streamed across this
pipe is gonna grow incredibly 5g is
gonna make it possible for us to connect
up to a thousand times more things than
we currently do with 4G from billions of
things to trillions of things those
things are going to be streaming sensor
information that are going to be high
density high daya rate as well as
continuous unlike the use of a phone you
use it when you need to access some
information in the case of robots in the
case of most sensors that are monitoring
the world or interacting with the world
the information is streaming
continuously we simply can't afford to
stream that amount of data over the
network continuously and be able to
operate most of these applications
sensibly and then lastly there's just
data protection matters all of the data
that is out in the edge and the point of
action can't possibly be streamed all of
it to the Internet and so these
fundamental
issues make it necessary for us to turn
computation and put it into the network
this network today is built out of
incredible gear that's made by the
telecommunication industry for of course
provisioning the data for security for
routing all of that all of that will
have to evolve to the next generation
just as the smartphone came out of a
modem plus a camera the next generation
of computing has to evolve out of a data
center which has communications but in
the future has to become
software-defined if it became
software-defined the number of
applications we could host at the edge
could be extraordinary and do the same
for the edge computing that the iPhone
did for the smartphone revolution so we
think that this Center here what is
otherwise known as the edge will become
a computing platform in the future it
won't just be a pipe and instead of
dedicated equipment that future will
become software-defined high performance
computing and we'll be able to layer on
all kinds of rich applications as a
result I think the iPhone moment if you
will the smartphone moment for edge
computing is here and a new computer a
new type of computer has to be created
to provision these applications and make
this vision possible and so today we're
announcing the world's first of this
type of computer we believe that in the
future in the edge will need to be a
high-performance supercomputing edge
server it's optimized for processing
sensor information the sensor
information could be video information
sonic information lidar information it
is optimized for multimedia and
interactivity with consumers and it's
optimized for artificial intelligence
applications or robotics applications
where you sense infer and act at an
instant we call this computer the Nvidia
egx
it's a edge computing supercomputer and
it's powered by our state-of-the-art
CUDA tenser core GPUs GPUs that are the
world's most advanced for artificial
intelligence computer graphics it is
secure from the base all the way to the
top from the moment that you boot this
computer it is secure has secure boot
with root of trust everything is
cryptographically encoded traffic inside
the Danner's data center traffic outside
of the data center are encoded and
decrypted and fully accelerated so it
doesn't burn the CPU even the network
even the storage could be connected on
the network so that a data center could
be composable disaggregated and
state-of-the-art and all of the traffic
that goes back and forth are completely
secure encrypted at all times and then
most importantly the software stack this
software stack is the software stack
that has taken the cloud computing
industry and ourselves and all the
developers thousands and thousands of
man years to make possible this is the
same software to access running in just
about every single cloud in the world
and this software stack makes possible
high-performance computing and
artificial intelligence inside our to
processors that are absolutely
state-of-the-art there's the new Nvidia
CUDA cores you detention court GPU as
well as the new Mellanox
smart NIC you can configure your
computer in all kinds of different ways
you could have more than one GPU with
each one of the GPUs you could select
one of three some of it extremely low
low-power some of it is extremely great
for computer graphics some of it is
optimized for artificial intelligence
depending on what you choose you could
up to 240 trillion operations for AI
processing you could stream a hundred
and forty HD video streams through this
one GPU and in a computer with four of
these almost thousand HD streams could
be streamed and each one of these
streams will be decoded image will be
processed you could do artificial
intelligence on it and coated and
dream it out again of course is based on
our latest generation ray tracing GPU so
computer graphics that are streamed from
cloud gaming or augmented reality will
be amazing
and has 15 up to 15 trillion operations
for our general-purpose GPU called CUDA
all of this processing basically
translates to one single node is
equivalent to hundreds of CPU nodes
inside a data center the most important
thing of course is the software stack
I've mentioned earlier we call it the
egx stack which is cloud native it is
secure from the moment that you boot to
authenticate attestation the way that
you stream networks all of it's secure
and it's fully accelerated with our CUDA
stack it is of course optimized for all
of the things that you would do at the
edge whether it's cloud gaming augmented
reality image processing sensor
processing as well as artificial
intelligence the Nvidia egx
edge super computing platform for this
new generation of applications we're
really excited today to announce that
Red Hat is joining us to turn this stack
to convert everything that we're working
on integrate everything we're working on
and make it a carrier grade stack Red
Hat as you know is the world's leading
data center open platform and they serve
the world's every one of the world's
fortune 500 and they of course has been
an incredible service to the
telecommunications industry they're they
power and run a hundred and twenty of
the world's telcos today so we're
incredibly excited to be working with
them to modernize to telco grade our
stack so that we could provision Red Hat
kubernetes to all of the telcos the rest
of the IT industry has joined us as well
every single computer maker datacenter
computer maker the world's leading
enterprise software may
have all joined us to take this platform
to market Addis Dell Fujitsu keel
Packard Lenovo QCT Super Micro Cisco
Microsoft Red Hat and VMware so I'm
super excited about this new platform I
want to thank all of our partner for
joining us let's give them a round of
plus all right let's take a look at what
this can do so this is the computing
platform well put this at the edge
well rack it up it will replace hundreds
of CPU servers we'll be able to do AI in
real time we'll be able to stream sensor
information of all different types it is
secure
it is cloud native we'll be able to
support sensors on a wide range of
geographies process information that is
space geo spatial and temporal close to
where you are where the action is now
the question is what application to
develop on top we have thousands of
developers that were working with but we
thought we would create one particular
application and open source it and give
everybody a chance to see what this new
type of application looks like writing
the first reference application we call
it the metropolis IOT and metropolis is
designed for sensors in smart cities
they could be smart city smart places it
could be used in retail can be used in
all kinds of different places where
information is coming in at a very high
data rate in this particular case we're
gonna have cameras that are gonna be
that are located on streets these are
real cameras located in real streets and
basically the stack works like this all
of this is streaming over Ethernet it
gets aggregated at a switch an enormous
amount of data is now streaming into our
server it comes in through our DMA it's
all completely secure in the data center
of course it comes in through our DMA so
that the CPU doesn't get burdened with
so many streams of video at one time we
decode all of them we can decode up to
140 of them we have
do image processing on a menu maybe you
want to crop it maybe want you want to
change the color space maybe you want to
take a fisheye lens and warp it back and
D warp it so there's all kinds of image
processing you have to do CUDA is
incredibly good at that
the deep neural network the AI
processing now runs through our tensor
course it could be one-year or network
but could be a whole bunch of newer
networks you could of course you could
detect you could classify you could
track you could estimate the pose you
could do all kinds of things in this
framework every one of these neural
networks are sitting inside a container
and those containers are provisioned by
kubernetes and orchestrated by
kubernetes we can then pipe that through
a graphics engine you could decide to
label it you could decide to fill in the
pixels and then you encode it and stream
it to the cloud you could decide to
encode it and store it locally if
there's anything that you detect throw
away the vast majority of the
information in the future we're not
going to record the earth we're not
going to record all the things that are
happening we're only going to record
what we need to record based on what we
detected and so based on what we detect
them we might encode it we might save it
stream it up to the cloud
Kefka is the streaming framework spark
streaming allows us to do large scaled
resilient data analytics on streaming
data and then we would visualize it this
entire pipeline is captured in the
metropolis IOT application framework
runs on top of the egx server this
application is a miracle all practically
in itself you're basically doing all
kinds of image processing in real time
and then streaming it all the way out to
the cloud this is sitting in the azure
cloud where we're doing the analytics ok
so I'm going to show you this
application step by step and our goal
our goal in IOT is to authenticate the
endpoints and so this system allows us
to authenticate adas test that it is it
should be allowed to send information we
would
prevision deploy the neural network we
would then decide how we want to
orchestrate it
okay so authentication deployment we
might decide to enhance it and then
orchestration ok so several step process
first step so this is our authentication
deployment we're doing this live so this
is actually running on an eg X server
these are live videos there are 87
streams we're detecting running an image
recognition and classification model on
top of that we're doing that in real
time and these are this is where the
cameras are located in the city
ok so we've now attenti kate and deploy
incredibly light application for this
edge supercomputer the next step we take
a look at it and we realize that in fact
some of the models aren't working as
well as we like so we might decide to
take this all of our models are
encrypted in the container we got also
shipping to you so they you have a pre
trained model that you could decide to
adapt using transfer learning and we
have transfer learning tools that you
notice that in fact we could do a better
job with this particular camera because
maybe the camera space the location of
the camera the orientation of the camera
isn't as well tuned for the network that
it's currently running and we decide to
optimize that and we can do that and
roll in the new camera the new model
without turning down the computer
without taking down the computer because
everything is sitting inside a container
and we swap one container for the other
container and direct deep stream to now
be looking at the second container to
new the new model it doesn't miss a beat
and so now we can enhance our model we
could decide then to deploy and scale it
out and so we've tried it in this many
number of cameras we could scale it out
with a whole lot more
and this is all just running on one GPU
is doing it fairly well there's all
kinds of new models you could decide to
put into it
and of course the goal isn't to just
monitor the goal is to find things and
so when we when we set up these this
network of cameras we weren't we didn't
know exactly what we would find but but
we recorded a variety of things and
whenever there are anomalies it would
detect it a story and so here's here's
one that that we found let's see if you
could detect the anomalies
I think that that she thinks things that
that something's wrong and so she she
got out for I would have stayed in the
car frankly and so so anyways she's a
great citizen she I don't know what you
did with the information but we caught
it for her nonetheless okay so so uh I
the world the world has 40 million miles
of roads the world has 40 millions miles
of roads and and the cities would like
to put put these cameras all over them
so that so that if there was an accident
if a piece of furniture fell off
somebody's back back in somebody's truck
of our fender fell off or a tire busted
and it's sitting in the middle of the
road that they could alert somebody as
soon as possible so so this this type of
application this particular application
now makes those kind of monitoring and
detecting anomalies you know in the real
world a real possibility okay so this is
this is metropolis metropolis IOT is
available in our GPU cloud registry as
of today if you take a look at what what
metropolis was doing it was taking live
video it was sensing it was inferring it
was perceiving and then was taking
action and it was trying to do that as
quickly as possible in the case of a
large number of cameras we have to do
that computationally extremely
efficiency efficiently because we're
trying to keep the cost down the more
efficient the computation is the lower
the cost it is in provisioning these
servers connecting to the number of
cameras that we have out there there's
another type of application now that was
only one sensor if we're one to infer
and we want to perceive the world better
we're going to have to have more sensors
the ability to have multi sensor or what
the industry calls multi modal ai is
extremely pot that extremely powerful so
that what you hear and what you see what
you sense and maybe even your apron
acknowledged contributes to what you
infer another way of saying context
matters in intelligence well we would
like to create a multi-modal AI and this
is our this is our first generation and
we call it the Jarvis SDK you have the
ability to do whatever you like with to
do with it we create the open framework
the application and the applicant the
models that are inside are pre-trained
there's several incredibly powerful AI
models that are making this possible
speech recognition is state of the art
the natural language understanding model
based on Bert is state of the art
incredible breakthrough recently in
conversational AI capability in the
industry then from there text to speech
and then speech synthesis all of those
models are running in the example and
mud to show you it is all completely
accelerated it's running at the edge and
the reason for that is because there are
so much sensor information is unlikely
that we'll be able to stream all of that
to the cloud and so in order to use all
of the sensory information to infer and
to recognize what is happening and do
what is appropriate we're going to have
to do this AI at the edge we call it the
Jarvis multi modal AI let me show it to
you
- a Mexican restaurant the nearest
Mexican restaurant is Luna Mexican
kitchen located at 1495 the Alameda San
Jose
it is currently 66 degrees and sunny in
San Francisco show us a Japanese one the
nearest Japanese restaurant is any key
sushi located at 3810 Mori Avenue three
minutes it will be mostly cloudy with a
high of 62 and low of 54 what's the
reading Viki sushi has four stars on
Yelp I chose the directions here are the
directions to an inky sushi okay so take
a look at what's going on here and what
you just saw first of all everything was
running in real time okay well let's
take a look at the nvidia backpack is
$100 what about the one on the right the
price of hex tote bag is $120 how much
does the white bottle why pantego bottle
is $21 it's the black one
insulated yes black contigo tumbler is
insulated what's the size of the center
one Rosewell bottle can hold 15 ounces
okay great I'll take that perfect I'll
charge it to your account so if you take
a look at what's happening first of all
the reason here there's literally one
session of Jarvis running it's able to
recognize who is talking and therefore
the question notice there was no hot
word because it noticed who is making
eye contact and who is asking the
question it also because it makes eye
contact and remember who's asking the
question it could keep the contextual
thread okay just like when we're making
a conversation with somebody depending
on who's asking the question and and
oftentimes we get the question asked by
somebody we're looking at and so the
ability to see somebody and recognize
what they're asking
allows us to have a reasonable
intelligent conversational thread and so
that's what's happening here notice how
fast it was if you're going to have a
conversation with somebody the ability
to respond very quickly matters a great
deal in the future it is very likely
that we'll even be able to just
interrupt each other in the middle of
the sentence and have a fairly
productive conversation between us in
the ami notice here it is impossible to
understand what she's saying unless the
AI could also recognize what she's
pointing at and so what's happening here
is recognizing the face recognising
because at the end she the AI was going
to charge it to her account and so
facial recognition matters the ability
to understand where she's posing or
she's posing and pointing at matters in
3d space because that could be almost
anywhere and so it's important to
recognize where she's pointing and then
to be able to classify and then answer
the question what is that object so that
the AI could recognize it and also
understand the context of that object
probably because it's a bottle it says
something about how much fluid it could
hold okay and so in the future these
kind of multi modal AIS will make the
conversation and the engagement that you
have with the AI much much better now
take this to manufacturing take this to
manufacturing where you're working among
robots and you would like to say things
like pass me that or hand me that or
hold this while I do that whenever
you're using things like pronouns he
she's it's and and that's those kind of
ambiguous words requires context and the
context could be part of either the
domain that we're in the conversation
we're already having or based on
computer vision what I'm pointing at
something that I'm demonstrating through
other sensations that I'm interested in
okay so multi modal AI
let's go back to that that slide please
thank you very much
so we called this Jarvis it's an sdk is
a tool and it comes with pre trained
models and these pre trained models are
are accelerated they're accelerated in
the sense that they run on tensor RT and
they run on our tensor core and they're
incredibly fast so that you can have a
conversation with it so that you can run
a whole lot of different a ice along
with speech recognition natural language
understanding text to speech and speech
synthesis computer vision computer pose
estimation you're doing also
classification and then you've got to go
figure out what is it that ultimately
was meant go take the necessary action
which requires potentially more AI
models and then come back and perform
the task or respond to a question okay
so this is what jarvis multiple multi
modal AI is it's an sdk it comes with a
whole bunch of pre trained models and
then you guys could use it and create
some domain-specific application out of
it the sdk will be available in december
ok and it runs on egx
well this platform this platform we've
been building for some time and we've
been working with with early developers
for some time we've been working with
early developers in smart city in
agriculture and manufacturing and
transportation and retail and in call
centers if you take a look at the type
of applications we can now address it is
absolutely gigantic and most of these
applications these industries are large
half of the world's population live in
cities there are thousands of cities in
the world that have a million people and
more forty million miles of road that
connects all the cities 40% of the
world's land mass is covered in farming
it is absolutely essential for all of us
to find ways to improve the yield of
farming not to expand and increase the
number of farms they're already almost
six hundred million farms somehow we
have to bring sensors and automation to
the edges of those farms today those
farms as you know has no Wi-Fi and
you're not going to be able to put a
person nearly at every single square
acre of a farm out there is 2 million
fact
we can measure a lot of things but you
can't measure workmanship we can measure
a lot of things but you can't measure
workmanship scratches small cracks burrs
fit and finish all of these things are
impossible today finally we have the
capability to put this type of AI at the
point of the factory and help the
millions of millions and factory workers
do better visual inspection ten trillion
miles are driven each year all of it is
gonna be a ton of animated someday
either fully automated or Auto assisted
those cars are going to be mapping the
world continuously every single car is
going to be a mapping car we're gonna be
mapping we're gonna be streaming sensor
information to the cloud over 5g the
data center close by will reconstruct
the sensor information into a map
high-definition map it would update its
memory of the map maybe some
construction is being done maybe a new
building got built maybe a new road got
opened up all of those roads in the
world will be mapped in real-time and
continuously not every part not every
car will contribute to mapping but a lot
of cars will contribute to mapping all
of that information will be streamed to
the cloud they'll be processed at the
edge cloud and it will be streamed back
to the cars millions of millions of
stores 30 trillion dollars almost of
industry 13 million retail stores if we
could find just a little bit more
efficiency the quality of life would go
up 5 million people are answering
questions of all kinds around the world
these types of applications for the very
first time we finally have the necessary
technology to go engage they're not
going to get solved in the cloud these
call centers are unable to stream all of
their data to the cloud because most of
that data to want to keep local for
either sovereignty reasons or privacy
reasons or other competitive analysis
competitive advantage reasons
so all of
these applications as it turns out would
like to have computing at the edge and
we've created a new computer to help
them do that the early engagements that
we're we're working on are really quite
amazing there are a hundred over a
hundred partners of ours working on
smart cities there are already four
million cameras in London there is just
impossible to to scale that out because
there's just not enough people to be
looking at all those cameras and so
there's all kinds of companies that are
cropping up all over the world to make
every place safer manufacturing I
already talked about fit and finish
samsung using it for samsung electronics
using it for the work that they're doing
in inspecting semiconductors BMW
inspecting fit and finish which is
incredibly complicated
Foxconn for their PC boards PNG billions
of people touch their products every
single day the quality of the product
matters greatly
keeping the manufacturing cost down and
low is really important to them and so
visual analytics is going to transform
the way that we do visual inspection
robotics robotics it's particularly
challenging first of all the AIS that
I've spoken about are gonna contribute
to enabling robots that will be able to
work among us today robots work either
in large cages if they're maneuvering
quickly or lifting heavy things or their
areas in the manufacturing floor that
you're simply not allowed across and if
you cross it instantaneously the robot
stops okay so the ability for you to
collaborate and what does call cobots
is limited because these robots can't
sense what's happening around it and so
the first thing of course is we have to
add sensory capability to the robots and
as a result it could sense infer and act
instantaneously and because it can infer
an act instantaneously because it has
Lola
see connection over 5g to a data center
that is the data center of the factory
for example it could now manage and
operate these robots with a great deal
of precision and fidelity and safety and
so the thing that we have to do is of
course we have to give this robot
sensory capability and then we have to
teach these robots how to be robots we
have to teach the robots how to be
robots teaching robots how to be robots
is impossible by programming because we
simply don't have the ability to skills
to program the incredibly complex
routines necessary for articulation
that's why a AI is so powerful the first
step for us is to number one create the
virtual environment that obeys the laws
of physics so that this robot can learn
how to be a robot then the second thing
we have to simulate the robot let it
learn to be a robot and then number
three deploy it in the factory floor
we're going to show you a demo of
something that works on the egx
and so this cloud data center will not
only be able to operate the robots but
it will create the virtual environments
for the robots to learn how to be robots
create the AIS that let the robots learn
how to be robots
and then lastly deployed the operation
on a large scale basis operating on
large number of robots and letting them
work together and so come on is going to
give us a demo of our first robot
[Music]
okay so what you're looking at right now
is a virtual reality environment this
virtual reality environment looks real
and it behaves really so according to
from the perception of this robot it is
essentially in the real world okay so in
this virtual reality world that obeys
the laws of physics that looks like
looks like the real world we're gonna
let the robot learn how to be a robot
and so now step two
step two
and step two here we go okay so here
Hammad is going to teach by showing the
robot how to be a robot
okay so Hammad you could see he's using
an iPad however he could be in virtual
reality
and he says robot this is how I want you
to pick things up the robot is going to
perform tasks notice the brick obeys the
laws of physics it recognizes the
objects of course is going to have to
generalize and diversify so that you're
independent of where we place things
that will still operate like a robot in
the future
okay not bad not bad
[Applause]
Hammad today
all right and then now we trained a
robot and we can now take the brains and
replicate it into a whole bunch of
robots and these robots of course will
have a sense of place it will be able to
recognize detect and recognize objects
even if the objects are placed slightly
off it understands what the mission is
and it will it will perform and once
once we're done developing the robot we
take the robot brain all of its software
and we deploy it into a factory of all
of these robots and they would perform
the manufacturing task with us ok these
robots are going to be are going to be
so acutely aware of its surroundings
that we could work right next to them
okay guys that's fantastic good job okay
so that's the Issac SDK it's gonna be
available in early January if you're
interested reach out to us
and then lastly Walmart Walmart is the
world's largest company it's kind of
it's kind of amazing to even fathom how
large they are there they're almost half
a trillion I think there are more than
half a trillion dollars in revenues 2.2
million employees and I think they they
hired they hired that the entire size of
Nvidia in you know by three by three
o'clock this afternoon and and so so
this company is gigantic this is this is
they turned their New York lab New York
store which is one of the largest
Walmart stores into a ai retail store
and it's powered by 400 GPUs every
second every second Walmart in that
store collects t' 1.6 terabytes of data
every second and what they're trying to
do what they're trying to do is figuring
out how to help customer service better
be make it easier for people to shop
make sure that they never stock out make
sure that all the products are fresh
make sure that they're all the products
are the best price to possible and so
they're trying to enhance
the quality of the customer experience
and and they have these these GPUs that
power 400 GPUs in this in this one
particular retail store and so the work
that we're doing with them is really
exciting you know as I mentioned the
retail industry is 27 almost 30 trillion
dollars large there are 13 retail stores
not to mention the number of warehouses
that it's backed up by and the
incredibly complicated logistic supply
chain that leads up to it the
opportunity for using automation to
improve the efficiency of retail is
extraordinary and the retailers that
were working with all over the world are
super jazzed up about applying AI in the
retail store in the warehouse and how
they manage their operations if they
could just squeeze out half a percent of
economics from a thirty trillion dollar
industry
it is absolutely transformative to them
the level of engagement the level
excitement that that we're seeing out of
out of out of retail is is not ours
alone and and in fact our work with
Microsoft in our collaboration with
Microsoft we're seeing a lot of mutual
interest in this area and so recently we
announced a partnership and Microsoft
has adopted the NVIDIA GPU to expand
their azure IOT offering they're going
to create these as your edge boxes
essentially computers like the ones that
I just described to you and they'll be
located they'll be put at the point at
the point of action solving exactly the
same problem that I just described to
you in dgx and their platform the IOT
Azure cloud the IOT cloud the IOT sphere
the azure edge box and the Nvidia
ecosystem are going to dovetail each
other perfectly and then together we'll
be able to support and help these retail
customers become more powered by AI
okay AI GPU the revolution is smart Nick
from Mellanox that makes this box
high-performance super secure from the
point of booting all the way to
Operation deployment cloud native
software stack that was made possible by
kubernetes in our partnership with Red
Hat's gonna be able to turn it into a
telco certified in telco ready stack the
egx computer that puts it all together
we're really still missing one stone and
that one stone turns out to be
incredibly challenging here here's the
thing if you look at if you look at the
the telco data center today in order for
this vision to be realized and
everything that I just described in
front of you the existing gear still has
to be purchased and the ability to get
ready for this new future with all of
these diverse applications require new
computers to be purchased this is one of
those classic chicken or the egg
problems is the killer app going to come
first and therefore we're gonna go build
our data centers to support it or do we
build our data centers to support it and
then their killer apps will show up and
you know my my attitude about most of
these things because we're in the
computer industry and we build computing
platforms is if you build it they might
not show up but if you don't build it
they can't show up and and so so we
start the process of building these
platforms and we hope that at some point
some amazing breakthrough happens would
it be amazing if these cloud data
centers were able to use exactly the
same computer that they were use for
cloud gaming for AR for artificial
intelligence for smart retail for smart
agriculture for smart transportation and
it happens to be exactly the same
computer for running the 5g ran
so that everything in the data center in
the future is software-defined
everything is just software running on
top of the supercomputer well it turns
out that if you're able to do something
like that the ability for you to create
a network of data centers to support the
changing dynamic workloads and the
changing dynamic traffic that happens
very naturally the utilization that will
result is incredible and therefore when
utilizations go up in a capital
intensive industry the cost of the
capital decreases there is so much
energy in wanting to make the data
centers the edge data centers Software
Defined there's so much energy in
wanting to make tell telecommunications
work wireless communications Software
Defined it is just an extraordinarily
hard problem and but the benefits could
be pretty extraordinary let me take it
let's just take a look at this benefit
and so so before we run this before we
run that this is from Thomas techies
he's the CEO of a company called on this
I they deal with gigantic data if you
guys ever need to work on large data go
to UPS I tell Todd that Jenson sent you
and and and they deal with data sizes
that are just gigantic and they do it
like they they treat the large data like
a video game and as a result you can
analyze your data incredibly fast and
find insight from it let's roll this
really quickly please
this is 2.7 billion rows of
crowd-sourced mobile phone data from our
partner tutela
running on the omni side GPU accelerated
database and analytics platform now
let's zoom into new york and
specifically let's look at the financial
district
we circled just lower Manhattan on the
map now check out this very tight work
day usage pattern in the heat map and
look at the time chart here in the
bottom right see how usage falls off
dramatically every night and all weekend
and all that activity has to shift to
other places like Brooklyn so let's
compare these two areas if we zoom out a
bit so we can see Brooklyn then shift
the focus over there we see a completely
different usage pattern
in those same two charts more traffic in
the evenings more and steady traffic on
the weekends the data needs of the
carrier don't go away they just move and
evolve both through place and time so
carriers need to be able to change and
reconfigure to meet these needs without
over provisioning so carriers need both
high performance and software-defined
infrastructure okay so you could see
that if if the world of Communications
was also software-defined the ability
for us to move the workload from one
data center to another data stream is
going to be increased now
that ability when you overlay on top of
this all of the other applications were
thinking about unless we're
software-defined is simply not going to
be possible and so we decided that would
it be amazing if this platform was also
a software-defined 5g network that runs
on our GPUs now if we're able to succeed
in doing that then this becomes the data
center computer at the edge it is
exactly the same date same computer that
I've mentioned earlier it has offered
off-the-shelf with off-the-shelf
components it is configured for all
kinds of different use cases you can run
5g on it IOT on it you can run XR on it
you can run 3d graphics on it of course
and you can run all kinds of
applications we haven't dreamt of the
entire software stack is cloud native
the entire software stack is cloud
native it's industrial-strength it's
been tested by billions and billions of
people up in the cloud and the IT
industry supports it there's just one
missing ingredient there's just one
missing ingredient we announced we
announced we
an SDK that basically does the through
the 5g ran physical layer in an SDK and
software it does the frequency
modulation demodulation channel
equalization and balancing it does the
error correction however there is so
much knowledge in building 5g radios at
scale and I can't tell you how excited I
am that the world leader in wireless
communications has been secretly working
with us to create the world's first
software-defined 5g rant
ladies and gentlemen Ericsson this is
and from Ericsson I would like to
introduce Frederik yaling Frederick
frederik terrific thank you for being
here
Frederick literally flew in from Sweden
just a second ago I think I think I was
taking a nap and it took me a while to
amp up you you were you were actually on
a plane that's correct yeah took one of
the last remaining direct flights from
Stockholm to Los Angeles so it was oh my
gosh well thanks for being here
yeah well we've been our teams have been
have been secretly working on this
project and and and I mean secretly not
not because there's there's any
particular secret but because we didn't
realize it could be done and so some of
the world's best computer scientists
some of the world's best wireless
communication experts have been working
together on top and creating essentially
this stack we called an outcome we call
aerial which stands for antenna the
stack that would would be the foundation
for Erickson to create essentially a 5 g
5g ran so my first question for you is
is you know we've been the industry has
been talking about software-defined
communications and software doesn't
defined data centers and this is this is
a area that is of extraordinary interest
and and the technology has been
complicated you know from your
perspective what do you think what do
you think took so long and and what do
you think about the moment that we're in
and and the implication to the future
sure let me shed some light on that I
think we've been we've been long history
building mobile networks for the past
let's say 25 years everything from 2g to
3G to 4G and to 5g and up until now I
think it's all been about
person-to-person communication via
mobile handsets and five does everybody
know that all the Bluetooth you guys use
was invented by Ericsson ok keep going
it's kind of mind-blowing Ashley
no but the heritage of this company is
really quite yes thanks for that and by
the way very impressive presentation is
good before and I think look 5d with
this on for something else it was not
only to connect people with each other
but was connect things which is with
with things and people with things and
then you get into a different challenge
of in you get then you get
in the broad environment yet again into
more of an IT oriented environment then
you gotta be able to leverage a broader
ecosystem it being Sdn it being a cold
space hardware it being whatever might
be virtualization and as an industry
we've been in all honesty struggling to
find alternatives that are there are bet
there are more higher performing than a
current current sort of bespoke
environment now over the past and we
mean bespoke for everybody it's
literally meaning hundreds of the
world's leading wireless communications
experts designing Asics exactly and
these Asics are incredibly complicated
yeah and now we have figured out a way
to then explore together with in idea
how we can define the next generation of
let's call it veer on utilizing the GPUs
that you're producing and combine it
with our many years of the wireless
technology knowledge and they build
something that is powerful and flexible
for our customers we have to understand
our customers the mobile operators
service providers across the world they
are looking for different architecture
enabling them to take advantage of 5g
our collaboration is figuring out an
efficient way of providing that again
combining the DGP use with our now of
course the world's gonna stay hybrid for
a very long time sometimes sometimes a
ad ran is perfect yeah it's the ideal
solution sometimes a dedicated system
sitting in seer an is the perfect answer
and because because that particular
datacenter or service provider isn't
offering a whole bunch of other
applications and they want to be
incredibly world-class and its cost
efficient as possible at delivering one
thing and and and yet there will be some
that would like to provide a whole bunch
of applications that sit on top you know
the thing that's that's really quite
amazing is if you take a look at the the
people that ultimately realized that
this the timing was now and that you
create that created the the 5g ran that
sits sits on some of our GPU you have to
realize that we're using a GPU that is
powering the world's fastest
supercomputer it is the most advanced
GPU in the world the single largest
processor in the world and it took the
world's best computer science
an algorithm and experts working on top
of that platform to make it possible and
so now now that we we realize that it's
possible the thing that's really quite
exciting is is the rate of innovation of
nvidia gpus is so fast and that won't be
able to take the hardware system cost
down as fast as possible and and then
there'll be other other applications
that we could layer on top of it the
type of algorithms that your researchers
are already imagining as a result of of
AI and high-performance computing at the
edge it's just incredible and that's
what we're looking at beyond then of
course the GPU collaboration on the
baseband side it's more around your
capabilities in AI incapability
supercomputing and apply that across so
that that's what we're looking for
it's an interesting thing we actually
it's about we launched - people don't
know that maybe but we launched five
year back here in the US with the first
five D Network so it's been a spin it's
been around for a year which is a life
length in many of these technologies but
we're currently around 34 networks
across the US and very proud to be part
of the US market here and we of course
surround a lot of use cases ourselves to
see what five you can and some of them
are shown here we are deploying a
factory here now with us exactly that
they kind of automated manufacturing
facility by 5g utilizing the low latest
networks - gamma phi the robots making
sure you got the intelligence out in the
cloud with low late this you can
manufacture in a former efficient way
and by the way also managed to flee it
straight out to the side to sector so we
try to we try to drink our own cold
kool-aid and this is a very important
market for us just as funny as you say
that I we have a robot simulator love to
work you guys it'd be great to have a
whole bunch of Isaac's you know doing
some doing some amazing tasks you're in
your factory you know the thing that the
thing that I'm super excited about and
and and I I think this is our first time
when we met you know we fantasized have
been dreamt about about the future of
telecommunications and an industry that
that that with each every generation
enabled a revolution
in computing and when I met when I
mentioned earlier the the Windows 95 and
the 56k modem the iPhone would have been
possible but not for data wireless data
networks and then now with the 5g
networks coming the the the the thing
that I'm incredibly excited about is is
all these data centers and all these
points of access around the world that
that is powered and we're where the
computation can now be placed closer to
where the point of action that having
the data can be and and the opportunity
for this industry to sit on top of the
platforms that you guys create and that
we create together that there will be an
application world that sits on top of
this and and and that's ultimately the
reason why we make something software to
fly it so that so that people could
invent applications we never thought of
and we see the same way the five years
of platform for innovation and I think
we just started at the beginning of it
but what can be done with this is
enormous for us as well yeah so today
today we we're not quite ready to show
you the results of our work we just
wanted to let you know that that the
world of 5g ran is coming Veeran is
coming and we're wearing were incredibly
honored and delighted to partner with
you guys and and to partner with the
people that that write the write that
write the books on teaching us about
wireless communications and and you know
you guys write the books on on on 5g we
write the books on computer graphics
yeah and so so you're looking at two
companies that apparently write a lot of
books thank you very much thanks for
inviting us today thank you so much
thank you thank you
guys Frederic Gatling uh I'll just make
one announcement about about Sweden if
you guys have not been to Sweden
absolutely go you know around summer and
I spent two continuous days drinking and
I took then it took me two months to
recover but but Sweden is one incredible
place I would characterize it as
delightfully strange delightfully
lovably strange and so really really
amazing people so Ericsson and video
working on 5gv ran on NVIDIA GPUs and so
that's those are the six miracles those
are the six miracles ai GPU for
extremely fast computing the networking
technology from Mellanox the security
technology from Mellanox that is able to
do it all at data rates line speeds the
fact that we now have the ability to
have an operating system an
orchestration system that allows us to
deploy quickly new applications in small
environment small computers to manage a
large fleet of systems not hundreds not
thousands like most companies but
trillions of devices in the future and
to be able to update them orchestrate
the service with very light touch in one
pane of glass a system that is created
that integrates all these great ideas
built for the very first time designed
to be an edge computer and then lastly
the work that ericsson and ourselves are
working on to virtualize the 5g radio as
a result what you're looking at is
essentially the six miracles that are
going to make it possible for us to put
AI at the edge to virtualize the edge
data center and to enable this new world
of
smart everything and so those are the
six miracles let's take a look at what
we can do and so these are some of the
fun things that we can do I've got to
show you a couple more this is the next
one is cloud gaming okay imagine if we
could play amazingly beautiful games
from everywhere in order to do that
latency has to be low we have to compute
this at the edge what you're going to
look at right now is running over 5g
this is only running over 5g and a data
center miles away okay and so because of
the low latency capability of 5g we're
going to be able to win we're gonna be
able to enjoy games amazingly beautiful
games running on these egx systems from
anywhere alright who's coming up here
David is that you alright so you brought
yourself a 5g phone and this is a 5g
phone you connected a little peripheral
to it so that you could enjoy the game
and let's see what you can do and this
is running over to 5g Network Verizon's
5g network locally all right let's take
a look all right you guys what you're
looking at here what you're looking at
here is a game on a phone this is a game
on a phone
frankly this makes no sense this is a
game on the phone
dave is one of our employees he's one of
one of our most senior software leaders
he's extremely good at playing video
games and that's what I pay him to do
[Laughter]
what do you guys think this is
incredible right
[Applause]
all of these all of these all of these
servers will have to be put near the
edge you would like you would like the
latency to be as low as possible and
with the low latency inherent in 5g
we're gonna be able to just deliver
every every every genre video game to
the phone wirelessly ok David good job
the H show what's your replay show how
did you do how did you do hang us Inglis
this this is David you know it always
looks good a first-person view and then
you look at it among the other cars and
then you realize what a terrible driver
you are you almost bumped into that guy
right in front of you you could pick
David out pretty easily he's the guy
that's driving it he's all over the road
I think he's the Ferrari
it looks so much faster when you were
driving huh yeah you got a glare there
sir
amazingly beautiful video game all right
you guys good job thank you and so we
have we have a service called GeForce
now and the way that we work is we
partner with telcos around the world so
that we could place these servers these
egx servers that I mentioned earlier
that's running an application we call
GeForce now and we have great
partnerships all over the world LG U+ in
Korea
Ross telecom in Russia Softbank in Japan
Taiwan Mobil in Taiwan Telefonica in
Spain and we're putting the service and
these servers in these data centers in
these countries so that people could
enjoy cloud gaming from anywhere these
servers are going to be precisely the
same servers that will then run AI for
intelligent agriculture and intelligent
retail it'll be the same server that
provisions the 5 GB ran the future is
software-defined and these applications
these low latency applications that have
to be delivered at the edge can now be
provisioned at the edge on these systems
let me show you one more application and
this one's odd mental reality augmented
reality of course has has has really has
really come in to come into the world
recently with applications and great
games like Pokemon var and so you could
you could a chase a Pokemon all over the
city
this level of AR is only only doable if
you have a powerful computer and this is
used for industrial design or this is
used for professional graphics and so
what we're going to show you now is
extreme augmented reality ok so if you
can see this
as you could see that cars on the stage
you guys are in the background it looks
like it's on the stage and Rosalyn's
gonna help us inspect this car ok so
take a look at look at what what
Roslin's sees you see a car this is on
Ruslan's phone let's zoom into it I want
to see you know look if I'm gonna buy
this car
this is McLaren I think goes for a
couple million dollars I'm gonna want to
see some of the detail look at that
don't shake
don't move dude
don't shake keep your core tight hold
your breath look at that look how
beautiful that is unbelievable well let
me go inside show me inside that's so
beautiful Frederick is gonna place an
order here free pretty soon I can
see him eyeing it that's a good-looking
car Frederick you and I can both get one
look at this can you see it can you feel
the leather okay
can you feel that look at the steering
wheel the leather on the steering wheel
can you see it so amazing amounts of
detail are you guys extreme augmented
reality you know one of one of the
things that that with windows 95
we consumer eyes 3d graphics and video
games became 3d with the with the
smartphone revolution computer vision
became consumer eyes if you take a look
at the modern computer vision technology
that's in most phones the capability is
really quite amazing it has to detect
and understand the surroundings register
itself to the surrounding track itself
relative to the surrounding it's posed
send that information in this particular
case sends that information to a data
center with the egx server there which
is running an incredibly complicated
computer graphics application it takes
that pose information it renders the
world within that pose it sends the data
back to the phone and that's what you
guys were seeing so cloud XR is the name
of our cloud XR is the name of our SDK
it's available today it works in both AR
mode as well as VR mode it allows you to
take your head mount display and its
pose information or your phone its pose
information stream it to the cloud
renders it sends it back it's all
perfectly registered
okay and so now whether your industrial
design or in entertainment or whatever
what not you can enjoy virtual reality
and augmented reality in real time all
because we now have a data center that
is powered by the egx and the low
latency of 5g cloud XR is available
today so that's it what we announced
today was a brand new computing platform
we call it the Nvidia egx edge super
computing platform it's made possible by
the combination of several new
technologies our advanced tensor core
CUDA GPU that does AI ray-tracing
incredibly great computation the latest
generation Mellanox NICs that does
security processing secure secure boot
it protects data in motion and data at
rest this is one incredibly secure
computer it is built on top of a stack
that is industrial-strength for the
cloud industrial strength for AI we have
a partnership with Red Hat to make a
telco core ready we have a partnership
with Ericsson to create one of the most
important applications for the future of
data centers the 5gv ran because of it
we can now really mobilize ourselves we
have that one killer app to start
installing these servers and let the
applications come to us over time we
have several applications we've already
built to stir up demand and to bring
application out to the edge GeForce now
as well as most metropolis IOT
metropolis IOT is available today it's
open source you could download it you
can modify it you could use it for smart
retail smart agriculture smart
manufacturing smart cities and then we
have a partnership with Microsoft that's
going to take this platform and theirs
together out into the world of an
intelligent edge because we see so much
demand for it and so this we believe is
going to be one of the pillars the
foundation to starting the smart
everything revolution
I want to thank all of you guys for
coming today thank you
[Applause]
Title: An Introduction to NVIDIA Iray Server
Publish_date: 2017-01-13
Length: 186
Views: 14800
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/eOER5PMD3rg/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: eOER5PMD3rg

--- Transcript ---

[Music]
hi I'm will from Nvidia today I want to
introduce you to a new way of
accelerating your rendering workflows
using a powerful distributed rendering
solution called Nvidia array server we
developed this solution as a companion
product for our array plugins as well as
products that already integrate array
like Douw studio you can easily
administer users and batch rendering and
take advantage of advanced features like
job modification and render elements
compositing from mobile devices and now
you can combine multiple workstations to
significantly accelerate image creation
here we'll be adjusting materials and
lighting during look development we want
our render to be as interactive as
possible in this case I'm running each
array plug-in on to Windows laptops and
a MacBook Pro while array server happens
to be on a powerful workstation that
could run either Windows or Linux I race
server in streaming mode lets you
connect your - studio viewport your
autodesk maya IPR and your 3ds max
activeshade sessions to a workstation
with multiple GPUs as expected when
rendering locally on a laptop GPU the
image takes time to resolve but time is
money
so let's connect to a workstation with
three and video Quadro m 5,000 GPUs and
see the massive improvement NVIDIA has
developed a flexible approach to queue
management with I ray server it has an
intuitive web interface that gives you
an easy way to remotely manage your
queue I'll submit a render job from
iRiver Meier now let's take a look at
the queue manager this is where we could
administer our users and the machines in
our rendering cluster let's look at the
hardware profile of the machines we can
add and remove these nodes as we need to
for the project we can also track the
progress of our job is the one that we
submitted from IRET for Maya once the
job is complete
assuming we have sign-off from our
client we can resubmit it and add any
additional render elements that we need
we can also use higher quality image
formats such as openexr
we can increase the resolution and the
quality
our unexpected deadlines can happen all
the time so if you're an administrator
we can add users we can add machines and
we can also adjust the priority of the
jobs in flight now that my latest render
is submitted I'm leaving the studio but
I'll need to check on the job later this
evening
so using Nvidia array server I can
monitor my progress on my phone wherever
I am this innovative solution will do
wonders for accelerating your rendering
workflows check out our website to
download the 90 day free trial and you
can also register on the advanced
rendering forum to find support and
tutorials for using iring thank you
Title: GeForce Garage– Building The GeForce GTX 1080 Ti Powered XForma MBX MKII
Publish_date: 2017-03-06
Length: 168
Views: 21480
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/EPsTOp-x2K4/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: EPsTOp-x2K4

--- Transcript ---

what's up everybody my name is Dwight
and welcome back to g-force garage you
guys just watch Derek and Joe build this
crazy beast of a machine if you've
missed it there's gonna be a link down
below thank you very much Derek and
aplomb man thank you very much Joe
thanks what you guys got to tell me more
about this machine all right tell me
about the case this is the xform of MK
ii chassis the owner of the company
Charles Harwood he own deck forma this
was his build up of Silverstone pjo 7
from like 2006 the original case was a
beautiful case I mean it basically is
this the same shape but just wasn't
water cooling friendly especially for
the time but this was Charles vision to
help people not have to go through all
the mods though basically this is water
flowing friendly how's the ball cool
what about the specs so we got the
latest and greatest we've got the 69 50
X 10 cores 20 threads and of course the
tooth front page 5 edition 10 in there
64 gigs of the classic Corsair
dominators
gotta have the best yeah I noticed you
guys also painted those yeah we did we
went for a custom paint job to give it a
little bit more or flash then of course
we dropped in a couple of the new 1085
just because because why not yeah I
notice you have some really cool
floating fans a pop standard comes with
a mesh in like however you can get a
floating fan grill some other
customizations includes all the custom
cables with this kind of case you kind
of have to do your own custom length cut
sets just because you have a window on
the back you have a large window in the
front and this case is pretty
unforgiving if you don't actually go
through that process and then the light
and everything up inside is a lumen ated
panel this particular panel is black
from the cases off by the second it
turns on you'll actually see the bottom
light up white that's the light you see
it's just that panel itself design any
additional light do you have a tube
that's actually going straight through
the floor it's one of the things you
very carefully but it is drilled
straight through and actually
illuminates the tube with the white
light
if it you can turn off the system or the
light control you can entities like a
flowing silhouette excuse Phil and then
up front we have the blood drip pattern
that can actually come in several
different colors poor sleep shows via
green for the film
imagine max good coloring on one last
detail we did special for GeForce garage
was a one-of-a-kind bag on the back
there usually it's the serial number
because there's going to be 250 cases so
each customer can choose which number
they want but for GeForce drives you
made a special one with color and logo
and everything like that so that about
wraps it up thanks everybody for
watching G forth garage and thank you
very much Joe and Derek for delivering
this crazy machine thanks for havin us
and if you want to see the build video
of it make sure to check out the links
down below until then we'll see you next
time
Title: NVIDIA Gaming Expo - E3 2014
Publish_date: 2014-06-11
Length: 157
Views: 18981
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/EvSdYq3hzLA/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: EvSdYq3hzLA

--- Transcript ---

every year thousands of industry
professionals flock to the Los Angeles
Convention Center for e3 we thought
there should be a way for gamers to
experience this so we made the Nvidia
gaming expo III is all about games and
we've got an amazing line-up of titles
like evolve dying light gauntlet war
machine Wolfenstein and many more all
these experiences are being powered on
GeForce GTX PCs with graphics cards like
GTX 780 all the way up to the all-new
Titan Z GeForce GTX doesn't just mean
desktop it also means incredibly high
powered notebooks that are slim portable
and easy to game on we got a great
lineup of titles like all new EverQuest
next landmark one of the ultimate
terrain manipulation games out there
right now Nvidia gaming technologies
extend far beyond graphics cards what
we're looking at right now is Nvidia
g-sync and only technology that allows
the GPU to talk to the display what does
this mean for gamers it means that we're
going to completely eliminate tearing
and stuttering in your games and provide
a silky smooth experience not only do we
have amazing desktop and laptop
experiences we've also got some pretty
stunning 4k surround demos what you'll
see is actually three 65 inch 4k TVs
powering an incredible project cars
driving experience a big part of the
evolving and video gaming platform are
the many different ways you can
experience and enjoy your favorite PC
games with steam machines you can take
your Steam library outside of your PC
take it into your living room or pretty
much anywhere you want with steam
in-home streaming and what would it
Nvidia event be without the ultimate
gaming portable Nvidia shield we've got
an incredible lineup of both Android and
PC games showing that you don't have to
compromise on your choice of games
console mode enables you to take all of
the great shield experiences that you
already have and bring them right into
the living room on the big screen
whether you want to play games like
League of Legends or Starcraft with
Bluetooth mouse and keyboard or kick
back with a Bluetooth controller you can
play however you like
what would a gaming event be without
eSports we have an incredible 6v6
titanfall stage where fans are going to
get the chance to team up with pros and
compete against other gamers for the
chance to win GeForce GTX hardware and
awesome prizes from our partners as you
can see people are having a blast
enjoying amazing games and demo it's
free music and food stay tuned to at
NVIDIA GeForce on Twitter and NVIDIA
GeForce on Facebook for the latest news
from e3
Title: Accelerating Data Science with NVIDIA-Powered Workstations
Publish_date: 2019-04-16
Length: 141
Views: 20786
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/F0FYJbwII88/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: F0FYJbwII88

--- Transcript ---

today most organizations have massive
lakes of data data that's that's so
cumbersome so big that for them to
interact with it often takes days you
want to get started on a beefed-up
machine that can crunch through your
data as quickly as possible our
customers are so excited about something
that is going to fundamentally change
how enterprises interact with their data
today what workstations allow you to do
is just get started right there and then
on your own personal machine and start
analyzing data unprecedented speeds what
I'm excited to announce is the data
science workstation powered by Nvidia
running r-tx GPUs layered on top of that
with a software stack of CUDA X ai and
that entire stack is something that
we're delivering as a platform to get
data sciences off the ground and running
faster with more horsepower more
capabilities than they've ever had
before it's really a story about a
platform libraries that have been
gathered together that we've accelerated
using crew 2x AI and something we call
Rapids which is this collection of
libraries that just dramatically
accelerates your ability to do things
like predictive analytics with your data
we're really excited about these
technologies because data logs tools are
the first mile of the data journey to
bring more data assets into those
workstations and data scientists can
just get started very very quickly on
their work we've really just given the
world time machine now they can predict
things with their data instead of just
digging through old logs and
understanding the past now they can
understand the future we're powering a
major service provider in the United
States five hundred million records for
over two hundred thousand access points
and we were able to visualize and
interact with that data in real time and
then we were actually able to predict
values into the future in terms of
outages and at capacity using Rapids and
it really does move the needle it's not
just quantitatively faster but it's
qualitatively
we see erotic is such a powerful way to
accelerate data science a connecticut
was funded with nvidia to hobb data
scientists built rapid models much more
effective and then deploy them in
production in their active analytical
applications what we're talking about is
something you can actually buy today
this is this sort of thing that will
change the way that we do business
[Music]
you
[Music]
Title: NVIDIA GTC 2018: Show Highlights
Publish_date: 2018-03-29
Length: 164
Views: 691543
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/f5cy6P49jTU/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLAWo8uI-EU9RtpWfRxw2G1hSLw8Rw
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: f5cy6P49jTU

--- Transcript ---

a record crowd more than 8,500 descended
this week on invidious 9th annual GPU
technology conference the world's
premier AI event held here in Silicon
Valley from machine learning to
self-driving cars from VR to smart
cities GTC brings together researchers
developers business leaders and startups
all looking to harness the power of deep
learning GTC has grown nearly 10 times
since its start in 2009 this year there
are more than 600 sessions 300 press
from around the world and more than 150
exhibitors the highlight as in past
shows was an inspiring keynote by CEO
Jensen Hall who unveiled major new
products and technologies but GTC at its
core remains of Developer Conference
many come here to bolster their skills
by attending one of the hundred and
sixty hands-on labs offered by Nvidia
steep learning institute measured by the
buzz the center of the show was the expo
hall Nvidia Zone booth featured a range
of AI tech demos from smart cities to
visualization attendees also got a peek
at the latest cars using deep learning
by Toyota for city's bends and others
the crowd favorite was the new
autonomous Robo car nearby in the
robotics showcase we demoed AI powered
systems automating tasks like
manufacturing and food delivery GPU
computing is allowing robots to work
alongside humans and improve workplace
safety
and in the VR village attendees immerse
themselves in virtual worlds generated
with Nvidia technology especially in the
collaborative holodeck platform most
popular was the ready player one demo it
wouldn't be GTC without startups six of
the hottest competed in the finale of
our inception Awards vying for a million
dollars in prizes the purse was split by
winners in three categories autonomous
systems enterprise and healthcare Nvidia
also named two winners of its global
impact award given the companies doing
groundbreaking work to address
humanitarian problems using Nvidia
technology and we hosted our fifth
annual women at GTCC event engaging
women doing breakthrough work and deep
learning and visual computing it's been
a record event and we're already looking
to regional gtcys and tech hubs around
the world later this year until then
find out how you can get your hands on
all the show content and weeks ahead
check the site for details hope to see
you next year
at GTCC
[Music]
Title: Mercedes-Benz Concept Car Powered by NVIDIA DRIVE at CES 2016
Publish_date: 2016-01-07
Length: 117
Views: 91600
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/fBwmH7TAupI/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: fBwmH7TAupI

--- Transcript ---

I'm Joseph Fagan senior 3d artist at
mercedes-benz research & development of
North America and behind us we have the
concept I am which is an aerodynamic
vehicle totally about efficiency
it's a plug-in hybrid and it's as
strikingly beautiful car there's two
modes there's the design mode which is
the standard mode for the car and then
there's the aerodynamic mode in
aerodynamic mode the car transforms the
front grille flaps close the tyre hubs
move out and the back fin actually
extends which is a pretty interesting
and beautiful transformation but when
you're inside the car obviously you
don't get to see that so we wanted to do
something inside that was equally as
striking - is a collaboration we did
with Mercedes that took our real-time
technologies that we have in our 3d
program right within their displays not
only for me to the gauges but we can
also offer a very high res 3d
representation of the vehicle that
you're in you can see different elements
of the car the back bend and the tires
and what's going on with those it almost
becomes like an interactive manual for
the car you can kind of really get a
sense
or what the car can do we're always
trying to push the effect see the detail
at the model how much fidelity is there
and that requires a lot of graphics
horsepower and the Nvidia Drive platform
allows us to push those boundaries and
to introduce things like HDR rendering
or image based lighting and keep a very
high frame rate so it's very fluid
we chose Nvidia because they're the top
of the game right now there's really no
one better doing computer graphics and
hardware I mean it's what they're doing
is amazing we knew that they would be a
great partner and they they really have
been fantastic
Title: NVIDIA Booth Tour at CES 2016
Publish_date: 2016-01-08
Length: 166
Views: 24631
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/Fdzy9wWyp5E/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: Fdzy9wWyp5E

--- Transcript ---

hey everybody its Dave Anderson I'm here
at CES 2016 at the Nvidia booth where
this year's focus is all about
self-driving cars one of the first demos
that I'd like to show you is actually an
in vehicle demo what we've done here is
we've taken a Mercedes GLA and we've
modified this vehicle to have a Drive px
on board and to the drive px
we've connected two new sensors we've
actually integrated these sensors behind
the bumper of the vehicle one is a
spinning lighter and the second is a
camera that's mounted here in the center
both are self-contained and concealed
behind the bumper of the vehicle and the
purpose of having two sensors is to show
you an idea that we call sensor fusion
so that's basically fusing the data from
both of these sensors the purpose of
these sensors is to actually give you a
view of the front of the vehicle where
we are detecting pedestrians walking
back and forth in front of the car one
of the benefits to having Drive px
onboard and using two different sensor
types is that we can work through bad
weather
like for example rain so let's make it
rain so now with the rain going you'll
notice that the pedestrian detection
algorithm is still actually running
flawlessly behind now let's go on to
some of the fundamental technology
that's running inside that drive PX
system
one of our other major announcements was
Nvidia Drive net a neural network that
we've developed using our Nvidia digits
dev box that can be trained with a wide
variety of different data sets allowing
us to detect things like cars people or
other objects that might be in the path
of the vehicle in this next
demonstration we outfitted the vehicle
with six cameras and four lidar sensors
we then drove this vehicle in the real
world we added to it a deep neural
network that allowed us to do object
detection detecting cars and trucks in
the field of view of the vehicle we then
overlaid all that information on a
detailed map and we created a location
for the vehicle inside the world that
then gives us a total situational
awareness around the vehicle but
obviously this is too complex to show a
driver in the Driver Information Center
we're receiving all the detailed
information from Nvidia drive px that
information is quite complex
so what we've done is we've distilled it
down into something that is visually
intuitive for the driver to feel
confident about the path and the
trajectory that the vehicle is going to
take it's going to be an exciting future
for NVIDIA self-driving cars
you
Title: GTC Spring 2021 Keynote Highlights with NVIDIA CEO Jensen Huang
Publish_date: 2021-04-15
Length: 130
Views: 34400
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/FKg4rAf5VgA/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: FKg4rAf5VgA

--- Transcript ---

what a gtc
it's incredible to think how much was
done in this last year
nvidia is a full stack computing
platform
we're building virtual worlds with
nvidia omniverse that will help build
the next wave of ai for robotics and
self-driving cars
we announced new dgx systems and new
software
megatron for giant transformers clara
for drug discovery
and cool quantum for quantum computing
nvidia is now a three chip company with
the addition of gray cpu
designed for a specialized segment of
computing focused on
giant scale ai and hpc and for data
center infrastructure processing
we announced bluefield 3 and doka 1.
nvidia is expanding the reach of ai we
announced an important new platform
nvidia egx with ariel 5g to make ai
accessible to all companies and
industries
we're joined by the leaders of it vmware
computer makers infrastructure software
storage
and security providers and to lower the
bar
to the highest levels of ai we offer
pre-trained models
like jarvis conversational ai merlin
recommender system
maxine virtual conferencing and morpheus
ai security
all of it is optimized on nvidia ai
enterprise
and with new tools like nvidia tau fleet
command
and triton it is easy for you to
customize and deploy on egx
and drive our end-to-end platform for
the 10 trillion dollar transportation
industry
which is becoming one of the world's
largest technology industries
from oren and atlan the first 1000 tops
soc
to hyperion 8 a fully operational
reference av car
platform to drive sim to drive av
we're working with the industry at every
layer
20 years ago all of this was science
fiction
10 years ago it was a dream
today we're living it
Title: Digital Domain Brings Virtual Humans To Life With Machine Learning
Publish_date: 2019-11-01
Length: 142
Views: 108443
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/fMwq9Xa2v2Y/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: fMwq9Xa2v2Y

--- Transcript ---

I am a fully simulated artificial
replication capable of being driven and
rendered in real time over the last
couple of years we've digitized me I've
got a digital avatar that looks pretty
close to what I look like and it runs in
real time it's not just a digital human
that we're rendering for a film where we
can have artists fix it up and
everything it has to actually be
realistic in the moment I'm the director
of a digital human group it's basically
a team of engineers and artists and
technicians all working towards the golf
queen most photorealistic digital human
possible written my backgrounds in
feature films creating the digital
characters like than ours and others for
big movies and what we started seeing is
that the hardware is first Nvidia
Hardware started getting to the point we
could start to sort of close that gap
from abuddin
offline for a feature film rendering to
be able to do real time and instead of
running hours per frame rendering in
milliseconds we're using NVIDIA hardware
and a whole wide range of new things
most recently were using in videos r-tx
architecture to do a lot of ray tracing
we're doing ray tracing occlusion ray
chasing shadows ray tracing everything
that's just one aspect of it how to make
did you done look real but he also has
to move real and behave real and that's
where Nvidia is machine learning
technology has really enabled us to do
the kind of training and machine
learning stuff that we wanted to do so
we have bolted cards we have our T X's
that we're using to the slam training
and inference through we also use in
video throughout our entire company to
make film we're moving a lot of our
rendering from CPU based renderers
to GPU based renders so how the
character behaves and how the character
looks is all based on Nvidia technology
and it's going to take us into the
future
really
who sees things at SIGGRAPH right now
that five years ago how I said no way
this is this is not possible now you
look at it and you go okay yeah I kind
of got how they're doing that this is
cool
the fact that we've changed our mindset
that much and the things that were
impossible are now possible more
exciting is that
[Music]
Title: NVIDIA Iray - The New Reality in Digital Design
Publish_date: 2016-02-09
Length: 112
Views: 46308
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/fq34dI47KjI/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: fq34dI47KjI

--- Transcript ---

there's a new reality in digital design
enabling you to visualize your creations
with complete confidence model with
natural physically based lighting
systems
pull from a spectacular selection of
materials and textures
enabled by the powerful Maxwell
architecture and made possible with
Nvidia array it was first put to the
test in the development of NVIDIA zone
products precise real-time feedback gave
the freedom to iterate without the need
for physical prototypes
now you can experience this giant leap
in realism integrated directly into your
favorite tool capture the way light
streams through a window in every season
to design the ultimate environment
explore materials that are so real you
can almost feel them
then share them across workflows for the
first time ever
you can now iterate more prototype less
and create your best work faster than
ever before
IRA takes the guesswork out of design so
you can move beyond digital
approximation to a new digital reality
Title: Project Cars - Gamescom 2014
Publish_date: 2014-08-20
Length: 159
Views: 31918
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/fSTqDEbfjAA/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: fSTqDEbfjAA

--- Transcript ---

there are many racing games out this
year
project cars is the only one which
offers a
authentic realistic uh really
technically advanced racing game out
there
really kind of like focuses on the
driver
has a wide variety of motor sports in
the game
and as i say it's like really really
technically advanced it's got one of the
most
powerful physics engines underlying the
entire system which lets us do really
really cool things like dynamic time of
day and weather and
pit stops and things like that so yeah
you know kind of like a hyper realistic
um shaders and things like that on the
car um
my new detail on all the cars and things
like that as well uh taking thousands of
reference photos of the tracks and then
recreating those uh not just
mathematically correct but also like
emotionally correct like recreating the
atmosphere of those with things like
wind and turbulence things like that in
the air we've been making racing games
for like over 10 years now
and we have a lot of expertise a lot of
pedigree in this kind of genre but by
involving the community in there as well
like they've been able to give us like
additional information to um you know
some of the guys actually live near the
tracks and go out and get the reference
photos for us as well but the third side
of that coin is we've got real racing
drivers as well helping us make the game
and these guys not only have visited
those tracks they've raced on those
tracks our collaboration with nvidia has
been fantastic you know we're also
working with panasonic as a as a trio of
uh people have been experts in our field
you know uh without the geforce uh card
we wouldn't even be able to do 4k
project cars is one of the only games to
run at 12k resolution you know we we did
4k and it just wasn't enough you know we
were now when i actually put three 4k
televisions next to each other and run
the game at 12k and it is phenomenal the
amount of fidelity and clarity they can
get you know is powered by the geforce
gtx titan card which
is just a fantastic piece of kit you
know people have got to if they're
looking for a brand new graphics card
out there like this is the you know our
recommendation of the one to go for um
and as i say it just we put so much
attention to detail into the actual game
that that level of clarity especially on
these amazing 4k tvs that there are
nowadays
yeah you get to really see like all the
details put in there
you
Title: Picture Perfect: NVIDIA RTX Studio Speeds Up Lightroom
Publish_date: 2019-11-13
Length: 125
Views: 11311
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/fsVYq_Ot2wc/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: fsVYq_Ot2wc

--- Transcript ---

Adobe's Lightroom is a tool that's used
by photographers to both manage their
photos and edit those photos and then
have a variety of options for output
it's an essential tool for a
photographer these days the thing that I
noticed most of all about nrt X based
laptop was the speed as a photographer
and the types of things I shoot are a
lot of concerts you shoot a concert you
might shoot three or four thousand
photos in an EP you got to bring those
photos and you got to decide which of
the photos are going to want to work
with and then you have to edit those
photos then typically get them back to
the band in that evening so that becomes
a very long process if you don't have
something fast to work with the benefits
of using an outtake studio laptop is
that it speeds things up much faster and
it's really good for me because I'm
always on the go I moved over to an
archaic laptop about six or eight months
ago now I'll never go back because it is
just changed the whole workflow process
once I'm putting something in that
machine and I'm starting to work on it
it's instantaneous I've picked out a
photo and worked with the lightening
adjusting luring all just super smooth
the new anti base enhance details
feature will really benefit me because I
started using a new camera which is 61
megapixels file with his large files
this AI base enhance details features
will really help me speed things up one
of the things that really blew my mind
is when you start using the enhanced
detail feature which is r-tx it
accelerated it's like night and day
compared to what you're used to
you can take the crowd shot lead singer
or something like that
and really enhance those two things
separately but they just pop right out
of this
that's a beauty that I've never seen
before
I love seeing what's starting to happen
with AI in the photography space we're
just scratching the surface think about
where this is all going you're able to
have that machine learning environment
you know types of photos you're taking
to help you organize the better you're
able to have some AI be able to help you
pick and choose the right ones for
composition and color that might be
going with something else that could
work it's really the birth of a next
generation of photography it's an
amazing
Title: Perceiving a New Dimension - NVIDIA DRIVE Labs Ep. 6
Publish_date: 2019-06-19
Length: 63
Views: 31134
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/ftsUg5VlzIE/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: ftsUg5VlzIE

--- Transcript ---

Today in DRIVE Labs, we're going to talk about
computing distance between the self-driving
car and objects in its environment using images
from a single camera and deep neural network
processing.
This is our DRIVE AV Mission 19A.
The green bounding boxes denote objects detected
on the scene, and the number shown at the
top of each box is the radial distance in
meters between the center of the ego car's
rear axle and the detected object ahead.
Here the DNN is computing distance to objects
on a very crowded hilly San Francisco street,
accurately detecting distance to both pedestrians
and cars despite the road's steep grade.
And here we have single camera DNN based distance
to object estimation running in snowy conditions.
Detecting distances to pedestrians and parked
cars, as well as cars coming from the opposite
direction.
These distance-to-object results are used
with other perception outputs to compute control
commands for various maneuvers such as automatic
cruise control or autonomous lane changes.
Title: Adam Savage steps into NVIDIA’s VR Funhouse
Publish_date: 2016-05-27
Length: 246
Views: 477316
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/fUL610qR6WM/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: fUL610qR6WM

--- Transcript ---

hey my friends an nvidia called me up
and told me they had some really cool
stuff for me to see in VR apparently
they've been experimenting with hyper
realistic physics engines so I'm about
to go see what they're working on hello
Adam hello rev how's it going good good
thank you for joining us thanks for
having me what we're gonna show you is
what we call VR funhouse it's our newest
game it's essentially a sandbox for
testing out the various types of physics
that we've been developing over the
years inside this virtual reality space
you guys gave me a chance to play around
in your funhouse the physics which is
the main point of this demo is amazing
that is really cool but the most part
its operating the way I would expect
things to operate within the world I've
played around with games that have their
own physics engines and for this to have
genuine physics that you can work with
that gets me really going oh I love the
swords specifically the metal
reflectivity that had a tremendous
effect on making it feel really real we
have a turbulent simulation so when you
wave the swords around in the air you
can see the confetti rolling around
excessive latency can make people sick
and specifically make me sick I have a
very weak stomach I did this for two
hours I didn't have any problem you want
to stop and rest I'm good to keep
playing alright
no latency was low enough and the frame
rate was high enough that I felt locked
into this space dude
I liked shooting at the plates I can
hold up the gun and get the sights
correct on the target I love the fact
that as I shoot the pottery that's
coming towards me the particles after I
shoot it come towards me and are still
in the ground and I can still shoot them
you're welcome to use both guns I think
my favorite thing though was the squirt
guns specifically when I realized I
didn't have to shoot all the way in the
air I could actually adjust the flow I
even noticed as I bring those streams
together you guys have built surface
tension into the lick this I could
squirt one stream through another and
start to see them interact with each
other whose viscosity answer yeah that's
freaking cool I'm somewhat familiar with
how much computational muscle it takes
to model hundreds or thousands or tens
of thousands of pieces of things as
they're exploding and dropping around
you to see the photo realistic stuff
that you guys are generating now
happening live in front of me it beggars
the imagination how much processing
you're doing in the back the haptic
feedback is really critical and I'm
totally clear that it's one of the
toughest problems to solve is invention
uh-huh the handles did this great job of
giving me different kinds of vibration
to give me a feeling of different things
that were happening oh my god I can feel
that's really awesome this is better
than anything I've seen in terms of the
physics of being able to manipulate the
world that I'm immersed in oh I can
totally get addicted to this you
mentioned you had some ideas on how we
can make our game better
I thought the swords were just a little
bit rudimentary the blade you have if
you cut it in a cross-section would be
like diamond shaped it's more realistic
to have what's called a hollow grind in
which the diamond shape the slopes of
the diamond are actually concave what we
really want to do is inspire others to
create similar sorts of experiences so
we decided to put out all of the source
code and all of the assets for
this game to the general public is
open-source so that they can modify it
and make those kinds of changes
themselves maybe we can scan some stuff
on my shop and absolutely okay good good
Reb thank you so much for letting me
play in your sandbox thank you for
coming and experiencing this with us
that was awesome
you
Title: Clara: Supercharging Medical Instruments with AI
Publish_date: 2019-03-06
Length: 118
Views: 12679
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/fvFO6uQvrfI/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: fvFO6uQvrfI

--- Transcript ---

[Music]
magnetic resonance imaging or MRIs are
commonly used to help diagnose a
significant number of medical conditions
from connective tissue injuries to bring
tumors some MRI studies require the
injection of a contrast agent into the
patient to better visualize specific
findings powered by nvidia gpus a nvidia
sclera sdk subtle medical has found a
way to reduce the use of the contrast
agent gadolinium in MRIs without
sacrificing the quality of the images
not only that because Clara is
containerized the implementation can run
on local workstations on-premise data
centers and even in the cloud using NGC
on the Left we have an MRI scan of a
patient with a large brain tumor located
centrally in the brain this MRI was done
using 100% of the required contrast
agent which helps us to identify and
separate the tumor from the rest of the
tissue and bone the tumor is seen here
as a white centrally located circle in
the middle panel we see the same MRI
image however in this example only 10%
of the contrast agent was administered
notice how the tumor here blends in and
is harder to distinguish from the
surrounding brain tissue here we see the
results of subtle deep learning model
powered by nvidia gpus and the Nvidia
claire sdk the model produces an almost
identical image to the full contrast
image on the left another great benefit
of this AI enhancement shown here is the
MRI scan can be performed in a quarter
of the time
this means less time in the scanner
trying to stay still and increase
patient comfort simply pull it Clara
enables the most advanced methods for
reconstruction segmentation
visualization and AI
Title: SHIELD GAMING - Metal Gear Solid 2 and 3 Dev Diary
Publish_date: 2017-11-28
Length: 65
Views: 16343
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/fw6FnO7Aerg/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: fw6FnO7Aerg

--- Transcript ---

the Metal Gear series has always been on
consoles and as such it naturally
translates over to the shield TV quite
well with the power of the x1 chip
players will be able to experience these
games with rich graphics and fast
gameplay since bringing Metal Gear
Rising revengeance to the shield
platform our team has gained a wealth of
knowledge and programming and optimizing
games for shields and a lot of that now
translates over into these games I think
our players will be delighted to be able
to play these games on the most advanced
streamer in the world
Title: GeForce Garage: Antec 900 Series – Rig Recap
Publish_date: 2016-01-26
Length: 239
Views: 31019
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/FwX-TzlSzzM/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: FwX-TzlSzzM

--- Transcript ---

hi I'm Andrew with NVIDIA you're
watching g-force garage now that we're
done with this awesome build I'm going
to have my friendly carrington come out
describe all the work that went into
this crazy case we're going to describe
some of the behind-the-scenes stuff as
well as all the components to make this
amazing PC possum
so tell us exactly what the finer
details are of some of these components
and processes here so like in the first
episode you saw rod do the case window
as you can see they're nice big window
Brian did the motherboard skirt to make
it nice and clean their kind of hid some
holes that we didn't want to see and
then you had John who did all the
beautiful cables as you can see they
look lovely so then we had Thomas who
came in and showed you guys how to do
some cool C&C stuff if he did these side
markers as one here and then one on the
back there wasn't enough room inside the
chassis to add another radiator so I
asked Thomas I said hey is there any way
you can make this capable the handle two
radiators so he said he could do it and
this is what he came up with this whole
front and whole top is all custom you
did these little fins that basically
just kind of drop in here so this is one
big solid piece of acrylic and then same
with this one Dwayne showed us how to do
some basic spray paint with a gun metal
gray we took the gun metal gray and had
Ron come in and put some air brush down
I took it a step further and took it
over to my auto body guy and he put down
clear for us and now we have more of a
reflective and nice makes all the green
pop really well you saw him paint the
power supply I actually had the power
spot cleared as well and then he went
ahead and sprayed up the motherboard so
there's two tone colors on the
motherboard tray some other things we
did I cut out this whole section here to
be able to see the reservoir because
what we ended up doing is water cooling
the whole system so you have a hard line
tubing and then James Walter was kind
enough to supply us with a badge for the
SSD so it matched all the aesthetics
aside I mean inside this thing is a
beast of a gaming PC too tight next
that's just GPU in the world so not only
does this thing look beautiful but it's
extremely powerful as well I got one
more thing though Lee you do it's very
special and I want you to do the honors
the official geforce garage case batch
lee i want you to be the one to adorn
this badge
our brand-new model cool I'd be honored
what do you think dude and that's it
that's it that's the build it's done so
now we want to say thank you so much to
all the modders that made this build
possible you guys are amazing and keep
doing what you do if you guys have some
ideas on what the next build should be
so if you need help on a particular
projects please let us know for the next
series by going to g-force calm / garage
and send us your ideas
at g4s calm / garage we have a ton more
content for you to check out if you
hadn't had enough if you like this video
there's going to be some more coming up
on the screen right now boom right here
go ahead and click it and there's a
second video for you to check out right
here whoo all right take that no no
seriously check that
Title: NVIDIA Automotive Tech on Show at CES 2015
Publish_date: 2015-01-08
Length: 119
Views: 12365
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/FX05jUnFnrQ/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: FX05jUnFnrQ

--- Transcript ---

hey everybody it's dave anderson uh with
nvidia here at ces 2015 at the nvidia
booth where this year's focus is all
about automotive i want to take a few
minutes and take you around the booth
and show you some of the amazing demos
and cars that we have on display
one of our major computer announcements
is nvidia drive cx featuring our tegra
x1 processor this will power your next
generation digital cockpit experience
this is our material showroom which
allows us to take real world materials
and via our nvidia drive studio convert
them into physically accurate content
for our nvidia drive cx
one of our other major announcements was
our nvidia drive px computer which will
enable application development for
semi-autonomous and fully autonomous
driving one of the applications is deep
learning using neural networks which
allows your computer to become
intelligent
another feature of nvidia drive px is
surround vision enabling your car to see
the world around it an application of
this is autonomous parking
one of the culminating points for our
technology is shown here in this
renewable vehicle it features a digital
cockpit powered entirely by an nvidia
drive cx computer
hope you enjoyed the tour can't wait to
take you for a ride in an nvidia powered
car
me
Title: Introducing GeForce Garage
Publish_date: 2014-07-21
Length: 53
Views: 66917
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/fZ_iaicJ4uc/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: fZ_iaicJ4uc

--- Transcript ---

hi i'm andrew with nvidia today i want
to introduce to you the geforce garage
our new diy workshop for gamers pc
builders and modders there's no question
that boutique system builders make some
of the best gaming pcs on the planet but
that doesn't compare to the pride and
satisfaction of building your very own
pc however building your own gaming pc
may seem like a daunting task to the
inexperienced
at geforce garage we're your resource
for everything pc building and we'll
walk you through the process step by
step whether you're looking for the
latest budget build guide a how-to on
the coolest mods or whether you have
absolutely no idea what you're doing
head over to geforce.com garage for all
the information you'll ever need we're
going to bring in some of the best
industry professionals to show you how
to do some of the most cutting edge mods
for your pc gaming rig
[Music]
Title: Connecting in the Metaverse: The Making of the GTC Keynote | Trailer
Publish_date: 2021-07-27
Length: 86
Views: 86511
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/f_Lv8BOjs4E/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLAXunS4ldLCGVX4dhJesAlIyXVHOQ
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: f_Lv8BOjs4E

--- Transcript ---

gtc is definitely one of our biggest
events that we do
it encapsulates industries from
healthcare to gaming
to automotive we spend a lot of energy
and time and blood sweat and tears to
present it in the best light possible
and this year we full on went into
omniverse
for our gtc presentation
creating a digital version of our ceo is
a big deal
we had so many variety of jensen to pick
from
we had 21 version and different varying
performance
and it was actually getting hard to
choose between the 21.
i focused a lot on drive sim and helping
them build real-time asynchronous
rendering worlds this was maybe like
three weeks out from when we needed a
little recording
and we went up to record and the
vehicles would all sit there and then
they move but their wheels all stay
behind
but um yeah we fixed that book when you
have
effects such as physics and fluid
they're
in omniverse because artists ask for it
and the developers
listen our creative team is now an
extension of our engineering team which
is really remarkable
i doubt that happens in many other
places if any
Title: BMW Group Celebrates Opening the World's First Virtual Factory in NVIDIA Omniverse
Publish_date: 2023-03-21
Length: 168
Views: 61962
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/g78YHYXXils/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: g78YHYXXils

--- Transcript ---

The world’s industries are accelerating 
digitalization with over $3.4 trillion being  
invested in the next three years.
We at BMW strive to be leading edge in automotive digitalization.
With NVIDIA Omniverse and AI we set up new factories 
faster and produce more efficiently than ever.
This results in significant savings for us. 
It all starts with planning – a complex process in which we need
to connect many tools, datasets and specialists around the world.
Traditionally, we are limited, since data is managed separately 
in a variety of systems and tools. 
Today, we are changing all that.
We are developing “Custom Omniverse Applications” to connect
our existing tools, know-how and teams – all in a unified view. 
Omniverse is cloud-native and cloud-agnostic 
enabling teams to collaborate across our virtual factories from everywhere.
I’m about to join a virtual planning session for Debrecen in Hungary – 
our new EV factory – opening in 2025.
Letʼs jump in.
Planner 1:Milan is joining.
Milan: Hello, everyone. 
Planner 1: Hi Milan – great to see you.
We’re in the middle of an optimization loop for our body shop.
Would you like to see?
Milan: Thanks – I’m highly interested. 
And I’d like to invite a friend.
Planner 1: Sure.
Jensen: Hey Milan! Good to see you.
Milan: Jensen – good to see you, too. 
Welcome to our virtual planning session.
Jensen: It's great to be here. What are we looking at?
Milan: This is our global planning team who are 
working on a robot cell in Debrecen’s digital twin.
Matthias, tell us what’s happening …
Planner 1: We just learned the 
production concept requires some changes.
We’re now reconfiguring the layout 
to add a new robot into the cell.
Planner 2: Ok, but f we add a new robot, on the 
logistics side, we’ll need to move our storage container.
Planner 3: Alright, let's get this new robot in.
Planner1: That’s perfect.
But let’s double-check - can we run the cell? …
Excellent.
Jensen: Milan, this is just incredible! 
Virtual factory integration is essential for every industry.  
I’m so proud to see what our teams 
did together. Congratulations!
Milan:  We are working globally to optimize locally.  
After planning, operations is 
king, and we’ve already started!
To celebrate the launch of our virtual plant,  
I’d like to invite you to open 
the first digital factory with me.
Jensen: I’d be honored. Let’s do it!
[Music]
Title: Introducing NVIDIA Dabbler 2.0 w/ DirectStylus 2
Publish_date: 2014-11-26
Length: 238
Views: 63144
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/G7JJqM8oMYQ/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: G7JJqM8oMYQ

--- Transcript ---

hello everyone my name is Louis I'm Emma
I'm a French artist Milligan Los Angeles
California I'd be taking you through
some of the basic functionality and some
of the newest features of w 2.0 on the
Nvidia shield tablet the first thing you
will notice about w 2.0 is the great new
look and functionality it is fully
aligned to the new Android 5.0 lollipop
material design language the whole loop
is a lot more flat and as the great
smooth animation when bringing in the
menus and submenus
it makes using the app a real enjoyable
experience first I will point out some
basic items to get you started in the
top left you can choose what mode you
want to double in whether it is sketch
mode watercolor mode or auto mode I'm
going to choose the watercolor here I
can choose a white brush a standard
brush a marker a pen and then you race
it from the menu on the right I'm going
to choose the standard brush and lower
my sensitivity level a bit now I'm going
to choose my color and adjust the level
of water on my brush you can see how
that looks on the canvas now I can
change the radius size of my brush here
and change my opacity level here one of
my favorite function in watercolor is
the gravity mode that you can choose
from the two menu this allows to hold up
the tablet and watch the pen flow down
the page naturally thanks to the GPU of
the NVIDIA Tegra k1 processor I can even
turn the tablet in real-time and watch
the pen switch direction now let's talk
about the newest features you will find
in developed 2.0 I'm going to pull up
the sketch I've been working on here
first is the new layering system which
can be found on the right-hand menu it
lets you create up to three separate
layers by just using the plus button
change their priority using the arrows
here and merge them using the merge
button very simple and entry you can
also pinch to pull up from the canvas
and pan around being sure to use two
fingers
this makes it easy to get to every part
of the canvas you can then pull out to
get back to the normal size of your
candidate so in the previous version of
the app you could mix custom colors by
just choosing one color and mixing in
clockwise into the mixing circle then
choose the second color and mix it in a
clockwise direction if you went too far
you could just spin in a
counterclockwise direction the new
dabbler lets you save those custom
colors in these and she loves you can
save up to 16 colors so you won't need
to keep mixing custom colors of finding
the color you're looking for with the
droplet tool let's say you want to use
some perfectly straight edges in here
I'll put some circles all you have to do
is choose the ruler from the tool
section and adjust the size and
direction of the rectangle using these
sides on either side if you want to
change the shape just up on one of the
sides then just use your brush or pencil
against it and you'll get the crisp line
you're looking for you can even record
your session just by tapping on the
share button and choosing this video
option the she'll tablet will record
your out session as long as you have
enough space left in your storage
there's a lot more to learn about Dobler
so as you're enjoying the app just press
and hold on any menu item to get a hint
of with the future - I hope you continue
to enjoy and create using the new tablet
2.0 on the Nvidia shield tablet to learn
more visit shield nvidia calm
Title: 'Real or Rendered' Demo at SIGGRAPH 2015
Publish_date: 2015-09-04
Length: 79
Views: 11577
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/gA9WKO_qcfQ/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: gA9WKO_qcfQ

--- Transcript ---

hi I'm Ian Williams and I'm going to
explain what we're showing you here
today at siggraph what we have here is a
physical real or ended we thought we'd
take this concept of real or rendered
that we've used in several comparisons
and take it to the next level and
actually have a physical drill compared
with a rendered drill and what we're
showing is a light box that contains the
physical drill we have a camera looking
at that and then on the computer monitor
here we're displaying that live video
feed along with a rendered version of
the drill and the purpose is to ask
people which they think is the real or
rendered version we actually spent some
time modeling the light box in 3d studio
Max and are able to recreate the
physical layout of the drills in and
that adds to the realism we can actually
do things like change the materials so
we can actually change to different
types of body material which have
different lighting characteristics so
you can see the effects that you see on
the rendered version would be like you
see in the real version so you can
really make design decisions about what
materials you want to use what type of
effects you want to see and make those
choices in the design application
without actually having to make the
physical object itself
Title: SportPesa
Publish_date: 2020-02-19
Length: 42
Views: 10907
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/gBef6-TKJQM/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: gBef6-TKJQM

--- Transcript ---

formula 1 is fast and not just the cars
the processes behind that creation need
to be streamline and time efficient as
the carbon-fiber bodies because every
millisecond counts in racing car
construction we were asked to enable
engineers to focus on innovation by
reducing development times so we move
the design process from pencil sketches
to CAD with a Dassault systèmes 3d
experience platform now engineers can
design wherever they are on whatever
device with 3d graphics to enhance their
experience and with built-in scalability
they can grow with demand ev3 fuelling
the race to digital innovation
Title: GTC 2016: NVIDIA News Summary
Publish_date: 2016-04-05
Length: 108
Views: 8669
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/gdP9oIPOnZY/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: gdP9oIPOnZY

--- Transcript ---

hi I'm Greg Estes and i'm here at the
first day of gtc which is already just
buzzing with the announcements that
we've had today Jensen finished his
keynote earlier today and we talked
about five new announcements the first
one is a new nvidia sdk all of our
software development tools in one place
across deep learning AI self-driving
cars professional design everything that
we're doing all in one SD cable the
second announcement that we had is i Rea
vr we're taking our array technology
with its photo real rendering and
realistic materials and bringing it into
the world of VR so that people can
visualize their designs and scenes in a
way never before possible the third
thing that we talked about is a
brand-new Tesla p100 the most powerful
chip that we have ever made the fourth
thing that we announced is a new dgx one
it is our own 8-way server designed
specifically for deep learning
applications it comes with acceleration
for all of the deep learning frameworks
and our own deep learning software on
there for researchers and developers
that want to take the power of the new
pascal architecture that we have in the
most powerful single note ever built
finally we showed our platform for
autonomous driving and self-driving cars
with drive works and dry PX and drive
we drew gasps from the audience when we
showed a new design from designer Daniel
Simon that's going to be a part of Robo
race which is going to be a race for
autonomous vehicles all built using the
Nvidia technology so we're super excited
about what's happening in automotive
it's been buzz-worthy with 5,000
developers and it's been a great day
stay tuned for what we're doing tomorrow
Title: 8K Video Playback is No Problem for NVIDIA RTX
Publish_date: 2019-02-05
Length: 70
Views: 10282
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/gdriBI359UE/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: gdriBI359UE

--- Transcript ---

[Music]
GPUs are really important to us because
right now our file sizes of a que are
really small you know they're raw
they're great to work with but they're
hard to play back with advantage of GPU
acceleration during video it's gonna
make working with our Falls much easier
for our entire workshop I can actually
play back aka real-time footage with
Nvidia CUDA acceleration with our TX in
real time something that I would never
even be able to dream about playing back
for cane when I was first release
just because you shoot an 8k it doesn't
really mean you're finishing a K when
you have a take a fall it's just a
bigger canvas to work with when I down
sample its gonna make a better looking
image I can stabilize a pan and scan
it's just gonna give you an overall more
natural looking image when you shoot
higher resolution and it's a good even
future-proof your work 4k TVs are common
right now
so a cake music we call it a future so
if you have a good project you can sell
it again your contents and a lot a lot
longer if you shoot to future-proof your
work as well
Title: The Art of Collaboration: NVIDIA, Omniverse, and GTC
Publish_date: 2022-08-10
Length: 2176
Views: 1427142
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/giHuu633Kus/hq720.jpg?v=62eafbc0
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: giHuu633Kus

--- Transcript ---

[Music]
i wanted to be like a ride i wanted to
be like a roller coaster
we wanted to do something more than just
create beautiful renders we wanted to
tell a story
we can simulate years a matter of days
on a single gpu
and when we calculate how much faster
that is we double checked because that
was nearly 100 000 times faster so yeah
that was a really cool aha moment
three and a half minutes 70 shots at 4k
in 14 hours
[Applause]
it's just mind-boggling for me that
we're able to get all these things done
in the short amount of time that we have
gtc is where we come together every year
to share our work with the world and one
of the biggest parts of that story is
where nvidians and our partners join
forces to help solve the world's biggest
problems
this year we decided to kick off the
keynote with a video that celebrates
this story using a seamless blend of
drone photography cg magic and of course
omniverse
we try and open every keynote with
some spectacle
video or animation that embodies the the
essence the spirit of nvidia
when you're first kind of coming up with
the idea you would do a layout and
storyboards and you would show it to the
big decision makers and be like what do
you think and you're kind of loosely
describing an idea and sometimes people
think the idea is more than it is or
less than it is or different than it is
what's nice about omniverse is we could
just take our layout throw it into
create and light it and we could show
like this is our idea and it's a pretty
good representation of what the final
product is going to look like i think
the biggest challenge that we
encountered was timing we needed to have
a lot of successful cues of the robots
and to do that we needed like an army of
nvidians to help us cue these robots to
pilot these drones so many elements to
coordinate we had a car we had the drone
itself we've had autonomous robots
running around doing their own thing in
the lobby we had a couple other robots
then we had like a blue angel kind of
squadron of autonomous drones in the
back courtyard so many things going on
it was a miracle that we pulled it off
we thought that a fly-through of campus
highlighting our new building would be a
great way to show how nvidia is thinking
forward ever expanding and find really
dynamic ways of you know bringing the
campus to life hi welcome to nvidia
at a certain point of time when we uh
turn a hallway and go into an elevator
we transition to all cg
using the elevator shaft as this conduit
to do that was great to the point where
people didn't even realize they were
moving out of live action and into a cg
world we went into our omniverse created
world we were also trying to tell the
story of what omniverse is being used
for right so not only is it being used
to create the thing like that we're
showing you see the isaac training the
robots the earth 2 sim it's you know
we're telling the story of omniverse
with omniverse omniverse was used at
every stage of production from very
beginning to the end and at times in
conjunction with existing dcc apps out
there our camera and animation
department was using maya to create the
camera moves and produce the animation
of the vehicles and the robot arms and
sending all that data downstream into
omniverse our asset builders were
constructing the world in 3d max and
maya doing the surfacing and substance
and creating the materials using mdls
inside of omniverse the effects team
creating the holographic simulations
those were done houdini and also sent in
the omnibus the lighting team was doing
all the cinematic lighting composition
depth of field
and then also rendering the shots out
for compositing later outside of
omniverse the platform enabled the team
to be creative have fun you know
frictionless be able to work smoothly
together and and get the job done more
than a story i wanted to be like a ride
i wanted to be like a roller coaster i
wanted to be exciting
i wanted to be
entertaining the ultimate appetizer to
get them ready for what the future was
going to be
[Music]
welcome to gtc
i hope all of you are well
as you can see nvidia lives in this
amazing space where creativity and
science meet and omniverse is no
different while often a focus of gtc
itself our creative teams also rely on
it every day to produce their best work
their latest production orchestrated to
perfection is no exception
it was a super long product meeting it
lasted like seven hours
and so i went to my car after this
meeting really tired and turned the
radio and suddenly it sings sing is
playing like boom boom boom boom boom
boom boom
[Music]
i saw this dg axis just like
acting like trumpets
david came to me with an idea he wanted
to figure out how to bring the data
center business to life in an engaging
entertaining way he had this idea of
creating a music video you know where
the whole data center would come to life
dance around and move to the rhythm
showing how the data center is
orchestrated to perfection
the first story about biore nayla it was
it was beautiful it was very similar to
what i i saw in my head however they did
contribute quite a bit to the story
after the music was recorded with it an
animatic
sync to the music
and that's why we presented to jensen
and he loved it but the one feedback we
left from that meeting was the
instruments are not fully clear who's
when what so we were able to associate
each of the hardware server components
with one of the instruments in the song
before the animation started we get the
storyboard we use that as a guideline it
gives us a overall
view about how all the shots connect
with each other once we study that and
then we start it in in maya we set up
the camera using the storyboard as the
guidelines and then we start animating
each of the assets and then we'll bring
in the music which is a big part of this
project so because all the motions will
have to be
in sync with the beats
in a dancing chip sequence i work with
michael in lighting because in this
sequence spotlight or lighting is almost
like an extra character's in there i
worked with rini on the h100 vignette so
all the dancing chips
and created the environments and gave
her the layouts for the environment so
she can then place the cameras and
because of the quick rendering of patch
tracing we can see things in
expedited rate you know so we can easily
see the problems and easily fix them and
then you want to use lit metal objects
know that it's very very difficult to to
light metal it's a lot about like
putting the thing that it's going to
reflect in the right spot it's a lot of
trial and error if you're doing
something that's a completely offline
render
being able to see what the final product
is in the viewport and be able to move a
card to get that perfect highlight on
the edge
massively time saving if you're seeing
an animation when you where your lights
are placed it may pop when it's
animating so you then you have to like
either adjust the materials if you like
the lighting or just the lighting values
just to make sure it doesn't really
catch your eye
after all the animations are done all
the assets are exported to omniverse
omniverse is kind of the hub for
everyone working omniverse is built on
usd so
everything kind of worked hand in hand
what we like to do is render first
middle lash just to test and see how
it's going to look throughout the
sequence without having to wait for long
render times there was so much rendering
that was going to be done
across multiple teams multiple projects
that nvidia built our own internal gpu
render farm substantial in size i
believe like 15 or 1600 gpu notes i
personally as an artist who've worked on
many shows i've never sent to a thousand
gpu render farm before and it is
glorious
so originally we're going to end
orchestrated perfection with an i.t
woman who is going to walk down the hall
maybe she heard something going on in
the server room
she flings the door open there's nothing
going on it's quiet silent she closes
the door and then the server room comes
back to life and that's how we end the
piece when we pitched the second review
to jensen jensen had actually a great
idea of like
why not just use tj
and for those that don't know tj is what
we call toy jensen
at first we're like oh geez that's a lot
of work most of these shots were already
finals so we had to go back and just
tweak him a little bit for a particular
character and he's much shorter and he
opens the door i thought it was going to
take a couple days but the next morning
they showed me a completed pass with toy
jensen replacing it woman looking
amazing and uh and we're like all right
we got it done it works
so then we asked ourselves if we could
render this in 4k right before delivery
with
16 to 24 hours left in our schedule so
we fired up the render farm
sent off 70 shots and all those thousand
machines hungry for rendering say i want
that rendering and they chew through it
14 hours later
every shot had rendered 4k
maybe one or two shots needed some touch
up it was
impressive the performance of the gpus
and omniverse and turning around a show
that quickly
three and a half minutes 70 shots at 4k
in 14 hours
all of these talented people that are
contributing to this hub that we call
omniverse
that get the benefit of like real-time
visualization of what's going on the
contributions of all these different
people
is no different from orchestration
and doing great orchestration is hard no
matter what field you're working on the
more people can participate and
collaborate the better for the project
and i believe that output is just going
to be way more interesting
[Music]
from one data center to another
omniverse helps bring it all to life and
when it came to introducing our hopper
gpu we travel from micro to macro using
billions of polygons soap bubbles and a
lot of teamwork
today
we are announcing the next generation
the engine of the world's ai computing
infrastructure
makes a giant leap for this gtc 2022
we had to announce the hopper
architecture on the h-100 gpu we wanted
to do something more than just create
beautiful renders we wanted to tell a
story a story of how such a small die
integrates to such a huge system like
the super pod
we didn't want to treat it as a
cinematic with different shots instead
we wanted to have something very fluid
so we chose to do everything in one
gigantic shot in a perfect world to pull
off an animation like this you would
need at least six months and that's
being generous we didn't have six months
we had about three weeks each element of
this animation is a complex model that
has many thousands of parts the scope
was way more than we've ever experienced
to the point where we had to figure out
an efficient way to prepare these models
so that the animation could be done we
had to work in parallel and overlap many
of the other parts of the creative
process in order to hit the deadline the
main animator deviced a rig that can
easily swap out a placeholder object
that he would work on to animate with
the real object once it's done prepping
so now that we have accurate files from
engineering now it's time for the fun
part to start talking about being
creative how are we going to make this
aesthetically beautiful we had to start
with the h100 gpu it was going to be
after all the hero right the
architecture we took tons and tons of
reference of piano black materials of
soap bubbles for the thin film we wanted
to encapsulate all of these things into
this dye to make it look as beautiful as
we possibly could we had to make sure
that the lighting for the small die
was just as consistent as the super pod
so light placement light intensity all
of those things were crucial to make
this shot work
one of the great things about omniverse
is how well it handles large amounts of
data this animation handled 6
billion polygons this was very
beneficial for the creative process we
could get into live sessions with our
directors and then we would make
decisions to make sure the materials
look the best they could can you rotate
it a bit so i could see a little more
dramatic reflection
we don't have to wait and animate a
thousand frames render it have it come
back and then decide whether we need to
move a light a little to the right we
can see instantly where the reflections
are sitting where the shadows are
casting all of that in real time
so once the look was approved for the
h100 and it was literally at the last
minute at the buzzer i quickly packaged
up the asset and then it was delivered
looking back after the animation is done
i'm just amazed at the teamwork and the
skills that we have to deliver such
beautiful product it's just
mind-boggling for me that we're able to
get all these things done in the short
amount of time that we had there were
many many sleepless nights
but we all pulled together as a team and
did it
from microchips to planets and the
world's first supercomputer dedicated to
climate change research earth2 and we
just got a glimpse of what earth 2 will
be capable of
forecastnet is an ai-based weather
forecasting model that we have developed
here at nvidia it's lightningly fast and
at the same time accurate it can provide
forecasts in a matter of seconds by
going so incredibly fast we're able to
generate extremely large ensembles of
weather predictions so what that means
is we're predicting the possible future
of the weather many many many times and
that can give you really good statistics
over what's likely to happen
so to train the forecast net model we
used what is known as era 5 re-analysis
data and what that means is that the
data that was predicted by the ifs
weather model from the european center
for medium range weather forecasting and
it's one of the most accurate
representations of the state of the
atmosphere that exists
we can make predictions at the
resolution of 25 kilometer planet scale
whereas earlier deep learning based
models
were making predictions in the range of
hundreds of kilometers
not only are predictions very accurate
they're really quite close to the
original ifs quality
using only a very small fraction of the
data that's available so we've used just
a handful of fields temperature pressure
humidity at just a couple of elevations
and we were amazed that we're able to
forecast the weather in six hour chunks
like six hour giant leaps with just this
very small amount of data and when we
calculate how much faster that is
compared to numerical weather models we
double checked because that was nearly
100 000 times faster so yeah that was a
really cool aha moment
so the forecast net is basically the
really fast prediction part of the model
on the other side you have omniverse
which gives you the beautiful rendered
ray trace visualization of the earth and
when you put these two things together
you can basically predict the data and
see it immediately so you can interact
with it in real time
visualization can be used to make the
data accessible and digestible for
almost everybody you can run it at home
basically and that makes it very
powerful it democratizes access to
weather forecasting but eventually we're
going to couple this to a much larger
system we're going to couple it to a
model that basically predicts the
climate and then uses that to initialize
many weather simulations nvtr modulus is
a hybrid physics ml framework that can
enable such hybrid physics ml methods
coming together for different kinds of
input data
we are aiming at at going to use
observational data incorporate data from
satellites from ground station
commercial aircraft to basically create
predictions which are much closer to
reality so i expect even up to a million
x speed up in a lot of these processes
that can lead to unprecedented
simulations and along with it new
discoveries
and we want to get right down to the
perspective of being on the earth and
being able to look around and see what
the weather or conditions are like at
some deep point in the future
forecastnet is just the first step of
our ambitious earth-2 effort where the
goal is to build a digital twin of the
earth and the entire point of the
digital twin initiative is to attempt to
help us forecast and prepare for a very
difficult climate future that's coming
our way so in future years you know
you're going to see a lot more wildfires
a lot more floods things like that and
everybody needs to be able to understand
what's coming to make the right
decisions and in order to do that you
need to make many predictions so you
don't miss those extremes
we want to get accurate
extreme weather predictions and this has
huge social as well as human costs
digital twins are set to change how we
see the world and together with drive
sim how we move around it our research
team is making that easier than ever
our drivesim gtc demo really focused on
our ability to create content so that we
can test not just the performance of the
vehicle in that scenario but all of the
edge cases surrounding it
we employ two different methods to
actually reconstruct real-world events
and enable them for re-simulation and
the first approach we construct the
scenario from real world data we extract
this deep map data into an intermediate
usd format called digital twin gt we can
turn this into anything that we want if
we want to add additional information
about lane lines is it double yellow
does it have reflectors we can add all
sorts of information at the usd level
which is natively supported in omniverse
art artists create all the content all
the city elements vehicles humans and
this allows you to do a lot of great
things
however the artist created content it's
really time consuming we know that we
need to create huge volumes of content
and you can't do it all by hand so we
need to be able to create that volume of
content automatically the second
approach on your approach is just using
ai to reconstruct the scene we're taking
an old drive that already happened that
is basically stored just as video and
use this neural construction method and
we're bringing it back to life
we basically separate the static scene
as well as a dynamic object and then we
use neural radiance fields to render the
background and composite the object back
into the scene we can recompose the
scene in a controllable way so suddenly
now we are the creator of the scenario
in the demo that we showed we kind of
purposely swirled it a little bit to the
left we wanted the other cars to react
in a plausible way to the eagle car
as we continue the demo we see on the
left this car hits this traffic sign and
there's a construction vehicle appearing
on the right what we wanted to show here
is we were able to virtually insert
objects into this modified drive now
suddenly this becomes a closed-loop
simulation
the main benefit of the second approach
is it's a lot less time-consuming for
humans you can just take real-world
recorded data and use these neural
construction methods to basically turn
them into simulation
so what comes next i mean we've talked
about digital twins and real time so you
put it all together and have that
ability to to reconstruct the world at
scale
we want to be able to simulate not just
one car driving in the digital twin
world but a whole fleet of vehicles that
are communicating with each other and
interacting with each other
wouldn't it be amazing that we can
identify any location pinpoint and then
click and we would be taken to a digital
twin representation of that location do
simulation training under different
randomized variations and that's where
the future is
i mean this is the holy grail of content
creation
[Music]
the same research team reinventing how
we train autonomous vehicles is also
exploring how ai can give animators new
ways to create faster more accurately
and more intuitively one of the biggest
challenges that we have with this
project was being able to train very
expressive models that can perform a lot
of different skills first the
breakthrough movement came when we
switched over from a cpu based simulator
to a gpu based simulator so isaac jim is
a massively parallel physics simulator
that runs on nvidia gpus and this just
makes the training of our policies
orders of magnitude faster which is the
only reason why this project is feasible
in the first place
the fact that we can simulate years of
simulated time in a matter of days on a
single gpu
it's just insane and there's no really
other viable alternative in order to
train our ace model we use a data set um
that consists of a large variety of
behaviors recorded using motion capture
from live actors this data set shows
behaviors
that range from common everyday motions
like walking and running to more
athletic behaviors showing a gladiator
character performing skills like sword
swings and shield brushes
and then it gets processed into a format
that isogyn can actually understand
which is then passed to our
discriminator in particular once we've
trained our character we can then
generate motions from the simulated
character and we can then take those
motions and apply them to a high
fidelity mesh character like the
gladiator character that you see in the
demo and this is done by using omniverse
kit which allowed us to retarget the
motions generating simulation to the
higher fidelity character and they also
allow us to generate high photology
renderings of the character to showcase
the motions produced by our system so
you can give it a prompt and maybe ace
generates a trajectory that's similar to
what you want but not exactly correct so
it's very quick when you're when your
interface is based on text you can
simply adapt your prompt and have ace
generate new trajectories so just like
you know director you know reshooting
scenes over and over again controlling
these animation systems with natural
language leads to a much quicker
iteration time which should you know
lead to intuitive easy to produce
results
in our work we develop a system called
adversarial skill embeddings or ace for
short and this is a system that allows
us to train physically simulated
characters that can perform a large
variety of reusable motor skills ace in
particular is an approach for making
physics-based simulations that are
realistic and look human-like ase takes
a data set of reference human motions
that one wants to imitate and can
produce new natural synthesized
behaviors that follow that reference
motions in the physics simulator the
goal of trying to make ace
interqueriable with language is that for
anybody should be able to create high
quality animations with ace people
should be able to do it quickly when
humans try to learn to perform new tasks
they will very rarely learn things from
scratch again so if you know how to walk
i'm not going to really learn how to
walk again if i need to do something new
so the idea for that is that we now want
to develop a model that can learn
reusable skills that we can then use to
synthesize motions for new situations so
for example as you can see in the gtc
demo the agent's not just able to do
these different low-level skills but can
apply to accomplish different objectives
like knocking over a block navigating to
a given task facing one direction while
walking in a different direction and the
idea behind all of these is a because
the underlying low-level checkpoint is
so capable these are all very quick
models to train and b because it's all
operating in the context of a physics
simulation they're all life like
realistic physically plausible behaviors
my hope with this line of work is that
eventually these natural language models
will allow us to
develop more accessible and expressive
interfaces if you're able to control
these complicated systems with an
interface as universal as language then
it should be really fast to iterate to
explore to experiment and make this
technology much more broadly accessible
to everybody
of all the technologies at gtc nothing
brings them together like toy jensen or
tj as we like to call them and for tj
2.0 the team challenged themselves to
help make him more natural intelligent
and engaging
back in spring gtc 2021 we created a
virtual jensen for about 14 seconds that
was showed up in the keynote so when we
came to the fall
we were thinking well
what's next what would be surprising for
our customers and viewers and what makes
sense for the technology we're bringing
to the world
we wanted something that was on one hand
fun but also could be taken seriously
and then we explored a toy version of
jensen hence the name tj
it was to showcase a completely art
driven avatar interacting with jensen
himself
toy jensen is powered by a very
complicated cloud-based computing
infrastructure called ucf unified
computing framework it starts off with
automatic speech recognition and then
that gets sent to the big nlp or natural
language processing which is 530 billion
parameters and then it goes to the riva
tts text to speech and then that gets
turned into a wav file and then that
comes back to audio to face which is
piped into omniverse it's this amazing
chain it's all happening in real time
and all we see is its character that
answers questions how do astronomers
look for exoplanets
most exoplanets are found by the transit
method that was for toy jensen the big
nlp or natural language processing is
the brain behind his answer and the
accuracy comes from how we tune the
model to be able to optimize where we
can search in a very particular area and
minimizes the errors of such a large
language model hi with the fall it was
the first debut of troy jensen we
received a lot of great feedback but for
2.0 we felt a few areas that can really
make a big difference his voice which is
very important was drastically improved
for gtc 2022 we wanted to have jensen's
timbre
combined with the prosody rhythm and
cadence of a virtual assistant we asked
jensen to record some data but we didn't
have enough of the virtual assistant
prosody that we were looking for
we have a model that allows us to
combine someone's timber it starts with
my voice with someone else's browser
from very high quality personal
assistants it starts with my voice
it starts with my voice
so we were able to combine those things
together and create dj 2.0
what is synthetic biology synthetic
biology is about designing biological
systems at multiple levels from
individual molecules up to whole cells
and even multicellular assemblies like
tissues and organs to perform specific
functions
troy jensen is unique because he has
emotion for tj 2.0 we saw the
opportunity to expand on this and we now
have anim graf working so we had
animation states that were driven
through ai piped through the animation
graph while toy jensen was talking we
also implemented full face audio to face
which allowed the upper portion of his
face to be driven through the wave file
so when he talks now he was more
expressive
thanks jensen i'm super excited to
explain how i was made
avatar one of the hardest things with
conversational ai is the response times
the latency if things slow down or the
response is wrong that's very
frustrating and that's a very hard thing
to control still as we were implementing
some of these new technologies into tj
2.0 we had a few show stoppers where we
had some serious performance degradation
a lot of people look at real time as 30
frames per second but for a really
convincing performance especially using
things like audio to face we really want
to hit 40 to 50 to give you that detail
that you would otherwise miss
eddie had to go in and figure out how to
rebuild full face audio to face to get a
performance improvement so there was a
lot of pressure on him to try to go in
and create a full face audio interface
that would function and work properly
[Music]
my dream of toy jensen is to go beyond
just the google search representation
through an avatar we should be able to
collaborate with toy jensen to solve
important problems in the world
hi how can i help my daughter spends a
lot of time coming in and out of my
office and she gets to see what i'm
doing and she asks questions
and it really makes me realize how
important the digital avatar will be for
the the new generation it can be a
character that she can ask questions and
get non-biased answers back she'll have
a vehicle that she can talk to a
character we're really excited about the
avatar having this connection with the
user
see you in omniverse
omniverse not only unites teams and
technologies but brings us closer to our
partners today more and more companies
worldwide are discovering the power of
this incredible platform to help solve
their real world problems
our collaboration with amazon robotics
was about building a digital twin of
warehouse facility that amazon plans to
build
it's very practical to build a replica
before you deploy something to the real
world so that we can basically
optimize it train and deploy the systems
virtually before we actually do it in
real life
what we did is basically help them set
up their first digital twin using our
omniverse platform so with omniverse we
built kind of the
ideal toolkit to construct the world
before you can even simulate it
and so what we did was we provided
output usd and animations and we also
provided the solidworks revit cad files
to the nvidia team for the cad data that
we have received from amazon we actually
used our connectors that connect
directly into omniverse the connector is
merely a way of translating the native
representation of the 3d worlds inside
each tool into a usd one the usd
basically can contain all the data and
all the information which is something
that previously was incredibly hard to
do usd has been a key
to actually helping us accelerate and
it's a fantastic building block
on top of that we also use pretty much
all the range of dcc tools available to
generate the rest of the content so the
artists are actually able to work within
whatever they're comfortable with and
we're not asking them to change anything
that they're doing thanks to our
connectors
the most powerful things about
the nvidia ecosystem of tools and
technology is that our technical artists
can use whatever tools they're
comfortable with the beauty about using
omniverse is that it gave life to those
applications that they have it was a
no-brainer that it made sense for us to
collaborate on this project
[Music]
during the collaboration the nvidia
technical artists worked with our
technical artists and helped get them to
understand the tooling better and some
of their practices we basically worked
with them and showed them how it's
possible to do things you know like
realistic material you know realistic
rendering why the lighting is important
how to use the systems within omniverse
and it was a constant collaboration
where every day they saw the
improvements and the things that we have
done we've seen the experiments that
they have done we discussed it together
and then we made a decision how to move
forward the autonomous drive because it
operates on sensors needs actual
photorealistic worlds to operate in and
sense as it's moving around there's
multiple components to it that go from
all the way to like the layout itself
you know where things are placed as well
as like the navigation that the
autonomous robots have to do as well as
navigation of the robots themselves you
can basically use the actual cad data
and all of this data once it's imported
to omniverse can actually be
rigged using our you know isaac sim
system and isaac sim is basically a
platform within omniverse for a robotic
simulation system so you can train them
for ideal pathing and then rearrange the
layout as needed
we created a
model based on synthetic data and for a
demonstration we wanted to actually
track the packages so we added
reflective tape to the packages tried
running them through and that's when the
model failed omniverse replicator is a
framework to generate synthetic data in
omniverse with synthetic data in a
matter of a day you can generate a new
packing material that is very reflective
that has different material properties
basically what we did is that we had
recreated an exact copy of all of their
packages which is something that again
digitally you can do very efficiently
and very quickly so then they were able
to train their perception system using a
huge data set it was very easy for us to
then go back to that synthetic data
generation
and we added to our pipeline the ability
to add reflective tape to the packages
and then they were able to retrain the
robotic arm which then worked flawlessly
the power of omniverse and the fact that
we were actually able again to
collaborate within omniverse and we're
able to use all the tools we use without
disrupting anyone's workflow it all
worked out in the end to be able to
deliver the project by the deadline when
they first saw it when i first saw it
i was amazed
my my initial reaction was just wow
yeah so i still remember this because it
was incredible as a matter of fact like
every single meeting during that period
i guess we started it with a quiz like
is this real is this digital
at nvidia we've always believed that the
sum is greater than its parts and
nowhere is that more evident in the work
that we do with our partners and the
beauty is we both commit our resources
to it because we both know that we're
going to get something out of it that's
going to help everyone just seeing that
digital twin and the life that we were
able to bring to it was phenomenal just
knowing that we were able to impact the
operation at amazon and we're
continuously going to do that i'm very
proud of it
the more we looked at the nvidia
technologies
the more we realized it was satisfying
other use cases because it's not just
providing one tool for us it's providing
an ecosystem of tools and technology
that's going to help accelerate multiple
areas for us
we hope you've enjoyed this behind the
scenes look at the passion talent and
hard work that goes into gtc and every
nvidia project product and production
we're always excited to share our body
of work with the world and look forward
to sharing even more
thank you and we'll see you at gtc this
fall
[Music]
you
Title: Accelerate AI-Powered Drug Discovery With NVIDIA BioNeMo
Publish_date: 2023-03-21
Length: 143
Views: 28352
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/GNL1z7hnj4w/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: GNL1z7hnj4w

--- Transcript ---

foreign
drug Discovery is incredibly hard
bringing a new drug to Market takes more
than 10 years and has just a 10 success
rate
between the complexity of biology and
the near infinite chemical and protein
combinations discovering drugs to help
cure disease is a truly heroic endeavor
accelerating this process is Mission
critical for drug Developers
there are three key stages to drug
discovery discovering the biology that
causes disease designing new molecules
whether those are small molecules
proteins or antibodies and finally
screening how those molecules interact
with each other
today generative AI is transforming
every step of the drug Discovery process
Nvidia bionemo service provides
state-of-the-art generative AI models
for drug discovery
it's available as a cloud service
providing instant and easy access to
accelerated drug discovery workflows
bionemo includes models like Alpha fold
esm fold and open fold for 3D protein
structure prediction
approach GPT for protein generation
esm1 and esm2 for protein property
prediction
Mega mole barge and moflow for a
molecule generation and diff dock for
molecule docking
drug Discovery teams can use the models
through bionemo's web interface or cloud
apis
here's an example of using Nvidia bio
Nemo for drug Discovery virtual
screening
generative models can now read a
protein's amino acid sequence and in
seconds accurately predict the structure
of a Target protein
they can also generate molecules with
desirable add me properties that
optimize how a drug behaves in the body
generative models can even predict the
3D interactions of a protein and
molecule accelerating the discovery of
optimal drug candidates
with Nvidia dgx cloud bio Nemo also
provides on-demand super Computing
infrastructure to further optimize and
train models saving teams valuable time
and money so they can focus on
discovering life-saving medicines
the new AI drug Discovery pipelines are
here sign up now for access to Nvidia
bio Nemo service
Title: NVIDIA Launches Turing for Real-Time Ray Tracing
Publish_date: 2018-08-14
Length: 154
Views: 150562
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/gTbWVt2_OWc/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: gTbWVt2_OWc

--- Transcript ---

hi i'm greg estes from nvidia and we're
here at siggraph in beautiful vancouver
canada and what a perfect place
to launch our brand new touring
architecture the reason touring is so
special is because it combines
four different processing types in one
gpu
the first is a new real-time ray tracing
capability using what we call
rt cores the second one is taking
artificial intelligence routines
deep learning and processing those with
what we call
tensor cores the third one is to use
the advanced shading capabilities that
we have to draw
graphics and the fourth is to do
simulations
the first turing gpus that we're
announcing are the quadro series
we've worked with many of our software
partners to be able to show this amazing
capability
the speed of light a universal constant
the first one was with our partner epic
and we showed for the first time how you
could visualize
something like that in the same way a
commercial might be done for television
but all in real time the second thing we
showed
was touring for architecture and
construction we took
a ray traced view of a building in
bangkok
and then we modified it in real time
using not one but two different industry
standard tools
revit and solidworks while we did
changes in those tools those changes
were reflected
with full ray tracing
[Music]
a classic use of ray tracing is
hollywood effects
we showed today how our new rtx software
technology combined with the turing
hardware
is able to create hollywood effects much
faster than they could possibly ever be
done
using cpu alone technology we think this
is going to have a great value to
artists
animators digital production people and
technical directors in hollywood and all
around the world
in a market that's been estimated to be
250 billion dollars
quadro is the first gpu purpose built
for real time ray tracing and we showed
it off in an amazing way
that combined this real-time graphics
artificial intelligence
and a wonderful demo that had the
audience dancing in the aisles
[Music]
Title: GeForce Garage: Cross Desk Series, Video 1 - How to Modify Your Chassis For Better Airflow
Publish_date: 2014-07-18
Length: 354
Views: 243362
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/gU5xuLnbPSA/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: gU5xuLnbPSA

--- Transcript ---

hi this is andrew with nvidia and you're
watching GeForce garage today we're
going to take a look at the first skill
guide based around our latest mob the
red Harbinger cross desk the cross desk
doubles as a high-end PC gaming case
allowing you to install up to two gaming
PC systems inside
it's accented with a clear glass top
showing off all your awesome components
today we have Lee Harrington from PC
jumping mobs calm in the studio he's
currently working on a build of the
cross desk so he's got quite a bit of
experience we're gonna show you some of
the basics of PC case modding but
specifically we're talking about how to
cut holes and panels in the case to
improve air flow
hey everybody leave from PC junkie mods
here we'll be using most of these tools
today first thing I have here are the
safety goggles if you're worried about
any metal shards flying up into your
eyes that that will protect you then I
also have the blue painters tape for
marking off the areas you're going to be
cutting next I have some files here then
I have my mm PC tech mod ruler it has
all the different thread sizes and hole
sizes then I have this tool this is
deburring tool I use it to clean up the
hole after I drill then I also have the
hole saw it's a four and a half inch
it's perfect for 120 millimeter fans and
then we have large drill over here it's
a little bit oversized for what you
would need to do to a standard PC case
you could probably get the job done with
the corded drill but I'm gonna be doing
steel today next we got the Dremel I
have a snake attachment on it this is
the most common tool used in the modding
industry I pretty much cut everything
with this and then we have a craftsman
drill nice handy cordless I have some
cutting oil we'll be using that today
because we'll be cutting steel and then
I have a variety of drill bits then I
have my nibblers very good for cutting
aluminum so let's talk about the chassis
that we're going to be mounting today so
we're going to try to optimize air flow
here we're going to take this mash out
because all that's doing is obstructing
the air flow into your chassis this is
your main intake right here the more air
you can get to come in the
longer-lasting parts you're going to
have the first step is going to be
work out the area and tape it the blue
tapes not really my guide per se the
holes are my guide okay so we have it
taped off before I actually cut my
nibbler has a pretty big tooth right
here so what I need to do is make a
bigger hole for the nibbler to start at
so what I'm going to do is actually
drill a hole so now the nibbler tooth
will fit in there and then we can start
cutting my nibbler needs a flat surface
area where the fan holes are out it
actually dips down so what I'll have to
do is I'm going to come back and cut
these corners out with the dremel
so now what I'm going to do is I'm going
to take my file and go ahead and clean
up these edges so one of the more
important reasons to do this is so you
don't slice your hand besides look you
still want to cut yourself by using the
file on the edge I can run my finger
across here now so I got the fan
installed now there's no obstruction in
front of the fan so we're going to get a
lot better airflow let's take some of
those basic principles and apply them to
the red harbinger cross desk lead this
thing already has quite a bit of
ventilation built into it so how can we
improve this thing we're going to
actually create an exhaust area for the
power supplies in the back we're going
to cut 120mm on your fan hole right here
in the back so what I'm going to use to
mark off the holes is a template lot of
templates are widely available on the
web right now I'm just marking them all
case my template moves
first drillbit was in Athens I used it
as a pilot drill bit so the second drill
bit that I'm using is a 1364 alright so
now what I'm going to do is I'm going to
tape off the area the reason being is if
you're cutting into the metal splinters
can pop up and those splinters can mark
your surface area
the whole stall is going to get hot and
I need something to cool it down I have
some cutting oil will do the job
so what we're going to use the deburring
tool to clean up the edge so that way
there's no sharp points and there's a
lot faster than a file sometimes with a
file you could end up d rounding the
hole and you probably not want to do
that if you can see the hole all right
Lee thanks so much for coming in thanks
for having me of course don't forget to
check out the next episode where we show
you guys how to do custom cable sleeping
thanks for watching GeForce garage the
ultimate resource center for designing
building and customizing your GeForce PC
you
Title: Meet Redshift: GPU Rendering with Ultimate Flexibility
Publish_date: 2017-11-06
Length: 185
Views: 190509
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/GXNeKuGWOZk/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: GXNeKuGWOZk

--- Transcript ---

[Music]
it's unlike a CPU movie asset that we
had and they rendered it with redshift
and it went from like minutes to seconds
the same equality redshift is the
world's fastest final frame render we
use the incredible power of the GPU to
render frames five to twenty times
faster than our competitors all the
founders needs to work for games
companies one of our artists that we
work with he just asked as one day like
you know wouldn't be really cool if you
could use a GPU to do ray tracing so we
set about like writing a prototype
that's got some very good encouraging
results I'm very early on and so we
decided that we would actually build on
the prototype rush to start a company
what sets us apart from other vendors
that are running on the GPU is that we
have an out-of-court architecture so we
accessed geometry and textures in and
out of poor fashion which allows you to
render scenes that far exceed the memory
capacity of the GPU
[Music]
one of the great things for us is that
Nvidia brings up new GPUs every year and
they come with like more CUDA cores and
different technology like different
architectures and more memory as well so
redshift actually just becomes faster
without even having to do any work the
GP 100 card which is doing quite new
that has like some really cool new
technology like the on-chip memory
redshift is very memory heavy we use
lots of memory in the on-chip memory of
the GP one hunting's that shaders and
things that are almost twice as fast
it's like an incredible performance jump
speed really has two major impacts one
is that you save money let's say it's 10
times faster you need 10 times less
Hardware to get the same job done the
other sort of more subtle but perhaps
more important factor is iteration time
so it's shorter and shorter we can make
that iteration time the more the artists
can iterate on their work the more they
can unleash their creativity and
experiment and just basically end up
with a better result in the end what we
hear mostly from our customers is that
you know because of the speed of the
rendering they can like iterate through
and like increase the quality without
actually like decreasing the performance
in some cases so they can like crank up
all the settings in redshift like all
the bouncers for lights and Gi that kind
of thing didn't get like way better
quality was also the game before without
the CPU render and again the results
back super fast as well
we're already in talks right now with
some very big studios that involved both
commercial and movies so it's very
exciting I don't see why not in five
years every major effects house will be
using like GPUs for rendering and
hopefully with redshift
[Music]
Title: GANverse3D: Knight Rider KITT Re-created with AI by NVIDIA (Full Version)
Publish_date: 2021-04-16
Length: 121
Views: 68897
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/gz5E9wszZSI/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: gz5E9wszZSI

--- Transcript ---

[Music]
nvidia researchers have created a
powerful breakthrough for 2d to 3d
models
now it's possible to create a 3d
animatable model
with a single 2d picture no 3d modeling
software or experience required
it's automatic and easy we call it
ganvers 3d most images are captured in
2d
ganvers 3d gives creators in gaming
architecture design
and more the ability to bring these 2d
images
to three-dimensional life in record time
with just a single 2d image ganvers 3d
can create a 3d model including the mesh
textures
and information needed to automatically
animate the object
it works for any static object from cars
to animals to human faces
and beyond the 3d model can be rapidly
brought into nvidia omniverse
the open platform for real-time virtual
collaboration
and simulation this means for example
a single photo of a car can be turned
into a 3d model
complete with working headlights and
blinkers and driven around
in a virtual world this gives creators
of any skill level
the ability to easily generate 3d models
for diverse uses
storyboarding previz game content
architectural models
and more ready to see this
groundbreaking tech in action
we uploaded a picture of kit the iconic
1980s
ai car from knight rider and brought it
to 21st century ai powered life
night industries 2000 here in full
3d i think that got their attention
michael
learn more at the link in our bio
omniverse really captured my good side
all of them
Title: Adobe and NVIDIA collaborate to build the best tools for creative pros
Publish_date: 2014-03-31
Length: 249
Views: 17817
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/h-T0oKPU_Q8/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLCWTvjvoWlXlcx0mujEdZ8xSfgfsg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: h-T0oKPU_Q8

--- Transcript ---

our relationship with nvidia is
fantastic it goes back many years and i
can honestly say we wouldn't be where we
are today with our professional products
without invidious collaboration
going back to
the cs5 time frame when we rewrote the
entire core of premiere pro from 32-bit
to 64-bit we re-architected to make sure
that we could take advantage of every
single compute resource in that platform
and that included really looking at gpu
so nvidia partnered with us to make sure
that became a reality and it's proven to
be an essential part of our file based
workflow strategy
nvidia and adobe have a very fluid
relationship it starts at the engineer
to engineer level where
engineers from nvidia and engineers from
adobe are in regular contact whether
it's regular
engineering meetings or face-to-face
discussions brainstorming sessions it's
almost like another engineering team
within nvidia we collaborate with them
that closely
we have a long history with nvidia we
started working together
on gpu acceleration that showed up in
cs5
that became the mercury playback engine
that premiere pro has used successfully
for many many years it accelerates
playback of virtually all types of video
uh allows us to run effects
uh in real time and it's just delighted
customers with its performance and
quality
probably the best example of gpu
acceleration in a creative workflow is
with premiere pro and mercury playback
engine but it doesn't stop there you
look at after effects for instance with
nvidia optics technology to bring 3d ray
tracing you look at photoshop with
different effects like blur modes and
other things that are accelerated there
you look at speed grade with the lumetri
color engine across a lot of different
products on a lot of different
capabilities the gpu acceleration is
going to be the number one thing that
affects the performance of the users
all the work we did with nvidia in the
early days is continuing to bear fruit
today one of the biggest trends is
centralizing compute resources this
allows a large media enterprise to have
a completely virtually connected
workflow we were able to take the same
technology
that we put into premiere
and use that in anywhere so now we've
taken the mercury playback engine and
we've moved that to servers so we can
centralize the
computation of effects
on the servers to support multiple
clients
and at the same time we get to take
advantage of all of the new encoding
hardware that nvidia has just added
that hardware really is made
anywhere as a product practical
before that i don't think anywhere could
have existed as a product
now folks can collaborate they can work
remotely over long distances
because of
the mercury playback
rendering of effects that we can get in
real time
and this
hardware accelerated encoding that we
get it's fantastic
one of the reasons our customers are
adopting creative cloud is so that they
can be ready for the future
we release updates to our products on a
regular basis and nvidia is there in
lockstep with us making sure that their
latest and greatest technologies work
with ours for creative professionals the
most important thing about creative
cloud is that together adobe and nvidia
can bring a much faster rate of
innovation to users they can get updates
more frequently nvidia can bring new gpu
accelerated capabilities working with
adobe to our users and that's the best
thing for everybody
Title: Marbles RTX on NVIDIA Omniverse
Publish_date: 2020-05-14
Length: 161
Views: 496033
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/H0_NZDSqR3Y/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: H0_NZDSqR3Y

--- Transcript ---

[Music]
[Applause]
[Music]
you
Title: Win an MSI Gaming Laptop or 1080 Ti! CES 2018 Giveaway!
Publish_date: 2018-01-05
Length: 61
Views: 22342
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/H25A3GrbY60/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: H25A3GrbY60

--- Transcript ---

hey everyone and Happy New Year I'm
Chris from Nvidia's official g-force
Channel here with a couple of special
announcements one is that I hope you all
know that there actually is an official
g-force Channel we've been live for a
little over a year and it's your
destination for all things PC gaming
GeForce Tech 4k gameplay exclusive
gaming trailers and the latest gaming
announcements from Nvidia so you should
probably subscribe if you haven't
already our other announcement is that
throughout January we'll be giving away
10 of these 10 atti cards and a new msi
GSC Riis stealth pro gaming laptop with
Nvidia max cue technology to subscribers
on our channel so you should definitely
subscribe if you want a chance at
winning we're covering all the GeForce
related gaming news coming out of the
CES trade show in Las Vegas and if you
comment on our videos you'll be
automatically entered to win we'll be
picking a few comments at random and
shipping out awesome tech to eligible
winners to get your 2018 started right
your first step is just hitting the
subscribe button below where you can
learn more and take in all of our great
gaming and PC tech content good luck and
enjoy g-forces coverage of CES
Title: NVIDIA AI Solutions for Efficient Supply Chain Operation
Publish_date: 2022-08-15
Length: 203
Views: 29625
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/he5I6ByoaB4/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: he5I6ByoaB4

--- Transcript ---

nvidia ai and simulation solutions are
delivering better than ever efficiency
and intelligence to the supply chain
ensuring retailers never miss a beat in
meeting customers expectations
from the moment a vehicle first
approaches a distribution center ai
streamlines the receivable process with
computer vision retailers have the
ability to detect and identify arriving
vehicles and count the boxes as they're
unloaded reading their labels for quick
identification
ai has already made sure that these
products are the ones in demand with ai
forecasting is smarter and more accurate
giving retailers insight into what their
customers want and when
as products move down the conveyor
intelligent automation monitors their
progress
ai can adjust the conveyor speed
speeding up or slowing down to maximize
employee productivity and safety
congestion is avoided as computer vision
applications identify when packages are
tilted or sitting too close together by
automatically adjusting the conveyor
speed ai eliminates downtime which can
cost retailers three thousand to five
thousand dollars per minute
the same intelligent automation boosts
quality control identifying damaged
boxes to ensure they never make it into
a disappointed customer's hands
and that's just the beginning of the
optimizations that ai can bring to the
supply chain nvidia omniverse makes it
possible to simulate distribution center
design improving workflows and
throughput with photorealistic
physically accurate virtual environments
retailers can develop test and manage
ai-powered distribution centers in a
digital world and then bring those
optimizations into the real world
as new products and processes are
introduced omniverse replicator and
nvidia tau can be used to create
photorealistic synthetic data to retrain
the real-time ai models
global teams are using omniverse to
design factories and distribution
centers in real time digital human
simulations test new workflows for
employee ergonomics and productivity and
robots are trained and operated with the
nvidia isaac robotics platform creating
the most efficient layout
these ai applications are deployed
efficiently and securely at the edge
with nvidia fleet command
in the real world autonomous mobile
robots safely navigate the distribution
center floor to sort store retrieve and
palletize products
this removes tedious manual labor from
employees shoulders and increases
efficiency and throughput
with nvidia coopt these robots also
bring new efficiencies to order picking
using simulation they're trained to find
the best routes in a virtual warehouse
when those learnings are brought into
the real distribution center orders are
picked and prepared for shipment more
quickly
ultimately ai solutions are the key to
creating the distribution center of the
future increasing throughput with
computer vision applications pick and
place robots and autonomous vehicles
from initial receival to truck loading
and dispatching ai adds intelligence
that streamlines the material handling
process and machine learning recommends
ideal routes for deliveries so once on
the road truck drivers can get orders to
customers faster and within shorter time
windows
explore our retail solutions on
nvidia.com to learn more about how
nvidia is optimizing every part of the
retail supply chain
Title: Accelerating Healthcare Innovation with AI | GTC 2022
Publish_date: 2022-04-04
Length: 1995
Views: 21027
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/HHmKOoABNWQ/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: HHmKOoABNWQ

--- Transcript ---

welcome to gtc 2022 i'm kimberly powell
vp and general manager for healthcare
here at nvidia
the applications of the ai in healthcare
continue to amaze and accelerate
i'm excited to share our growing suite
of computing platforms that are enabling
the ecosystem to make incredible
progress in digital biology drug
discovery and real-time medical devices
gtc is an incredibly unique conference
for healthcare bringing together
pioneers in academia startups software
platform makers in every industry from
healthcare delivery genomics research
medical devices and drug discovery
to mention a few carolyn chung chief
data officer at md anser cancer centers
puts an ai into context using the
metadata of medical imaging to boost ai
performance
joshua meyer lead ai scientist at absy
will present recent breakthroughs with
their in silico protein drug discovery
platform
regina barzley professor mit computer
science and ai lab on ai breast cancer
and personalized patient screening
bill pine senior director of surgical
robotics and medtronic will give us a
glimpse to the technologies transforming
the field
ola engfist head of molecular ai
astrazeneca shows how ai is accelerating
drug discovery
and mohammed al-qarishi professor at
columbia university will present
openfold democratizing the prediction
and modeling of protein structures
you and ashley from stanford shares the
amazing achievements of accelerated
genome sequencing for critical care
where every second counts
not to mention a distinguished panel
discussion on ai for molecular dynamics
and drug discovery featuring de shaw
research roy vent discovery astros
astrazeneca and nvidia
i'm honored to host such a diverse set
of disciplines with a unified
understanding that ai is a game changer
we invented nvidia clara as a
computational platform to enable the
healthcare industry to leverage its most
important data and apply the latest
computing technology
today healthcare data is 30 of all the
world's data and by 2025 healthcare data
will be growing at the highest compound
annual growth rate of any other industry
at 36 percent
bringing the state-of-the-art and
accelerated computing ai simulation and
advanced graphics data can be
transformed into robotic assistants drug
designers and early detection systems
nvidia clare is our ai computational
platform where we are developing domain
specific tools ai models accelerated
applications and frameworks to address
the industry's most important challenges
monae is a domain specific open source
training framework for healthcare
nvidia flair is an open source framework
for distributed training and privacy
preserving ai model development created
to address healthcare data challenges
but a paradigm needed by every industry
clara has over 40 pre-trained models
across imaging drug discovery natural
language processing and computer vision
and our nvidia clara parabrix is a suite
of enterprise genomics analysis software
recently announced nvidia clara hall
scan is a full stack platform for
hardware systems to system software and
sdks that are enabling the ai
software-defined medical device
revolution
nvidia clara software suite and full
stack platforms are giving the industry
the tools needed to accelerate discovery
create new applications and generate new
business models
let's take a closer look at monae
when i had an incredible year a
breakthrough year
addressing ai development end-to-end has
proven to be tremendously valuable
it engages the healthcare industry
experts who may not be ai experts first
onei label uses ai assistance to reduce
the time and effort of annotating new
data sets and trains a model to a
specific task by continuously learning
from user interactions
when i delivers data center scale
training of domain specific model
architectures like 3d unit for medical
imaging segmentation problems
advanced training methods like automl
which use neural architecture search
automatically finds the best model
architecture according to the given data
this is a huge boost in researcher
productivity
the monai contributors make sure to keep
pace with state-of-the-art approaches
like self-supervised training pipelines
to explore vision transformers and apply
them to downstream tasks
once a model is trained onei makes it
easy to deploy into real world
environments by packaging it up into
clinical applications that plug right
into healthcare hospital i t
infrastructure
mana is a community-led open source
consortium
formed by the world's leaders in
healthcare ai cultivating open research
and accelerating the pace of innovation
global integration of one eye increases
its accessibility on every cloud onto ml
opera platforms and packaged into
familiar open source tools
when i was approaching fifty thousand
downloads per month seven and a half the
monthly download rate just one year ago
research and development is heating up
and publishing is coming from all the
top academic medical centers around the
world
don't miss monayah gtc there's one eye
developer meetup
when i an amazon stagemaker and guys in
st thomas's nhs trust using monai as the
ai operating system for hospitals
over the last five years new instrument
sensors robotics and automation have
created the digital biology revolution
sequencers microscopes diagnostic
imaging devices can capture biology with
increasing throughput and resolution
in particular the last two decades have
brought tremendous advancements in
genomic sequencing
a process that involves conversion of
dna from living organisms into digital
information that can be read processed
and analyzed by computers
dna sequencing the foundation of digital
biology has reduced in cost dipping
below a thousand dollars and exciting a
global trend to generate over 40
exabytes of genomic data by 2025
the availability breadth complexity of
digitized genomic data is growing
rapidly new sequencers are coming to
market for both short and long read
sequencing and the ability to do both
nvidia is working with all sequencing
companies to improve and accelerate the
primary analysis phase of base calling
today's genomics applications are
trending to the use of combination of
both long and short read data building
new demand for sophisticated analysis
applications
global programs are underway to sequence
all living organisms to create
population-specific reference genomes
to centralize millions of human genomes
with their healthcare outcome data and
to rapidly sequence critical care
patients to identify rare or inherited
diseases that have treatments
as digital biology databases grow from
petabytes to exabytes
modern information science approaches
and ai will power breakthroughs in drug
discovery synthetic biology
that have broad reaching effects not
only for healthcare but agriculture
energy manufacturing and climate science
in just the last six months a team of
researchers at uc santa cruz and google
developed a deep learning based analysis
pipeline that produces state-of-the-art
variant calling results for oxford
nanocore data
the team showed that long-reed nanocore
based methods outperform the industry
standard short read single nucleotide
variant identification at the whole
genome scale
simultaneously a clinical sequencing
trial to shorten the time of sequencing
critically ill patients from today's two
weeks to hours was underway at stanford
led by dr ewan ashley
nvidia joined forces with sneha goenka
an electrical engineer phd student at
stanford to help accelerate the pipeline
with the efforts of the entire group a
reduced end-to-end time for sequencing
dr ashley's patients achieved the
guinness world record in january
finishing in just 7 hours and 18 minutes
from a previous 14 and a half hours
let's hear a bit more about how this
impacts patients
[Music]
patients with rare or undiagnosed
diseases often undergo a diagnostic
odyssey spending time money and numerous
hospital visits to uncover the cause of
their disease rapid whole genome
sequencing can help identify the genetic
cause of a disease and match patients to
the right therapies quickly
but sequencing a patient's whole genome
can take weeks and isn't a part of
standard care
so a team from stanford oxford nanopore
technologies nvidia the university of
california santa cruz and google set out
to accelerate a whole genome analysis
workflow
they developed a world-record
ultra-rapid dna sequencing technique
from sample to diagnosis in just seven
hours and 18 minutes their results were
published in january in the new england
journal of medicine it was a phenomenal
collaboration between all the parties so
we're talking about
putting something on a breakfast and
having actionable data
at lunch time
oxford nanopores promethion 48 dna
sequencing instrument was critical to
their achievement the promethion 48 uses
flow cells which contain an array of
tiny holes protein nanopores embedded in
an electro-resistant membrane as dna
passes through the nanopore an electric
signal is generated which is classified
into base pairs using a deep learning
model powered by nvidia this is called
base calling the team used 64 gpus in
the cloud for base calling alignment and
variant calling genetic variants were
identified by a pipeline using nvidia
clara parabrakes
curation and final diagnosis happened
after that
this ultra-rapid nanopore dna sequencing
technique helped a three-month-old
infant who was suffering from seizures
by identifying the correct genetic
variant in just hours a definitive
diagnosis was made quickly a standard
epilepsy gene panel that was ordered at
the same time took two weeks to return
results and didn't even include the gene
affected this is an exciting moment
because this has the potential
to fundamentally change critical care
nvidia was so inspired by this work we
partnered with stanford and oxford
nanopore to democratize this capability
to clinical research everywhere
today we are announcing unapp a
containerized version of the ultra rapid
nanopore analysis pipeline that runs end
to end on a single dgx a100 node
dramatically reducing the compute
infrastructure so sequencing can be done
right in the clinic
for the first time oxford nanopore
prometheion can now locally base call
and a line while the sequencing is
happening making real time sequencing a
reality not only does this dramatically
reduce the computing infrastructure
complexity but it also significantly
reduces the compute cost per patient by
two thirds
from over five hundred dollars to under
two hundred
we will continue to partner with
stanford and oxford nanopore so you can
get your whole genome sequenced while
you nap
get it
the increasing volume of biomedical data
in biology and chemistry requires the
development of new methodologies and
approaches for analysis
applying ai to digital biology is
elucidating biomedical knowledge that
has evolved over human history and lies
within the language of dna and protein
sequences
biology has become an information
science working towards an end-to-end
modeling of disease pathway the genes
involved and the drug target
interactions
with rapid advancements in accelerated
computing and ai we now have the
computational scale necessary to
simulate more and more complex
biological process and speed the pace of
discovery
with over 60 billion in bc funding over
the last two years alone the field is
exploding with new companies who are
building ai drug discovery platforms to
address the over 10 000 diseases with
only 500 approved therapies
let's look at some amazing breakthroughs
that happen in just the last couple of
months
first a survey of some research deep
minds dm21 set out to improve the last
30 years most popular method dft
used to predict properties of systems in
chemistry biology and materials
deepmind developed a machine learned
density functional
this is able to accurately model complex
systems such as hydrogen chains and
charge dna base pairs
we are now one step closer to the
universal functional
which could allow us to solve quantum
chemistry problems at perfect accuracy
equibind created by researchers at tu
munich and mit
is addressing a fundamental problem in
drug discovery of predicting how a
molecule interacts with a protein
there are several important quantities
where the molecule binds
the active site how the molecule binds
the pose
and how well it binds the infinity
equibine reaches state of the art with
respect to pose and it's much faster
than the competing methods like docking
at the university of washington levy and
baker combined an ai design protein with
crispr and showed it's possible to
awaken individual dormant genes by
disabling the chemical off switch
this new approach allows researchers to
understand the role of individual genes
in a cell and begin to learn what makes
cells healthy age or mutate into cancer
next let's take a survey of a few of the
exciting companies in the space
orbnet denali created by company entos
is unlocking a new class of molecular
simulation
researchers use molecular simulation as
a computational microscope to analyze
how molecules react with proteins the
most accurate but too computationally
expensive is quantum mechanics the most
efficient the least accurate is
empirical force fields
orbit denali is a machine learned
potential that learns the interactions
between atoms and is as accurate as
quantum and almost as efficient as
empirical force fields
in silicon medicine late last year
announced the first ai discovered novel
drug for a novel target that entered
into phase one human trials
and silico has an end-to-end approach
using deep learning for target
identification and generative model
models for small molecule generation
this is an amazing industry first
and absci has the first generation of
their fully in silico protein drug
discovery platform
and they are announcing exciting results
here at gtc provided a target antigen
this version one of their platform uses
graph neural networks to design
antibodies de novo and refines their
properties using large language models
applied to their protein sequences
one of the most powerful deep learning
architectures that have emerged in the
last couple of years is the transformer
transformer neural networks are powerful
ai architectures that move us beyond ai
applications that find patterns or learn
from repetition into ai that can learn
from context and create new information
transformer based models train on
unlabeled data sets in a self-supervised
manner and improve their accuracy with
size
this is why nvidia created nemo megatron
an accelerated framework for training
large language models and can scale to
trillions of parameters
nemo megatron manages partitioning
models sending data and collecting
results and uses mixed precision to get
the best possible performance
transformer neural networks typically
have a two-stage paradigm pre-training a
model with fine-tuning for a task
this paradigm is well suited for digital
biology where labeled data is scarce and
unlimited data is tremendous
large pre-trained models have implicitly
encapsulated the knowledge of the domain
for which it was trained and can be used
to search features and generate data
in the case of proteins in chemical
space where the training data sets are
much smaller than all possible
combinations
pre-trained models can generate
molecules or sequencing
not in the database in which it was
trained
in the clinical nlp domain a major
challenge is data availability cost to
label data security and privacy
pre-trained large language models can
generate realistic de-identified
synthetic training data
let me share some of the incredible work
we are doing with our partners
today we are announcing the next
iteration of mega mole art that was
jointly developed with astrazeneca
megamolbar is capable of training large
chemical language models with one
billion or more parameters using
nvidia's nema megatron framework it
includes scripts for accelerated data
pre-processing and training that
implements model parallelism
a new version of the mega mobile model
that achieves super computing scale
molecular generation
and achieves high validity and
uniqueness of 98
where validity measures how well the
model captures chemicals constraints and
uniqueness checks that the model
produces molecules not seen in the
training set
several downstream ucases are now
possible such as molecular optimization
fine-tuning a nemo
and creating proprietary molecular
feature stores
sign up for early access starting in
april 1st
large language models are critical
enabler of today's ai drug discovery
platforms
today we are announcing our next
generation of cambridge one
collaborations with four amazing aid
discovery startups
alcomov's antibody drug discovery engine
is identifying novel targets and
therapeutics in the area of
neurodegenerative conditions in cancer
data sets of antibody sequencing are
exceptionally large
50 times larger than the burt
transformer corpus for natural language
processing
cambridge 1 will help scale language
models that represent antibody sequences
in computational space to increase
understanding of antibody structure and
their function
instadeep an elite member of the nvidia
partner network delivers ai-powered
decision-making systems for the
development of next-generation vaccines
and therapeutics
leveraging publicly available 12 billion
nucleotide sequences from 450 000
species
cambridge one will train a large
language model using some of that data
and make it public so healthcare
researchers can derive new insights from
the rapidly growing sequencing datasets
peptone is a next generation
therapeutics company that works on
intrinsically disordered proteins idps
proteins that lack structure and are
deemed undruggable
their platform oppenheimer runs a super
computer stack running ai and simulation
cambridge one will take peptone's ai
model development to exceed billions of
parameters and predict millions of
proteins and enhance the platform
to describe discover more plausible
drugging targets
and relation therapeutic uses single
cell profiling human genetics and
functional genomics with machine
learning
to create maps of causal relationships
with that drive disease
cambridge one will be used to build
language models from dna and rna
sequencing data that could open up areas
of biology that almost impossible for
wet lab experiments
like how changes in dna sequence affect
the gene expression in the human brain
so i've talked to you about transformers
and large language models being used for
biology and chemistry but of course they
apply to natural language processing
applications across healthcare
we have exciting work underway with
jansen johnson and johnson's
pharmaceutical division to help ensure
safety of drugs on an ongoing basis in
the industry this is called
pharmacovigilance
the jensen r d team in nvidia used
nvidia biomegatron a state-of-the-art
pre-trained language model that
understands the nuances of biomedical
literature
an nvidia nemo training framework for
easy customization and fine-tuning
using nvidia nemo and biomegatron jansen
improved adverse drug event detection
recall by 12
to reach 88 while maintaining precision
this results in classifying many more
adverse events which will help to
increase patient well-being and discover
potential positive and negative side
effects
jensen further optimized the new model
using nvidia tensor rt and triton to get
a 2x performance gain in model inference
improving the cost and operational
efficiency for jansen and their
intelligent automation team
biomedical and clinical language models
are dense and filled with
domain-specific jargon which is why
domain-specific models are needed
clinical data has additional
complexities including clinical jargon
siloed data
and it contains protected health
information
yet the opportunity for clinical nlp is
tremendous for research and improving
patient care
today we are announcing the university
of florida and nvidia made the world's
largest clinical language model called
syngatortron
that can synthesize clinical data
naturally de-identified and mimicking
unstructured clinical
datasets syngatortron is used as a data
factory to generate large volumes of
synthetic clinical reports at various
types
as an example we prompt syngatortron to
generate millions of synthetic discharge
summaries
we show that pre-training clinical
language models on synthetic data
results in models that meet or exceed
the performance of models trained on
real-world de-identified data sets
for the first time we're releasing
gatortron pre-trained models to the
community
free and open source on ngc
two models will be made available
the model trained on real-world
de-identified data gatotron og
for original and a model trained on
synthetic data gatotron s
heterotron models substantially reduce
the barrier for developing clinical
applications used across the entire
industry for hospital systems pharma
insurance and contract research
organizations
this is an amazing contribution to the
field
we talked about the digital biology
revolution giving us new tools to
diagnose patients and discover drugs
we talked about the power of
domain-specific language models that
have broad applications across the
healthcare domain
now let's talk about the future of ai
and medical devices
today the healthcare industry is faced
with incredible challenges
aging and growing populations
chronic disease driving up demand for
medical care far beyond the supply of
medical professionals
by 2030 one in six people will be over
sixty
six and ten adults in the u.s have to
manage chronic disease
even today only one-third of the world's
population has access to medical imaging
and surgical treatment
it's now possible through continuous
sensing computation and ai for medical
devices to detect measure predict
simulate adapt guide
essentially become robotic in a way that
augments clinical care teams
helping to meet the demand maximize
efficiency and increase the access to
modern medicine
nvidia clara holoscan is an open
real-time computing platform for the
healthcare industry providing the
computational infrastructure needed for
scalable software-defined end-to-end
processing of streaming data
clara hall skin is an all-in-one
computer that supports third-party
front-end sensors and combines nvidia s3
chips
nvidia connectx high-speed i o and gpu
direct rdma
nvidia oren system on a chip
and rtx a6000 discrete gpu for ai
imprints and advanced rendering
claro holoscan is cloud native and
supports over-the-air updates
based on a microservice engine to run
modular multi-modality microservices
allowing developers to create low
latency streaming applications running
on device while passing more complex
tasks to the data center resources
let's take a look at holoskin in action
for microscopy
light sheet microscopy invented by
chemistry nobel laureate eric becksig
uses high resolution fluorescent
microscope to create movies of living
cells
giving researchers the ability to study
biology in motion
the challenge is these microscopes
generate three terabytes of data per
hour
and some experiments can take a full day
to complete and then an additional day
to process and visualize
nvidia clara holoscan makes it possible
to process and visualize the microscopy
data in real time
at the advanced bioimaging center at uc
berkeley researchers are using holoscan
and ai to automatically detect
interesting biological events and auto
focus the microscope while the
experiment is running
claro holoscan can process the data in
real time because it pulls the data
directly from the sensor
into the pci express frame grabber and
uses gpu direct rdma for gpu image
processing deconvolution and deskewing
holoskin applications are composed as
graph of microservices that seamlessly
distribute processing from device
compute to data center resources using
zero memory copy to ensure high
streaming performance
here holoskin streams process microscopy
images to the data center running a
microservice of nvidia index for
interactive large scale volume rendering
let's take a look at what light sheet
microscopy can see
[Music]
now with claro holoscan and nvidia index
we can visualize the entire large volume
of living cells in real time as the data
is being recorded directly from the
microscope
watching these living cancer cells move
about we can see normal healthy biology
and malignant processes at the same time
the fluorescent marker rendered in blue
marks nuclei which we see splitting to
form two cells from one cell a hallmark
of cancer is cell division occurring
more frequently and with less error
checking than normal healthy cells
using berkeley's lattice light sheet
microscope the ultimate high resolution
allows scientists to see what is hidden
to normal light optics not seen using
traditional microscopes as we zoom in
watch cancer cells display what is
thought to be a rare event even for
cancer cell lines see one cell split
into three cells this phenomenon has
only been reported anecdotally in a
couple of scientific publications
scientists do not yet know what we will
see but this technique enabled with
real-time processing and visualization
now allows the scientific community to
discover new unseen events like this
let's see what the future has in store
we are working with the entire sensor
ecosystem to support clara holoscan so
every medical instrument can accelerate
their development and become real-time
sensing systems
working with aja blackmagic deltacast
yuan magewell and kaya
our global partners are providing video
capture cards supporting sdi hdmi dvi
inputs commonly used in radiology and
endoscopy and surgical displays
verisonics and us for us provide
ultrasound transducer front ends with
access to raw rf data to develop
reconstruction and image processing
algorithms for software defined
ultrasound
and spectrum general purpose
digitizer cards support ultrasound
radiation and laser sensors
the nvidia clara hall scan developer kit
works with multi-modality sensor
front-end support and it has a cloud
native sdk with reference applications
to give developers a head start
claro hall skin developer kits are
available today with the oren based kits
coming second half of 2022.
clara holoskin is being adopted by the
leaders in the industry from top medical
device makers to new robotic and digital
surgery platforms
carestream fully integrated intelligent
x-ray room uses continuous ai for
patient placement ai image processing
and image quality and denoising
siemens vex ct nuclear imaging system
auto adapts ct dose and perform smart
zooming
active surgical provides surgical
guidance with tissue perfusion and blood
flow visualization for critical
real-time guidance to surgeons
hapley robotics a next-generation
physical simulation training platform
playstation for surgeons
is set out to train 2 million surgeons
in over 260 procedures
an imfusions platform is a
self-navigating ultrasound acquisition
robot that fuses with 3d mr mri data and
segments in real time
the applications for clair holoscan are
endless
today we are announcing claroholoscan
mgx a medical grade platform for
real-time software-defined medical
devices
holoscan mgx is a three-chip scalable
reference design for embedded and edge
computing
based on nvidia orin to deliver 250 ai
tops and scaling to over 600 ai tops
with the rtx a6000 discrete gpu
nvidia connectx 7 delivers streaming i o
with up to 200 gigabit ethernet
and gpu direct rdma for ultra low
latency processing
mgx has safety security and
manageability built in
with a safety microcontroller that
supports running a real-time operating
system
a baseboard management controller for
over-the-air updates and monitoring and
an external route trust for boot
security
the holoscan mgx reference design will
be available from our global partners
ad link advantech dedicated computing
kontron lead tech mbx onyx portwell
prodrive ryoyo and yuan
are all announcing today support for
clara holoscan ngx
nvidia provides the mgx stack with over
10 year whole stack support
the core of mgx stack includes the
operating system drivers and supported i
o devices
nvidia's ai inferencing acceleration
libraries and reference applications
holoskin mgx is a medical-grade
full-stack platform that customers build
applications on dramatically reducing
engineering investment and providing the
architecture for a software-as-a-service
business model
nvidia clara hall scan is a
one-of-a-kind end-to-end platform for
the healthcare industry from ai
development to production ai deployment
with the world's most advanced ai
development frameworks monae and nvidia
flair for model development
and now a complete deployment
architecture with holoscan developer
kits and production deployment on clara
holoskin mgx medical grade platforms
the impact of ai in healthcare
applications is accelerating
the digital biology revolution has begun
sequencing costs and throughput are
generating a tsunami of genomic data
that the world is just learning to
understand
today with oxford nanopore breakthrough
technology real-time sequencing is
becoming a reality for critical patients
transformers and large language models
are helping to shed light on the lower
levels of biology that human evolution
is written
cambridge one is helping scale ai
protein and drug discovery platforms to
take advantage of the recent
breakthroughs in billion and even
trillion parameter chemical and
molecular language models
nvidia's mega mobile provides a training
framework for chemical language models
capable of generating molecules
ai tools built specifically for
healthcare like monae
invite the industry experts to get
involved and build ai applications that
help solve their most important
challenges
tens of thousands of ai applications are
needed in radiology alone
clara holoscan completes the picture
giving ai models a platform for
development now with holoskin mgx a
medical-grade ai computing platform the
medical device industry can evolve to a
software-as-a-service business model
giving doctors the latest innovations
and extending healthcare access through
ai and robotics
the future is bright have a fantastic
gtc
Title: GPU Technology Conference News Highlights - March 25, 2014  GTC
Publish_date: 2014-03-25
Length: 134
Views: 21239
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/HjH7Um0CD58/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: HjH7Um0CD58

--- Transcript ---

welcome to GTC 2014
hi I'm Adam here at invidious GPU
technology conference in San Jose
California now in its fifth year GT C is
bigger and better than ever there are
over 3,500 people here from 50 countries
to hear keynotes
attend more than 500 sessions and meet
with the brightest minds in computing
this is the number one show for the GPU
ecosystem companies like Google Facebook
Pixar Boeing Honda and Audi are all here
along with scientists and researchers
from universities and leading
laboratories to explain how they're
solving important computing challenges
there's been a ton of great news here at
GTCC I want to walk you through four
particularly interesting items we
announced the Pascal will be our next
GPU after Maxwell Pascal will introduce
several new features including envy link
Envy link is a new interconnect for
high-performance computing that will
enable GPUs and CPUs to share data 5 to
12 times faster than they can today it
will eliminate a long-standing
bottleneck and pave the way to exascale
supercomputers we introduced a
professional new rendering system called
array visual computing appliance IRA VCA
dramatically accelerates ray tracing to
enable designers to create and interact
with photo realistic models in real time
it will allow designers to replace
physical prototypes with 3d rendering of
incredible detail we unveiled an awesome
new graphics card called the geforce gtx
titan z built around two Kepler GPUs and
twelve gigabytes of dedicated frame
buffer memory titan z is engineered for
next generation 5k and multi monitor
gaming and we announced a new
development module called Jetson TK won
the world's first mobile supercomputer
for embedded systems Jetson will enable
developers to create smarter robots
Jones that avoid moving objects and
self-driving cars this concept was
dramatically illustrated when an Audi a7
drove across the stage and parked itself
during our CEOs keynote
this is just some of what's been going
on here at GTCC it shows how Nvidia is
the leader in visual computing and why
visual computing is more important than
ever
Title: How AI Helps Autonomous Vehicles See Outside the Box - NVIDIA DRIVE Labs Ep. 14
Publish_date: 2019-10-23
Length: 87
Views: 51610
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/HS1wV9NMLr8/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: HS1wV9NMLr8

--- Transcript ---

In autonomous vehicle software, deep neural
networks are often trained to detect an object
using a bounding box around the object.
Today in DRIVE Labs, we're going to talk about
using a DNN to provide pixel-level accurate
object shape information.
This is known as panoptic segmentation.
In the top video, we see pixel-level shape
segmentation results for different object
classes, with vehicles in blue, pedestrians
in red, and drivable space in green.
In the bottom video, we see segmentation of
these object classes into individual instances,
as shown by their unique object IDs.
So, each of the pedestrians at this intersection
is detected and tracked as a different instance.
We detect object shape details, including
the rearview mirror on the bus coming towards
us, unusually shaped trucks, and the precise
shape of a trailer.
Despite occlusion by the tree, this car is
still detected as a single instance.
Here we see accurate segmentation of drivable
space around traffic cones and on-road traffic
signs.
In this clip, the panoptic segmentation DNN
is segmenting both cars in drivable space
accurately in an unstructured environment.
Panoptic segmentation of objects and free
space provides a deeper and richer understanding
of complex scenes, such as dense traffic with
vehicles occluding each other, or construction
zones with unusual object shapes, or corner
cases, such as pedestrians carrying large
objects.
Title: NVIDIA Announces Project Holodeck
Publish_date: 2017-05-12
Length: 74
Views: 67050
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/hUsP7fsjrdg/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: hUsP7fsjrdg

--- Transcript ---

[Music]
[Music]
[Music]
[Music]
[Music]
go ahead and draw that mustache stay
still yeah that is one you could see
from behind the ears buddy
I choose you go figure that that's good
Title: NVIDIA DRIVE Ecosystem Partners at GTC 2019
Publish_date: 2019-03-21
Length: 128
Views: 7737
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/hzRjUqAnXz4/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: hzRjUqAnXz4

--- Transcript ---

over 94% of all accidents happen because
of driver error if you think about all
the stuff you could do in your car
instead of you know driving so what we
are showcasing here is really the future
so what is yet to come
automated driving is not something that
somebody can do by themselves it takes
an ecosystem of partners there's a whole
new frontier out there we see the inside
the cabinet is the next one so a
guardian has done has developed a single
sensor that's not inside the vehicle
that detected everything that goes on
inside the vehicle we think Nvidia
provides a great platform in the night
driving we're actually using the high
dynamic range camera which requires a
even bigger flow of the information to
be processed the media is providing
extremely powerful deep learning chips
massive amounts of sensor data need to
be processed in real time safely and
efficiently and our system uses Nvidia
to do that
Nvidia being leader in GPU technologies
is enabling us to design and develop our
systems at the increased pace that
everything is happening at nowadays and
we're right now we have been using GPU
extensively for deep learning model
training and the inference this car is
not really a vehicle just to go from
point A to point B it's like a third
home for me it's homework and then the
car I'm spending most of the time and I
should be able to naturally interact
with it and that's why you need this
intuitive interaction with the car the
Nvidia drive is actually powering over
in MBU x infotainment systems without
having the powerful NVIDIA GPUs we
basically can't really resolve the
nitriding issue time is driving really
it's more of a matter of when not if
what's cooler than enabling the future
and changing the future of
Transportation that's why I'm excited
about this
[Music]
Title: GeForce Garage– Making The GeForce GTX 1080 H-Tower
Publish_date: 2016-06-09
Length: 252
Views: 44963
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/H_6rS5UxEX8/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: H_6rS5UxEX8

--- Transcript ---

what's up everybody my name is Dwight
and welcome back to g-force garage I'm
here with my good friends Bob and rod of
BS mods who just finished building
Tessie our amazing new H tower there's a
reveal video that you guys can go watch
in the description down below this video
is going to be about what's inside of it
and what they did to make it so cool so
this is the H tower from n win I know
how badass it is but I want you guys
fill us in on why it's such a badass
case it's an amazing chassis it's a one
of the Signature Series and it's a
limited production chassis it's an
automated chassis so it opens up
displays your hardware for you so the
first stage that the sides open up then
the second stage here is where you can
tilt the motherboard tray so now you can
check out your hardware it's easy to
work on you can get to everything and
you know it's a real fun case to just
play with and show off you know how long
it take you to break down it probably
took an hour you know and we could have
done it faster but we thought it might
be a good idea to take a lot of pictures
orientation of the arms the springs that
are inside the arms the spring
orientation all that was critical and
getting it back together so what about
the paint job - you guys got this really
cool jet black mirror gloss finished
we're kind of known for our paint jobs
and we use a lot of automotive finishes
primarily and so this is a acrylic
urethane jet black and several coats of
clear and then wet sanded and polished
and buffed out it looked really
beautiful but NVIDIA had asked if we
could throw a little something about
tessellation on there and we reached out
to a friend of ours romli Christensen of
bLUE HORSE Studios Braun's been on the
on g-force garage a couple of times very
talented guy we kind of turned him loose
on it and it went from a simple
tesselation design and kind of evolved
into a dragon which kind of gave it life
really so I know that this case actually
comes with lights on all the arms but
you guys removed them on the back and
put your own crazy ones in the front
right so we really want the case to up
and show off the hardware so we put some
10 watt LEDs we also hook these lights
up to a relay because we want it to be
able to remotely
turn lights on so don't look directly at
them okay
shield it shiver eyes go there you go no
it's not right those are actually
incredibly bright lights well what about
the the specs of the PC like what's it's
actually inside of it so we went with an
Asus x99 WS board we have 64 gigs of
2666 dominator memory a 1200 watt
corsair power supply a samsung 950 pro
and DME drive at 59 60 X then the
granddaddy of it all we've got our
GeForce GTX 10 80s so I also notice that
you guys have a custom cooling setup
here so yeah we used EK parts here we
went with a 360 rad the pump as well and
then also the block and we've run 16
millimeter tubing so it's a little bit
larger diameter and you can't really see
and then for the coolant we went with
the Aurora silver it's kind of really
cool like cloudy affective and I notice
that all of this is on top of a
different box I know that this box
doesn't come with the case the box are
talking about as a pedestal that we
custom-built it kind of puts on a show
it's got some lights at the corners that
flash a hidden smoke machine inside so
between the smoke and the lights
flashing and everything it's it's quite
the show
thanks mom rod for coming out you know
you guys built another amazing amazing
PC we had a blast doing it yeah thank
you it's always a pleasure you guys want
to see the reveal video again the link
will be down below so check it out it's
really cool smoke and everything lights
nuts you guys want to see more of Bob
and Ron go check out youtube.com forward
slash BS mods and check out more of
their projects and everything then
watching g-force garage see you guys
later
Title: A Team of Extraordinary Technologists
Publish_date: 2019-12-18
Length: 115
Views: 76547
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/I5AOnsSPpYo/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: I5AOnsSPpYo

--- Transcript ---

I'm a research scientist and I work in
the new experiences group and we look
ahead 10 or 15 years to try and see what
kinds of new displays are on the horizon
my main area of research is foliated
rendering or gaze contingent rendering
that involves using an eye tracker and
trying to save computation by rendering
peripheral images at lower resolution I
first got interested in vision science
because I found out that I was stereo
blind that inspired me to enroll in a
vision science ph.d program at Berkeley
while I was doing my PhD in vision
science you have to program experiments
and I first learned how to program
during that time and I discovered that I
actually really liked programming a lot
more than doing experiments even so I
decided to do a master's in computer
science at the same time I was really
looking for an internship that combined
my vision science in computer science
expertise Nvidia was a great opportunity
for that to work in VR research here so
the culture is definitely positive
towards disagreement and also voicing
your opinions on things people ask hard
questions that's part of the nature of
the job and that is actually very
comforting because you can have the
discussion and it makes the ideas
stronger things move really quickly here
and we have a philosophy of
meet risk head-on so that means that
people pitch ideas to each other all the
time and it's a very fun process to like
get excited about other people's ideas
that have people get excited about your
ideas I really look forward every
morning this is seeing my team and
engaging in ideas with them and also to
playing with all the cool technology
that I get to work with here it's
exciting to be solving really hard
problems every day
[Music]
you
you
Title: NVIDIA Keynote at COMPUTEX 2022
Publish_date: 2022-05-23
Length: 3153
Views: 2084981
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/icEa4c9USS4/hq720.jpg?v=628c5fec
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: icEa4c9USS4

--- Transcript ---

Once upon a time, it was all fun and games.
NVIDIA started off by making chips for video game machines.
Graphics became serious business when people started using NVIDIA for blockbuster movies, medical imaging devices, and
the world’s most powerful supercomputers.
And then one day, researchers discovered that our technology was perfect for AI.
Today, NVIDIA is the engine of AI.
Engineering the most advanced chips and systems
and the software that makes them sing.
So robots can lend us a hand…
Cars can drive themselves…
And even the Earth can have a digital twin.
We live to tackle the world’s biggest challenges.
And don’t worry.
We still love our fun and games.
Welcome to Computex!
Our close partnership with the innovators in Taiwan
is foundational to our work.
We have a lot to cover today, across the breadth of our company – gaming, robotics, data center, and, of course, AI.
Let's start there.
AI is transforming large markets, and every day we work closely with our partners to help bring new AIs to life.
We collaborate on the systems, the physical and software infrastructure, the AI frameworks and AI applications – a
platform that is continuously growing.
Data centers themselves are transforming into AI factories 
as companies manufacture intelligence to process every engagement,
every product, every recommendation to deliver great customer experiences.
This transformation requires us to reimagine the data center at every level, from hardware to software, 
from chips to infrastructure to systems themselves.
This data center transformation will drive massive business opportunities for our partners in Taiwan.
Delivering AI factories for training and inference combined with the continuously growing need for traditional high
performance computing, comprise a $150Bn market for our ecosystem partners.
In addition, AI is enabling the new market of digital twins, where we can reproduce, virtually, the complex products the
world wants to build, and extend all the way to a digital twin of the world itself to study climate change.
Digital Twins opens up a new $150Bn market.
A third workload is also emerging:
cloud-based gaming.
Streaming games from the cloud to all devices expands the datacenter market by another $100Bn.
Combined, these workloads represent a half-trillion-dollar market opportunity that our partners can tap into
by leveraging our open platform.
There are 4 key elements of this data center transformation that require us to reimagine these modern AI factories.
First, we need 3 processors working in harmony to handle different aspects of these massive new workloads:
 CPUs, GPUs,  and DPUs.
CPUs manage the overall system, 
while GPUs are the workhorses that perform the computations, 
and DPUs handle network traffic securely and perform the in-network computing to optimize performance.
Of course, as training scales up, faster interconnects are necessary to allow the compute to scale.
These massive workloads consume exaFLOPs of compute, so infrastructure that excels at energy efficiency ensures
sustainability in AI factories.
NVIDIA GPUs are so efficient at compute that if all the world's current AI, HPC and data analytics workloads were
running on GPU servers, we estimate we'd save over 12 trillion watt hours of electricity per year! 
That’s the equivalent of taking 2M cars off the road annually!
Getting AI running in production requires both tools to manage development workflows
and tools to run the models for inference.
But it also requires tools to manage and deploy AI models among fleets of servers 
distributed across central and edge data centers.
This requires robust software to run AI factory operations 24/7.
Powering these modern AI factories requires end-to-end innovation at every level.
These include Hopper GPUs, Grace CPUs, and BlueField DPUs 
as building blocks networked together by Quantum and Spectrum switches.
All of these combine to deliver the infrastructure of the data center of the future 
that handles these massive workloads.
Finally, getting all of this to run seamlessly requires NVIDIA AI Enterprise software, 
which delivers robust 24/7 AI deployment.
At every scale, NVIDIA has worked on this entire canvas of innovation so that our partners can take advantage of our
open platform to deliver state-of-the-art servers and racks for the modern AI data centers.
Earlier this year, NVIDIA announced the H100 GPU, the most advanced chip ever built.
It's an order of magnitude leap in performance over A100.
Built with a custom TSMC 4 nanometer process, it features 6 groundbreaking inventions.
A faster, more powerful Tensor Core—6X faster than its predecessor, Ampere.
It's built to accelerate Transformer networks, the most important deep learning model today.
The 2nd-generation multi-instance GPU partitions the GPU into smaller compute units 
that allow CSPs to divide each H100 into 7 separate instances.
This greatly boosts the number of GPU clients available to data center users.
Confidential computing allows customers to keep data secure while being processed, 
maintaining privacy and integrity from end to end on shared computing resources.
Our 4th-generation NVLink allows GPUs to communicate faster than ever before, 
at 900 GB/s bandwidth between server nodes,
and scaling up to 256 GPUs to solve these massive workloads in AI factories of the future.
The DPX instructions are dedicated cores that speed up recursive optimization problems, 
like gene sequencing,
protein folding,
and route optimization—up to 40X faster.
For the largest-scale AI factories that need to solve workloads like conversational AI agents 
and planet-scale digital twins, 
NVLink now scales across servers.
The new NVLink Switches allow up to 256 GPUs or 32 HGX servers to communicate at NVLink speeds.
This switch and interconnect network forms the NVLink Switch System.
NVLink Switch System allows up to 20.5TB of HBM3 memory 
at an incredible 768 TB/s of memory bandwidth.
This delivers an exaFLOP of compute in a single pod—
truly we have reimagined the data center.
The performance boost over Ampere is incredible.
Training the latest Transformer-based models, the combined benefits of Hopper's raw horsepower,
 the new Transformer engine with FP8 Tensor Core, 
NVLink with SHARP in-network computing, 
and NVLink Switch, as well as the latest Quantum-2 InfiniBand, results in a 9X speed-up!
Weeks turn into days.
For inferencing, the benefit is even greater.
H100 throughput is up to 30X higher over A100.
It's the most significant leap we've ever delivered.
NVIDIA H100, the new engine of the world's AI infrastructure.
Our Taiwan partners have helped accelerate the world's servers.
Now, they're designing the world's next generation data centers with NVIDIA accelerated computing.
Let's take a closer look at the chips in our open platform that deliver this incredible acceleration.
Here's Brian Kelleher to tell us more.
Thanks Ian.
NVIDIA has been built on a consistent flow of innovations and technology,
delivered through a new GPU architecture family every two years.
NVIDIA focuses on inventions that solve new challenges and open new markets.
Our work in accelerated computing, AI and machine learning, edge computing, and Omniverse are examples of how NVIDIA and
our partners have created unserved new markets.
We’ve established a reputation for unmatched roadmap execution, designing the world’s most complex silicon with an
expectation that the first silicon out of fab will go directly to production.
That dependability helps align our company, and just as importantly, 
helps align our partners through trust in our roadmap.
Our new data center roadmap includes three chips: CPU, GPU, and DPU.
We’ll extend our execution excellence and give each chip architecture a two-year rhythm.
One year will focus on x86 platforms.
One year will focus on Arm platforms.
Every year you will see exciting new products from us.
The NVIDIA architecture and platforms will support x86 and Arm – whatever customers and markets prefer.
3 Chips; Yearly Leaps; One Architecture.
Let’s dive into the latest announcements and discuss the impacts to our partners’ servers…
Grace is our first data center CPU.
Grace is built for AI workloads that have emerged only in the past few years.
Grace is built to power AI factories – a new breed of data centers.
Grace is on track to ship next year and will be available in two form factors.
Grace-Hopper, shown on the left, is a single superchip module with a direct chip-to-chip connection between the Grace
CPU and the Hopper GPU.
The CPU and GPU communicate over NVLink-C2C – a low-power memory-coherent interconnect at 900GB/s!
Grace will transfer data to Hopper 15X faster than any other CPU can 
and will increase the working data size of Hopper to up to 2 TB.
Grace-Hopper is built to accelerate the largest AI, HPC, cloud and hyperscale workloads.
We’ll also offer Grace in a superchip made of two Grace CPU chips connected coherently over NVLink-C2C.
The Grace superchip has 144 CPU cores, an incredible 1TB/s of memory bandwidth, 
and twice the energy efficiency of existing servers—
the entire module including 1TB of memory consumes only 500W.
Grace is based on Arm, the world’s most popular CPU architecture, and rapidly growing in hyperscale cloud and edge
computing.
Grace will be amazing at AI, data analytics, scientific computing, and hyperscale computing.
And, of course, the full suite of NVIDIA software platforms will run on Grace.
The enabler for Grace-Hopper and Grace Superchip is the ultra-energy-efficient, high-speed, memory coherent,
NVLink chip-to-chip interconnect.
Future NVIDIA chips – CPUs, GPUs, DPUs, NICs, and SOCs – will integrate NVLink just like Grace and Hopper.
Our SERDES technology is world-class, with expertise established over decades of designing high-speed memory interfaces,
NVLinks, and networking switches.
NVIDIA is making NVLink and our SERDES open to customers and partners who want to implement custom chips
 that connect to NVIDIA's platforms.
In addition to NVLink-C2C, NVIDIA will also support the developing UCIe standard announced earlier this year.
With NVLink that scales from die-to-die, chip-to-chip, and system-to-system, we can configure Grace and Hopper to
address a large diversity of workloads.
We can create systems that range from Grace CPU-only to accelerated with up to eight Hopper GPUs.
The composability of Grace and Hopper’s NVLink gives us a vast number of ways to open new markets 
and address customers’ diverse computing needs.
Here’s Ying Yin to tell us more about these cutting edge systems.
Thank you Brian, Advanced technology needs world-class partners.
Collaborating with the world’s best system makers, we have created a broad array of data center solutions.
Together, we offer hundreds of configurations of x86 and Arm systems to power the world’s need for HPC and AI,
 and we are preparing new systems for Hopper and BlueField.
These systems are open for all partners to expand their markets by leveraging our ecosystem.
Today, we’re introducing Grace-based reference designs for the 5 massive new workloads of reimagined data centers:
CGX for cloud gaming
OVX for Digital Twins and Omniverse
HGX for HPC and Supercomputing
and, last but not least, a new HGX architecture for AI.
It features our most powerful AI training and inference platform using the Grace-Hopper CPU-GPU superchip module.
All of these servers are optimized for NVIDIA accelerated computing software stacks,
and can be qualified as part of our NVIDIA-Certified Systems lineup.
Let’s take a closer look at two of the HGX systems.
Today, we are announcing HGX Grace and HGX Grace Hopper systems.
NVIDIA will provide the Grace Hopper and Grace CPU Superchip modules 
as well as their corresponding PCB reference designs.
Both are specifically designed for OEM 2U high-density server chassis.
Our partners can modify the reference design to quickly spin up the motherboard, 
leveraging their existing system architectures.
Since OEMs already widely use the 2U high-density chassis, they can easily repurpose it to build Grace-based servers.
We’re pleased to announce these OEM hardware titans who will be part of the first wave of providers.
The Grace systems will start shipping in the first half of 2023.
Let’s now talk about how we connect these servers to build racks and clusters with NVIDIA networking.
Here’s Michael Kagan to tell us more.
Thank you Ying Yin.
The wave of AI sweeping the world is accelerating the demand for computing and creating new services that further demand
even more data processing power. 
Data centers are becoming AI factories. 
This is the new unit of computing based on
software defined infrastructure, delivering software defined services. 
It is a single harmonic computing engine,
delivering millions of services to billions of users.
NVIDIA networking solutions are based on three key components. 
BlueField DPU, the data processing unit that connects compute nodes to the data center network, 
the InfiniBand Quantum switch, and the Ethernet Spectrum switch.
The NVIDIA BlueField DPU is the computing platform running the data center operating system.
BlueField offloads and accelerates networking and storage services presenting virtual infrastructure at native bare
metal performance.
BlueField is an essential part of cross-tenant performance and security isolation at the data center scale.
Its operation is optimized automatically through built-in AI accelerators.
A few months back, we introduced BlueField 3, the 400-gigabit DPU.
BlueField-3 integrates high-performance compute cores, exposing fully-programmable data-path acceleration for network,
security, and storage.
BlueField is a zero-trust infrastructure computing platform with hardware-based platform attestation and transparent
“always on” data encryption.
DOCA is the infrastructure framework for cloud native data center. 
It simplifies development of networking, storage,
security, and infrastructure management services.
DOCA is designed to host certified third-party infrastructure services.
The NVIDIA InfiniBand Quantum network platform is designed for AI and HPC workloads. 
It is the foundation of cloud-native supercomputers that deliver bare metal performance with the convenience of cloud usage.
The NVIDIA Ethernet Spectrum networking platform is the fastest and most efficient Ethernet platform for the enterprise
cloud data centers.
NVIDIA maintains a strict cadence introducing a new generation networking platform every other year. 
This year, we introduced the 400 gigabit end-to-end networking platform,
the fastest end-to-end network solution in the world.
The BlueField DPU along with the Quantum and Spectrum networking switches 
comprise the infrastructure platform for the AI factory of the future.
Thanks Michael.
Solving challenges with AI requires a full-stack solution and NVIDIA provides the software that brings these hardware
innovations to life.
NVIDIA AI Enterprise is a suite of software to power the end-to-end workflows of AI and data science.
From data preparation and analytics with RAPIDS,
 to model training with TensorFlow, 
to real-time inferencing with Triton.
You can think of this software as the operating system of AI.
This software is fully supported by NVIDIA to run on leading enterprise platforms from cloud to data center to edge
to help enterprises and organizations start AI projects and keep them on track.
On top of this core software, NVIDIA has created frameworks to help solve specific challenges. 
For example, Riva is a framework for speech AI, 
Merlin for recommender systems, 
and Metropolis for vision AI.
NVIDIA AI Enterprise is available through our partners around the world.
When it comes to reimagining the data center, NVIDIA has the complete, open platform of hardware and software to build
the AI factories of the future.
NVIDIA is Taiwan's partner to scale this technology portfolio into products, to move from servers to racks 
to data centers that manufacture intelligence.
We've talked about AI factories. Now let's talk about the next wave of AI: robotics.
Here's Deepu Talla to tell you more.
We are entering the age of robotics—
autonomous machines that are keenly aware of their environment 
and that can make smart decisions about their actions.
This drive towards automation makes robotics a major new application for AI.
Across industries such as manufacturing, retail, agriculture, logistics & warehouses, delivery, and healthcare,
we see a clear demand for automation.
Robots of all forms and sizes, with wheels, with arms, with legs, with wings, on the ground, in the air, under water,
and even robots that are stationary watching other things that move and providing outside-in perception 
are increasingly being deployed.
NVIDIA Isaac is our robotics platform. It has four pillars.
The first pillar is about creating the AI, a very time-consuming and difficult process that we are making fast and easy.
The second is simulating the operation of the robot in the virtual world before it is tried in the real world.
It is far safer, cheaper, and faster for robots to be born in the virtual world before existing in the physical world.
The third pillar is building the physical robots.
We will show the tools that help bring real robots to market.
Last, the fourth pillar is about managing the fleet of deployed robots over their lifetimes, 
typically many years if not more than a decade.
Now, let’s double-click into how we are simplifying the creation of AI for robotics.
Training AI models requires data—lots of data.
Capturing real-world data and human labeling is necessary but not sufficient.
With synthetic data generation, or SDG, 
corner cases can be added and model development bootstrapped.
Image attributes such as lighting, textures, and colors can be randomized to ensure diversity in the dataset.
And the SDG tool delivers the datasets with perfectly labeled data.
Augmenting the real training dataset with synthetic data is increasingly being used to improve accuracy 
and also reduce time to create or update an AI model.
Using the NVIDIA Omniverse platform, we have created Isaac Replicator for SDG in robotics applications.
Starting with a good AI model dictates how fast the model can be adapted to a particular use case and target device.
NVIDIA pre-trained models vastly speed-up the model creation time.
Several of our customers have reported up to 10X improvement.
Imagine hiring an engineer for a critical job.
You can hire a high school student and train them for a few years starting from the basics, or you can select an
experienced engineer and train them in a few weeks in the job’s expertise.
NVIDIA pre-trained models are essentially experienced engineers and are available on NGC for download.
The NVIDIA TAO toolkit allows you to fill in the gaps.
TAO stands for Train, Adapt, and Optimize.
You take a great engineer and then delta-train them for your specific environment.
You can take any model, whether it’s an NVIDIA pre-trained model or your own model, and use this toolkit to create AI
models optimized for both accuracy and performance.
Every month, we are seeing several thousands of downloads of our pre-trained models and TAO toolkit.
Synthetic data generation plus pre-trained models and TAO greatly simplifies AI model creation.
Make sure you try these out if you are building AI for robotics.
Now let's talk about the second pillar.
Robotics is not easy, particularly building and testing a physical robot.
Imagine you want to build a robot arm and it weighs 500 pounds.
And it’s made of solid metal,
capable of manipulation and gripping.
And it needs to work with a human in a manufacturing plant.
I certainly do not want to be that human crash test dummy.
Until it has been tested a million, billion times, I'm not going to be comfortable operating beside it.
I don’t mind being the human in the virtual world working with the robot in a simulation.
We can simulate thousands of robots in parallel.
Simulation makes it safe, cheap, and fast.
NVIDIA Omniverse brings together high-fidelity graphics and accurate physics 
to become a platform to simulate the real-world—
an environment for creating digital twins.
Using NVIDIA Omniverse, we have built Isaac Sim for robotics.
With Isaac Sim, various robot 3D models can be imported.
All the robot sensors can also be imported.
Isaac Sim can be interfaced with the ROS ecosystem.
Several companies are using Isaac Sim for simulating both navigation and manipulation.
Our focus on Isaac Sim is closing the Sim2Real gap, 
where the simulation closely matches what happens in the real world.
Today, we’re announcing Isaac Sim 2022.1 release, 
which introduces new features that make it the simulator for the age of AI robotics.
Features include a new tool called Cortex, 
which makes it easy to program co-bots, like the large robot arm I mentioned earlier.
We also have added Isaac Gym, 
which allows reinforcement learning to be leveraged to train robotcontrol policies
in minutes, as opposed to days.
Along with SDG capabilities of Replicator mentioned earlier, 
these new tools bring the power of NVIDIA AI to robotics simulation.
Now let's take a look at a demo of Isaac Sim in action.
Successful development, training, and testing
of complex robots for real-world applications
demand high-fidelity simulation and accurate physics.
Built on NVIDIA's Omniverse platform, Isaac Sim
combines immersive, physically accurate,
photorealistic environments with complex virtual robots.
Let’s look at three very different AI-based
robots being developed by our partners using Isaac Sim.
Fraunhofer IML, a technology leader in logistics,
uses NVIDIA Isaac Sim for the virtual development
of Obelix—a highly dynamic indoor/outdoor
Autonomous Mobile Robot, or AMR.
After importing over 5400 parts from CAD and
rigging with Omniverse PhysX,
the virtual robot moves just as deftly
in simulation as it does in the real world.
This not only accelerates virtual development
but also enables scaling to larger scenarios.
Next, Festo, well known for industrial automation,
uses Isaac Sim to develop intelligent skills
for collaborative robots, or cobots, requiring
acute awareness of their environment, human partners and tasks.
Festo uses Cortex, an Isaac Sim tool that
dramatically simplifies programming cobot skills.
For perception, AI models used in this task were trained using
only synthetic data generated by Isaac Replicator.
Finally, there is ANYmal, a robot dog developed by a leading
robotics research group from ETH Zurich and Swiss-Mile.
Using end-to-end GPU accelerated Reinforcement
Learning, ANYmal, whose feet were replaced with wheels,
learned to 'walk' over urban terrain within minutes
rather than weeks using NVIDIA's Isaac Gym training tool.
The locomotion policy was verified in Isaac Sim
and deployed on a real ANYmal.
This is a compelling demonstration of simulator
training for real-world-deployment.
From training perception and policies to hardware-in-loop,
Isaac Sim is the tool to build AI-based robots
that are born in simulation to work and play in the real-world.
So we talked about the first two pillars of the NVIDIA robotics platform.
Now let’s switch gears and talk about building real-world physical robots and deploying them.
NVIDIA Jetson has become the de facto AI platform for edge and robotics applications.
Jetson has over 1M developers. And over 6000 companies are using Jetson for production.
With over 150 partners, ranging from system builders to application software companies, the breadth of edge AI and
robotics products being deployed continues to grow.
Sharing the same architecture as NVIDIA data center platforms allows us to run the latest and greatest AI cloud-native
software on physical robots. We call this JetPack SDK.
DeepStream SDK is being downloaded more than 10K times every month and accelerates vision AI applications.
Riva, our conversational AI SDK, is now available on Jetson.
We’ve been working with Open Robotics to accelerate ROS on GPUs. We call it Isaac ROS.
In addition, we have several robotics algorithms available, we call them Isaac GEMS.
There has never been a better time to build robots with all of the NVIDIA Isaac software tools now available.
And we continue to add and improve the software stack.
NVIDIA Orin has set a new bar for edge AI as evidenced by the recent MLPERF competition—
 up to 5X measured performance over previous generation, Xavier, 
while maintaining 100% software and form-factor compatibility.
Powered by the Ampere Tensor core GPU and twelve ARM A78 CPUs, 
it delivers up to 275 Tera operations per second. 
Basically a server in the palm of your hand.
The Jetson AGX Orin developer kit is available now at distributors worldwide.
Production modules starting at $399 will be available starting in July.
The Orin NX module is just 70mm x 45mm size for the full computer: 
CPU, GPU, networking, memory, and power management.
Production systems from partners with Jetson Orin are available now.
Many partners are announcing their products this week at Computex. 
More than 10 partners are in Taiwan.
These systems come in various form factors tailored towards specific industries and use cases.
Systems come as fanless or with a fan, varying degrees of IO, ruggedized or commercial, and other such parameters.
Autonomous mobile robots are one of the fastest growing segments of robotics due to the growth of e-commerce, 
supply chain challenges, and shortage of labor.
In addition to warehouses, AMRs are being deployed in hospitals, retail stores, factories, campuses, and airports.
The technology challenge is that AMRs operate in highly unstructured environments, even though they move slowly.
To accelerate the development of AMRs, we created Nova Orin.
Nova Orin is a reference design for state-of-the art compute and sensors for AMRs.
It consists of two Jetson AGX Orin and supports multiple sensors 
such as two stereo cameras, four wide-angle cameras, two 2D lidars,
one 3D lidar, and up to eight ultrasonic sensors.
The reference architecture will be available later this year.
The software stack on Nova Orin will consist of the navigation stack 
along with additional NVIDIA software application frameworks, 
such as DeepMap, CuOpt, and Metropolis.
DeepMap provides an accelerated framework for 3D map creation, deployment, and dynamic updates of the deployment space.
CuOpt provides accurate, dynamic route planning, and scale-out capabilities to hundreds if not thousands of AMRs in a
single large warehouse or factory.
Metropolis brings outside-in perception and situational awareness 
while the AMR itself can only do inside-out perception.
And finally, NVIDIA Fleet Command provides a secure fleet management capability.
Today we talked about the NVIDIA robotics platform.
The four pillars: AI training, simulation, building physical robots, and deploying robots.
This is possible because we leverage NVIDIA investments in AI, high performance computing, and graphics from the bottom
to top of the stack, starting with hardware systems and system software, to AI and Omniverse platforms, and finally to
domain-specific application frameworks.
This is the industry's most comprehensive end-to-end robotics platform and we continue to invest in it.
Happy roboting!
And one more thing.
DRIVE Hyperion is the computing and sensing architecture of our self-driving car.
It is central to our entire AV platform.
It consists of sensors, networks, chauffeur AV computers, a concierge Al computer, a mission recorder, and safety and
cybersecurity systems.
Hyperion is designed for a full self-driving car with a 360-degree camera, radar, lidar, and ultrasonic sensor suite.
Importantly, it's open for the entire industry, enabling them to build all types of vehicles. 
This is a key reason why Hyperion is being adopted all over the world.
Hyperion version 8 will ship in all new Mercedes-Benz vehicles starting in 2024,
 followed by Jaguar and Land Rover cars and SUVs in 2025.
This platform evolves and has a great future roadmap.
At our most recent GTC we announced Hyperion 9 for cars shipping starting in 2026. 
Hyperion 9 will have 14 cameras, 9 radars, 3 lidars, and 20 ultrasonic sensors.
It will process twice the amount of sensor data compared to Hyperion 8, further enhancing safety and extending the
operating domains of full self-driving.
Today we are excited to announce Foxconn, Quanta Computer and Desay as our newest DRIVE Hyperion supplier partners to
help OEMs scale this breakthrough technology into production.
Now, let’s talk about where it all began, NVIDIA Gaming. Here’s Jeff Fisher.
Hi everyone.
This is our 3rd virtual Computex, and like you, I am eager to get back in person to see all our great friends and
partners.
Taiwan is the birthplace of the PC ecosystem and the spirit of Computex is to celebrate the incredible journey that
built this $500B industry.
NVIDIA's journey started here as well, and together with our amazing partners, we are delighting and empowering hundreds
of millions of gamers and creators.
Over the past 20 years, we launched multiple generations of gaming GPU architectures, each pushing the industry forward.
Our latest, NVIDIA RTX, introduced real-time ray tracing and AI, once again reinventing graphics.
We turned MaxQ into a game-changing approach to laptop system design.
Beefy transportable desktops have transformed into thin, portable powerhouses.
And we looked beyond the PC into gaming monitors, showing the world buttery smooth gaming with NVIDIA G-SYNC 
and setting the standard for image quality.
And let’s consider the market we have built together.
PC Game Hardware is projected to be a $67B market this year 
and to grow double digits over each of the next several years.
100M new PC gamers were added to our ranks in just the past two years.
80M creators and broadcasters are fueling an economy of $100B.
920M people will watch game live streaming this year, 
up 1.5X in the last 3 years, and over a half a billion people will watch esports.
To our partners, thank you again for the commitment and passion to deliver amazing products
 that inspire gamers and creators, year after year.
NVIDIA remains dedicated to building the ultimate platform for gamers and creators.
At its heart is GeForce RTX, powered by our Ampere architecture, 
with 28 billion transistors, 40 Shader TF, 78 RT TF,
and 320 Tensor TFs,
it is the world's fastest GPU.
Ampere features 2nd gen RT cores for real-time ray traced cinematic graphics, 
and 3rd generation tensor cores, 
which power NVIDIA DLSS, our groundbreaking AI rendering technology.
For competitive gamers, we invented NVIDIA Reflex, providing the lowest latency and best responsiveness.
For game live streamers, the RTX platform includes NVIDIA's advanced video encoder, 
engineered to deliver the highest quality video stream alongside maximum game performance.
RTX is also your AI-powered home studio. 
Our Broadcast app leverages Ampere's dedicated Tensor Cores to turn any room into a live streaming station.
For digital artists, we built NVIDIA Studio, 
an RTX-powered platform that includes dozens of SDKs and accelerates the
top creative apps and tools, including NVIDIA Omniverse.
Finally, the RTX platform is constantly optimized and improved with Game Ready and Studio Drivers.
Our customers depend on GeForce to just work across every system configuration and thousands of games and apps, 
out of the box, every time.
Our drivers are an invisible force that makes our platform like none other.
We are so proud of this effort, I invited Thiru to tell you more.
At NVIDIA, we want gamers and creators to get the best gaming and app experience on day zero. 
We work closely with developers to ensure our drivers deliver the best possible performance 
and the reliability you count on to game more and create faster.
For many years, we were working very closely with the NVIDIA team. 
It's like a safety net of constant cooperation on
making sure that the drivers are supporting the latest update in optimization on both ends, 
and that secures the gameplay experience for all the players.
 If you really want to see the game at its best, the way we actually imagine,
the way we designed it, you need to get the Game Ready Drivers.
Our engine is so purpose built and finely tuned to give you the best experience playing Doom Eternal, 
and the driver team at NVIDIA are almost like part of our team so we can achieve the vision that we have.
NVIDIA Studio Drivers provide our artists, creators, and 3D developers the best performance and reliability. 
As I always say, it's very hard math to get these things to work, and the companies are working together to ensure that we're doing
the math, so our creators don't have to.
Our mission with drivers is to be invisible, so gamers can just play and creators can just create—faster than ever.
RTX momentum continues to build.
The cinematic look of Ray Tracing and performance-boosting AI are defining the next generation of content.
Now there are over 250 RTX games & applications, doubling since last Computex.
And GeForce Gamers continue to upgrade, with over 30% now on RTX 
and logging over 1.5 billion hours of playtime with RTX ON.
Agent 47 returned to HITMAN III in this dramatic conclusion of IO Interactive's HITMAN trilogy,
which has sold over 50 million copies.
I'm happy to announce that today, the most successful game of the franchise is getting a big update
 with ray traced reflections, shadows, and NVIDIA DLSS.
Let's take a look at HITMAN III with RTX ON.
The popularity of Formula 1 racing continues to grow as does Codemasters’s F1 racing game.
I'm excited to announce that the next season, F1 22, will launch July 1st with RTX ON.
Gamers will feel even more in the drivers seat with the cinematics of ray tracing and the performance of DLSS.
HITMAN III and F1 22 add to the RTX momentum and join a number of new games that will be turning RTX ON.
Now let's talk about NVIDIA Reflex.
Immersive gameplay requires low latency.
 It connects your mind to the game.
 It’s also critical for competitive gaming.
It seems obvious that low system latency helps all gamers, not just the pros. 
But by how much?
To measure this, we recently conducted the System Latency Challenge. 
The largest study of its type.
Partnering with Kovaaks, the popular aim trainer, we collected data from 20 thousand gamers. 
Measuring aim accuracy
across a range of system latencies. 
From 25ms to 85ms.
What we found was interesting.
High skilled gamers, or the top 25%, hit 2X the number of targets at low latency.
And the least skilled gamers, or the bottom 25%, increased their shot accuracy by 2.5x.
Here is Tion to tell you more about Reflex.
Game and performance isn't just about FPS, it also involves latency. 
We created NVIDIA Reflex to reduce system latency.
By integrating Reflex directly into games, we cut latency in half. 
Over 35 games have adopted Reflex, including eight out of the top 10 shooters. 
Now, over 20 million gamers play with Reflex on each month.
With NVIDIA Reflex, the game and the graphics driver coordinate to dynamically reduce system latency.
 Gamers should definitely turn on Reflex. 
It's one of the first settings I turn on when I'm playing a game that supports it. 
And in Valorant, we turn it on by default on hardware that supports it.
When you think about it, targeting 60 frames per second, every frame is only 16.6 milliseconds long. 
On PC, competitive players are trying to get higher and higher frame rates, 
100 frames a second, 200 frames a second.
 When you get to those numbers, frames happen in just a handful of milliseconds.
 So being able to reduce system latency by a few milliseconds,
that's a really big impact for players. 
NVIDIA Reflex was really easy for us to integrate into Fortnite. 
And in fact, we've made it a plugin in Unreal Engine 
so that all Unreal Engine developers can easily enable it in their games as well.
The Reflex ecosystem continues to grow.
In addition to games, it is featured in 22 monitors and 45 mice.
This will soon include Icarus, a gritty PvE survival game 
where you explore a savage alien wilderness in the aftermath
of terraforming gone wrong.
Icarus, which already features DLSS, adds Reflex next month, 
so every gamer can better survive this hostile environment.
Today, we are introducing the newest member of the Reflex family: 
the ASUS ROG Swift 500Hz Gaming Monitor.
The lowest latency, highest refresh rate G-SYNC Esports display ever created. 
It has been designed from the ground up for competitive gaming 
featuring a brand new ETN panel for maximum motion clarity, 
G-SYNC Esports mode with adjustable vibrance,
and of course the NVIDIA Reflex Analyzer.
Gamers, creators, and students have made GeForce high performance laptops the fastest growing PC category. 
Over 60 million people are gaming and creating on them. 
And this past year, over 35% more RTX laptops were sold, with 3X growth in Studio laptops.
Today there are over 180 models for gamers and creators that feature our RTX 30-Series GPUs and Max-Q technologies.
Since we introduced Max-Q technologies at Computex 5 years ago, 
it has transformed GeForce laptops into thin and high performance machines.
This year, we announced 4th gen Max-Q, bringing new innovations and even more power efficiency.
CPU Optimizer allows the GPU to balance system performance and power. 
It utilizes a new low-level framework we developed with CPU vendors 
to improve power efficiency and boost frame rates.
Rapid Core Scaling delivers more performance for creators on the go. 
The GPU can sense the real-time demands of the application
and scale to the optimal number of cores, for a performance boost of up to 3X.
Battery Boost 2.0 enables gamers to play for longer while unplugged by finding the optimal balance of GPU and CPU power
usage, battery discharge, and image quality.
And Advanced Optimus delivers the smooth, stutter-free gameplay of G-SYNC while offering better performance, latency,
and longer battery life.
At Computex, we are showcasing new gaming laptops from MSI, ASUS, Gigabyte, and others. 
These gaming laptops feature cutting-edge designs and the incredible performance of the RTX 3080Ti and 3070Ti.
There are also exciting new NVIDIA Studio laptops for content creators, 
like the Asus Zenbook Pro 16X, the Acer ConceptD5, and the Lenovo Yoga Slim 7i Pro X.
NVIDIA Studio is our platform designed to enhance and accelerate digital artist workflows.
It's an end-to-end approach, 
starting from our GPUs that include dedicated hardware to accelerate ray tracing for gamers and creators,
AI features to simplify content creation,
high performance encoders and decoders to accelerate video editing, 
and CUDA for compute-intensive tasks like image processing and simulation.
There are now over 200 NVIDIA Studio-accelerated apps.
We have also developed our own applications for NVIDIA Studio, 
including Broadcast for live streaming, 
Canvas for painting landscapes with AI, 
and Omniverse for advanced 3D design and collaboration.
NVIDIA Studio and RTX laptops deliver a significant performance advantage in Ray Tracing and AI acceleration.
These laptops are up to 6X faster in 3D rendering than the fastest MacBook Pros with M1 Max processors,
and up to 3 times faster in AI processing, unlocking new workflows for artists.
3D Designers and artists are the builders of the next digital frontier.
Vast virtual worlds filled with factories, homes, shops, museums, robots are now being built.
Today, a 3D artist typically works sequentially across multiple applications. 
Exporting and importing large files many times along the way.
Omniverse was designed to unlock the potential of 3D design 
and allow creators to collaborate on large interconnected spaces.
Omniverse is an open platform, connecting the industry's leading 3D tools including Adobe, Autodesk, and Epic's Unreal Engine
into a shared, single environment.
And it’s fully accelerated by RTX Ray Tracing, AI, and compute.
Over the past year, we have seen a 10X increase in Omniverse downloads, with 120K unique users creating or developing on
the platform.
Omniverse is the future of 3D content creation and how virtual worlds will be built.
And we continue to update Omniverse with new capabilities.
Omniverse Cloud has added Simple Share. 
With one click, users can send an Omniverse scene for others to view.
We’ve added Audio2Emotion, an AI-powered animation feature that generates realistic facial expressions 
based on just an audio file.
Omniverse XR is now available in beta. 
You can open your photorealistic Omniverse scene 
and experience it, fully immersive, in Virtual Reality.
And Omniverse Machinima has been updated to make it easier than ever for 3D artists to create animated shorts.
I'm in the Omniverse.
And check out our Made in Machinima contest. 
It’s in full swing. 
Create an easy animated short with Omniverse materials,
physics, and game assets for a chance to win top of the line RTX Studio laptops.
Over the past 20 years, NVIDIA and our partners have dedicated ourselves to building the best platform 
for gaming and creating.
Hundreds of millions now count on it to play, work, and learn.
RTX has reinvented graphics and the momentum continues to grow. 
There are now over 250 games and applications.
Gaming laptops are the fastest growing PC category and MaxQ 4.0 is delivering a new level of power efficiency. 
These are our most portable, highest performance laptops ever.
Massive, interconnected 3D destinations are being built today. 
NVIDIA Studio and Omniverse are designed to enable
collaboration and construction of these virtual worlds.
Finally, I want to thank our partners for working so hard alongside us to bring new innovations to the market and
helping build this amazing PC ecosystem.
Thank you for watching. 
We wish you—and everyone around the world—safety, peace, prosperity, and good health.
Title: NVIDIA Solutions Bring New Headquarters Building to Life
Publish_date: 2017-10-26
Length: 201
Views: 46687
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/iCKx8LidOdM/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: iCKx8LidOdM

--- Transcript ---

we were competing with that I believe
three other architects and we had come
and presented Nvidia and Jensen many
times we were getting ready for this
penultimate presentation and then on a
note from Nvidia saying Jensen just
wants to meet with how alone it turned
out to be a three-hour conversation
one-on-one and that conversation started
off with the question from him was are
you ready to find my soul I've never had
a client asked me that question before
from that moment I knew this project was
going to be special endeavor was a great
project to work on we were involved in
terms about working on the ray tracing
technology physically-based running for
architects is important because it
enables them to more rapidly show their
customers what something's really going
to look like after it's built instead of
operating with 2d drawings or 3d models
we can accurately model how the interior
space and the exterior space is gonna
look like at any time of day the finish
of a certain material what a color of a
surface really does impact what a day
lighting will do to a space or what a
space will feel like
so having that degree of accuracy with a
ray-tracing tool really allows us to be
able to be confident about the choices
that we make during the early design
phases ray-tracing and general scales
quite nicely across multiple nvidia gpus
even a workstation could produce a new
rendering within a few minutes maybe a
full building within a few hours using a
VCA we could take that two day
turnaround and literally turn it into
something that was done in less than an
hour why that's important to us as
designers is that that allows us to
iterate where we're not just creating a
final product but it's something that we
could feel like we could make changes
based off of what we saw at the moment
VR was very much integrated into the
design approvals process here any new
design ideas would not be approved
without having seen it through VR
there's a difference of looking at
something on screen and looking at it as
you're surrounded by it and if you're
accurately modeling the light and the
materials you really feel like you're
sitting in endeavour when you're really
just starting to be our environment it's
really revolutionized the way that
architects deal with their clients
it's a first chance where you actually
get to not only understand what the
building will look like but you start to
really understand what a space watching
feel like when you're actually in it
it still gives me goosebumps
I'm very excited to see people working
in it playing in it that's why I'm an
architect
I'm really hoping to build places that
transform people's lives
Title: Introducing NGN - the NVIDIA Gaming Network!
Publish_date: 2016-06-03
Length: 73
Views: 38095
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/IFf9w7m_4NQ/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: IFf9w7m_4NQ

--- Transcript ---

ngn is storytelling engine is PC GeForce
interviews with developers I think all
of that kind of adds up to make a really
cool and unique experience lights ng n
is n GN is the latest tech VR GeForce
garage pull in on this and Jim is shield
MGM is Android gaming indie game
ng n is eSports we're cloud 9 here's the
storm thank you for joining us behind
the scenes at our GeForce bootcamp and
GN is FPS MOBA live streaming it's a
lifestyle it's life itself
I'm really really really excited for
this it's an awesome destination to
learn more about and get more immersed
in the world of gaming ng n it it's you
me it's community and Geon is the Nvidia
gaming Network engine is tight
Title: GeForce @ E3 2019 - $50K in Prizes! GPUs, Laptops, a BFGD, and More!
Publish_date: 2019-06-10
Length: 45
Views: 14770
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/IGheE_xY_iQ/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: IGheE_xY_iQ

--- Transcript ---

what's up pc gamers unbranded from
nvidia geforce and you're probably
wondering why I'm surrounded by some
pretty fantastic gaming hardware its e3
season which can only mean one thing
it's time for our e3 2019 RTX on
sweepstakes whether you follow Jeep or
some social media or YouTube we've got
updates on the latest PC game analysis
from the show floor and we're giving you
the chance to win from our massive
gaming prize pool we've over $50,000
with the stuff to win including a $5,000
open BFG d this main gear vibe stage for
PC built for 4k gaming and 20 GeForce
r-tx GPUs if you want some of this swag
get the details on how to enter over on
our official GeForce youtube channel and
don't forget to hit that subscribe
button we've got our world of III
coverage including exclusive gameplay
videos interviews and trailers you won't
find anywhere else I'll see you there
Title: NVIDIA Turing Provides OTOY OctaneRender with Unprecedented Speed
Publish_date: 2018-08-17
Length: 62
Views: 20632
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/IJ77a0erU4w/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLC0epUcWpEXgfhtdZhxIfkSqT_CHw
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: IJ77a0erU4w

--- Transcript ---

[Music]
so octane is the world's first and
fastest unbiased spectral render on the
GPU it's been used in film and
television productions in the opening of
West world we are also integrated in
unity and we just yesterday announced
that we're going to be shipping octane
for unreal next year so when I first was
made aware of the ray-tracing hardware
acceleration in Turing I lost my mind
because I knew that rate racing partner
was the future so on Pascal octane bench
was getting about 400 million raised a
second and on touring it's about 3.2
billion so that's an 8x improvement in
speed and that's gonna be huge for
visualization product design and many
other things that are now possible in
real time rasterization has basically
controlled the way games are created and
programmed and with ray tracing on RT X
you're gonna see games that look real
it's going to be easier to make them and
they'll look absolutely beautiful this
changes everything
[Music]
Title: NVIDIA ISC21 Special Address
Publish_date: 2021-06-28
Length: 3854
Views: 131338
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/iJavSDp7qCo/hq720.jpg?v=60da0aa2
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: iJavSDp7qCo

--- Transcript ---

hi
my name is john joseph he's vice
president of hpc sales at nvidia
welcome to isc 21 and the nvidia special
address
it's been an exciting year for us with
many announcements
from the a100 gpu to the mdr networking
technology
all the way to our first arm grade cpu
but that's not it we also announced a
lot of
software frameworks which enable
scientists to use our hardware platforms
to accomplish their goals
we had a number of wins on the euro hpc
program
and we also announced our cscs
arm-powered grade cpu system
in switzerland with cscs data sciences
in nai have had a big impact to advance
modeling and simulation
accelerated computing is driving
exponential growth in mary areas
like manufacturing and drug discovery
the need to solve large
problems has driven people to larger
systems using
ai and accelerated computing
but for more on that i'll have over i'll
hand it over to
mark hamilton our vp of solution
architectures and engineering mark
take it from here
thanks john and welcome again everyone
to our isc
21 special address
isc has always been a time for the hpc
community
to gather and celebrate our successes
from the past year
in what a year it's been when we went
into isc
20 a year ago the first digital isc
i'm not sure anyone thought that isc 21
would still be digital only yet here we
are
i have to reminisce a little bit and
think back
to two years ago in many other years in
frankfurt
anyone who's ever been to an isc in
person in frankfurt
and gone to any of the vendor events at
the marriott hotel
will certainly remember that walk
out the front doors of the convention
center
past the lunchroom across what seemed
like a vast
and endless parking lot through the back
gates
out to the street corner wait for the
street light
cross the street cross another street
the tram lines across hamburger alley
make sure you're not going to get hit by
a train into a vendor session
maybe the nvidia vendor session at the
marriott hotel
and if you work for nvidia you probably
did that walk five or ten times a day
ten minutes each way so being digital
does have its advantages so let's go
ahead
and get started with the show before
that a quick reminder
this is live and for the last 20 minutes
we'll be having a live q a
so if you think of any questions as i'm
presenting
go ahead and put them in the q a window
we have a distinguished panel
of ai and hpc experts from nvidia
that will help me address your questions
at the end
for years we've seen industries
transformed
by each pc including nvidia
for 20 years nvidia's worked with
companies like industrial
and magic using hpc
to create amazing scenes of special
effects
in movies i know many of you have
probably watched
loriza skywalker epic
lightsaber battle on the dying death
star
as waves from the ocean rolled over
the actors over the boat
ilm simulated 135 million
gallons of water in filming this scene
which would of course have been
impossible to do
without nvidia gpus and today
and since that film i'm sure many
countless
young kids have replayed that out on a
summer lawn
or in or in their bedroom dueling away
with lightsabers
probably a few you adults at the show
too
our own products nvidia isn't just about
what we build nvidia is about how we
build it
in simulation has become a culture at
nvidia
our processors are fully simulated
before we ever send the designs off to
the fab to be built
we don't press the go button until we're
really sure
that that processor is going to come out
error free on day one
that leads us to quicker time to market
not having to repeat build cycles in the
fab
in higher quality products
and of course especially over the last
year we've all used the internet more
to buy things to work from home
maybe even to shop and play a little bit
in internet services are today powered
by super computing
recommender systems are at the heart of
every internet
service and of course natural language
programming
nlp has become the poster trial
for ai and high performance computing
so that's our culture at nvidia and let
me share with you a few of the things
we're going to talk about
in the special address first
digital biology covid19
is thrust digital biology into a new
light
in this year has been the dl moment for
digital biology
ai flops hpl
in the top 500 list continues but
there's a new list
hplai and in fact
look at every new system that's been
announced or has gone operational in the
last year
ai flux is the new standard by which
supercomputers are measured
and in part as we said because of
natural language programming
processing nlp impossible to do without
a supercomputer
another new technology that isn't brand
new but you've heard talked a lot
a lot more in the last year is quantum
we'll talk about invidious contributions
to quantum
and finally arm arm's been in the news a
lot
but it's arms partners that have made so
much
progress for the hpc community so we'll
talk about
what some of the armed partners have
done in the hpc space
industrial hpc is a tipping point
back in 2006 when we launched cuda
molecular dynamics was one of the first
applications
that was gpu accelerated the national
supercomputing center
in the us at the university of illinois
was one of the first to run nandi
on gpus and 15 years ago
this large state-of-the-art simulation
on namdi
was about a million atoms being
simulated
for maybe 20 nanoseconds
fast forward to today in the summit
supercomputer
and we see models 1 billion atoms
being simulated for 500 nanoseconds or
more
even so that isn't enough
to make real breakthroughs design new
design new vaccines design new drugs
and we see people looking to have
fourfold
increase in what they can do in digital
biology
at the same time we've seen an
exponential growth
in ai models natural language processing
a poster child for ai and hpc
started out in 2017 with models like the
transformer
65 million parameters at the time seemed
huge
but look at today's state-of-the-art
models being
open gpt-3 from open ai
175 billion parameters
nvidia's megatron and advances such as
universities of florida gatortron based
on the megatron model
are making and in fact in the last few
weeks
we've seen the first nlp models
published
with over 1 trillion parameters
and this isn't stopping we expect by
2023
researchers will need to model
models with over 100 trillion parameters
while this may seem far-fetched think of
the human brain the human brain has
roughly 140
trillion synapses while a synapse
doesn't equate
exactly with a parameter in an ai model
it's a good approximation
finally back to molecular dynamics
look at what's being done today by
combining gpus
plus ai
usdoe researchers using the summit
supercomputer
modeled covet 19 with deep drive nd
they were able to run a 305
million atom parameter for a number of
milliseconds
this represents a 10 million fold
increase in processing power from that
2006 era
the md model 10 million times
the processing growth of atoms per
second
being simulated with a combination of
gpu
plus ai and that's just the start we see
industrial hpc
delivering transformational
breakthroughs across industries
an example after example ge aerospace
is worked with oak ridge national labs
to do large eddy simulations
simulating jet engine operations
leading to more more efficient jet
engines
in drug discovery the us is of course
not alone
we've seen the entire world double down
on on digital biology in covet 19
simulations
in france jensey developed a code
called tinker hp and tinker hp
was ported to gpus
for for increased increases in
performance
today tinker hp is available on the
nvidia
ngc software hub we're helping
democratize
high performance computing in ai and
taking work
that gen c does and making it available
to the world
and of course francis observed this as
well
cubit a french pharmaceutical company is
taking tinker
md is working with six different
pharmaceutical companies to advance drug
discovery
using the tinker nd code
this continues in industry after
industry
for instance automotive
we know that automotive robots are
simulated
often before the software is put out
into the factory
but bmw's gone far beyond that with
omniverse
bmw is working to simulate the entire
factory
not only the robots how cars move
through the factory
and how the the humans operate inside
the factory
leading them to design more efficient
cars in the factory of the future
in weather simulation the weather
company is now using ai
to accelerate weather for high
resolution weather forecasts
to bring more accurate weather you can
now tell not just is it going to rain
in your country or your part of the
world is it going to rain in your city
what is the temperature going to be more
accurately than ever
and finally tech manufacturing another
area
where we see ai and hpc coming together
there's a new class of algorithms
physics-inspired neural networks
that are addressing adding ai to the
front end of cfd simulations
to do things that simply weren't
possible before
ai with brute force computing
let's go ahead and take a look
on what's possible this is a demo of
nvidia's simnet
let's go ahead and roll that tape
what we're showing here is an
application of simnet which is a
framework for running physics and form
neural networks on gpus
as an example let's take a look at how
you can use simnet to optimize the
design of a heatsink
this involves solving a coupled fluid
flow and heat transport problem
we've got a heat source in this chip on
the board here and a heat sink design on
top of it that we want to evaluate over
a number of different conditions in the
chip
what we're going to do is search over a
variety of different configurations of
this heatsink
in this case we've taken two of the
parameters at the fin height
the center and outer fin heights are
varied over a number of different
conditions
we've also taken the volume of fluid in
the metal along with all of the
materials that are in the heatsink and
the surrounding air
and modeled them as a point cloud of
about 3.3 million points
we then train a neural network on all
those points using the various physical
parameters
and map those onto a loss function
governed by the partial differential
equations of never stokes fluid flow and
heat transport
the training takes about four days on a
single dgx1
once the training is complete we can
evaluate and explore
25 000 different various configurations
in a matter of seconds
here we are visualizing a subset of
those you can see the heights of the
center versus the edge fins are changing
as i move these sliders with the
temperature map and flow
instantly reflecting those changes what
we're looking for in this optimization
problem is a condition where we've got
minimum temperature for tmax
while still remaining underneath this
gray plane here
which is the maximum pressure drop that
our cooling system can provide
what we ended up finding was that the
optimal configuration was not a
completely rectangular block
but rather a roof structure this makes
sense because the heat is actually
generated here in the middle
at the top of the chip if you look at
any other configuration either the
temperature or the pressure drops will
increase
and finally we are showing a simulation
of transient flow
with colored smoke vectors the green
vectors are showing flow that is slow
and the blue and purple vectors indicate
faster flow you can see here that there
is slow flow through the core of the
heat sink
but then there is very fast flow that
comes over the top
which helps pull the heat from the
center as it rejoins the streamlines on
the other side
we can also turn off and on different
layers of the simulation
to literally see what happens when we
change parameters like the height of the
fins
or the pressure drop threshold note here
that some combinations parameters show
what seems to be cool temperatures
internally
but result in large drop in pressure on
the back side of the heat sink
using simnet we can see the effects of
each change in real time
allowing us to quickly find a design
that produces optimal cooling with
acceptable pressure drops
then we can quickly get that design into
manufacturing confident that it will
perform as expected
the first time these tools are something
that we're using internally at nvidia to
help model our heatsinks and do design
space optimization
we expect to have a paper describing the
core of the physics and form neural
network technology out soon
well i'm sure after that simnet video
everyone's going to want to run out and
open up
their dgx a100 take a closer look
at their heatsink please don't do so
while the system's plugged in and
operational
work with your sysadmin on that anyhow
let's go on and talk about omniverse
omniverse
is nvidia's platform for connecting 3d
worlds
it can accelerate scientific computing
discoveries
here's a little bit about how omniverse
works omniverse has connectors
to dozens of different 3d data sources
all of the typical 3d design tools that
you might work
that you might use in design and
manufacturing
in others as well as
as well as is as well as hpc design
sources
with our paraview omniverse connector
you can connect directly in
to in-situ data from hpc simulations
because everything is physically modeled
it gives you a life
like realistic view which is of course
displayed beautifully within video
graphics and nvidia gpus
but furthermore because it's physically
based omniverse can be connected into
simulators it can be used not only to
it can be used to build a digital twin
whether that digital twin is of a
manufacturing plant
or of the entire planet it's physically
accurate
and can be used in collaboration
let's take a look now at what a digital
twin of the earth
might look like
high resolution weather simulations as
provided by dkrz's icon code
and vast amounts of observational data
allows us to determine the state of the
earth with unprecedented fidelity
but exploring and working with this data
can be a challenge
technologies like the nvidia omniverse
paraview connector
allows scientists to interactively
analyze weather and climate data
in 3d to combine information from
multiple sources
and to explore what-if scenarios this
takes us
ever closer to a realistic interactive
digital twin
of the earth let's look at two examples
in august 2019 hurricane dorian
developed into a massive category 5
hurricane
causing over 5 billion in damage in the
bahamas and the southeastern u.s
with sustained winds of 185 miles per
hour
viewing high resolution simulations like
these integrated forecasting systems
simulations
in omniverse allow scientists to analyze
the behavior of storms such as this
in detail interactively and in full 3d
this not only aids in understanding the
physics of weather systems
but also allows inaccuracies in models
to be identified
and refined with the omniverse
paraview connector visualization
modalities of paraview like streamlines
become available within omniverse and
can leverage the rtx renderer
or be fused with data from other sources
these sources can be static
or computed on demand such as mapping
service surrogate model
or an ai-informed physical model
here we show ai-based image segmentation
of satellite data
which was used to detect flooded areas
and to classify land use before and
after dorian hit
the network performing the segmentation
can run as a service and be triggered on
demand
once we have interactive access to all
this earth science data
a next step is to play through different
scenarios on-demand evaluation of
surrogate models
or access to real-time physics engines
can help exploring the impact of our
actions in this virtual world
with access to real-time physics models
as enabled by omniverse
we can not only explore pre-computed
data but also simulate on the fly
here we use real-time fluid dynamics
simulation to show the vapor plume
emerging from a cooling tower
a user can quickly explore the impact of
the plume depending on the wind
conditions or the steam output
similar models can be used to
interactively explore the dispersion of
aerosols or contaminants
nvidia omniverse is a powerful platform
for massive scale digital twin
simulation
we're excited to see where it takes us
next
i know many of our super computer center
customers have
downloaded and are working with early
access version
of omniverse today again i'm excited too
as we start to be able to travel again
and i visit some of you in person to see
some of your results
but as we all worked from home over the
last year
it's been amazing to see the next wave
of supercomputers
going into production and being
announced
nvidia's newest supercomputer cambridge
one
we built our first cloud native
supercomputer
in the uk thousands of miles away from
our headquarters
as we were locked down during the
pandemic
cambridge one is now live and we're
working with our partners
in the healthcare space to do some
amazing work
in the uk you'll see cambridge one on
the top 500 list
and more to come on that system
last week tesla automotive announced
their new industrial ai supercomputer
about 6 000 a 100
gpus in nvidia infiniband networking
tesla is of course well known for their
self-driving car software
and even with the largest fleet of
self-driving cars
in the world tesla still needs to
simulate
to run massive ai simulations in order
to continue to improve their software
and reach their goal of full
self-driving
they've put a stake in the ground with
this system and i think this is going to
be
a bar than any other automotive company
that wants to build a full self-driving
system will need to compete against
a few miles away also in california
nurse went and installed the perlmutter
system
ai for scientific discovery
perlmutter is now the fastest ai
supercomputer in the world
over 6 000 nvidia a100
gpus it's a cray hpe system
we're really excited as it goes into
production about the work they'll be
done
we've also seen a new class of
supercomputers
based on our dgx superpod architecture
recursion pharmaceutical a big pharma
company
that was just a startup a year ago many
of you may not
have heard of and they wanted to get a
supercomputer
up and running as quickly as possible if
they're
at their headquarters in salt lake city
they spent just took took just a few
weeks
to install the djx super pod there a
digital twin
of other djx super pods around the world
we now have 13 dgx super pods
on the top 500 list
but of course supercomputing isn't just
for on-prem anymore
we've seen a number of the major csps
cloud service providers launch
supercomputers microsoft
azure alone has not one but four
new supercomputers on the top 500 list
in fact in the top 30 list number 26
through 30
located around the world you can now
rent a top 30 supercomputer
in the cloud directly from azure once
again
this system is based on the dgx superpod
reference architecture
uses nvidia's a100 gpu
and nvidia infiniband networking
two systems in europe that have been
announced in the last year
that we're extremely proud about first
is the leonardo super computer etchanica
in 10 ai exoflops
it'll be an amazing system for ai and
hpc
giant scale problems and the world moves
ahead not long after leonardo was
announced
cscs in switzerland announced albs
alps will double leonardo's performance
using our next generation
grace armbase cpu
and it will deliver 20 extra flops of ai
performance
it'll be one of the most capable hpc
systems equally capable in ai
the top 500 list was released just a few
hours ago
in fact just an hour ago i'm sure many
of you have already studied that list
and great to see all of the nvidia
systems on both
the top 500 and the green 500
delivering energy efficiency to the
world's fastest super computers
super clouds cloud native super
computers
these aren't just the systems in azure
or in aws but on-prem systems
every day need to have the same
attributes
that you expect from the cloud a
supercomputer today
can't just be locked up in the back room
and made available on a private network
for 10 or 15 researchers the investment
required for state-of-the-art ai
supercomputers
means they need to be shared
and they often need to be shared across
organizations
the super cloud can be made
secure with nvidia's bluefield dpu
our dpu provides multi-tenant bare metal
performance
while still providing the security you
expect of a cloud-native system
only a few miles from our own cambridge
one supercomputer
we saw the university of cambridge
deploy a system
on the top 500 list using nvidia's
bluefield 2
dpu the cambridge system
was built by dell using our own hgx
4 gpu platform i'm very excited to see
that system
uh with its very high ranking on the
green 500 list
so congratulations to dell into
university of cambridge
for that system
of course dgx a100 is a great platform
but we don't keep it to ourselves it's a
reference architecture
that as we said is shared with all of
our oems
and all of our cloud service providers
you know at
isc we always like to make a few new
announcements
in this year we're expanding hgx beyond
the traditional hgx8
in four-way nv-link based platforms
and introducing hgx with our pcie-based
gpus it's a perfect platform
for building a system with your oem for
using a system
that your cloud service provider is off
has offered
when you use the hgx platform we're
guaranteed that
all 2000 of the nvidia gpu accelerated
applications
those that are on our ngc software hub
and many others available from leading
software providers around the world
we'll run to nvidia certified standards
of course it's going to include our
nvidia gpus
includes new performance technologies in
the storage space such as magnum io
and of course to make that dpu usable
is doka doke is quickly becoming
to the dpu what cuda was to the gpu
so we're super excited about all the new
hgx
ai-based platforms that our oem partners
are announcing this week at the show
powering many of them will be this new
gpu
the nvidia a100 pcie 80 gigabyte
gpu the world's highest performing
pcie based gpu now many of you are
familiar with the a100
80 gigabyte from our earlier hgx
4 and 8 gpu platforms this is the same
a100 80 gigabyte gpu
fit in a pcie form factor for the
broadest range of servers possible and
we have
every leading oem lined up in supporting
this new pcie gpu and it has all the
same features
that the earlier a100 systems had
so i'm not going to talk about all of
those but one of those i do want to
highlight
is the multi-instance gpu or mig
meek was new in the a100 architecture
i think a lot of people aren't focused
on it yet
when you build a supercomputer you're
thinking about putting multiple gpus
together in a server
and then even connecting hundreds or
thousands of servers together
so you don't think about a fraction of a
gpu
but again many times across a general
purpose center
people run large codes but you also have
a wide variety of users on
some may just be doing development some
may use smaller codes
and of course with the adoption of ai
across the hpc landscape
as those ai enhanced hpc applications
get deployed
you have more and more of a need for
inference and these are all areas where
mig can help
mig takes the gpu and lets an a100
look to the operating system like up to
seven
gpu instances so a single pcie server
with one
a100 can appear to you
can appear to serve seven different
users each
under the impression or looking like
they have a gpu
with a fraction of the performance and
again
you don't have to decide ahead of time
do i need a small gpu
or a big gpu make can be dynamically
configured
in your system so one of the great
features
i'm really looking forward to more
adoption of the a100
in both the pcie as well as our other
hgx
form factors
now on to some of the other exciting
announcements in the networking space
we're excited at the show to be
announcing
the details on our ndr 400 gig
infiniband systems
of course 400 gig gives you twice the
bandwidth
of 200 gig hdr but it's a lot more than
that
look at these results that we've
achieved on early ndr systems
improving natural language processing
deep learning recommender systems and
computational fluid dynamics
codes that need to run on every ai
supercomputer
and of course because power is important
ndr delivers 60
lower power performance for your super
computer
that translates directly into more power
available
for to run your applications
one of the real areas where we've seen
improvement in ndr
is in ai acceleration we're moving
certain types of ai
acceleration into the network sharp
sharp accelerates ai primitives in the
network switch
in ndr switches provide 32x the sharp
acceleration
has made the current hdx or hdr based
infiniband systems so popular
and look at the put look at the
performance on a petabit per second base
1.6 petabits a new
a new petabit being the new reference by
which we need to judge switches
so we're super excited and we expect on
the top 500 list
in the fall we'll see the first ndr
deployments
like the current family of hdr in
earlier switches
ndr comes in all sorts of sizes
and shapes from from the smallest
top of rack switch with 64 ndr ports
all the way to a new record-setting 2048
ndr ports in our largest switch
now one of the hallmarks of infiniband
is not only being leading edge and
performance but being backward
compatible
we know that many of you don't have the
luxury of replacing your entire data
center
when you buy a new system you need your
new supercomputer
to be backward compatible with your old
supercomputer
so you can mix and match networking so
of course
ndr is now also supporting ndr200
so you can take that new switch and
double the number of ports
beyond that it's compatible with all the
earlier generations of infiniband as
well
so it can plug and play with earlier
existing
infiniband switches in infiniband
servers
super excited again but the advances ndr
is going to bring
across the industry and this is
something industry is learning in a
trend that started in super computing
super computer was always about the data
center you bought one super computer and
it was in your data center
but as systems become more and more
complicated data center
is becoming the new unit of compute
customers want to buy a data center at a
time
and they're going to put different types
of architectures
in that data center of course scale out
scale out is a classic hpc design
and technologies like magnum io will
accelerate the storage
for scale out and the increasing traffic
in east-west east-west traffic in your
data center
virtualization is still important for
the enterprise
and with new gpu support by all the
leading hypervisors
we see scale up computing continuing
stacking up multiple vms with vmware
and nvidia ai enterprise software
on top of nvidia certified pcie-based
servers
is the perfect platform to run small and
medium scale
ai and hpc workloads and finally
new applications are designed as a
series of microservices
those microservices may run on a
hypervisor based system
or natively in a gpu container
so nvidia is building reference
architectures for all of these
with gpus and nvidia networking
optimized
for each of the three styles of
computing
gpu direct storage
gpu direct rdma has been
known for many years in the infiniband
space and we're extending it
with gpu direct storage it's the same
idea
except applied to storage take the data
directly from storage
either on a locally attached direct
storage on storage that's attached via
the network
and create that data path flowing direct
from the pcie switch to the gpu
this of course requires collaboration
across the ecosystem
with the gpu the network the storage
provider
and the file system provider we're
working with all the leading
hpc storage companies listed here
quantum there's billions of dollars of
new funding pouring into quantum
you perhaps wonder what does nvidia have
to do with quantum computing
well first of all specialists agree
the future is not just quantum computing
but hybrid quantum classical systems
and we expect when quantum systems reach
industrial types of tipping points
they'll be used in hybrid
systems with gpu accelerated systems as
well
but meanwhile a challenge remains to
build
useful quantum computers the world needs
to do quantum simulation
and it simply takes too long the quantum
state vector simulation
can take 10 days today on a two-socket
server
with the new coup quantum sdk we
accelerate that from 10 days
to 2 hours and that's for tens of qubits
to build a meaningful system let's
experts agree
you'll need on the order of thousands of
qubits
and in fact doing a tensor state
simulation
with thousands of qubits would take
about nine years
we can't wait nine years just to do
quantum circuit simulation
if we want to see quantum adoption we've
taken that nine years
down to four days on the dgx a100
so we're working today with all of the
leading quantum labs
and ensuring they have access to coup
quantum
so we'll be hearing a lot more over
about q quantum as we evolve that
software
over the coming months now on to our arm
partners
this is an area where we've seen
tremendous growth of so many partners
over the last year nvidia is further
accelerating that with our arm hpc
developer kit
will be available uh this quarter
and coming soon featuring featuring of
course
an nvidia a100 nvidia's bluefield 2 dpu
and an armed cpu from our partner ampere
computing
the ultra arm cpu
in video grace this was our big
announcement in gtc
and of course the alps based
supercomputer
launching in 2023 is one of the first
systems
that will use the grace architecture
grace solves the gpu to cpu
memory bottleneck increasing by orders
of magnitude
the performance of accessing that cpu
memory
in giving you a cash coherent memory
space
between the gpu and the cpu it'll use
our fourth
generation envy link technology
connecting not just the gpus
but envy link all the way out to the
grace cpu
this is designed for giant scale hpc and
ai applications
and you can see some of the performance
improvements that we're simulating today
as we continue to simulate the grace
chip
coming in 2023
so as we've talked about the hpc
community
needs to address a diversity of problems
and that requires architectural
flexibility
one size does not fit all in an hpc data
center
an arm and our arm partners enable that
architectural innovation
if you look across the landscape of
existing
and announced arm architectures
many countries cdac in india
e3 in korea ampere computing in the us
s.i perl representing uh the
the work of the european processor
initiative
and the processor design for next
generation european supercomputers
amazon graviton in the cloud and of
course nvidia grace
you'll see different types of
architectural innovation across all of
these
all of these partners in all of these
processors
creating a computing ecosystem is hard
we've worked for 15 years
to create the nvidia ecosystem on top of
x86
and of course that still represents the
majority of the servers
being shipped
we love x86 as a partner and we'll
support x86
cpus for the for forever
however i think there's there's a lot of
room for architectural innovation
and i'm really looking forward to that
arm number growing with all the
architectural innovation
going into arm
we've got a great ecosystem
lots of developers over 150 sdks
adoption by all the major oems and csps
and we're really looking forward to the
next year
in high performance computing and i
can't wait
to everything willing be back in
frankfurt in person
next year for isc 22.
so there you have it we've reached a
tipping point for industrial hpc
digital biology it's been the dl moment
we've seen the rise of cloud native
supercomputing
expanding arm ecosystem and of course
our latest pcae-based a100
in ndr-based infiniband launching here
at the show
so with that we'll go right into
our q a panel panel we've got a panel of
experts here
and they're all live no no takes here on
the q
a and so let's go ahead and start with
the first question
how accurate are ai simulations in
comparison to traditional
simulations um
let's see uh gitaka why don't you take
the lead on that
okay thanks mark for the question and
thanks
audience for this is a great question i
think this comes up often
uh you know how accurate the ai models
are for a hpc use case
so i would want to like when i answer i
would like to step back
and remind that you know simulation
application is also an approximation
where it is trying to use the laws of
physics to approximate
and predict what the behavior of the
material or the particle
would be but over time that simulation
application has become more and more
mature
and definitely it's uh the results are
have a higher trust factor now with the
advent of ai
you have to see that the ai model is
getting trained
using the data coming out of the
simulation so yes
initially the ai model
may not be that accurate but what the
researchers are doing is
they run the simulation and the ai model
in a tight integration
so wherever the ai model is weak
they use the simulation to create more
data set
or more training data set and that is
used to train the model
and over time this process is called
active learning
and over time the ai model becomes as
accurate as a
stimulation it's almost like an analogy
of a professor
and a student initially the professor is
teaching the student
and as the student gets more and more
practice
the student and the professor they
actually work as peers
and they can give feedback to each other
and that's what
ends up happening as the ai model gets
more and more accurate
it is actually then able to inform what
simulations to run
in the future and they can work in a
tandem so that's the process
where how you know people are making
their ai models more and more accurate
and yes when they start they may not be
that accurate but over time they do get
accurate
and we are seeing that trend that was a
great answer
again okay and you know the only thing i
think i'll add is
you know people will continue to use
traditional physics-based simulations
when they need to
but when they don't need to they'll
accelerate them with
uh with ai with data science-based
applications
and if you think about it the output of
simulations as you said
are not always 100 correct right that's
why when a coveted vaccine candidate
is generated by a simulation it's then
first tested out in the lab right and
then after it's tested out into the lab
then it starts going into human trials
and so again if if we had waited for a
hundred percent accurate
uh physical simulation we probably would
have seen
would not have seen yet the first covid
vaccines or certainly i've seen fewer of
them
so again it's amazing to see the
progress there
next question
does the performance of gpu applications
stack up on
arm versus x86 um
tim you know you're working with a lot
of our our
arm software why don't you take our
first pass at that we'll see
who else might want to jump in sure
uh well i mean the simple answer is that
we're seeing great performance on
uh on on arm-based systems as compared
to x86
uh and you know we take a look at that
both from the perspective of gpu
accelerated applications
and applications that aren't gpu
accelerated and we see just tremendous
performance
i think a lot of that has to do with the
maturity of of the
software stack and tools both from arm
as well as from nvidia
and it's it's looking very very good
[Music]
as so i i think that you know uh arm
and x86 are both great cpu architectures
right and
what nvidia focuses on is really
accelerating
uh accelerating the performance on
the gpn but also on
accelerating at the data center level
right
it's not about any individual processor
but it's about looking at things at that
full stack when we build our dgx super
pod
right our largest djx super pod we have
in-house
celine on the top 500 list
has over 4 000 gpus but it's not just
about the gpus
it's about accelerating the entire
performance
and in fact you know i'll let everyone
do the math you know you can go look at
other systems of different numbers of
gpus on the top 500
list and look at the ratio of the
performance
and then look at ml per or look at other
applications
right the cpu adds value
the gpu adds value the network adds
value
the software adds value but being able
to integrate and optimize
across that full stack is really i think
where
video's value comes through
and one quick point i'll add there uh
mark and this probably gives credit to a
lot of the work that tim and his team
does
um when we work with developers on
porting and working on on arm
specific applications the experience
that i that we saw time time again was
that they got that performance
with very minimal effort so i think
initially as we started thinking at the
supporting exercise and moving from x86
to arm but using i think a lot of the
tools that tim
and team has built around the hp sdk as
well as just the arm ecosystem as a
whole it has built in terms of
allowing that port to happen very
seamlessly and to get that performance
almost out of the box i think it's been
sort of a nice
surprise for a lot of the folks who've
been engaging with the applications
importing from from army x86
and you know that that will continue to
uh to let our customers decide
you know uh who has the best innovation
for them
right uh there's of course different
choices in x86
uh processors and you'll see many
different choices
of of arm processors as you already do
today and i think
customers will often pick the ones that
are
that have the right level of innovation
and deliver the right value
for them in in their application and
that might be different at the edge
in in the the data center in an
industrial data center and out in a
cloud use case
next question
so in the heatsink example how exactly
is this different from random or guided
search
algorithms used in optimization problems
this was described almost exactly as a
differential
equation
do you want to take that
yes i can take that so uh yes
your i mean simnet is doing
you know the design space exploration
just like you know what you described in
the question
but the point of simnet is that it is
able to do
a exploration of much more permutation
and combinations within
the same time or even in less time i
think
it's not it's about you know how simnet
does it and how it makes it easier
uh for the users to explore a lot more
uh
variables a lot more geometries
and still be physics informed and do it
in a matter of seconds or minutes or
hours versus
days and weeks
great uh next question
what are the programming models
supported by the hpc
sdk for the arm hpc dev kit
we've tamed outs one right up your alley
all right
i mean it's a great question um and and
with an easy crisp answer
all of them uh so the hpc sdk uh you
know
has had full support for armed server
for well over a year
in fact it's first released in may of
2020
arm was treated as a as a platform of
equal importance to uh to x86 and power
and so from automatic acceleration of
iso standard languages with cpu plus and
fortran
to incremental optimization with
directive based programming whether you
choose open acc
or openmb target offload and of course
support for
programming in cuda to directly target
the gpu and get great performance
when hosted by an armed server all of
those programming models are fully
supported
on the arm hvc dev kit and the hpc sdk
great thanks tim uh next question
what is the availability of ndr
infiniband uh good luck we'll let you
take that
appreciate that mark uh so we we
definitely were excited to announce the
ndr switch systems
uh today i believe um and we will start
to
sample out and send ndr switches to our
partners uh and customers in couple of
months for now
so it's pretty much uh going out that
will deliver
not just uh a great benefits on the
networking side
meaning going to the 400 gigabit per
second and a bigger switch rate digs so
you can connect more nodes with less
switches and
save uh power and save real estate
but we're also bringing new e-network
computing engines
meaning if you look at a network for
super computing it's it's not just
speeds and feeds anymore right it's
how you move compute to where the data
is
how you can improve the performance of
applications
by bringing compute everywhere and by
bringing the in-network computing
engines
such as sharp and the new engines as
part of ndr
we're able to analyze more data and
doing more rhythmics on the data as it
moves within the data center
and you presented some of the advantages
of that when improving
scientific or cfd applications nlps and
recommendation models and so forth so
short answer couple of months we start
shifting the
the ntr devices and will be in a full uh
uh production or uh general availability
by the end of the year
thanks galad and you know i know our our
internal architects are
super excited about uh getting ndr
and of course we're not going to just
ship
faster switches we're going to optimize
all of our designs starting with our own
dgx super pot
and again we're still working on the
different design trade-offs there
but that's one of the things that i
expected super computing
we'll be talking more about how ndr how
can help to optimize new dgx super pods
and other oem-based systems that follow
that same reference architecture
next question right then it's great it's
great that we bring in the full stack
right so it's more than just
a component perspective not just the
network not just the
the gpu not just the cpu but actually
bring the full sdk the full software
stack and can optimize everything from
the
applications all the way to the drivers
underneath
you know i i know uh uh before the
melanox acquisition
uh we work very closely with you right
but uh but there's nothing
like having uh all those melanox
engineers
in the same virtual room in in the same
virtual building
and you know every single piece of
nvidia software every piece of
nvidia hardware every engineer is
thinking about how do we take advantage
of these new features so
it's very exciting times
can you talk about how dp is an
increasingly
prevalent role in hpc ai architectures
well i know who i'm going to hand that
one off to but
you know i know a lot of our customers
are thinking about
that we had last week we had our
internal technical training for uh
for uh hpc and usually for isc
normally we just have internal speakers
for it and
and last week i was very privileged to
have
asked paul collegia of university of
cambridge because he's deployed
one of the first top 500 supercomputers
with dpus
and he came in and talked about just
this but he's not on the q a panel but
glad i know you work a lot with paul
at the university of cambridge so i'll
let you take this question
thanks mark and uh we did uh have a
session at iac together with paul and
actually talking about
cloud native super computing and so
forth
so if we look on on hpc we can
definitely see that hpc is going
everywhere
right ai is going everywhere hpc serves
more applications
more users and this is where you need to
start
bringing more multi-tenancy support into
the supercomputers that we build
more security right we want to work with
more industry applications you want to
work with medical companies
you need to secure the data so if you
look on that
one of the most complicated workload
right now on a supercomputer or a data
center
it's actually the data center operating
system it's actually the data center
infrastructure workloads that's become
one of the most complex workloads
and it takes a great toll on the cpu
performance
and and when we build supercomputing
it's all about performance right we need
the massive compute power
in order to actually solve the problems
that we want to solve
so this is where we kind of stuck how do
we
keep their bare metal performance and
bringing multi-tenancy and
isolations all the infrastructure
management that we want
this is where the dpu comes to the
rescue right the dpu
it's a full data center on a chip it is
being built
to run the infrastructure management uh
and and not just offload the
infrastructure management from the cpu
but also to accelerate that
with engines around securities file
system and so forth
so look if moving forward every super
computer will have a dpu
every super computer will have a dp in
every server to run the network opera
to run the infrastructure operating
system maintain bare metal performance
on one side and multi-tenancy support on
the other side
and one quick note it's not just there
because if you look on the world of
scientific computing
we're starting to go all the way to the
edge do the first analysis at the edge
and then
send the data after the the first
analysis to the main system
the dpu being a full data center on a
chip
it's a great component to be an edge
component h device component
is multiple dps for example with gpus
that's where we're going to see the edge
computing being started
that's a great point of dpu at the edge
but you brought up healthcare and so we
don't have time for any more questions
but
go a little bit just one minute over
here uh
nicola i'm going to put you on the spot
you work with a lot of our health care
customers
and and you know privacy of healthcare
data is certainly paramount uh how do
you see
the the dpu helping out in the
healthcare use case where
we go all the way from from edge
instruments to
of course super computers
we can't not show for sure yet but i i
expect to have
a major influence so privacy is a major
component for medical applications it's
not
that we handle with random data this
data is
genomic data this is patient data this
data should not be leaked it really has
to
to remain secure so dpu can really
accelerate the process there for for the
secure compute there
but i also see a lot of applications in
terms of uh
like big data centers or supercomputers
so i don't expect for example that every
hospital probably will have their own
supercomputer
but there's a reason why we do have a
healthcare team specializing on this
application because
it was also mentioned in the discussion
before it takes more than just
put compute somewhere to actually solve
a problem
to really tackle an application it has
to have the combination of
the algorithms that come from the
researchers
the infrastructure as well as the
compute so if you
consider for example the genomic world
if you have every every single one of us
has
billion of information in the dna and
every
uh to make actually inside of this
information it takes a lot of compute
to actually compute that and it's it's a
combination of
getting this information by a sequencer
accelerating this process
getting this information in the storage
getting this information from the
storage to the compute
and and vice versa to actually make
insights of that
well thank you and you know thank you
very much
to all of the panel i get to work with
you every day and can ask you questions
every every day i'm sure our audience
wishes we had another
hour that we could ask more questions
but i'm sure there were some questions
that we didn't get to
we'll follow up with anyone that asked a
question and uh by the end of the week
we'll have the video and the slides
posted
on the nvidia download center so i have
a great virtual isc
participated in the sessions um come to
the nvidia booth
and you can ask more questions there and
again
until we're able to meet in person thank
you all for coming
and have a great week
thank you mark that was great a
fantastic presentation and
great discussion with the panel lots of
exciting news
lots of announcements and we look
forward to having further conversations
with you
uh we're ready uh with our teams to have
the
the in-depth discussions and to go even
deeper in what we bring to market i wish
you all a great isc 21
and hopefully we get to see some of you
face to face either at
sc21 or next year in frankfurt at isc22
thank you so much bye
you
Title: GeForce Garage is MOVING to Official GeForce Channel - SUBSCRIBE now!
Publish_date: 2017-03-31
Length: 43
Views: 16467
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/ikX_1Dfmpy0/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: ikX_1Dfmpy0

--- Transcript ---

hey guys white here just letting you
know that GeForce garage is moving over
to the geforce channel be sure to
subscribe to stay up to date with all of
our amazing builds and guides that are
going to be coming soon this past year
we've shown some awesome rigs and we've
brought in some awesome get some of the
best in the industry and there's more
great modding content like that to come
same great show just on a new channel
you just need to be subscribed to
g-force to keep up to date with GeForce
garage as well as the cluesive works at
new tech upcoming games contests
giveaways and more
[Music]
Title: Interview with Ubisoft
Publish_date: 2014-10-01
Length: 264
Views: 22656
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/IrdUA1r2muk/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: IrdUA1r2muk

--- Transcript ---

welcome to be south Montreal this is our
lobby but obviously
did not come here to see this so follow
me and I'll show you the Assassin's
Creed unity floor come on alright guys
so this is where the magic happens
this is only part of the large room that
we have on Assassin's Creed unity
there's about close to 400 people here
in Montreal on the teams which is a
really really big team but you know we
need them because of this the sheer size
and ambition of the game that we're
trying to build with unity when we were
thinking about unity very early on this
is like almost four years ago we were
looking for a new setting to sort of you
know reinvent the brand for the first
fully next-gen iteration of the
franchise we looked at many cities we
decided very early on that we wanted to
go back to a European setting you know
like large sprawling city Paris is
arguably one of the most beautiful city
ins in the world it is the most visited
city in the world is actually a city
that looks still looks brand new it's
been really well preserved so it was
really good opportunity for us to
showcase all of the new technology that
next gen was going to bring to us so for
us unity was all about lighting it was
all about investing into a rendering
unity so we decided to tackle everything
from the ground up or I should say from
the sky down and I think that's what
makes you need see a better looking game
than black flag even if it was already a
good looking day
our partnership with Nvidia on the
Sasson screed unity really helped us
push the boundaries of what we were able
to do in terms of graphics you know
given this more serious I would say
story like a little bit more complex
story we felt that we needed to push the
boundaries of what we were able to do in
terms of performance capture to really
convey that emotion that actors could
actually bring to the table
you know like pushing characters to the
maximum level of detail having
performance capture that really
surpasses anything we've done before and
then use camera effects stuff like depth
of field camera lenses really try to you
know mimic the effect of cinema but
within the game world I guess I first
heard of nvidia when i was just in my
beginning of GPU programming or graphics
programming even so when the first
GeForce came out and then transform and
lighting became a thing so where the GPU
finally started doing something else
that wasn't checked texturing and
rasterization and that was the start I
guess of modern GPU programming I
remember first working with Nvidia back
on far cry 2 you know that partnership
with Nvidia really helped us push our
graphics to the maximum Assassin's Creed
unity is going to be the first that is
purely for next-generation systems so
next generation consoles and obviously
PC so I think PC gamers will be able to
experience ECU in all of its glory as
well as 4k what we're seeing today is
really a trend that's sort of forking
games into two you have the very very
high level triple layer quadruple a
games I should say into how like unity
and many other games where we're really
pushing immersion to a next level right
in the case of Assassin's Creed it's
becoming almost like time travel right
we're we're really faithfully
reproducing the the city of Paris as it
was during the French Revolution and I
think all of these Triple A games are
trying to go that way and then you have
the second trend where we're seeing a
lot of huge boom in terms of indie games
it really allows for people with that
don't have the means that large
corporations have to actually express
you know their their creativity and make
a contribution to the gaming industry
thank you very much for the tour thanks
Nvidia and remember that pick up the
game on November 11th enjoy leave go go
yeah go go go
you
you
Title: Enabling the Secure, Hybrid Accelerated Data Center with NVIDIA-Certified Systems
Publish_date: 2021-01-26
Length: 134
Views: 10943
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/iRPFO2YVs-Y/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: iRPFO2YVs-Y

--- Transcript ---

from more precise product
recommendations
to accelerated drug discovery
enterprises in every industry are
adopting accelerated computing to
deliver cutting-edge products
advanced research and revolutionize
business but they face many challenges
data is expanding generated by users
applications and devices
ai models are growing in size and
complexity modern data center workloads
from ai to visual computing to hpc
are increasingly diverse and to handle
these workloads networking needs to be
reliable
fast and secure enterprises require
systems that work out of the box
can run dynamic workloads at scale and
can deliver the performance that
accelerates time to solution
to address these challenges nvidia and
its ecosystem of partners are delivering
nvidia certified systems that securely
run
accelerated modern workloads at any
scale from data center to edge
nvidia certified systems combine the
unparalleled computing power of nvidia
ampere architecture gpus with nvidia
mellanox
secure high speed networking this
creates the ideal foundation for running
gpu optimized software from the nvidia
ngc catalog
as well as commercially available
applications seamlessly and to ensure
optimal performance on single node and
multi-node clusters
the systems pass a rigorous
certification process
so what are the benefits of nvidia
certified systems
they provide a unified accelerated
platform and simplified deployment of ai
data analytics visualization hpc and
virtualized enterprise apps
they're optimized for performance and
are highly scalable accelerating time to
solution and delivering the ability to
meet future workload demands
they secure workflows with key features
such as transport layer security
internet protocol security and inline
cryptography
and backed by enterprise grade support
they give enterprises the confidence to
develop and deploy
accelerated workloads at scale
nvidia certified systems provide the
necessary foundation to enable the next
generation of hybrid
secure data centers for enterprises
across all industries
click on the link below to find out more
Title: Path Perception Ensemble - NVIDIA DRIVE Labs Ep. 1
Publish_date: 2019-05-01
Length: 129
Views: 25052
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/iTZ9GoN2Q0k/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: iTZ9GoN2Q0k

--- Transcript ---

Solving for autonomous driving is a huge engineering
problem, and the way that we tackle that here
is through a divide-and-conquer approach,
where we tackle one critical piece of functionality
at a time.
Each of those pieces of functionality, we
call an NVIDIA DRIVE AV Mission, and today's
DRIVE Mission, DRIVE Mission number 62, is
called Path Perception Ensemble.
In this DRIVE Mission, we leverage an ensemble
technique, which is a machine learning method
that combines several base models and produces
an optimal predictive model.
In our Path Perception Ensemble, the base
models include three different deep neural
networks, LaneNet, that you see here in yellow,
PathNet, shown in orange, and PilotNet, shown
in blue.
These DNN outputs, along with the output of
a high-definition map, are combined into our
Path Perception Ensemble output, that you
see here, visualized by the thick green center
path lines, predicting the center path for
the ego-lane, as well as the left-adjacent
and right-adjacent lanes.
In this clip, we see Path Perception Ensemble
in action in a construction zone, in dense
traffic, with pretty much no lane lines on
the road, and the white lines you see are
actually computed by ensemble.
They are not physical lane line markings.
That center path is still green, which shows
agreement between the underlying Path Perception
Ensemble components.
When they start to disagree, it turns yellow,
and when disagreement gets stronger, it becomes
red.
The result of this analysis can be used to
determine when autonomous operation is safe
and when control should be properly transitioned
to the human driver.
In this video, we are using Path Perception
Ensemble for a series of autonomous lane changes.
The ego-lane that we're currently in is visualized
in green, left-adjacent lane is shown in red,
and right-adjacent lane is shown in blue.
And as we change lanes, the color scheme changes
accordingly.
You can also see that the color of the obstacle
fence computed for each car changes as cars
move to and from different lanes.
This functionality was incubated from scratch,
through our NVIDIA DRIVE AV Mission number
62, Path Perception Ensemble, and it will
be shipping in the NVIDIA DRIVE software 9.0
release.
Title: Win a new GeForce RTX 2080 Ti! - on the NVIDIA GeForce Channel!
Publish_date: 2018-08-21
Length: 58
Views: 93912
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/iUVwc-ruJ90/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: iUVwc-ruJ90

--- Transcript ---

hey there it's Julian the guy from the
NVIDIA GeForce Channel you are a
subscriber to the GeForce Channel right
see a lot of you nodding your heads but
there's about one two to two and a half
thousand of you who are not which means
you all are missing out on a huge
giveaway we are giving away 10 brand-new
insanely powerful GeForce r-tx 20 ATT
eyes on our channel which means to win
yeah you really need to be a subscriber
for those of you who want to see more in
depth gaming PC modding g-sync watching
goodness and you're eager to see what
our new ray-tracing GPUs can do come on
over right now we're showcasing all the
highlights from our RT X launch event
and over the next few weeks we'll be
showing you tons of coverage from
Gamescom as well comment on these videos
as a subscriber and you'll be
automatically entered to win if your
name gets picked at random and you're an
eligible winner you could be one of the
first gaming on a brand new RT X GPU get
this started right just hit the
subscribe button below and I'll see you
on NVIDIA GeForce Channel with all the
20 series goodness good luck and I'll
see you on NVIDIA GeForce
Title: NVIDIA Pascal, GeForce GTX 10-series Announced
Publish_date: 2016-05-07
Length: 102
Views: 51309
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/j1oJBIchWUo/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: j1oJBIchWUo

--- Transcript ---

hey guys this is telling Peterson with
NVIDIA and I'm here in Austin Texas
where Dreamhack is going on it's a
massive gamer event first time in North
America and we've just launched the GTX
1080 which is incredibly fast it runs
2.1 gigahertz overclock we also
introduced for new technologies first
and sole which turns your PC gaming into
a digital photography studio so you can
compose your best moments edit them
hyper visualize it it's actually
incredibly great technology the second
thing we brought to market is an
addition to VR ones what VR works does
is now models sound in three dimensions
so we're simulating physically bouncing
up walls doing all the inclusion so in
your ears it sounds like you're in those
hallways the third thing of course is
SMP which stands for simultaneous
multi-project
which allows us to inside the GPU match
the display orientation that you're
using so things like VR are faster and
more immersive things like surround
gaming dramatically increases your field
of view so you're looking at the side
panels and they actually look like
you're looking through a window into a
3d world we also launched gtx 1070 which
is at an incredible price point and has
incredible performance just like gtx
1080
Title: Localization Helps Self-Driving Cars Find Their Way - NVIDIA DRIVE Labs Ep. 17
Publish_date: 2020-01-08
Length: 129
Views: 24674
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/jcKnb65wpWA/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: jcKnb65wpWA

--- Transcript ---

Today in DRIVE Labs, we're talking about our
localization software that uses data from
sensors like cameras, radar, LIDAR, or a fusion
of these to localize or place the ego car
on a map.
This clip shows single camera localization
to a high definition map.
On the right, we see the bird's eye view of
the 3D map and the ego car localized to it.
The map has several different types of features
or landmarks.
Lane boundaries are shown in green, road boundaries
in dark blue, and the center of the ego lane
and magenta.
Horizontal intersection lines are in yellow,
while vertical yellow lines denote poles,
and yellow rectangles are traffic signs and
traffic lights.
After the ego car is localized to the map,
we can project the 3D map into 2D camera image
space and generate the video on the left.
If the 2D map projection matches with what
the camera sees in the real world, this means
that the map is correct and that localization
to that map is accurate.
Here, our localization is running in the second-longest
road tunnel in the world, which has no GPS
signal.
Nevertheless, we maintain accurate lateral
and longitudinal localization using just a
single front-facing camera, mass market, inertial
sensors, and the vehicles wheel tick data.
In this clip, we are seeing results from camera
localization using both the front and rear-facing
cameras.
We note that of sun glare reduces front camera
performance, we can use the rear camera to
maintain localization accuracy.
This clip shows localization to a 3D map that
has camera, radar, and LIDAR layers.
In this map shown on the right, the concentric
circles around the ego car represent the live
LIDAR scanned data, while the blue dots are
live radar scans.
We fully localize the ego car to each individual
map layer and we also fuse the individual
per sensor localization results together.
The fusion enables us to generate localization,
confidence metrics based on agreement and
disagreement analysis.
In the NVIDIA DRIVE Software 10.0 release,
we have opened up our camera-based localization
API to help enable this capability on consumer
vehicles.
Title: GTC November 2021 Keynote with NVIDIA CEO Jensen Huang
Publish_date: 2021-11-08
Length: 6420
Views: 1692985
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/jhDiaUL_RaM/hq720.jpg?v=618a414c
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: jhDiaUL_RaM

--- Transcript ---

Welcome to GTC!
We have a lot of exciting things to show you,
so let's get started.
We have a jam-packed keynote, but let me first
share with you this year's I AM AI
a celebration of groundbreaking work by scientists
and researchers around the world.
I am an explorer
Finding a path through earth's frozen expanses
And peering into darkness in search of water
on the moon.
I am a guardian
Watching for signs of pollution from the sky
And protecting us from danger here on earth
I am a visionary
Learning from the past
To better predict the future
And keep us safe for generations to come
I am a helper
Keeping our most valuable species healthy
And our crops abundant
I am a healer
Making our hospitals smarter for the future
And unneeded treatments a things of the past
I am a creator
Opening new paths to innovation
And new forms of artistic expression
I am a storyteller
Giving emotions to words
(in German) And breaking down the language
barrier
I am even the composer of the music
I am AI
Brought to life by NVIDIA, deep learning,
and brilliant minds everywhere.
Accelerated computing starts with NVIDIA CUDA
general-purpose programmable GPUs.
The magic of accelerated computing comes from
the combination of CUDA, the acceleration
libraries of algorithms that speed-up applications,
and the distributed computing systems and
software that scale processing across an entire
data center.
We have been advancing CUDA and the ecosystem
for 15 years and counting.
We optimize across the full-stack , iterating
between GPU, acceleration libraries, systems,
applications, continuously, all the while
expanding the reach of our platform by adding
new application domains that we accelerate.
With our approach, end users experience speed-ups
through the life of the product.
It is not unusual for us to increase application
performance by many x-factors on the same
chip over several years.
Imagine the joy of a researcher whose simulation
completed in half the time just by downloading
new software.
As we accelerate more applications, our network
of partners growing demand for NVIDIA platforms.
Starting from computer graphics, the reach
of our architecture has reached deep into
the world's largest industries.
We start with amazing chips, but for each
field of science, industry, and application,
we create a full stack.
We have over 150 SDKs that serve industries,
from gaming and design, to life and earth
sciences, quantum computing, AI, cybersecurity,
5G and robotics.
We introduced 65 new and updated SDKs at GTC
this year.
One of the major new industries that is accelerating
with NVIDIA is Design Automation.
I am thrilled to see Ansys, Synopsys, Cadence,
and Dassault accelerate the simulation of
thermal, mechanical, and 3D electromagnetics
for RFI and signal integrity.
A super exciting development is the work we
are doing with Ansys to accelerate Ansys Fluent,
the world's leading industrial fluids simulation
package.
Early results with the Ansys multi-GPU solver
show one DGX will replace 30 high-end dual-CPU
servers, leading to big savings in system
cost and power.
With the same total budget, customers can
scale to much larger simulations.
The number of developers that use NVIDIA has
grown to nearly 3 million - by 6 times over
the past 5 years.
CUDA has been downloaded 30 million times
over the past 15 years and 7 million last
year alone.
The adoption of accelerated computing is accelerating.
Our expertise in full-stack acceleration and
data-center-scale architectures lets us help
researchers and developers solve problems
at the largest scales.
Our approach to computing is highly energy-efficient.
The versatility of architecture let us contribute
to fields ranging from AI, to quantum physics,
to digital biology, to climate science.
We have some great new acceleration libraries
for you today.
The first is ReOpt an accelerated solver for
Operations Research optimization problems,
like delivery vehicle routing and warehouse
picking and packing.
There are 87 billion ways to deliver 14 pizzas. It's not so easy for Dominos to deliver pizza in under 30 minutes. 
Operations optimization is needed for last mile delivery, but also warehouse and manufacturing logistics.
Route planning is an extremely hard logistics problem.
At industrial scale, even small routing optimizations can save billions of dollars. 
This example uses a virtual warehouse in NVIDIA Omniverse to show the impact of optimized routing in an automated order-picking scenario.
 An optimized plan results in orders picked in half the time and half the distance traveled. 
Current routing solvers can take hours to rerun and respond to incoming new orders.
NVIDIA ReOpt runs continuously and dynamically reoptimizes in real-time. 
When new orders come in after the robots deploy? Re-Optimize. 
When a robot goes offline? Re-Optimize.
NVIDIA ReOpt responds in seconds and scales for thousands of locations.
For the first time, we can now capture the dynamic behavior of the real world and respond not just intelligently, but optimally. 
Re-optimize your logistics today. 
Quantum computing, relying on the natural
quantum physics phenomenon of superposition
and entanglement, has the potential of solving
problems that grow with combinatorial complexity.
Nearly 100 teams around the world in universities,
science labs, enterprises, and startups are
doing research in quantum processors, systems,
simulators, and algorithms.
It is expected to take another decade or two
to build a useful quantum computer.
In the meantime, the industry needs a superfast
quantum simulator to validate their research.
So we created the cuQuantum DGX appliance,
with an acceleration library for quantum computing
workflows that speeds up quantum circuit simulations
using state-vector and tensor-network methods.
The first accelerated quantum simulator will
be Google Cirq.
The speed up is terrific - here are results
of quantum Fourier transform, Shor's algorithm
used to break public-key cryptography, and
Google's Sycamore circuit.
A simulation that takes months can now be
done in days.
We are working on optimizing all the popular
simulators.
NVIDIA research achieved a major milestone
in quantum algorithm simulation using 1,688
qubits to find a solution for MaxCut of 3,375
vertices.
This is the largest-ever exact quantum circuit
simulation 8 times more qubits than ever simulated
before.
With cuQuantum on DGX, quantum-computer and
algorithm researchers can invent the computer
of tomorrow, with the fastest computer today.
The cuQuantum DGX appliance will be available
in Q1.
Python is the programming language of scientists
and ML and AI researchers.
Python has a rich ecosystem of libraries                          
Pandas for data analytics on data frames; NumPy for
analytics of n-dimension arrays and matrices;
Scikit-learn for machine learning; SciPy for
scientific computing; PyTorch for deep learning,
and NetworkX for studying graphs and networks.
There are nearly 20 million users of Python.
Today we are announcing cuNumeric a drop-in
accelerator for NumPy.
Zero code change.
cuNumeric accelerates NumPy, scaling from
one GPU, to multi-GPU, to multi-node clusters,
to the largest supercomputers in the world.
The parallelism is done implicitly and automatically.
cuDF is Pandas-like.
cuML is Scikit-Learn-like.
cuGraph is NetworkX-like.
They are part of NVIDIA's RAPIDS open-source
Python data science kit.
RAPIDS has been downloaded half a million
times this year over 4 times more than last year.
cuNumeric is built on Legion, which schedules
tasks across the CPU, GPU, and DPU computing
units across a data center in a similar way
as a modern CPU schedules instructions across
its ALUs and load/store units.
Like modern out of order execution CPUs that
automatically extract instruction-level parallelism
and dynamically reorder the execution, Legion
extracts task-level parallelism and dynamically
reorders and dispatches the execution of these
tasks, often out of order,
across the entire data center.
Legion is a data-center-scale compute engine
and cuNumeric is a data-center-scale math library.
NumPy was downloaded 122 million times in
the last 5 years.
NumPy is used by nearly 800,000 projects on
GitHub.
Developers are going to be thrilled with cuNumeric.
The scalability of cuNumeric is excellent.
On the famous CFDPython teaching code, cuNumeric
scales to a thousand GPUs with only a 20%
loss from perfect scaling efficiency.
ReOpt.
CuQuantum.
CuNumeric.
Three fantastic new libraries.
Let me show you the roadmap of my talk.
I'll update you on big initiatives we're working on
and introduce new ones that will shape our industries.
A constant theme you'll see - how Omniverse is used to simulate digital twins of warehouses,
plants and factories, of physical and biological systems, the 5G edge, robotics, 
self-driving cars, and even avatars.
You'll see how leading-edge computer graphics,
physics simulation, and AI came together to
make Omniverse possible.
And how the computing platforms and acceleration
libraries we built lay the foundation to make
Omniverse a reality.
Data center scale computing.
Million-X Science.
Omniverse.
AI.
Avatars.
Robotics and Self-Driving Cars.
We have a jam-packed GTC.
But before we jump into data centers, I want
to show you something we've being building
a conversational avatar, Toy-Me.
You're going to see speech understanding,
natural language processing on the largest
model ever trained, speech synthesis with
my own voice, character animation, and beautiful
ray traced graphics.
All in real time.
Toy-Me was made with some amazing technologies
that have become possible only recently,
and barely so.
I asked a few friends to ask this cute little
guy some tough questions.
Let's take a look.
Hi.
Hello.
Hello, Toy Jensen.
Hi, please ask me anything.
What are the greatest threats of Climate Change?
The greatest threats of climate change are
the effects on human health, food security,
water supplies, and the environment.
How do astronomers look for exoplanets?
Most exoplanets are found via the transit
method.
That was the case for the TRAPPIST-1 planets.
Tell me more about that method.
The transit method is when a planet passes
in front of its star.
The amount of light block tells astronomers
the size of the planet.
The duration of the transit tells astronomers
the orbital period of the planet.
What telescopes use that method?
The Kepler space telescope used the transit
method to find thousands of exoplanets.
I have heard about a satellite called test
that is being used to find planets.
How does it work?
TESS is a space telescope that looks for transits.
What roles do proteins play in our cells?
Proteins are the building blocks of the body.
They are the basic material of which all living
things are made.
They are needed for the structure-function
of the body's tissue and organs.
How do they work?
Proteins are made up of amino acids.
Amino assets are the basic building blocks
of proteins.
There are 20 different amino acids that link
together in different combinations to form
different proteins.
How do they catalyze chemical reactions?
Proteins are biological catalysts.
Catalysts are substances that increase the
rate of a chemical reaction without being
consumed by the reaction.
Enzymes are proteins that catalysts increase
the rate of biochemical reactions in the body.
Thank you, goodbye.
Sure, anytime.
In distributed computing, the network is the
vital central nervous system of the computer.
The network connects thousands of GPUs into
a giant supercomputer, determines its scalability
and ultimate performance.
Today we are announcing the NVIDIA Quantum 2
the most advanced end-to-end networking platform ever built.
Quantum 2 is a 400 Gbps Infiniband platform
and consists of the Quantum 2 switch, the
ConnectX 7 NIC, the BlueField 3 DPU, and a
whole bunch of software for the new architecture.
Quantum 2 is the first networking platform
to offer the performance of a supercomputer
and the share-ability of cloud computing.
This has never been possible before.
Until Quantum 2, you get either bare-metal
high-performance or secure multi-tenancy never both.
With Quantum 2, your valuable supercomputer
will be cloud-native and far better utilized.
Quantum 2 has some great new features.
Performance Isolation keeps the activity of
one tenant from disturbing others.
A telemetry-based congestion-control system
keeps high data-rate senders from overwhelming
the network and jamming the traffic for all.
Generation 3 SHARP has 32 times higher in-switch
processing to speed up AI training.
A nanosecond precision timing system can synchronize
distributed applications, like database processing,
lowering the overhead of waiting and handshaking
needed to avoid race conditions.
Nanosecond timing will also allow cloud datacenters
to become part of the telecommunications network
and host software-defined 5G radio services.
If NVIDIA's Selene DGX supercomputer were
equipped with Quantum 2 today, the total bandwidth
would be 224,000 GBytes per second, or roughly
one and a half times the total traffic over the internet.
Quantum 2 starts with the amazing new Infiniband
switch chip.
57 billion transistors in TSMC 7nm as big
as our A100 GPU.
It has 64 ports at 400Gbps or 128 ports at
200Gbps.
A Quantum 2 system can connect up to 2048
ports versus 800 ports in Quantum 1.
That s over 5 times the switching capacity.
And Quantum 2 can scale up to 1 million end-points
within the 3-hop Dragonfly topology.
That's 6.5 times over current generation.
This networking speed, switching capacity,
and scalability is coming just in time for
the giant HPC systems that the world needs
to be build.
Quantum 2 switch is sampling now.
Quantum 2 offers two networking end point
options: CX-7 and BlueField3
CX-7 is the fastest NIC ever built.
8 billion transistors in TSMC 7.
CX-7 doubles the data-rate of the world s
current fastest networking chip, CX-6.
And doubles the performance of Mellanox's
famous capabilities like RDMA, 
GPU-Direct Storage, GPU-Direct RDMA, and in-network computing.
A 256-thread data-path processor does
crypto at line rate.
CX-7 will sample in Jan.
Quantum 2 also offers BlueField 3 Infiniband.
16 64-bit Arm CPUs to offload and isolate
the data center infrastructure stack.
BlueField 3 is 22 billion transistors in TSMC7.
BlueField 3 is sampling in May.
The NVIDIA Quantum 2 the most advanced networking
platform ever built.
Quantum 2 will be available from the top computer
makers and in supercomputing centers 
all over the world.
It's going to give high-performance computing
quite the boost.
Cloud computing and machine learning are driving
a reinvention of the data center.
Container-based applications give hyperscalers
incredible abilities to scale-out 
allowing millions to use their services concurrently.
The ease of scale-out and orchestration comes
at a cost - east-west network traffic increased
incredibly with machine-and-machine message
passing and these disaggregated applications
open many ports inside the data center that
need to be secured from cyber-attack.
A new type of processor is needed to offload
the CPU of the burden of processing 
the networking, storage, and security software.
NVIDIA's BlueField DPU, an infrastructure
computing platform, is designed to do exactly that.
BlueField offloads and accelerates the infrastructure software, 
which is consuming some 30%, and growing, of the CPUs.
For multi-billion-dollar data centers, the
freed-up capacity can be a giant cost savings
or throughput boost.
The reception of BlueField is phenomenal.
Today, we are announcing BlueField DOCA 1.2,
a suite of new cybersecurity capabilities
that make BlueField the ideal platform for
the industry to build their Zero Trust security systems.
Protection at the perimeter and workgroup
segmentation are no longer sufficient.
Every touch point of applications, data, users,
and devices are potential attack surfaces.
Since BlueField is the networking end point,
we can secure a data center at virtually every touch point.
We are delighted to announce the leading cybersecurity
companies are working with us to provision
their next generation firewall services on
BlueField: Checkpoint, F5, Fortinet, Juniper,
Guardicore, Palo Alto Networks, Trend Micro,
and VMWare.
The BlueField ecosystem is expanding.
The cloud data center movement affects every
computing company.
There are now 1400 developers working with
BlueField.
And now cybersecurity companies on BlueField
can provide zero-trust security as a service.
Until every attack surface is secure, we should
assume security will be or is already breached.
State-of-the-art cybersecurity platforms monitor
and study the torrential user-machine and
machine-machine transaction logs yet they
only parse a fraction of that data
looking for anomalies.
With accelerated computing and deep learning,
we can process and study everything.
We created Morpheus, a deep learning cybersecurity platform that can monitor and analyze subtle
data center characteristics generated by every user, machine, and service.
Morpheus is built on NVIDIA RAPIDS and NVIDIA AI.
Workflows in Morpheus create AI models, digital
fingerprints, for every combination of app
and user to learn their usual patterns and
look for abnormal transactions.
These abnormal transactions, which may represent
only a handful of millions of events, would
trigger a security event and alert an analyst
to respond.
Just as many computing fields have enjoyed
tremendous advancements with accelerated computing
and deep learning, with Morpheus, we are bringing
this superpower to cybersecurity.
Be sure to go see the talks by F5 and Splunk
on the cybersecurity works we are doing together on Morpheus.
NVIDIA Morpheus harnesses the power of GPU
computing to help protect networks in a way
never before possible by creating customized
models tailored to your environment.
Observing every detail of your network activity,
Morpheus employs unsupervised learning to
understand typical behavior patterns across
multiple dimensions without relying on 
predetermined labels of good or bad.
As the AI learns, it creates not a single
model, but potentially millions of models
each one with a specific digital fingerprint
that is constantly scanned and analyzed.
And because the models are running on NVIDIA
GPUs, they can be scaled out, and parallelized
to support massive networks, enabling cyber-security
practitioners to apply enhanced capabilities,
to detect anomalies quickly and reliably.
The software revolution of deep learning is
coming to science.
This is extremely exciting and will make a
big impact.
Three connected dynamics will give us a Million-X
leap in computational sciences.
Let me explain.
First, accelerated computing, re-inventing
the full computing stack from the chip and
system, the acceleration libraries, to the
applications gave us a 50x boost.
Second, the boost, launched deep learning,
triggered the modern AI revolution, 
and fundamentally changed software.
The software that deep learning writes is
highly parallel, making it even more conducive
to GPU acceleration and scalable to multi-GPU
and multi-node.
Scaling to large systems like DGX SuperPOD
gave us another 5,000x speed-up.
Third, the AI software written with deep learning
can predict results 1,000 to 10,000 times
faster than software written by hand, busting
open completely the way we solve problems
and the problems that are even solvable.
50x times 5,000x times 1,000x gets us 250,000,000x. 
Of course, the mileage will vary and much depends on the scale you invest.
But when a solution to a worthwhile problem
is within grasp, the investments will come
look at the investment that are going into AI,
robotics, self-driving cars.
The signs are clear, accelerated computing
doing AI at data center scale will give a
giant boost in simulation performance.
How do we apply deep learning to science?
Science obeys the laws of physics Newton,
Maxwell, laws of thermal dynamics, Ohm's law,
Bernoulli's principle, the law of conservation
of energy, to name a few.
Researchers are creating AI models that learn
physics and make predictions 
that obey the laws of physics.
The application of machine learning to improve
physics simulation has been growing incredibly.
Let me highlight just a couple that I know of:
Karniadakis and the team at Brown described
PINN, physics informed neural network.
Li, Anandkumar, a team at Caltech and NVIDIA,
described FNO, Fourier Neural Operator, that
can learn to approximate any partial differential equation.
The same team recently combined the benefits
of PINN and FNO into PINO, 
a universal function learner that obeys the laws of physics.
PINO can learn from a principled-physics simulator
or observed data.
Once trained, it can emulate the principled
physics models at extremely high speeds.
And equally importantly, this model is highly
parallelizable, and so can scale to very large
systems to get a combined Million-X factor.
Virtual screening is one of the major pillars
of modern drug discovery.
It involves finding a drug chemical that will
bind-to and inhibit the function of a protein
in the pathway of the disease.
The virtual screening process is a molecular
dynamics simulation of the atomic forces between
the chemical and protein.
The atomic forces of molecules are determined
by its 3D structure.
The 3D structure of a human protein, the long
strands of amino acids, is revealed through
x-ray crystallography and cryo-electron microscopy.
Using this painstaking method, only 17% of the roughly 25,000 human proteins have been decoded.
In the vast space of human disease, without the 3D structure of most human proteins, 
computer-aided drug discovery is limited.
Just this year, the researchers of AlphaFold
and RossettaNet taught their AI's to predict
the 3D shape of proteins from just their amino
acid sequences.
Overnight DeepMind decoded over 20,000 human
proteins.
The tedious process of decoding a protein
is turbocharged by an AI model.
There are hundreds of millions of animal, plant, and bacterial proteins that can now be decoded.
Meanwhile, AI models are now able to learn
characteristics of known effective chemicals
and generate other potentially effective novel
chemicals.
Millions more potentially effective chemicals
meet hundreds of thousands more protein structures,
opening up a gigantic unexplored space of
new opportunities.
The opportunity space has increased many orders
of magnitude a million fold.
This has created overnight a massive molecular
simulation bottleneck.
How do we get the Million-X leap to engage
this opportunity?
Physics-ML might be the answer.
Researchers at NVIDIA and Caltech, used physics
ML methods to teach a graph neural network
to replace the expensive quantum calculation
of atomic forces in a molecular simulation.
OrbNet predicted Schrodinger's equation.
The result is a 1000-fold increase in simulation
performance.
Entos, is a super cool company focused on
using machine learning and computing 
to revolutionize drug discovery.
This is a simulation of HSP-90, a chaperone
protein that helps other proteins to fold properly.
You are watching something amazing.
The video shows a simulation of the chemical
reaction that is happening between
the HSP-90 protein and a candidate drug.
The candidate drug is forming a chemical bond
with an amino acid of the protein.
These chemical reactions are rare events,
so we have to simulate at long time scales.
And because electrostatic simulations cannot
model the atomic bond, quantum methods are
needed to compute free energy of the reaction.
This simulation took 3 hours on one GPU.
Without the OrbNet Physics-ML, it would have
taken over 3 months.
The future of drug discovery is computational
end-to-end, modelling the disease pathway,
the genes involved, the drug-target interactions,
and the off-target interactions.
With the confluence of Million-X acceleration,
ML for protein and chemical structure prediction,
and physics-ML simulation approaches, we are
witnessing the dawn of a biology revolution.
For climate science, we may finally have a
way to simulate the earth's climate 10, 20,
or 30 years from now, predict the regional
impact of climate change, and take action
to mitigate and adapt before it's too late.
Severe droughts are happening around the world.
This is not caused by lack of rain but higher
evaporation from rising temperatures.
The dryness is causing more wildfires.
Predicting climate change, so to develop strategies
to mitigate and adapt, is arguably one of
the greatest challenges facing society today.
We don't currently have the ability to accurately
predict the climate decades out.
Although much is known about the physics,
the scale of simulation is daunting.
Climate simulation is much harder than weather
simulation, which largely models atmospheric
physics and the accuracy of the model can
be validated every few days.
Long term climate prediction must model the
physics of earth's atmosphere, oceans and
waters, ice, the lands, and human activities,
and their interplay.
Further, simulation resolutions of 1 to 10
meters are needed to incorporate effects like
low atmosphere clouds that reflect the Sun's radiation back to space.
Ignoring these contributions accumulate to
significant error in long term predictions.
This is 10 to 100 thousand times higher resolutions
than any weather simulation today.
There are no computers big enough that we
can build.
We need a computer science breakthrough.
Today we are announcing NVIDIA Modulus a framework
for developing physics-ML models.
Train physics-ML models using governing physics
and data from principled models and observations.
Modulus has been optimized to train on multi-GPU
and multi-Node.
The resulting model can emulate physics 1000
to 100,000 times faster than simulation.
With Modulus, scientists will be able to create
digital twins to better understand large systems
like never before.
One important problem we can apply Modulus
to solve is climate science.
Climate change is reshaping the world.
The largest reservoirs in the U.S. are at
their lowest levels in two decades - some
150 feet below where they were.
The combination of accelerated computing,
physics-ML, and giant computer systems can
give us a Million-X leap and give us a shot.
We will use principled-physics models and
observed data to teach AI to predict climate
- in super-real-time.
We can create a digital twin of the earth
that runs continuously to predict the future,
calibrating and improving its predictions
with observed data, and predict again.
Researchers trained a physics-ML model using
atmospheric data from ERA5 of ECMWF.
The model took 4 hours to train on 128 A100 GPUS.
The trained model can predict hurricane severity
and path at 30 km spatial resolution.
7 days of prediction takes only a quarter-of-a-second
on a GPU
That s 100,000 times faster than simulation
Hopefully in a couple of years, data will
stream into a digital twin of Earth running
in Omniverse and an ensemble of physics-ML
models will predict the climate.
Let's talk about Omniverse.
"The Internet changed everything" is surely
an understatement.
We are always connected now.
The internet is essentially a digital overlay
on the world.
The overlay is largely 2D information - text,
voice, images, video.
But that's about to change.
We now have the technology to create new 3D
virtual worlds or model our physical world.
These virtual worlds will obey the laws of
physics, or not.
There can be AI or friends with you.
We will jump from one world to another like
we do on the web with hypertext.
This new world will be much larger than the physical world.
We will buy and own 3D things like we buy
2D songs and books today.
We will buy, own, and sell homes, furniture,
cars, luxury goods, and art in this world.
Creators will make more things in virtual
worlds than they do in the physical world.
We built Omniverse for builders of these virtual worlds.
Some worlds will be built for gathering and games.
But a great many will be built by scientists,
creators, and companies.
Virtual worlds will crop up like websites today.
Omniverse is very different than a game engine.
Omniverse is designed to be data center scale
and hopefully someday planetary scale.
The portal of Omniverse is USD, Universal
Scene Description essentially a digital wormhole
that connects people and computers to Omniverse,
and for one Omniverse world to connect to another.
USD is to Omniverse what HTML is to websites.
Omniverse is futuristic.
Omniverse can connect design worlds things
created in the Adobe world can be connected
to those in the Autodesk world through Omniverse
enabling designers to collaborate in a shared space.
Changes by a designer in one world are updated
for all connected designers essentially like
a cloud share document for 3D design.
Omniverse will revolutionize how the 40 million
3D designers in the world collaborate.
Companies can build virtual factories and
operate them with virtual robots in Omniverse.
The virtual factories and robots are the digital
twins of their physical replica.
The physical version is the replica of the
digital since they are produced from the digital original.
Omniverse digital twins are where we will
design, train, and continuously monitor robotic
buildings, factories, warehouses, and cars of the future.
Let me show you some of the fundamental technologies
that make Omniverse possible.
We are releasing a big update to Omniverse
today with some exciting features.
Showroom - an Omniverse App of demos and samples
that showcases core Omniverse technology graphics,
physics, materials, and AI.
Farm - a systems layer that orchestrates the
processing of batch jobs across multiple systems;
workstations, servers, bare-metal or virtualized.
Farm can be used for batch rendering, synthetic
data generation for AI, or distributed computing.
Omniverse AR streams graphics to phones or AR glasses.
Omniverse VR, the world's first full-frame
interactive ray-traced VR
Since launch late last year, Omniverse has
been downloaded 70,000 times 
by designers in 500 companies.
Along with us, the community, companies, and
tool providers are building Omniverse connectors.
There are 14 available now with 15 more coming soon.
Bentley announced that iTwin with Omniverse
is now in early access.
Bentley is not just connecting to Omniverse,
they are building their digital platform on it.
Bentley is used by 90% of engineering firms
and has nearly 2 million users of Bentley iTwin.
Heat Recovery Steam Generators, or HRSG, converts
the hot gas out a combustion turbine into steam,
which drives a steam turbine to generate electricity.
Corrosion is certain, so inspection and maintenance are needed.
Siemens Energy estimates that by predicting
corrosion accurately, they can reduce inspection
during regular maintenance and unplanned down time by 70%.
Reducing the industry 5-7 days can save nearly $2B a year.
HRSG corrosion is a multi-physics problem
with a combination of flow characteristics,
water chemistry, and operating conditions.
Using NVIDIA Modulus Physics-ML framework and Omniverse, 
we've created a digital twin platform with Siemens.
Let's take a look.
This massive Heat Recovery Steam Generator
uses hot gas exhaust 
to convert water in the pipes into steam for the turbines.
Predicting corrosion to avoid downtime is challenging.
Reduced order models aren't extremely accurate,
and full simulation takes expertise and time.
Siemens Energy is developing a digital twin
with NVIDIA Modulus for the multiphase turbulent flow.
With point cloud data to train a physics-based
AI model, they can infer high fidelity flow
- shown by the streamlines - in seconds.
This could reduce downtime by 70%, saving
the industry $1.7B a year.
BMW produces 1 vehicle per minute.
Each with roughly 25,000 parts.
There are 5 million parts on the factory floor
at any time.
At GTC Spring, BMW showed us how they are
building a digital twin of their Rogensberg factory.
They have since expanded to 3 other factories
totaling 10 million square meters.
Their engineers are using Isaac Gym, built
on Omniverse, to teach their robots new skills.
Let's take a look at the digital twin factories that BMW is building.
BMW has continued building on the momentum
of their factory of the future initiative
we showed you earlier this year.
On average they produce one new vehicle every
minute.
Expanding the initiative to now include Digital
Twin projects at three additional factories
which total more than 6 million square meters.
Meeting BMW's demands for continuous improvement
and innovation requires simulation of complex
production scenarios to speed output, increase
agility and optimize efficiency everywhere.
NVIDIA Omniverse introduces new simulation
capabilities everywhere BMW needs to make
critical decisions and introduce automation.
BMW and their partners have also been connecting
new users and tools to expand the Omniverse ecosystem.
Spanning every aspect of the BMW factory of
the future, Omniverse allows BMW to collaborate
and simulate from the entire plant level down
to comprehensive engineering detail .
Ericcson is building a digital twin of a whole
city to configure, operate, and continuously
optimize their fleet of 5G antennas and radios.
This is a really great story take a look.
There are 15 million 5G microcells and towers
planned for global deployment in the next five years.
Ericsson is using NVIDIA Omniverse for building
digital twin environments to help determine
how to place and configure each of their sites
for the best coverage and network performance.
In Omniverse, Ericsson builds city-scale models
that are physically accurate down to the materials
of the buildings, vegetation and foliage.
Then, wireless network components are added
including the precise location, height, and
antenna pattern of each transmitter.
Ericsson built a custom Omniverse Extension,
enabling them to integrate radio propagation
data and leverage Omniverse's RTX-accelerated
ray tracing to quickly visualize and calculate
the quality of the signal at every point in
the city.
Because Omniverse materials are physically
accurate, the intensity of reflections are
precisely determined.
Antenna beamforming and signal paths can be
accurately simulated and visualized.
In the simulations, the lobes signify transmitter
antenna beamforming, and the straight lines
are signal paths.
The colors of the signal paths denote strength
in decibels, and data throughput, 
from blue as the weakest to red as the strongest.
Visualization is a critical capability for
Ericsson.
With Omniverse VR, network engineers can virtually
explore any part of the model, teleporting
to any location, anywhere in the world, at 1:1 scale.
As they tune the network for optimal performance
or identify path disruptions, they can literally
see the effects of their adjustments in real
time things which aren't visible in real life!
In Omniverse, Ericsson can perform true-to-reality
remote simulation of entire 5G networks, enabling
them to design more efficient and reliable
networks, conduct remote field trials, 
and speed up deployments.
Someday, that fleet of antennas will use AI
to learn the best beamforming and signal strengths
to optimize quality of service and throughput
in the city, while conserving energy.
Omniverse as you can see is a foundational
platform for digital twin virtual worlds where
AI systems are created.
Let s talk about building AI models and systems.
Graphs are the native format, the most natural
data structure, of the world's data.
Whereas CNNs learn from spatial data and RNNs
learn sequences, 
graph neural networks can learn relationships.
How molecules connect to each other in a protein.
How people are connected in a social graph.
How roads are connected to towns and cities.
All can be described as a graph.
Deep Graph Library (DGL) is a Python library
built to implement graph neural networks on
top of existing deep learning frameworks.
We are working with the DGL community to accelerate
GNN processing like we have with CNNs, RNNs,
and Transformers.
From constructing the graph, to sampling the
subgraphs, and projecting graphs into a DNN
framework, we are accelerating the workflow
so that developers can train and inference
graphs with billions and trillions of edges.
GNNs are the new go-to model for financial
services, drug discovery, digital biology,
and cybersecurity.
Our early graph engagement partners have seen
excellent results.
PayPal significantly improved their collusion
fraud detection.
Amazon used it to improve Amazon Search and
reduce abuse and fraudulent sellers and buyers.
They processed graph sizes impossible before.
Pinterest scaled search and recommendations
to half a billion users.
We will have early access in December.
Transformers are models that can learn sequence
patterns in parallel.
This breakthrough sped up language model training dramatically,
which led to self-supervised language learning.
No longer limited by human data labelling,
giant self-supervised transformers benefit
from the troves of digital knowledge on the internet.
The recent breakthrough of Large Language Models
is one of the great achievements in computer science.
There's exciting work being done in self-supervised
multi-modal learning and models that can do
tasks that it was never trained on called
zero-shot learning.
Ten new models were announced last year alone.
Training LLMs, large language models, is not
for the faint of heart.
Hundred-million-dollar systems, training trillion-parameter
models, on petabytes of data, for months,
requires conviction, deep expertise, and an
optimized stack.
We created Nemo Megatron, a framework dedicated
to training speech and language models of
billions- and trillion-parameters.
It is optimized to scale out to gigantic systems
and sustain the highest computation efficiency.
Our researchers trained GPT-3 on NVIDIA's
500-node Selene DGX SuperPOD in 11 days and
together with Microsoft trained the Megatron
MT-NLG 530 billion parameter model in six weeks.
With Nemo Megatron, any company can train
state-of-the-art Large Language Models.
Once trained, how do we run the large language models?
Inference response time has to be sufficiently fast to be useful.
On a high-end dual Xeon Platinum CPU server,
inferencing Megatron 530B takes over a minute
for many applications, that s basically unusable.
GPU-accelerating these models is also challenging
because the model sizes require much more
than the frame buffer size of a GPU.
GPT-3, with 175 billion parameters, needs
at least 350 GB of memory.
Megatron, with 530 billion parameters, needs
over 1 TB of memory.
So, we created the world s first distributed
inferencing engine - NVIDIA Triton now does
distributed processing across multiple GPUs
and multiple nodes.
GPT-3 will fit easily on an 8-GPU server.
Megatron 530B will distribute across 2 DGX systems.
The performance is incredible.
From over a minute to half a second.
The capability and implication of Large Language
Models (LLM) are profound.
LLMs can answer deep domain questions, comprehend
and summarize complex documents, translate
languages, write stories, write computer software,
understand intent, be trained without supervision,
and are zero-shot, meaning that they can perform
tasks without being trained on any examples.
LLMs are pre-trained on general knowledge
and can be retrained
 to effectively serve new domains.
There are 20-35 languages that represent 80%
of the world population.
There is easily a hundred industrial or science domains.
And within them plenty of use-cases.
Sweden is working on digitizing its history.
Samsung is building a smart speaker for the
over 200 million Portuguese speakers in South America.
VinBrain is training a Vietnamese LLM for healthcare.
JD is building an LLM for their e-commerce
services to engage their half-a-billion customers.
Rakuten is building a Japanese LLM for their digital services.
ServiceNow is building an IT/help desk chat bot.
Xiaomi, the world's largest phone maker, is building an AI assistant.
Customizing large language models for new languages and domains, 
is likely the largest supercomputing application ever to come along.
There are many AI models that are now matured
and industrialized for broad enterprise use.
Computer vision, speech recognition, recommender
systems, graphs and trees, time series models,
generative models, variable encoders, and
now large language models.
There are excellent applications and uses
of AI in leading companies across the world's industries.
It's great to see them present their work at GTC, be sure to go watch their talks.
There are now 25,000 companies running AI on NVIDIA.
Financial companies are looking to reduce
fraud on over a billion credit card purchases
a day loss that costs the companies and consumers
over $35 billion dollars a year.
Customer and contact centers are overwhelmed. There are over half a billion calls a day.
This is a $20B industry.
And of course e-commerce product service recommendations
for what is soon-to-be $10 Trillion dollar industry.
For all companies, automation is vital to growth.
And AI is the most powerful automation technology
we have ever known.
Video conferencing is the most important apps
for most of us today.
We are doing 15 billion meeting minutes a day.
Microsoft has over 200 million active users.
We are delighted to work with Microsoft to
develop live captioning across 28 languages.
This is an invaluable feature for those who
are deaf or have hearing difficulties.
Every captioning session is personalized for
each meeting to understand names of people
and specific jargon.
Video conferencing technology is going to
advance very fast.
Going forward, a great deal more AI will be infused.
In addition to background and noise removal,
there will be AI for all kinds of amazing
things language translation, eye contact,
re-lighting, and much more.
Let's talk about inference.
AI is a new way to write software, and inference
is running the software written by AI.
Inferencing is challenging on multiple dimensions.
The computation intensity of the networks
is high, but that s just the start.
AI is data-driven, so the movement of data,
the pre-processing and post-processing of data,
all play in its performance.
NVIDIA's CUDA GPU architecture shines at processing
this end-to-end pipeline
AI applications have different requirements
response time, batch throughput, continuous streaming.
Different use cases use different models,
and deep learning architectures are complex.
There are different frameworks.
There are different machine learning platforms.
There are different platforms with different
operating environments, from cloud, to enterprise,
to edge, to embedded.
There are different confidentiality, security,
functional safety, and reliability requirements.
And the world has a large installed base of
different CPUs and GPUs, each with different
capabilities and performance characteristics.
The combination of all those requirements
is gigantic.
Inferencing is arguably one of the most technically
challenging runtime engines the world has ever seen.
Today, we are making the biggest release of
inference tools ever.
First, NVIDIA's TensorRT compiler has been
integrated natively into TensorFlow and PyTorch.
Many developers inference directly from the
frameworks.
It's easy.
It always works.
But it is slow.
Now, with 1 line of code, ML developers can
get a 3X boost without lifting a finger.
Just 1 line of code.
Tree models are ubiquitous especially in finance.
It is naturally explainable and new predictive
features can be added-on without fear of regressions.
Today, we are announcing that our Triton inference
server will do inferencing on DL as well as ML models.
The performance is fantastic and game changing.
Here is an example on the IEEE fraud detection dataset.
The goal is to improve detection rate while
responding in time to block the transaction
in this chart, that's staying to the right
of the red line, which marks one-and-a-half
milliseconds, the longest allowable processing time.
For small trees, both CPUs and GPUs can do so.
However, with large trees needed for better
detection rates, inference time remains under
1.5ms for a GPU, while a CPU now takes 3.5ms
way too long to block the transaction.
With this release, we open NVIDIA GPUs to
the world of classical ML inferencing.
Now, with one inference platform, Triton lets
you inference DL and ML, on GPUs and CPUs.
Announcing a major upgrade to our Triton Inference Server.
Inference on all models, any framework, multiple query types.
ML and DL.
For all platforms, cloud, on-prem, edge, and embedded.
Multi-GPU, multi-node.
On CUDA, x86, and Arm.
One engine, NVIDIA Triton, for all inference workloads.
The performance of Triton is spectacular across
the board from imaging to speech AI, natural
language processing, recommenders, and reinforcement learning.
For CSPs, Triton drives up the utilization and throughput of their infrastructure,
freeing capacity for new growth.
For users, Triton drives up throughput while, reducing cost.
This is one of the major benefits of the NVIDIA
platform with our full-stack optimization
and rich ecosystems, customers enjoy boosts
in performance and new functionality,
throughout the entire life of use.
Years after purchase, our chips keep getting faster and better.
The more you buy, the more you save.
Every company in every industry is looking
to increase automation.
To automate, we need to program computers to recognize patterns 
and execute a task repeatedly and safely.
But the world is unstructured the range of
tasks humans perform in an infinite range
of circumstances is impossible to describe
in programs and rules.
Advances in AI have opened new opportunities
to automate tasks unimaginable before.
In computer industry parlance, the edge is
where computers touch the world.
A large number of edge applications today
can be processed in the cloud for example,
people using phones connected to cloud services.
For many edge applications, transit to the cloud is not possible for response time, data security
or reliability reasons, or the practicality of data transport costs for continuous high-speed sensing.
Edge applications are essentially robotic
applications they perform similar tasks
under similar requirements as self-driving cars.
The unifying concept of edge computing is
the need to process some combination of sensors,
high-speed IO, data processing, signal and
physics processing, AI inferencing, and computer graphics.
This is the robotics pipeline and must be
processed in real-time.
Processing time translates to safety, cost,
capacity, and ultimately usability.
So how do you build the AI applications that
process the robotics pipeline?
We created the NVIDIA Unified Computing Framework.
It lets us compose containers and microservices
into a fast pipeline by chaining the processing
of dedicated accelerators, the CUDA GPU, Tensor
Core AI, RTX graphics, networking, security,
and fast IO.
UCF lets you build AI applications that can
process the robotics pipeline.
UCF can create applications that run in the
data center or embedded systems.
Buildings, warehouses, factories, farms, and
roads will be able to sense in the future.
Metropolis is our video processing and analytics
platform.
From streaming video, it can detect, track,
count, infer 3D pose, 
and even reconstruct full 3D scenes in the future.
We support cameras today, but it's an easy
extension with UCF to support lidar, depth sensors,
imaging radars, ultrasonics, and infrared.
Metropolis is full-stack and open, like all
of our platforms.
Customers can use the Metropolis application
as-is or customize our graph.
The stack can integrate 3rd-party 5G radio,
accelerated by our Aerial CUDA PHY.
And NVIDIA-certified edge computing systems
are available from every computer maker.
Mavenir, the leader in software-defined 5G solutions,
builds 5G core and virtualized radio access networks.
They used the Metropolis platform to create
a fully optimized construction kit to do AI-on-5G
for industrial applications in factories, plants,
public spaces, farms, and places where IT is limited.
MAVedge-AI will be available to early access
customers in Q1.
The combination of rich sensors, computing,
and AI at the edge will inspire a wave of
new ideas applications that are simply not possible today.
Here is an example of work we are doing with Verizon.
Volumetric video can offer exciting entertainment experiences.
Here, Yahoo Ryot Studios places live-action
rugby footage in a variety of settings.
Traditional edge compute architectures cannot
process 3D volumetric data 
from 50+ high resolution cameras in real-time.
Verizon adopted a breakthrough approach which
bypasses the CPU by using Magnum IO to write
video streams 10X faster, and process up to
250TB of video per half hour.
In the future low latency 5G, edge computing,
and NVIDIA GPU acceleration will enable Verizon
to open new options for putting you in the game
In this chapter, I've shown you how we train
new models like physics-informed ML, graph
neural networks, and large language models,
and how Triton is one engine for all inference workloads.
And using UCF, how you can compose these models
into edge applications.
Our platform is full-stack and open, runs
from the cloud, on-prem, to edge and embedded.
With partners across the industry, we've built
a rich ecosystem that connects NVIDIA AI into
whatever IT infrastructure, software platform,
workflow, or integrator you choose.
Still, there remains a great deal of engineering
to stand-up these stacks.
For that last-mile, we are partnering with
Equinix to pre-install and integrate NVIDIA
AI into their data centers around the world.
We're making it easy for enterprises to test
drive their workload.
And when you are ready to scale-out, the full
recipe is available to our network of partners
to help you do it at Equinix, your own data
center, or anywhere.
We call this LaunchPad.
You will find LaunchPads all over the world
in Silicon Valley, Dallas, Washington D.C.,
London, Paris, Amsterdam, Frankfurt, Singapore, and Tokyo.
If there are other locations you would like a LaunchPad, let us know.
In the near future, there will be billions
of robots to help us do things.
Some will be physical robots most will be
digital, virtual robots.
Some virtual robots will be fully autonomous,
others semi-autonomous or even teleoperated,
that is to say, with human-in-the-loop.
Maxine is our avatar platform our virtual robot platform.
We've been showing you pieces of the technology
for some time.
Today, I'm going to put all together.
Maxine can be autonomous or teleoperated,
realistic or artistic.
Maxine can be used for a broad range of applications,
for example, customer service, live, 
on the web, or in Omniverse.
It can be used for video conferencing.
Or to animate game characters.
Or be integrated into a robot.
The fundamental technologies of Maxine are
just becoming possible.
Computer vision, neural graphics, animation,
speech AI, dialog manager, 
natural language understanding, recommenders.
These are foundational technologies we've
been talking about for some time.
This was pretty much impossible 5 years ago
and barely so today.
First, speech AI.
Today we are announcing the public release
of NVIDIA Riva neural speech AI.
This is the input/output of Maxine.
We ve dedicated significant R&D and built
DGX SuperPods to make Riva possible.
Riva speaks English but recognizes 7 languages 
(English, Spanish, German, French, Japanese,
Mandarin, and Russian).
Riva will speak more languages in the future.
Riva can close-caption, translate, summarize,
answer questions, and understand intent.
Riva's accuracy is world-class and the response
time is unrivaled.
And with only 30 minutes of training, Riva
can be tuned to a specific voice, for example,
that of the brand ambassador of your company.
Riva can be fine-tuned for vocal pitch, duration,
and energy for human-like expressiveness.
Riva can be deployed in any cloud or at the edge.
Early customers have seen excellent results.
The formulation of a problem is often more
essential than its solution, which may be
merely a matter of mathematical or experimental
skill.
To raise new questions, new possibilities,
to regard old problems from a new angle requires
creative imagination and marks real advances in science.
This is a simplified diagram of Maxine's UCF
graph, Unified Computing Framework.
The Maxine compute graph consists of video,
audio, graphics, and IO processing; AI models
include vision, speech, animation, language, recommenders.
Maxine runs NVIDIA AI on EGX servers and Omniverse
on RTX servers.
And all of it has to be interactive let's
take a look.
Hi.
Welcome to Shannon's Cafe.
How can I help you?
Can I have a cheeseburger with a side of onion rings, please?
Would you be interested in our popular toppings
for cheeseburger, like bacon and fried onions?
Yeah, let's do it.
Done.
Added cheeseburger with bacon and fried onions.
What else?
What protein options do you have?
We have multiple options, I recommend the
double protein burger.
Would you like that?
Do you have any vegetarian options?
I can get you a black bean patty instead,
would you like that?
Sure.
This is a Metropolis application. We call it Tokkio, a talking kiosk.
The little animated robot is making eye contact
and tracks the customer.
From speech recognition, to natural language
dialog manager that infers intent and actions,
to recommendation, to natural speech, Tokkio
responds in about 2 seconds, very interactive.
That was Maxine in autonomous and artistic mode.
What if you would like to use Maxine in a
teleoperated and realistic mode?
This would be useful for customer service
or video conferencing.
Let's take a look.
NVIDIA's Maxine is a remarkable innovation,
one built to advance how we collaborate in
this brand new online world of ours.
With Maxine, you can speak several languages,
(in German) all at once
Riva converts the text to speech in those
languages.
Omniverse takes over and converts the speech
to 3D facial animation.
Alex's German, French, and Spanish-speaking
avatars are streamed simultaneously.
Depending on which version of Alex's avatar
you choose, she will speak to you in that language.
(in Mandarin) better share your ideas
Eye contact is critical to engage your audience,
so Maxine keeps my eyes on you even when they
are looking down at the script
In this case, Maxine uses computer vision
to track Alex's face and recognize her expressions.
The 3D animation animates a virtual, but realistic,
avatar of Alex.
because you not actually looking at me, but
rather a portrait that Maxine brings to life
from a cafe filled with noise that would normally
drown me out, like this.
(that s the magic of Maxine!)
Let's talk about robots.
Future medical instruments will become robotic.
Recent advances in AI, physics-ML, ray tracing,
and the computing advances we've spoken about
will also revolutionize medical instruments.
The algorithms will be reimagined by AI.
The instrument will be reinvented by edge
computing architectures.
The business models will be revolutionized,
as instrument sales will be replaced by 
Medical Device Software-as-a-Service solutions.
These dynamics are great for the patients,
for the hospitals, and the instrument makers
of this $200B industry.
The industry needs a software-defined imaging
platform to build this future on, just as
the auto industry needed a software-defined
AV platform.
Today we are announcing NVIDIA Clara Holoscan
- a software-defined, programmable, imaging platform.
Holoscan is the culmination of many years
of planning.
It takes all of NVIDIA's technology to make Holoscan.
The last two pieces of the puzzle are just
coming on-line. Unified Computing Framework,
that I've described, and a new chip, Orin,
a superfast sensor-processing robotics chip.
The base Holoscan platform consists of Orin
and CX-7.
Orin can process the entire robotics pipeline
- sensors, physics, AI, imaging, and graphics,
in a single chip.
12 Arm CPUs.
5.2 TFLOPS of FP32
250 TOPS for AI
740 Gb/sec of high-speed IO to connect sensors
You can optionally add an A6000 Ampere GPU
and get another 39 TFLOPS of FP32 and over
600 TOPS of AI inference.
With Clara Holoscan UCF, instrument makers
have a development platform to build real-time
applications that connect these powerful engines.
The Holoscan platform is open 3rd-parties
can build upon Holoscan's interfaces and APIs,
researchers can do great new science, and
instrument makers can integrate Holoscan into their solutions.
We are delighted that AJA video systems, Kaya
instruments, Verisonics, and US4US are building
front-end sensors to support Clara Holoscan.
Holoscan applications can be deployed fully
in-instrument, in the hospital's datacenter,
or a mixture of both.
This allows companies to develop applications
that require more computing than is in the
device or to upgrade the installed base of
devices years after deployment.
NVIDIA Clara Holoscan is a full-stack, open
platform for next generation software-defined instruments.
Holoscan is our 3rd robotics platform.
We've got some great updates on our other
two - Isaac and Drive.
The robotics industry is growing incredibly.
Our Isaac ecosystem is now over 700 companies and partners.
That number has grown 5 times in the last 4 years.
AMRs, or Autonomous Mobile Robots, are being
deployed in giant warehouses to handle the
incredible growth of e-commerce fulfillment.
Cleaning robots, restaurant and retail automation
robots, last-mile delivery robots, moving
telepresence robots are all being worked on.
A robot perceives the environment, reasons
about where it is and where it needs to go,
what it needs do, and then develops a plan to do it.
There are three interconnected workstreams
in building a robotics system:
First, train the robot's AI models to perceive.
For training, we have NVIDIA AI and DGX.
Second, in a simulator, train the robot to
manipulate or navigate.
For simulation, we have Isaac Sim on Omniverse
running on RTX.
The Isaac Sim Omniverse simulation will also
serve as the digital twin of the robot when deployed.
Third, operate the robot in the environment.
Here, Isaac running on AGX does the perception,
localization, mapping, and planning or, otherwise,
the robotics pipeline in real-time.
If the robots are connected over 5G and orchestrated
from a central server in the warehouse,
we would operate the Isaac stack on EGX.
So, DGX, RTX, EGX, and AGX systems and their
appropriate software stacks make up the
 end-to-end machine learning loop.
Isaac is a full-stack and open platform.
Isaac now supports the ROS ecosystem, the
large open-source robotics community.
ROS has 700,000 developers and is growing fast.
The Isaac runtime can now be a node in the
ROS framework.
For example, to do object detection, segmentation,
3D pose estimation, visual odometry,
or point cloud processing.
All of that can be 10X faster.
Instantly, ROS developers get a giant boost
in performance and benefit from the algorithms on Isaac.
ROS developers can also import the ROS URDF
robot definition format directly into 
Isaac Sim to simulate their robots.
Isaac Sim is the most realistic robotics simulator
ever created.
It's built on Omniverse.
Sensors are modeled.
Physics is simulated.
Environments are photorealistic.
Robots in simulation are running their actual
stack (either SIL, software-in-the-loop or
HIL, hardware-in-the-loop).
The robot is connected to an actual map.
It really feels like it's in the environment.
The goal is for the robot to not know whether
it is inside a simulation or the real world.
We strive to minimize the Sim2Real domain gap.
Training data is incredibly hard to create
for robotics unlike cars on roads,
the world of robots is far more random.
Cars follow lanes and avoid other cars.
Robots have no lanes and are designed to make
contact.
It is impossible to collect and label all
the scenarios to train a robot.
Isaac Sim Replicator is an engine to generate
synthetic data to train robots.
Replicator simulates the sensors, generates
data that is automatically labeled, and with
a domain randomization engine, creates rich
and diverse training data sets.
The ROS community will be supercharged end-to-end
with Isaac Replicator, Isaac Sim on Omniverse,
and Isaac ROS.
Someday, everything that moves will be autonomous
either fully or mostly autonomous.
By 2024, the vast majority of new EVs will
have substantial AV capability.
We are developing an end-to-end flow for building
autonomous vehicles, as well as a full-stack
in-car AV system, and a global cloud map.
NVIDIA Drive is a full-stack and open AV platform.
Customers can decide to use just our development
flow, parts of our driving computer, connect
to our cloud map, or partner with us end-to-end.
We are working with companies building cars,
SUVs, sports cars, trucks, vans, robotaxis,
and food delivery vehicles.
Autonomous vehicles are robots and the same
3 pillars of machine learning development
apply training models with NVIDIA AI on DGX,
simulation and synthetic data generation with
Drive Sim on Omniverse, and a real-time robotics
pipeline with Drive AV on the Orin robotics chip.
The first goal is to transform the data from
surround sensors into a 4D world model.
The left image is showing the surround cameras,
simulated by Drive Sim.
The right image is the world model, essentially
the mind of the car.
With a high-integrity and high-precision world
model, we use it to avoid obstacles, localize
to a map, reason about the environment, and
plan paths to reach your destination.
It starts with the sensor and computing architecture
of the car.
The design should allow for high-fidelity
sensing, redundancy, and fail-over safety,
with sufficient computing power and programmability
to process software improvements for the life of the car.
This is Hyperion 8 the architecture of 2024 models.
The sensor suite is 12 cameras, 9 radars,
12 ultrasonics, and 1 front LIDAR.
All of this is processed by 2 ORINs.
For the devkit, we include Ampere GPUs to
give plenty of performance headroom so that
engineers can have the best environment to
prototype new software.
Hyperion 8 is available today.
For anyone developing an AV, or sensors for
AV, Hyperion 8 is an ideal platform.
We collect petabytes of road data from around the world
and have some 3,000 trained labelers creating training data.
Still, synthetic data generation is a cornerstone
of our data strategy.
Here you see a scene through the simulated
surround cameras, with data labeled automatically,
and, on the right, some of the AI models that
were trained with the data.
Drive Sim Replicator is a synthetic data generator
for autonomous vehicles and is built on Omniverse.
The lens models are simulated and consider
motion blur, rolling shutter, LED flicker,
and doppler effect.
We work very closely with sensor makers to
accurately model their sensors.
The camera, radar, and lidar sensor models
are path traced.
The materials are physically simulated for
accurate beam reflections.
We built a lidar-materials library, and we
are also building a radar-materials library.
Replicator is a game changer for us.
Replicator bootstraps the AI labeling tools
and the AI models before Hyperion 8 is even
built and any data has been collected.
Replicator can label ground truth in ways
that humans cannot tracking moving objects
across sensors, velocity, distance, occlusion,
severe weather conditions.
It is accurate and low cost.
And it augments data where we have known gaps.
Mapping is a critical pillar of driving it
is the collective memory of the fleet and
can be considered another sensor.
A couple of months ago, we welcomed DeepMap
to our company.
DeepMap is a world leading expert in mapping
for autonomous driving.
Between DeepMap and our existing mapping team,
we are scaling out globally.
We do both survey mapping and fleet mapping.
Fleet mapping crowd-sources, or with one car,
incrementally, builds up a drivable map.
With each drive, more of the route is perceived
and reconstructed in 3D.
Survey mapping is a fleet dedicated to mapping.
We will have a fleet dedicated to mapping
the most popular areas in the world.
Survey maps primes the fleet even before it's launched.
It also serves as the ground truth data for
our cloud mapping-AI system.
Since the last GTC, we've turned on urban
driving and autonomous parking.
We are now running Hyperion 8 sensors, 4D
perception, deep learning-based multi-sensor
fusion, feature tracking, and a new planning engine.
This is our Mercedes Hyperion 8 driving a
route of urban streets and highways near our headquarters.
You will see Mercedes negotiate merges, crosswalks,
intersections, a roundabout, a cloverleaf,
merge contenders, cut-ins and pedestrians.
Enjoy.
AV will revolutionize how cars drive and will
greatly improve road safety.
The inside of the car will also be revolutionized.
The technology of Maxine will reimagine how
we interact with our cars.
With Maxine, your car will become a Concierge.
Maxine will show you what is in the mind of
the Chauffeur, precisely, and use neural graphics
to reconstruct a 3D surround view,so that
you can have confidence in the autonomous driving.
Maxine will summon valet mode.
Search for a parking spot.
And park the car.
And Maxine, with all the amazing avatar technologies
we are building, will seem incredibly smart.
What different driving modes can you do?
I have max range, super hush, sport, and super sport.
Which would you like?
I don't want to be late. Let's go super sport.
Got it. Super sport it is.
All of this will run on the new Orin robotics chip.
Future cars will be your personal AI chauffeur
and AI concierge.
The technologies I've shown you today make all this possible.
Here we've applied it autonomous vehicles.
But the technology can be generalized to all
kinds of robotic applications.
A robotics stack for navigation and manipulation.
And a robotics stack for human interaction.
It's going to be pretty darn amazing.
We covered a lot today - let me put it together
for you.
Accelerated computing launched modern AI and
the waves it started are coming to science
and the world's industries.
It starts with 3-chips, GPU, CPU, and DPU,
and systems - DGX, HGX, EGX, RTX, and AGX
that span from cloud to the edge.
150 acceleration libraries for 3M developers,
from graphics, AI, sciences, to robotics,
serving $100 trillion of industries.
NVIDIA accelerated computing is a Full-Stack,
Data Center Scale, and Open Platform.
Quantum-2 is the most advanced networking
platform ever built, and with BlueField 3,
welcomes cloud-native supercomputing.
Cybersecurity is a top threat of companies
and nations.
We announced a three-pillar Zero-Trust framework.
BlueField isolates applications from infrastructure.
DOCA 1.2 enables next-generation distributed
firewalls.
And Morpheus, assuming the intruder is already
inside, uses the superpowers of accelerated
computing and deep learning to detect intruder activities.
We introduced new deep learning frameworks.
Nemo Megatron trains Large Language Models.
LLMs will be the biggest mainstream HPC application ever.
Graphs can now be projected into DNN frameworks.
And NVIDIA Modulus builds and trains Physics-informed
ML models that can learn and obey the laws of physics.
GPU acceleration, data center scale, and physics-informed
ML will give us Million-X speed-ups, and revolutionize
drug discovery and climate science.
Triton, an inference server for all workloads,
now inferences forest models and does multi-GPU
multi-Node inference for large language models.
We introduced many exciting new libraries.
ReOpt for the $10T logistics industry.
cuQuantum to accelerate quantum computing
research.
cuNumeric to accelerate NumPy for millions
in the Python community.
The next wave of AI is enterprise and industrial
edge - where AI will automate at the point-of-action.
We offer several edge robotics application
frameworks Metropolis, the New Clara Holoscan,
Isaac, and Drive.
We highlighted three important technologies
needed to enable edge AI Unified Computing
Framework, the new Maxine, and Omniverse.
NVIDIA Unified Computing Framework is built
for robotics applications.
Edge applications are different than cloud.
They stream sensors, do signal or physics
processing, AI inference, speech, 
computer graphics all in real-time.
Clara Holoscan, built with UCF, is a new software-defined
medical instruments platform and runs in the
data center or Orin, our new superfast robotics processor.
The new Maxine is an avatar platform.
Maxine connects computer vision, Riva speech
AI, and avatar animation and graphics into
a real-time conversational robot.
Our Metropolis engineers used Maxine to create
Tokkio, a smart kiosk application, a talking kiosk.
This will be useful for smart retail, drive-throughs,
and customer service.
Our Drive engineers used Maxine to create
Concierge.
You could imagine Maxine being integrated
into future Isaac and Clara Holoscan applications.
Another demo showed Maxine as a videoconferencing
avatar doing simultaneous multi-language conferencing.
And Omniverse, our virtual world simulation
engine, was a common thread throughout our entire keynote.
Robots, AV fleets, warehouses, factories,
industrial plants, and whole cities will be
created, trained, and operated in Omniverse
digital twins.
I have one more announcement.
We will build a digital twin to simulate and
predict climate change.
The last supercomputer we built was called
Cambridge 1, or C-1.
This new supercomputer will be E-2.
Earth Two the digital twin of Earth, running
Modulus-created AI physics, at Million-X speeds,
in Omniverse.
All the technologies we've invented up to
this moment are needed to make Earth Two possible.
I can't imagine a greater and more important use.
See you next time.
What are the greatest...
The greatest are those who are kind to others.
Title: NVIDIA Brings Content Creation to the Next Level with RTX at SIGGRAPH
Publish_date: 2019-08-02
Length: 161
Views: 9343
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/Jih4Va5kzOQ/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: Jih4Va5kzOQ

--- Transcript ---

hi I'm Lexi Whitmore from Nvidia here at
SIGGRAPH 2019 the biggest show for
computer graphics all week we've shown
thousands of sea drop attendees how r-tx
is pushing the frontiers of creativity
with real-time ray tracing and AI so
artists can quickly create realistic and
accurate design during the show we made
three big announcement first we unveiled
that just one year after Archie X was
introduced it's now emerged as the
industry standard with product designers
architects and game developers in this
short time over 40 applications with our
TX technology have been launched by the
world's top software makers and major
renderers second we announced 10 new
designs for RTX studio laptops and
professional-grade mobile workstation
bringing the total number of our TX
studio models to 27 our third
announcement is new research from Nvidia
attendees at sea Graff channeled their
inner buzz aldrin and did some
moonwalking in our stunning Apollo 11
experience the demo mirrors your
movements on the moon surface in
real-time through its AI pose estimation
capabilities nearby we're showcasing the
exciting work of our award-winning
research team Goggan is an interactive
paint program that uses Ganz to turn
your doodles into gorgeous landscapes
and masterpieces the demo won Best in
Show and audience choice at real-time
live on Tuesday night that's not the
only award our research team is bringing
home on Monday the team won see graphs
Best in Show emerging Technology Award
for their work on prescription AR a
prescription and vetted AR display well
this wouldn't be see graphed without
technical sessions hundreds of attendees
stopped by our talks and listened to
luminaries from across the end
street nvidia was involved in more than
half of the 50 ray-tracing sessions the
team showed off the latest in real-time
collaboration on our omniverse platform
attendees were blown away by how easy it
is for multiple artists to work on a
single project and they can't wait to
get started the highly anticipated beta
release is set for this fall the
ecosystem of partners adopting our check
server is expanding helping users handle
the complex workflows in the M&E
manufacturing and AEC industry our TX
server can be used as a platform for
multiple workloads including
virtualization and interactive or batch
rendering stay updated on all our news
from the show and watch on-demand
sessions at Nvidia comm / C draft 2019
Title: Siemens Energy HRSG Digital Twin Simulation Using NVIDIA Modulus and Omniverse
Publish_date: 2021-11-10
Length: 53
Views: 90937
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/JLboPXn6sKI/hq720.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AHUBoAC4AOKAgwIABABGEUgSyhlMA8=&rs=AOn4CLCNuAhwJBmuN_-8m6N_MJlgUXyeQQ
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: JLboPXn6sKI

--- Transcript ---

this massive heat recovery steam
generator uses hot gas exhaust to
convert water in the pipes into steam
for the turbines
predicting corrosion to avoid downtime
is challenging
reduced order models aren't extremely
accurate and full simulation takes
expertise and time
siemens energy is developing a digital
twin with nvidia modulus for the
multi-phase turbulent flow with point
cloud data to train a physics-based ai
model they can infer high-fidelity flow
shown by the streamlines in seconds
this could reduce downtime by 70
saving the industry 1.7 billion dollars
a year
Title: Physics-ML Predicts Extreme Weather Globally in 0.25 Seconds
Publish_date: 2021-11-10
Length: 42
Views: 118687
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/JnGPxZ9glVk/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: JnGPxZ9glVk

--- Transcript ---

the furrier neural operator physics ml
model predicts the precise path of
hurricanes a full four days in advance
and can provide near real-time updates
as conditions evolve
here we're looking at the massive 2016
hurricane matthew the red line shows the
observed track of the hurricane the
contours show the ai prediction the
black dots show how closely the actual
path of the hurricane followed that
prediction the white cones show the noaa
forecast
being able to accurately predict the
behavior of complex weather systems
around the world is a key step towards
building a digital twin of the earth
Title: Celebrating Robotics in Seattle: NVIDIA Opens New AI Research Lab
Publish_date: 2019-01-11
Length: 147
Views: 21388
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/JT2viTz_0jU/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: JT2viTz_0jU

--- Transcript ---

[Music]
collaborative robotics we might even
call the holy grail of robotics right
now it's one of the most challenging
environments to get these robots to
operate around people and to do useful
things in unstructured environments one
of the most challenging collaborative
domains is the kitchen environment right
now so we've chosen that as a testbed
so we can develop a lot of these
technologies study the system in this
space and take a lot of what we learn
and apply them across the board to these
collaborative domains
many academic Institute's have been
studying a lot of these problems in
isolation these areas of detection and
tracking and motion generation but for a
real-world robotic system all of these
things need to communicate with each
other and do so fluently in order to
make an impact in the real world so we
are bringing together experts from all
of these areas along with software
engineers and the resources of Nvidia to
build a system and study this system at
the system's level on the perception
side we're using a technique we call
pose CNN which was developed both here
and at the University of Washington
which is a deep learning technique for
taking the image of the area in front of
the robot and detecting where objects
are in that space and then that
information is fed to a technique we
call dense articulated real-time
tracking which is an optimization based
tracker developed at the University of
Washington which gives us continuous
perceptual feedback of where those
objects are at any moment in time and
that information is fed then to the
motion generation layer which uses a
system we call romani motion policies
which is able to get the robot to
generate real-time fast reactive
adaptive and human-like motion in these
collaborative spaces one of the things
that's great about romani emotion
policies is that when it detects
obstacles or movements of obstacles
it just has to bend the existing
behavior we're developing the system as
a model of what other groups can use we
intend to share this with the broader
research community so they can build
their own research on top of it
you
Title: SHIELD Quick Start Guide
Publish_date: 2015-10-19
Length: 154
Views: 21936
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/jwcqNoLPcgw/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLCzD9DBfMyEjIc7DhgqEcHZ0Kx3HA
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: jwcqNoLPcgw

--- Transcript ---

we're gonna help you get started with
your new Nvidia shield by walking you
through some of the basic functions
let's start with the simple set up just
connect shield to your TV with the
included HDMI cable and AC adapter
shield is a 4k device so it works out of
the box with any ultra HDTV once plugged
in the shield will automatically boot up
then prompt you to connect to either the
included controller or the optional
shield remote press the center button on
the remote or tap the Nvidia button on
the controller the controller will blink
quickly and then display the default
language page just scroll down the list
using either the d-pad or left analog
stick until you see the language you
want then select it by hitting the a
button you'll want to connect shield to
your Wi-Fi find the correct network and
follow the prompts to enter the password
if at any point want to go back to the
previous menu press the B or back button
on your controller you can use an
Ethernet cable to connect to your
network directly instead of using Wi-Fi
next link your Google account to access
all the movies music and apps available
from Android TV if you're using an at
gmail.com email you don't need to type
that part in if you don't have an
account you can always create one by
going to accounts goal.com slash sign up
on your PC or mobile device Android TV
is organized into a series of rows the
first row will contain suggested content
based on your viewing or playing habits
just select any of the tiles to watch
you can also adjust a system volume
directly from your control
you'll most likely see a shield software
upgrade here as well you definitely want
to run that right away to update your
system with the latest features and
enhancements the second row is shield
hub where you'll find tiles to stream
and download games and watch Netflix
your shield comes with google apps like
movies and TV YouTube and music go to
Google Play Store to find popular apps
like Hulu and Pandora or do a quick
search to find many more you can always
tap the Nvidia button to do a voice
search
Red Bull TV the last rows for settings
there you can access the settings menu
change your Wi-Fi add accessories or
power down you can hit the home button
at any time to return to the home screen
you're now ready to enjoy shield
Title: AI Conference 2018 Australia Wrap Up Video
Publish_date: 2018-09-17
Length: 140
Views: 32928
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/K1l1CruzNyM/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: K1l1CruzNyM

--- Transcript ---

[Music]
hi I'm John Gillooly and I'm coming to
you from the Sydney International
Convention Center where we're wrapping
up the two-day AI conference for 2018
[Music]
on day one we had almost 350 people take
part in deep learning institute courses
covering everything from finance to
computer vision
Takuto on day 2 which kicked off to the
keynote speech from nvidia
vice-president for solutions
architecture and engineering Marc
Hamilton we had almost 600 people attend
track sessions to learn more about deep
learning in the 3 track sessions
attendees learned about large-scale AI
and research AI for enterprise and IV a
autonomous machines and robotics these
attendees came from a wide range of
areas like finance retail energy and
construction than academia all coming
together to hear more about the latest
developments in deep learning attendees
also had a chance to meet with our
partners ranging from some of the
biggest IT companies in the world down
to the latest and smallest startups
attendees had a chance to check out
Nvidia's holotape a vr collaborative
environment where they could check out
the architectural bottle of the new
building at the Nvidia booth we were
showing off a range of AI technologies
including zgx station and a cpu cloud
products these are designed to put super
computing power into normal people's
hands we also had our latest Quattro
r-tx GPUs based on the just announced
cheering architecture thanks to the
power of AI and real-time ray tracing
this is going to forever change computer
graphics we heard success stories from
local partners like Monash University
University of Queensland Mac's Kelson
who are doing amazing things locally
with deep learning and with that the AI
conference is coming to a close please
join us again next year when we show off
the latest advancements in the world of
a
[Music]
Title: Multiple Artists, One Server: Enhance Production With NVIDIA EGX Platform
Publish_date: 2021-05-12
Length: 298
Views: 9234
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/kdCkp_WW-iI/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: kdCkp_WW-iI

--- Transcript ---

A VFX shot like this is created through a 
collaboration of multiple artists that bring  
different skills and tools to the project. 
Each of the artists builds upon the work of  
the others creating elements that all interact 
with each other. We see how changes to the lights  
and displays affect the environment and all the 
little graphical elements on the displays that  
bring additional light to the composition. There's 
also just enough dirt and dust around to add to  
some of the realism. Scenes like this one are built 
from many large files, often requiring hundreds of  
gigabytes. When multiple artists work on their 
own systems, often from home, the long download  
and upload times for those files start to add up, 
hindering productivity and collaboration.   
Thanks to the NVIDIA EGX platform, the scene we're looking 
at was created in a completely different way. 
All the artists work in a different location on 
the same machine at the same time. The EGX platform  
eliminates tedious file transfer bottlenecks 
and provides easily configurable hardware   
to help artists accomplish their unique tasks. 
Let's take a look at how we did this. Each of the three  
artists created a different element using tools 
that are designed for the tasks that they're doing.  
There are many apps that can be used here, but 
for this project, the 3D artist uses Max on  
Cinema 4D and OTOY Octane to model and create the 
environment and set the mood with the lighting.  
The motion graphics artist uses Adobe After Effects 
to create the animated content on the main center  
and background displays. And finally the compositor 
uses Blackmagic DaVinci Resolve Studio to assemble  
all of these pieces together and to add more 
details to create the final look. Powered by the  
NVIDIA Virtual Workstation Software, each of the 
artists remotely accesses the NVIDIA EGX platform  
to work, enabling a distributed production. They 
create in a virtual workspace also known as a  
virtual machine or VM for short. Each VM can have 
a different amount of GPU and system resources  
assigned to it so each artist gets exactly what 
they need for their work -- whether it's just a part  
of a GPU or multiple. Although the tools that 
we're using here take advantage of NVIDIA RTX  
to speed up the creative process, Octane is an RTX 
accelerated renderer that becomes more interactive  
with more GPUs. So for our project, we'll allocate 
half of our GPUs in the system for that artist. 
Resolve leverages AI and CUDA acceleration of 
our RTX GPUs to deliver real-time effects so  
we're going to give it two GPUs. After Effects only 
needs a small amount of GPU so we're only going to  
give it a fraction and have the rest available 
for more artists for future rendering tasks.  
All of this is easily adjustable through NVIDIA's 
Virtual Workstation Software so as the production  
needs change, the system can adapt with just a few 
clicks to update the VMs. Let's see how the artists  
are putting each of their VMs to work. Starting 
with the motion graphics, we can see that the  
artist is making animated and stylized content for 
the final shot. Once the animation is established,  
the artist arranges it into elements so they 
can be added into the hanging cluster of TVs
during the composition. All of this work is being 
done in Adobe After Effects which utilizes the  
video encoder built into the GPU to take exports 
from hours to mere minutes. Since the artist is  
accessing the EGX platform through VMs there are 
even more production benefits here. Artists can get  
fast and secure access to files because the server 
is in the data center right next to the storage.  
Additionally, since the artist can interact with a 
compressed and encrypted view of the desktop,  
the content never leaves the data center. All artists 
working on the same project have immediate access  
to the same files and productions can keep their 
production IP secure. As the motion graphics are  
being created the 3D artist is bringing the base 
of the scene to life with materials and lighting. 
Here they're working with Octane to leverage 
NVIDIA's Ampere GPUs to render scenes up to 
two times faster compared to previous generation GPUs. 
This speedup delivers an interactive experience  
that saves time so the artist can make many more 
creative decisions. When the lighting is just right  
it's time to render. The NVIDIA EGX platform can 
help productions also stay on schedule because  
any unassigned GPUs can be used for rendering in Octane --  
or any queue manager. When the artists are  
done working for the day a simple logout frees 
up their resources to render until they come  
back the next day and start creating again. While 
the platform renders and continues to provide a  
workspace for the motion graphics and lighting, 
another artist is working on bringing rendered  
sequences into DaVinci Resolve Fusion to composite 
them together. Compositing brings all the elements  
together in an adjustable composition so the 
artist can refine to get just the right look .
Compositors fix any issues with the images, apply 
color corrections, change the look of the lights,  
and add  atmosphere like smoke or dust to the scene 
to get just the right look all without needing to  re-render. 
During this stage it is really important 
to be able to see these details and colors clearly  
because this is the last stop for final 
adjustments. When artists work on the EGX Server,
the experience is the same as working on 
a local machine so even the smallest grain of  
noise can be found and fixed. A VFX sequence like 
this is often one shot in a long list of curated  
visuals that are used to tell a larger story. 
These stories can take an army to bring to life  
and the EGX platform's flexibility helps studios 
meet production timelines and goals confidently.  
It's scalable, secure, and completely configurable 
so studios can enable artists to play to their  
strengths no matter where they are, or what tools 
and what hardware they need to get the job done.
Title: Deep Dive into Omniverse Kit Introduction | NVIDIA Omniverse Tutorials
Publish_date: 2020-12-28
Length: 1889
Views: 22170
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/Kdj_ry1mvmM/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: Kdj_ry1mvmM

--- Transcript ---

welcome to our video about omniverse kit
sdk
in this video we will review the high
level concept
of building applications using omniverse
kit and how to get started
omniverse is a platform to simulate
reality
it's connected with many of the
applications that are used in the
various industries it used a nuclear
server to bring collaboration to the
platform
kit is a toolkit built on usd to create
apps to solve your problem
for example build robotic editor
scenario editor for self-driving car
it use python for scriptings and has a
core in c plus plus for efficiency
you benefit from all our great
technology at nvidia from ai to vr and
physx
that are bringing to the simulation
platform they include the
state-of-the-art renderer
real time fully raytrace multi-node
multi-gpu to take advantage of our tech
here is a quick overview of the stack
for omniverse kit
at the bottom you can see what comes
directly with kit
that include many sdk to do physx
rendering compute graph and many more
all of those coordinate the data that is
natively stored and manipulated
using usd above that you have access to
many core extensions we are building to
augment the platform
finally at the top you can see the
omniverse apps
they are composed using those building
blocks into purpose-focused apps
you of course can bring your own code
and build extensions to put into the mix
to make it all these less abstract i
thought i would go over an omniverse app
and show you how it's constructed this
is omniverse create
it is an omniverse app for content
creation that was used to assemble the
marble rtx demo that hopefully you've
all seen by now
it contained the general functionality
that you would expect from an
application of that type
but it is not built as a monolithic
applications
let me show you how instead of being one
application it is made of extension
those extensions are kilt
building block that you can assemble in
many ways to create different types of
app
at the higher level they are all written
in python to assemble the ui the
workflow and the general feature set
when required or for performance reasons
or to access some c plus plus api
c plus plus plugin can be added to the
extensions and we use
binding to connect with the ui those
extensions also include their icons
images configurations
all they need to run individually
let's go over some of the core one now
the rtx viewport extension a leverage
omniverse rtx renderer
to provide an incredibly beautiful view
on your data leveraging all of the rtx
technologies
from nvidia's gpus it is scalable
from real time even on large scenes and
accurate
using ray tracing or pi tracing mode it
leverage
mdl materials to be physically accurate
the geometry are read directly from usd
and your assets
require no modification to render in
real time
the omniverse content browser extension
it provide a great way to boost your
omniverse nuclear server organize your
data and find the files you want to work
or collaborate on
alongside showing your omniverse servers
you also work with all of your local
data
so you don't always need to be connected
to work there is a rich set of api
that enables you to add your own actions
to the files
powered by diptag the omniverse server
are
searchable in new ways using ai
classifications
but these two is modular based on
extension
again build entirely in python mean that
you have
full access to its source code you can
use it to learn
and build your own connect with your own
servers add new file types or icon
also it is not itself monolithic a core
principle of kit
is extreme modularity so we always look
for opportunity to build small building
blocks
that we can reuse here you can leverage
the nucleus file path fields
the file grids or this tree view
independently
and use them into your own extensions
even the search
is added as a separated extension
connected
to the nucleus ai indexing services
we have a great collection of usd
widgets and window extensions that you
can use to work with your data
the stage window extension leverages the
stage widget extension
to build a rich browsing experience of
your stage data the stage window
contains
all of the informations on the object in
the scenes that enable you to manipulate
them
the property window provides you access
with all of the informations on the
object's attributes
and the various informations about usd
it is also
fully extensible and all of the sections
into it
come from dedicated extensions that are
targeted toward
different primitive types and then you
can build your own
finally the layer window gives you
access to a powerful layering system
that usd provide
for rich compositions but also a great
place to find omnivorous feature around
layer management
and live collaborations ultimately
all that you see in create and other
omniverse apps come from extensions
they are the atomic building block of
the omniverse apps
here are some examples i picked to give
you a sense of the things that you can
leverage into your apps
but also use them as example for
extensions that you want to build
as again all of those are written
primarily in python so you have full
source access
on the left to right a toolbar example
that we use to manipulate the usd scenes
context menu frameworks for you to build
your own context menu
profiling data that shows you what's
going on in your scenes and the
performance you're seeing
a ui for picking dates a great graph
framework that we'll
talk a little bit later into the
presentations here's an example of movie
maker so you can capture great
cinematics of your scenes
even the menu bar is customizable and
extensible for your own needs
this is just a small set of what we are
working on to share with you
for our beta release of omniverse kit
let's do a recap about the kit
extensions they represent the building
blocks of any kit applications
in the system almost everything is an
extension
they are version control so you can load
different version
at different times they can depend on
other extension
so you can build complex tree of
extensions
and then have the right behaviors
happening automatically
they're also hot reloadable which is
really great for development
as we see in some of our other
presentations
with kit we provide a great extension
manager
the extension manager enables you to
browse all of the extension in your
system
for there you can see their different
names and their properties
you can search for them and for each
extensions you get detailed informations
on its dependency
its change log and some of its important
properties
the extension manager is a great place
for you
to start discovering applications other
people have made
you can find their source and their
descriptions
inside these applications and so you can
start using them to build your own
here we have a large collections of
extensions
the viewport the graph view the staging
layer windows
and many more scripting engines the
documentation is showing there
your app is really ready to get some
users
let's talk a little bit about our ui
system omniverse ui
when we started the project we wanted to
have a really fast hardware accelerated
ui
that would be lightweight and open we
based our framework initially on the
dear imui library
some of the key features that we're
going after is to have a modern fast and
lightweight ui framework
it is the foundation of the omniverse
kit user interface
we wanted our ui to be declarative and a
very dynamic layout
it has to be fully styleable using
html-like stylesheet workflows
we also have implementations for
featuring
um ui streaming with low sui qualities
and then working on xr vr and ar
rendering to project those widgets into
the 3d world
in terms of platform we are supporting
full c plus and python using bindings
and eventually potentially other
languages
we support linux and window and for
those we have direct excels
and vulkan and then vulcans on window
and linux
we have other platform plans like on mac
or arms
in the planning here's what the stack
look like
we already discussed about drm gui being
one of the pieces
at the lower level the graphics api we
have level of abstractions for graphics
input and widowing that enable us to be
cross-platform and cross-rendering api
and then on the top we're building our
omni ui c
plus library to enrich all of these
i'm gui widgets and all of those are
python binded
so you can interact with them completely
in pythons leveraging the full speed of
the c plus plus api
you get access to a rich documentations
these
extensions omniverse ui documentations
is built entirely as a kid applications
using
omni-ui as the widgets there's no html
or other language here
everything here that you see is omni-ui
it enabled for fully interactive
documentation where all of the buttons
and sliders and images
do the things they do in your app but
also represent an incredible sample
for you to look at how to build these
things and so whenever you see something
in the documentation that you like
there's a source file that includes the
entire python code
that will let you know how to do that
okay let's talk a little bit about the
ui declarative syntax
here on the left you see the code and on
the right
the window is generated only kind of
highlight couple of really interesting
items there
we dynamically declare on the stacks
those stacks
um so so they are easily sort of seen
probably something that you've seen in
swift ui or maybe just html
so with the vertical stacks then we go
into the next block
we create a z-stack so we have layers in
depth
of stacks where we can create an image
here this splash screen
and then i've layered it on top of it
with a ui label
to put this little create label and then
i've used a placer that enables you to
place items anywhere and those placer
have loads of interesting features
around being draggable and so you can
build really rich and interactive
ui you put spacer there's different
types of pacer
that could sort of be dynamics as we go
down we have the styleable widget so
here
we're creating this simple declarative
ui label
and then we make a font size of 18. the
styles are cascade
like like in styling sheets so so they
could come from the base classes and
they could go down
to your ui again a separator an
horizontal stack so h stack horizontal
stack
with two buttons and here we see the
simple single line declarations where
none of these variables have to be
captured ui buttons name of the buttons
some actions that's it that's all you
need for a button and if you want some
style
you say ui button style and then you add
a style
some background color and here some some
label colors
um so if you go down you have these
model base widgets
and so sliders and and fields and all
these things
they're built on models um they enable
you to have really rich
binding between your data and then the
sliders um
and then here's a more advanced ui here
we see again a ui that has
a little bit of a data provider and so
it build dynamically
so you know 20 30 lines of code on the
left
and then a full reach window on the
right all of these
again fully auto reliable you know when
i build that samples i just you know
move things around
press enters and this image recreates
itself constantly
so really interactive experience for you
here's some documentation from the style
just want to highlight that
the style sheet is is very very flexible
it's really similar to what you'd see
in something like html you create your
styles
they will cascade through the stacks and
the layouts and
they can really enable you to do loads
of different things they have
style for over and click and enabled and
then
different types of styling so they
really provide you a rich way
to make your applications be very unique
and also it's very quick to do some
simple coloring and styling for your
item so so you really have a good place
to start
on working with your designer to build
incredibly beautiful apps
here is an example uh internally where
we really sort of
use that to the maximum and the team
that build the vue app
build on kit wanted to have a more sort
of
you know look that much the ac
experience and so
they wanted to be more like white and
these buttons and color
so work with the designer and then they
have a distinctive looks where they
could use these styling sheets and omni
ui
to build very rich and distinct
experience
for their customers in in the app
some example here were just to show
breadth of the type of widgets that you
have
but everything that you'd see but but
again they're really simple to
instantiate it
dropbox you have sliders you have fields
that can be draggable
again here we see the different styling
that are applied to these labels all
these different things
you have those collapsible frames color
picker
all of these really easy entirely built
in python
again available in the sample so you can
pick them up and go from there
there's a really great tree view it
serves as a tree view but you can use it
for list view as well
and table view and so it's very flexible
system
that use an abstraction of model view
controller
so you have your model so you can write
your own model that binds to your data
to here see
we show the stage but you could show any
types of lists
or trees and then you have a full
control over the look of this widget
and so it's not really just a other
widget can have an icon and a label
and and you have a fixed number of
columns or things like that it's very
much
programmable where every single cell in
that table
will have callbacks that enable you to
create anything these widgets can have
sliders they can have buttons images
anything you like really you're in
charge of creating what's the look of
these cells
so very flexible very fast we use it to
do search and
display very large number of items
we have a really great graph framework
and again here
the extreme modularity of our platform
was really a key design principle
the entirety of the graph framework
extensions
that is available in kit is written in
python so you have full source access
you can look at it you can hack it you
can you know improve it do anything you
like with it
you really have a full access to it here
a couple of examples of the
things that we're using it internally on
the left side you see
that misti demo that we've seen in some
of our gtc demo
that's the brain of misti using the
omnigraph and then the ui to represent
these nodes
on the right hand side is some of our
designs around building
material graphs and mdl editors and at
the bottom
some of our proceduralism around compute
graph
one really interesting aspect of the
graph networks is
it has you know two really interesting
api entry points
um it's a kind of uh i call it bring
your own model and look
so the graph framework is around
coordinating the nodes providing you a
platform for all of the things that you
need to do
using graph framework that are generic
and then
simple api to use again in python to
implement your own
model and so the backend what's what's
your where where where is the list of
nodes
what's their name what's their you know
children's how they're connected
all of these informations really belong
to the type of graph that you want to
build
so you have models abstract models that
you implement and then you bring your
own model
into the graph system there is a default
look
and that's the look that we've seen into
the previous slides
for the node but here again you have
full flexibility to write programmable
on the look
so here i just want to show a little bit
of how that structure
the nodes themselves are templated nodes
with you know entry points
so you have an old background you have a
node header
you have node ports you have a node
footer
each of those can have plugs on the
sides where you can connect
different types of informations all of
those
virtual and virtual functions that are
you know i have a base class implemented
that we provide a default look
that you can you know use directly if
you don't need or just
use as a reference to build your own
again it's fully in python so it's
it's completely accessible um and then
because all of those are
these functions you can start doing
really interesting things where
you can only implement some of them so
here on the left hand side we see some
examples where
only the header and a single port were
implemented so you have these standalone
widgets
you see somewhere the headers has been
enriched with an image preview
you can see somewhere the ports actually
have widgets in them
and all of those although they're built
and set up in python
they're rendered using this really fast
hardware accelerated
rendering back end and so they can
render a large amount of nodes
at really fast performance without any
speed
delay here i wanted to show some of the
example of some of the research that we
did initially
there's you know a lot of nodes in the
industry and many of them really
beautiful here the autodesk
bifrost one the latest uh from maya
really beautiful design nice nodes so we
wanna see hey can our
framework enable you to replicate that
type of structure and so we see on the
right hand side omnigraph
python definitions describing those
nodes showing how to kind of create the
label
and the color and the placement of some
of these things to replicate the design
that was
given for the autodesk we we then
you know ask our designer to work on
some interesting design that would be
unique and specific to
our application platform and so we see
at the bottom left
this design from adobe xd photoshop
xd whatever like the flexibility is
there that the designer
creates their own look they decide where
they want to put things they obviously
have some constraints because
they're not but very few and then on the
right hand side
that's the implementations of this
design in code
directly into these delegates that are
transforming
the different function calls into ui
elements and then we could
be able to do a really close one for one
match
into those and then again bring that
flexibility that
this code could be different per nodes
and very programmable
let's discuss a little bit about
building microservices
using omniverse kit there's really two
main ways to build microservices using
kit
first the nucleus-based microservices
nucleus represents a coordination point
in between a lot of different
applications
all of these applications can in real
time collaborate into the data
modifying it from different places so
every users can change the different
pieces
at the time and then you can integrate
in that flow kit applications and we
discuss how they can be created headless
with users or no
using any of these extensions that
you've made available that will in real
time
modify this data as well so in that
sense kit
a kit application connected to nucleus
using the live synchronizations become a
micro services of that ecosystem
and then you have the rich power of
building apps that can contribute to
that
the other ways that you can build is
we've also seen that kit could be a very
lightweight small runtime effectively
not very dissimilar to
to python and it's a small thing that
they will run apps
um and then we can leverage that
including a control port
layer that we've integrated inside an
extension
that will enable you to use you know
type of remote control
behavior using http or sockets or any
type of transport protocol
and then connect directly into
lightweight headless apps
that can run in containers be deployed
in kubernetes you know
they run on localhost as well they use
open api to be able to describe those
those api front-end uh very easily and
and with all of the tooling from the
industry
these control port extensions that that
ship with kit
is is very simple all written in python
of course you've
heard the themes by now and it's sort of
layered on top of
open source technology to build a very
simple ways to connect
apps to a kit micro services here on the
left we see what would happen into a kit
server
one of these app that runs on kubernetes
let's say that exposed
entry point and then the extension would
just say
give me the control port register an
entry point and so i want a registered
entry point when someone called me
with file copy with some payload you
know i want to trigger
some actions python functions whichever
that can leverage
any of the feature that are integrated
inside
the kit sdk we also have a small client
in python that leverage some of these
things but but generally
from any client anywhere where you have
python you are able to just import a
request
and make a simple http request that will
poke into that endpoint and then grab
the data from a kit running remotely
and so that enables you to really
natively connect
kit in the server running as a micro
services on any type of infrastructure
and any type of client is able to
connect to it remotely
and then leverage its power we've built
internally really interesting micro
services that extend this concept
because of course you're not limited to
just run headless and with some of these
you know cpu-only microservices you have
the full access
to what kit can do in terms of
leveraging the rtx rendering extension
physx and many of our other ai
simulations framework
and so you're able to generate large
amounts of data
as a micro services deployed on the on
clouds or farms
and it's been really really interesting
and i think will be really interesting
to
see what you can build with this
functionality here again there is a lot
more to discover
and learn about micro services make sure
to watch the deep dive video we prepared
for you
i want to discuss a little bit on how
you have access to all of these
sdks using omniverse kit
we really bring all of the technology
from nvidia and other places
inside kit so you can access them
directly from your app
of course usd is fundamental to the
applications
it represents your data all of the data
is stored in usd
and natively inside the application this
is the mechanism to modify it
it leverage an industry standards that
has been beyond
only media and entertainment but as
reached now automative industries
and other types of verticals and
constructions and manufacturing
it represents a great flexible open
format
that we can all leverage to build
applications that can talk with each
other
of course we discuss many times of the
rtx technology and the rtx viewport
it brings into your app an incredibly
fast and scalable renderer
that means that you can render beautiful
images of
great complexity with many lights here
an image of the marble at night
cinematics that
was shown a couple of months ago all
rendered
inside kit using the rtx viewport
technology
but it's not just the rtx viewport that
you can use you can really use
any viewport at the core we're
leveraging hydra
which is an abstractions on the viewport
technology
to communicate between usd and then the
values renderer
leveraging hydra and the idrive viewport
you can bring different types of rounder
into kit and contribute to this
ecosystem
here in that screenshot i showed the hd
storm
gl runner that comes with pixar but
using the similar type of framework
you could bring the um hydra compatible
rounder
that is made for arnold or pixar
on the man or maybe your own in your
company or the owner you've developed
and so you can bring your own rendering
and then you can contribute to the rich
ecosystem
of the other extensions via hydra and
usd for your data back-end
in order to provide material definitions
that are open
and in supporting into many of these
renderers mdl was provided
as an open source project by nvidia a
few years ago
we continued to invest in that
technology greatly for all our apps and
the rtx rounder fully leveraged
its power to build the physically
accurate image that you've seen
we've also contributed the usd schema
that enable you to
represent these mdl file natively in usd
so they are able to be encapsulated into
this file and moved
from different router and we constantly
work with all of the different
industry player material x and osl to
consolidate to have an open shading
format
that enables us to work on great
technology inside
our app and of course we have all of the
nvidia simulation sdks
that are you know been built and
provided through the omniverse kit
platform
of course physx that provides you great
accurate
and fast physics simulations
recently announced and shared with the
general public
audio to face the incredible technology
that converts
audio signal into face shapes so you can
have character talking automatically
a lot of simulations are on fluids here
we see some image from floor but we also
contributed recently
the nano vdb framework that enable you
to see
large amount of uh volume data rendered
in the gpu at incredible pace
and then a lot of work on ai pose
estimations and different things that
will
come to to be you know shared with you
through
the kit and the various sdks
now that you have all of these sdk let's
see some apps like let's see some
example
of what was built using omniverse kits
and those sdks and those frameworks into
something
that is useful to the end users
here again is back to these simple graph
apps and
wanted to show you that you can build
simple apps with few extensions
so here is an extensions an app that was
built with graph extensions that we've
written
internally to share with other teams how
to build graphs
um it is just an extension for the graph
couple of small extensions for listing
nodes and components
and it serves as a simple example that
you can build from
here is another example of a great app
the isaac c-map
it is built on top of the kit sdk here
we can see at the center that it
leveraged
the beautiful imagery that the rtx
renderer is
generating it is built around usd for
the data
organizations and connections of these
robots it offers skews
and connects to the isaac simulation
framework and then is able to leverage
all of the ecosystem of tooling in the
kit applications
to provide a great app for our robotics
community
vue is one of these other great kit app
that we build and shared with the
community
not so long ago it is built using the
sdk
and leverage this great customization
that we talked about using the omniverse
ui
wheel styling to build really nice and
unique interface
it leveraged all of the connectors that
bring the data from applications like
revit or rhino
into the omniverse and the rtx rounder
to
visualize in real time beautiful large
scale rendering environments
audio to face that has been shared
recently again here we see the app
streamline kit experience that use
few widgets focus on showing the
view of the character talking and be
able to be represented there
some of the informations very focused
app
few extensions the power of the rtx
renderer
and the ai framework directly into your
app
and then back to what we discussed at
the beginning omniverse create
of course is built with the kit sdk it
was you know
used to create the incredible marble
game in the marvel at night rendering
here we see it using a lot of these core
extensions
that we've built and tr and used a lot
to refine and perfect
so we can share them with you as
building blocks that you can use your
own apps of course you can use create
to do incredible things and build art
but you can also use those different
building blocks
to then build your app so here i want to
just
try to complete a little bit that
presentations with a recap on few things
that i really want you to take away with
you
first omniverse is really built on a
flexible
extension system at the core it's
building blocks
that you assemble into apps that make
sense for what you want to build and you
want to create
we provide a rich set of extensions that
is been
you know production used and driven that
are served as
example or building blocks ready to use
for you to use in your
app so you don't have to start from
crash
and finally in the ecosystem
you get access to a large amount of sdks
that are really empowering you to build
great physics apps you know beautifully
around their apps
things with you know ai renderings of
fluids
work on on you know ai simulations pose
estimations
and all of these sdk come into life into
kit
you know for you to be able to use them
in a really easy way
and compose them into your app i am
really looking forward to seeing
what you will build with omniverse kit
you
Title: NVIDIA CES 2017 News Recap
Publish_date: 2017-01-05
Length: 126
Views: 8418
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/keSzoA-Ctuo/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: keSzoA-Ctuo

--- Transcript ---

hi this is Danny Shapiro from Nvidia and
we're at CES 2017 in Las Vegas we just
concluded the opening keynote of the
world's largest Consumer Electronics
Show in one electric our our CEO Jenson
Wang talked about how NVIDIA has the AI
computing company is revolutionizing
everything from gaming to home
entertainment to the future of
transportation first he unveiled our new
GeForce now service it'll enable a
billion new consumers to
high-performance GeForce GTX PC gaming
experiences streamed from the cloud to
their own PC or Mac computer if he
showed off our new shield TV it streams
TV and games from the cloud and provides
amazing AI capabilities accessible
anywhere in the home with the new spot
AI microphone it's the best most
complete entertainment experience
anywhere and in the automotive space we
announced a series of important
partnerships based on our Drive px 2 ai
card computers they also described new
partnerships with two of the world's
largest automotive suppliers Bosch and
ZF they're working to take our Drive px
2 technology into production for
manufacturers to build self-driving cars
trucks and other vehicles
he also unveiled that we're teaming up
with two of the world's most important
mapping companies in addition to our
previous announcements of Baidu and
TomTom we're now working with here and
zenrin in Japan we'll work together with
all of them to accelerate global HD
mapping and ensure seamless integration
with Nvidia Drive DX systems all over
the world all in all it's clear that
NVIDIA has moved to the center of the AI
world bringing deep learning to the home
and to the car if you're in Las Vegas
come check out our booth in the North
Hall where you can see this technology
for the home
and the new AI car we're also go for a
ride in bb-8 our self-driving car where
there's nobody behind the wheel
[Music]
Title: NVIDIA CEO Jensen Huang’s Teratec Keynote: The Industrial HPC Revolution
Publish_date: 2021-06-24
Length: 1090
Views: 396887
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/kirkvFf5ytI/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: kirkvFf5ytI

--- Transcript ---

[Music]
the universe of supercomputing
is expanding in all directions
simulating and visualizing what we could
not see
finding meaning and vast expanses of
[Music]
data
and tapping into ai to accelerate
life-changing discoveries
[Music]
for us all
[Music]
welcome to terror tech i'm delighted to
be with you today
to talk about high performance computing
a topic close and dear to my heart
hpc is an essential instrument of
science
helping scientists better understand our
world and universe
and though several industries have
benefited profoundly from hpc
they are the exceptions we believe a
confluence of advances have put us at
the inflection point
in the beginnings of the industrial hpc
revolution
when we hear about the digital biology
revolution or the future factory
revolution
or the digital twin revolution the same
fundamental dynamics are at play
my talk will highlight the dynamics that
will drive
the super exponential super moore's law
advances in computing
that have made hpc finally
a useful tool for industries it will be
followed by a discussion of how hpc
systems
will be extended to serve the diversity
of applications of industries
let me start with our own industry and
how it has been wholly transformed
by high performance computing nvidia's
success
is partly due to what we build but much
has to do with how we build it
nvidia codified early on the philosophy
of building our chips
fully in silico and only sending to fab
what we believe will be perfect nvidia
simulates
everything and invests whatever it takes
to enable that to happen
we've invested billions in our chip
design infrastructure
in hardware and software to simulate our
chips and systems
simulation lets us find problems at the
least
costly phase of a project and ultimately
get to market sooner with a high quality
product
but it is much more profound a
simulation culture
naturally forces concurrent hardware
software co-design
and naturally aligns internal teams a
simulation culture
has profound implications throughout
every aspect of product development is
integrated
failures and results are reproducible
and continuously improving
with all past learnings captured in the
tools and flow
the compounded benefit over time is how
we scaled and moved
so fast in 1997 riva 128
was a 4 million transistors chip it was
built
by 50 engineers 23 years later
a100 is 15 000 times
larger and took 5 000 engineers without
simulation
it is impossible for the film industry
to bring us to galaxies
far far away in the rise of skywalker
industrial light magic simulated
millions of gallons of water
on tens of thousands of servers cgi let
ilm
create impossible shots it's clearly not
possible to summon the spectacular waves
they simulated
but cgi has opened up so much more the
ability to shoot from any angle
iterate until the shot is perfect and
reuse the algorithm whenever an ocean is
needed
the creative freedom is astounding if a
director can dream it
cgi can make it happen simulation has
transformed e-commerce
media the internet there are trillions
of items
and billions of people and join the
internet
on just a small four-inch screen
predicting user intentions and
preferences
is essential to connecting users and
items
by learning tens to hundreds of
attributes for each product
and tens to hundreds of attributes for
each user
internet service providers use
hyperscale data centers
running recommender systems to predict
your
implicit preferences recommender systems
effectively simulate your preferences
and predict what you'd enjoy buying
reading or watching
data comes from every possible
interaction you have with their services
recommender systems are some of the
largest scale commercial hpc systems
and they run continuously some 30
million servers are deployed each year
into hyperscale
and cloud data centers a significant
number of these
are learning predictive features and
user preferences
each of these industries have been
revolutionized by hpc
but industries that make products based
on physical and biological sciences
have problems that are too large and
complex to properly capture in
simulation
to aid in creating actual products
whether to decode
and understand biomolecules discover
novel drugs
find more sustainable food and fuel
sources or
safely operate autonomous machines to
make and deliver goods
industrial hpc can only be widespread
when companies can design and stress
test
the entire product in simulation there's
a threshold of scale that needs to be
achieved
until now the scale of simulation
possible in hpc was simply too small
to be useful to industry as the saying
goes
having the tallest ladder is useless if
you're going to the moon
we need a rocket two computer science
breakthroughs have increased
hpc simulation scales dramatically the
first is cuda gpu acceleration
let's see the rate of progress by
examining the scale of simulation
scientists are doing with namdi namdi is
a molecular dynamic solver
that is used to simulate chemical to
protein interactions
for virtual drug screening or simulate
protein folding
to predict its 3d structure in 2006
uiuc accelerated namdi with nvidia gpus
and did a 20 nanosecond simulation of 1
million
atoms 20 million atom nanoseconds
today with gpu acceleration researchers
are simulating
1 billion atoms for 500 nanoseconds or
500 billion atom nanoseconds that's
one million times larger in 15 years
moore's law would have been a thousand
times in fact the largest top 500
supercomputer
increased only 400 times in that period
gpu accelerated computing drove super
exponential growth
this was achieved because of cuda gpus
multi-gpu systems
advances in networking and full stack
optimizations
this fully integrated approach to
computing is what we call
gpu accelerated computing still we're
only approaching
microsecond time scales we need to
simulate at millisecond time scales to
observe
important cellular processes like the
covet 19 spike protein in action
and second time scales to observe
protein folding actions
that's another three to six orders of
magnitude larger
and would have taken another 10 years to
achieve even with gpu accelerated
computing
then deep learning arrived deep learning
models are universal function learners
its effectiveness and the accessibility
of nvidia gpus
have attracted scientists globally to
join deep learning research
we're seeing gigantic breakthroughs
regularly ai model sizes have increased
four orders of magnitude in just four
years
to over a trillion parameters now we
expect a hundred trillion plus parameter
models in a couple of years
for a sense of scale the human brain is
about 150 trillion synapses
researchers at doe labs use deep
learning with namdi to simulate
305 million atoms of the covet virus
over one millisecond time scale to
observe its spike protein in action
in the past 15 years we went from 20
million
atom nanoseconds to 305
million million atom nanoseconds
a 10 million fold increase
gpu and deep learning have given hpc
super exponential growth industrial hpc
activity has noticeably jumped since
deep learning
particularly in digital biology and drug
discovery
financial services manufacturing and
transportation
leaders see the inflection coming and
are jumping on the platforms now
in anticipation of super exponential
advances
researchers at serbon university worked
with jensey
crns and nvidia to gpu accelerate tanker
hp
a large-scale molecular dynamic
simulation they simulated 38
microseconds
of the covavirus spike protein in action
a result researcher jean-philippe said
would have taken years
or a few million cpu cores to achieve
six leading pharmas are starting to use
tinker hp for direct discovery
transformers a breakthrough ai model
that learns
sequence patterns in parallel has
achieved incredible natural language
understanding results
google's burt openai's gpg3
nvidia's biomegatron are examples
language understanding technology has a
potential to profoundly impact society
by democratizing computing giving
everyone access
gen c and the big science open
large language model collaboration
project is bringing together
500 researchers from 45 countries to
develop an open source language model
for industry and scientific communities
the transformer
is not only revolutionary for language
understanding
these models are also useful to learn
the syntax rules of smiles
the language of describing chemical
structures the language of chemistry
nvidia and astrazeneca developed an ai
model that could be used for drug to
target
reaction prediction molecular
optimization
and de novo molecule generation it was
trained on the zinc
chemical compound database of the
billion commercially available drug
molecules
physical simulations of fluid flow are
critical to understanding mechanical
systems
from designing efficient turbines and
windmills to designing modern data
centers
geometry aware multi-physics
computational fluid dynamics simulations
can take days to complete limiting the
scale of the design we can explore
nvidia researchers developed simnet a
physics
informed neural network simnet is an ai
driven multi-physics cfd simulation
framework
an ai model that obeys the laws of
physics gpu accelerated computing and
deep learning
are giving super exponential growth in
many fields
while super exponential scale will open
industrial use of hpc
the diverse use cases of industrial hpc
is driving
architecture advances beyond just scale
industrial hpc cannot be limited by the
walls of the supercomputing center
industrial hpc will be distributed
across multiple sites
extending to the edge connected to
remote sensors
sometimes running batch simulations and
increasingly
continuous digital twin simulations
industrial hpc is cloud native
and hybrid cloud the central
supercomputer will be incredible
great at first principles physics
simulations as well as data analytics
ai model training and physics ai
fusion simulation approaches will be
used by most industries
in transportation hbc will build and
continuously
update hd maps from sensor data streamed
from millions of cars
hpc will be used to develop self-driving
ai models
while hpc optimized for visualization
will simulate the full car stack in
virtual cities
in manufacturing visualization optimized
hpc
will create virtual environments where
robots use deep reinforcement learning
to learn skills
hpc will run digital twin simulations
that are visually and physically
accurate
some supercomputers will be fully
dedicated to monitor and process
continuous streams of telemetry of the
environmental
meteorological atmospheric oceanic
satellite imaging and human and
agricultural pollution data
coming from billions of sensors all over
the world
historical data can be visualized in
many modalities
and the data will drive ai models that
predict weather in the next hours or
climate change in the coming years
destiny is such a computer and will be a
digital twin of the earth
accelerated computing and ai will be
used throughout
and yet the diversity of applications
and system bottlenecks will drive
diversity of system architectures
and this is the beauty of arm the open
licensing model that allows the world to
create purpose-built
ships and systems amazon graviton is
excellent for hyperscale
ampere computing has built a great cpu
for hyperscale cloud
and high concurrent user applications
cdac in india
and etri in korea are also building
supercomputing cpus
fujitsu has built a supercomputing cpu
with strong vector processing and high
memory bandwidth
marvell is excellent at storage servers
and 5g base stations
here in europe scipro is building a cpu
for european
exascale supercomputing and nvidia is
building a cpu optimized for gpu
accelerated computing
of large data problems like ai building
the cpu is the first step of a long
journey
to creating a computing platform that is
useful for researchers
building a useful computing platform
that supports the diversity of
applications
system configurations and use cases in
different fields of science and
industries is a giant endeavor
and with a tiny exception computers are
not standalone or isolated appliances
rm systems need to be integrated into
today's infrastructure
software stacks and workflows arm needs
support of third party companion chips
a broad range of systems domain specific
solvers and applications
middleware storage and file systems
networking mature development stacks and
sdks that support popular programming
models
and languages and data center management
software arm is barely one percent of
the world's data center today
so inspiring the ecosystem to fully
support arm will be a long journey
we believe the time has come for the arm
model in hpc and though a long journey
it's one nvidia is quite familiar with
nvidia can jumpstart the arm hpc
ecosystem
over the past two decades we have
created a strong ecosystem for nvidia
accelerated computing
our platform accelerates all key hpc and
ai ecosystems we provide 150 sdks to our
two and a half million developers
we work with server makers throughout
the world as well as cloud providers
to offer our platform to customers the
hpc community wants diversity and is
eager to have nvidia offer
accelerated and ai computing for both
x86 and arm
with community interest we will support
both isis
from cloud to super computing centers to
the edge
let me mention one more thing quantum
computing
though still early stages of research
holds the promise to simulate classes of
problems that grow exponentially complex
with scale
like quantum chemistry and cryptography
though a couple of decades away from
broad commercial use there is immediate
important research to do
and nvidia can make contribution a
number of ways
simulating quantum circuits to verify
the results of a research
quantum computer providing a platform
for quantum optimal
algorithm researchers architecting
hybrid quantum classical systems
as well as accelerating the many quantum
solvers already in use for drug
discovery or material sciences
nvidia ku quantum is a tensor processing
sdk
and speeds up quantum circuit
simulations tremendously
there's a large and vibrant community in
france and around the world
our computer scientists are ready to
help you accelerate your quantum
research
science uses hpc to test the limits of
new theories
industries need hpc to test the limits
of new products
until now the products based on physical
and biological sciences
are too large and complex to properly
capture in simulation
gpu acceleration and ai has changed that
gpu acceleration and ai boosted the
scale of simulation
by 10 million fold in a little over 15
years
super exponential growth to industry
hpc is no longer a tall ladder trying to
reach the moon
it's now a rocket ship the question for
industry
is no longer what can computers do the
question
is who will use it first to
revolutionize their industry
the time for industrial hpc has come
thanks for joining me today have a great
terror tech
Title: NVIDIA DRIVE Concierge Powered by Omniverse Avatar at CES 2022
Publish_date: 2022-01-04
Length: 96
Views: 16378
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/kKXhnhsJDMo/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: kKXhnhsJDMo

--- Transcript ---

welcome cheryl your calendar shows your
ces hotel check in for tonight if we
leave now we can avoid traffic on 880.
perfect drive me there
activating autopilot
i'm ready to enjoy the ride but it's a
little warm in here
should i roll down the windows or turn
on the air conditioner
you know what open the sunroof
opening the sunroof for now
are there any important messages for me
there's one from jensen he asks can you
meet for dinner tomorrow
great can you find me a michelin star
restaurant in las vegas
the first result is no boo located at
3799 las vegas boulevard
who is the head chef there
nobu's head chef is no beaumatusab
what is his signature dish
his signature dish is black cod with
miso
what wine goes well with that
no booze head some layer recommends a
crisp white wine like a sauvignon blanc
with black cod miso
sounds like a winner are there
reservations available for dinner
tomorrow
there is availability for a party of two
at 7 30 p.m
perfect please book it can you respond
to jensen to let him know to meet us
there tomorrow
okay should i send it
yes please
okay sending
Title: How AI Helps Autonomous Vehicles Perceive Intersection Structure - NVIDIA DRIVE Labs Ep. 20
Publish_date: 2020-05-27
Length: 111
Views: 25198
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/KPLTA4S_3Yo/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: KPLTA4S_3Yo

--- Transcript ---

Hello everyone.
We hope that you are staying healthy, staying safe.
We are working from home
welcome to DRIVE Labs, the “Home Edition”
today we’re going to talk about using a deep neural network to understand the structure of an intersection
things like how many lane lines are there in each direction?
Where are the intersection entry and exit
lines?
the application for this is autonomous intersection handling
in both semi-urban and urban scenarios
The DNN is able to detect and classify different types of intersection structure features
including intersection entry line points for
the ego car, shown in red
intersection entry line points for other cars shown in yellow
and intersection exit line points for all cars, shown in green
The DNN also detects non-drivable lane lines, as shown in black.
We see that the perception is robust to both partial and full occlusions
and that the DNN is able to predict both painted and inferred intersection structure lines
We also note that these are all per-frame DNN detection results
with no tracking or fusion of any kind applied
The ego car can determine where to stop for the intersection based on the closest intersection entry line
can decide how to exit the intersection using all of the intersection exit line information
So in this case, the DNN correctly predicts
that the ego car could exit the intersection
by either proceeding straight through, taking a left or right turn, or making a U-turn.
And we also note that this perception information can be used to extrapolate how many lanes
and which types of lanes the intersection has
These DNN perception results can be used in several different ways for autonomous intersection handling
They can be used to generate paths to navigate the intersection.
They can be used to create a map of intersection structure.
And they can also be combined with previously mapped results, where available
to create additional diversity and redundancy.
Title: NVIDIA TensorRT at GTC 2018
Publish_date: 2018-05-29
Length: 51
Views: 14735
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/kQBsJPrLfAI/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: kQBsJPrLfAI

--- Transcript ---

tensho RT is in videos programmable
inference accelerator and we use that to
allow people to deploy neural networks
into production in cars penis centers
etc hence RT optimizes neural networks
so that they deliver answers quickly and
they do so with a throughput that allows
businesses to use AI power applications
at a lower cost we're showing an
application from a company called really
and really is using AI to revolutionize
the way that highlight reels from sports
events are gathered consortium them do
that by delivering high throughput so
they can look at lots of hours of video
in a very short period of time 10 CRT is
available to in video register
developers through developer Nvidia calm
and to users who are using applications
that have embedded 10 sorority
Title: Accelerating Deep Learning Research with NVIDIA DGX Station A100
Publish_date: 2021-08-03
Length: 94
Views: 12202
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/KVedp1Ij--g/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLAwxUWL6DBRbbDyrfENuk4RcXjMiw
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: KVedp1Ij--g

--- Transcript ---

i'm brian catanzaro and i'm vice
president of applied deep learning
research at nvidia
and my job is to help us prototype new
ways of using deep learning that will
make nvidia better
we're in the middle of a new industrial
revolution
artificial intelligence is changing the
way that we do everything
from analytics to recommender systems to
conversational ai
self-driving cars applications in
robotics and agriculture and healthcare
and we could use better tools to do that
the dgx station a100 brings the full
power of our a100 gpu
with the speed of our version 3.0 tensor
cores
as well as faster envy link that
connects these gpus so that you can
scale a model to multiple gpus at once
it is a very powerful very flexible
workstation it has server grade cpus
when you're building a system to train
the world's most advanced artificial
intelligence
of course you care about the speed of
the processor but you also need to care
about the rest of the system including
the i o
and the storage we need to be able to
stream through enormous data sets
as we're training a model and we need to
communicate potentially between multiple
servers
in order to train something even bigger
the thing that's so great about the dgx
station is that anybody can set it up
you don't need an it department
the dgx station a100 is the fastest
computer
that a person can buy and plug into the
wall and use without an i.t
department
Title: NVIDIA SHIELD Showcase - The Unboxing
Publish_date: 2013-07-30
Length: 183
Views: 93913
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/kwMcG51EksY/hq720.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AH-CYAC0AWKAgwIABABGHIgSigvMA8=&rs=AOn4CLA4jAwNkBGtnMuC4ab_rjhBUX3J6Q
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: kwMcG51EksY

--- Transcript ---

[Music]
hey guys i'm will and this is shield
this is actually shield in the retail
packaging what you'd probably find on
your doorstep when you order shield or
what you can expect to pick up in a
store
but today i want to show you what the
unboxing experience looks like so let's
get started on the front of the box
you'll see shield open and show in a
game on the back side you'll see shield
closed with the tag on the side we have
some light specs of course shield is
powered by tiger 4.
it's got a console grade game controller
and because it's running 100 android it
connects to google play
now on the back of that is just a simple
shield logo but what you really care
about is what's on the inside of this
box so let's check it out all right so
what we're going to do is put the box on
its side you're going to find the first
seal along the edge here i'm going to
take my blade and just run it across and
just cut that seal nice and cleanly
there we go so we're going to take the
sleeve off
put it off to the side and put the box
down and locate the last seal here
you're going to see an arrow you're
going to want to make sure that arrow is
pointed upwards this one is so i'm just
going to take my blade
just cut that seal off real nice like
that and what that allows you to do
is bring
shield
out of the box
so here you have shield but before we
get into shield too much i'm going to
lift him off and put him to the side
underneath you'll find shield
documentation
take that off you find your shield
charging brick as well as your shield
micro usb cable
i'm going to take these out put them out
off to the side as well
so now let's get back to shield we're
going to lift up this green tab here
and that free shield from its tray it's
going to move the tray off to the side
and there you have it their shield
you're going to flip up the display so
you open the lid and you see the plastic
right here letting you know that this
shield multifunction button here is also
the power button i'm just going to press
that to power it on get my fingernail
underneath the plastic to
take it off like so while we're waiting
for it to boot up let's take a look at
what shield comes with alright so we're
just going to flip shield around here
and take a look at the back side we have
all these ports here we have the mini
hdmi port the micro usb port the
headphone jack and finally the micro sd
card slot now to charge shield you're
just going to take your shield charging
brick and your shield micro usb cable
connect them like so
right into shield over here
and we're just going to take the
charging brick and plug it in to get
your charge on so that does it for our
shield unboxing experience find out how
to set up your shield here
and how to set up pc game streaming for
your shield here
[Music]
[Music]
you
Title: Neoscape Creates a New Wave of Real-Time Experiences with NVIDIA Quadro RTX
Publish_date: 2020-06-12
Length: 205
Views: 28308
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/l1H8RjlElt8/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: l1H8RjlElt8

--- Transcript ---

[Music]
new escapes a full-service marketing
agency focused primarily on the real
estate industry providing services like
3d visualization animation film
interactive experience all basically a
one-stop shop for all your real estate
marketing needs we do branding strategy
graphic designer iPad applications
interactive applications renderings film
you name it it's a little over a year
ago our client out of New York City came
to us to help them develop deliverables
around the highest observation deck in
the Western Hemisphere the biggest
deliverable there was developing
essentially to ride films for the
elevators that take you up to the
observation deck and back down to the
ground level we were rendering basically
all of Manhattan it's 70,000 frames of
animation at a K and so rendering these
out is a massive challenge we had a
great relationship with our partners at
Lenovo and NVIDIA so we reach out to
them to see what was new and what we
could use to make things easier on us
these given a very unique workflow
involving some cutting-edge bleeding
edge software applications they had a
need for Quadro r-tx type GPUs end to
end within that environment so we
provide them ThinkPad mobile
workstations power boy NVIDIA Quadro X
GPUs and we scaled up to a more powerful
desktop workstation that could handle
multiple r-tx GPUs and the night to then
our Lenovo our TX server that can hold
it to four quadrille TX GPUs as we were
testing Lavina on L&O of p9 20 we took
Quadros our TX 6000 I figure it would be
a good test to bring in the whole data
set into Lavina because it gives us the
ability to see everything in real time
and just develop the look and feel of it
to our amazement they did not break not
only was not breaking it was performing
quite well as we saw the advantages of
using Quattro r-tx GPUs we realized that
it was not going to be cost effective to
have more than one GPU in all of our
workstation now that we have the Lenovo
our TX server every artist across the
company has access to the GPUs right
away we started to use the v-ray next
GPU renderer that takes advantage of the
whole machine the CPUs energy
and we went from one-hour renderings to
seven-minute renderings the freedoms we
get to iterate more rapidly and freely
are really fulfilling on a number of
levels from a creativity standpoint from
a workflow standpoint to a storytelling
standpoint we have another project in
the elevator these won't be in a motion
graphic based we figured we'd render the
whole thing in our GPU render farm in
Unreal Engine that allowed me to work on
my Lenovo workstation as well as my
Lenovo p53 mobile workstation I could
write trace that I could render it and I
could still do everything that I had to
do without compromise GPU rendering for
us is an absolute game changer we see it
as revolutionary not just for neo scape
but for our entire industry we were
absolutely limited by ability to render
things and ability to move big datasets
and all of those limitations and
constraints are now falling away thanks
to Lenovo workstations and mobile
workstations and using a media squadron
our TX technology I feel confident that
we can do anything anywhere it is
absolutely a game changer and we
couldn't do it without tremendous
partners like Lenovo and Nvidia
you
Title: Chaos Group Harnesses GPU Rendering for Construct Movie with V-Ray RT
Publish_date: 2014-05-19
Length: 136
Views: 52485
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/LBOGD5bjXd8/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: LBOGD5bjXd8

--- Transcript ---

construct is an independent short film
that myself and a group of friends and
artists are putting together in our free
time nights and weekends it is a fully
animated short film it'll be about eight
minutes long and we're rendering
entirely on the GPU NVIDIA GPUs I
reached out to Kaos group makers of the
B ray renderer and I said I want to try
and take v-ray RT and translate that
into a real-time context inside
motion-capture morning working with
Kevin on construct was actually pretty
interesting because it sort of opened
our eyes in terms of the potential of
what our software can do and where it
can be used and we thought it was a
perfect opportunity for us to push the
technology of v-ray RT especially on the
GPU he started to see that he could
actually harness this technology to
mirror basically what live-action
cinematography does so we just shot a
take of roto and Darren fighting each
other now we want to review that take so
what we can do now with v-ray promotion
builder is replay to take while its path
tracing in real time so this is the tape
we just captured playing back in real
time and at any given moment we can
pause it and it resolves to final
quality this is a great way to evaluate
how the lighting and shaders behave in
the tape that you just captured well I'd
like to replicate the live action
workflow as closely as possible and a
big part of that is being able to see in
the monitor on the camera that I'm
capturing the highest fidelity imagery
as possible I'd like to think that
there's implications outside of the
motion capture process and more about
the process of filmmaking as a whole the
ability to compose a frame to color and
lighting in the same way a live-action
cinematographer would has huge
implications not only on the efficiency
and the production standpoint but also
creatively you watch live-action
cinematography there are always
conscious of the color and framing and
light and shapes that are moving around
in the scene in front of them that's a
huge part of how you would
it was a frame what this technology is
enabling me to do is to see the moment
with more clarity and more fidelity and
be able to respond creatively to a
broader range of artistic considerations
Title: Mirror's Edge Catalyst's Graphics & Gameplay Wows Gamescom Attendees
Publish_date: 2015-08-13
Length: 161
Views: 17890
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/lEQEsTTmZbc/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: lEQEsTTmZbc

--- Transcript ---

Mirror's Edge introduced gamers to
futuristic first person parkour action
back in two thousand eight and much like
the game's iconic hero faith fans have
been climbing the walls for more ever
since fortunately their prayers will be
answered next year when EA my new
releases Mirror's Edge catalyst we
supposed to Eric odel doll design
director at dice to find out what
players to expect from this long-awaited
game samira's Edge catalyst is a reboot
of the mirrors edge franchise we're
starting over basically we're building
on the themes of the first game the
beautiful architecture and is there
but we're basically rebuilding the world
a lower from scratch and we're also
making some changes to the gameplay of
course first and foremost combat is a
part of our core courses or free running
it's about achieving flow chaining moves
together and as long as you keep
chaining moves bullets won't hit you
you're safe first person free running it
kind of screams for larger environments
we are free to explore and basically
choose your own path through the
cityscape basically building a massive
city that gradually opens up for you
and letting the player choose the wrong
path through and choose what to do and
when to do it with a switch from our old
engine to frostbite there's a there's a
lot of things that we can do
lots and lots of time and perfecting our
animation tech to get a really smooth
experience we're using several different
reflection techniques together to create
a world where you even in a first-person
game you see yourself quite often almost
all
basically every area of the game is
changing because we're finding these
strengths of frostbite and we're using
that to our
to just make something that we believe
is truly special
Title: NVIDIA Quadro RTX Powers Real-time Global Illumination for Architectural Visualization
Publish_date: 2019-03-21
Length: 114
Views: 14029
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/liQxEGzmHps/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: liQxEGzmHps

--- Transcript ---

build media for the last four years have
been investing some time into real-time
game engines for architectural
visualization real space is our
interactive interface which is built
upon the Unreal Engine our clients they
use it to walk their potential buyer
through a unbuilt apartment accuracy is
definitely key for us because a buyer
needs to know what they're purchasing
it's one of the biggest purchases of
their lives
I meet cousins all founder and CEO of
dough one we're demoing a virtual
Helsinki here where you can actually
enter in virtual reality and go through
the different touristic spots we're
building everything for the Unreal
Engine and as we were using the cheapest
cars in the beginning that was a
challenge because our scene is so large
so we got ourselves a Quadro card that
has been a game changer for us as we've
been able to actually have enough memory
to run the experience in real time and
even get those raytrace shadows and
reflections and even the global
illumination in place as soon as we plug
the Quadro Arctic's and and using the
new ray tracing feature in Unreal Engine
we're getting proper translucency proper
reflections so that the overall image
quality in real time is getting even
closer to pre-rendered outputs that you
were doing our bottleneck was that crap
is fired but now with the quadrille
cards we can actually expand the city
with more speed and we can actually get
way more locations in part where it's
all experienced and there's thousands of
objects being rained at all in real time
and you can fully navigate every single
room
are we getting amazing reflections from
the courtyard space and all the windows
and glass it's still for sale in New
Zealand and I know the person to talk to
it really is pretty amazing actually
we're really loving it
[Music]
Title: NVIDIA VR Funhouse: Jamie Hyneman of MythBusters Experiences Next-Gen VR
Publish_date: 2016-07-21
Length: 137
Views: 36918
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/ljm82enu0mI/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: ljm82enu0mI

--- Transcript ---

we're on the nvidia campus that's their
new building it's not quite ready yet
but they tell me they have a new VR
demos it is ready so we're gonna go
check that out I've run into VR a couple
of times in the past VR funhouse was an
eye-opener being able to interact with
the stuff that you're seeing having very
convincing sound having haptic tactile
feedback I've just never seen anything
that was as interactive as what I saw
here one of the most impressive things
that I was seeing here was actual
physics that had been applied to these
objects nice if you're punching
something and it recoils in the case of
the balloons you had all this kind of
little particles that were left over
when the balloons popped
it's not just about rendering something
it's about actually seeing things behave
like real objects in the real world
that's fantastic
the rubber and those things was so
realistic that you could pull it back
kind of like a slingshot and let it go
and it would launch itself across the
room the best target practice that is
ahead that's I almost think that's more
fun than the real thing you never run
out of ammo I was impressed that they'd
gone to the level of realism of me being
able to continuously fire until I could
pretty much make the whole target
backboard fall down
all of those details that you've been
able to build into these virtual
environments equal more than the sum of
its parts
that's what makes you actually feel like
you're we live there I can feel the
sword interacting with the other sword
and my hand is on it so swiping through
that I'd be cutting my hand off and it's
like a concern I'm like ow ow ow ow you
know with these kinds of environments
you're getting put in a situation where
the line between what's real and what
isn't gets blurred pretty quickly I'll
be really excited to see how far this
technology is going to be taken
Title: NVIDIA Opening Keynote Highlights at CES 2017
Publish_date: 2017-01-06
Length: 2222
Views: 21148
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/ljSpat74w10/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLBeL6X1vjoBebNXKL2UCKOThV_8tA
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: ljSpat74w10

--- Transcript ---

these four areas we've been endeavoring
for some time and then all of a sudden
in the last several years an enormous
breakthrough happened researchers all
around the world working in a new field
a new technique of machine learning
called deep learning met the GPU and the
Big Bang of artificial intelligence
happened this technique allows software
to write software allows computers to
learn from experience and data and
allows the computer to recognize complex
patterns that are easy for you and I but
incredibly hard for computers and it
does so by hierarchically building up
feature representations to represent
very complex information complex
patterns but building it up from
hierarchies of simpler patterns this
ability to perceive the world is just an
enormous breakthrough and I'm going to
show you why why this foundational
technology foundational capability was
so important it just had one incredible
challenge one incredible handicap and
that is the amount of computation
necessary is absolutely enormous and
then one day the AI researchers met the
GPU that we invented and the Big Bang of
modern AI happened the achievements have
been fast and furious some of the things
that we've been able to accomplish in
just the last several years absolutely
mind-blowing you have all heard about
the alphago achievement demis hassabis
and his deep mine team was able to teach
a computer how to play go the most
complex game we know how more
variability than all the atoms in the
universe more moves and yet this
computer was able to learn go from the
world's masters and then play the master
of our modern era and beat it a network
has been able to synthesize our voice
instead of our voices stitched together
from a whole bunch of little tiny chunks
this network is able to learn the
tonation of our voice and from the words
that we feed it synthesize how we would
speak a network was able to learn how to
walk by itself just by teaching it the
kinematics of a robot
have to repeat a trying the robot was
able to stand up and walk driving is a
skill it's not mathematics kids can
learn how to drive adults drive and yet
we do no computation whatsoever we do no
Newtonian physics whatsoever in our head
we just drive we've been able to teach a
car how to drive the achievements that
you see in front of you it was just
impossible until recently and all of a
sudden because we're now able to
understand the complex nature of the
world we're now able to apply artificial
intelligence to solve problems that we
had never conceived of in the past the
enabling technology behind all of these
great achievements is GPU computing that
GPU had to benefit over 23 years to be
fueled by the single largest
entertainment industry in the world
video games to us is propels the science
of our company g-forces
also thriving and vibrant in the last
five years it has doubled in revenues
there are 200 million GeForce gamers
around the world the dynamics that are
driving this business is
multi-dimensional of course it is a
global industry and before anyone has a
game console everyone has a PC and
almost every single human today is a
gamer there are several hundred million
core gamers in the world today I expect
there to be several billion gamers
someday our technology is also fueled
and this market is also fueled by the
amazing production value of the video
games that continues to come out in the
last five years gaming technology has
increase in performance by a factor of
10 and now 4k HDR and virtual reality is
coming gaming is no longer just about
games gaming is now the world's largest
sporting event in fact it is very very
likely that eSports will someday be
larger than all of the other sports
events combined PC gaming is thriving
GeForce is thriving and all of that is
propelling the incredible R&D budget
that we're able to support today the
first announcement is to connect GeForce
the number one gaming platform in the
world to the number one social platform
in the world GeForce has a software
platform above it that's connected to
tens of millions of gamers
and this software platform today is able
to allow you to capture pictures capture
a video capture a VR picture so your
video game could be shared with other
people who want to spectate and watch it
in VR and you could also live broadcast
our platform is connected to all of
these social networks and today we're
going to connect it directly with just
two clicks to Facebook and to help us
celebrate this incredible connection
we're going to share with the world
never-before-seen footage of Mass Effect
Andromeda
let's welcome Erin Flint to the stage
Erin it's great to have you here thank
you so much gents like you know you're
gonna tell us a little something about
Mass Effect yeah absolutely we've been
on this game for almost five years now
so it started as an idea we'd wrapped up
the Mass Effect trilogy and knew we
wanted to tell a new story with new
characters and new places to go and
explore and so we reached out decided e
new technology to do that that's all we
turned to frostbite and started working
with the frostbite team and really
investing in that and building on that
and that's allowed us to do all sorts of
really cool things larger environments
ever before more dynamic more detailed
characters than ever before to tell
stories with and even better game
playing out with verticality and
destruction the thing that's really
amazing about Mass Effect is the story
is really really deep I mean your
explorers that are traveling to the
andromeda galaxy
and they're able to do so of course at
faster than speed of light absolutely
they're able to use this incredible
element that they found call element 0
on mars and as a result reduce their
mass to zero
that's excellent chances that's amazing
Wow that's great geez and and you and I
both know that unless your mass is zero
you have no shot of traveling at the
speed of light I've heard that yeah and
therefore mass effect
ladies and gentlemen has effect
[Music]
are you alright Sammy's leader The
Tempest that were stranded but alive no
choice but to investigate PB signal
Pathfinder this console controls the
gate
[Applause]
[Music]
[Applause]
[Applause]
there are two billion PC users in the
world we estimate some billion users
with integrated graphics or Macs or
older pcs or thinner like notebooks that
would love to be able to enjoy games but
simply don't have the capability to do
so and maybe they're they're rather
intimidated by opening up their computer
in the first place and building
something new and we thought that would
it be amazing if we were to put a
state-of-the-art GeForce gaming PC
powered by Pascal in the cloud like AWS
these supercomputers in a cloud could be
shared by millions of gamers around the
world they could try video games for the
very first time they could launch a game
whenever they want wherever they want
would that be utterly amazing and it's
incredibly hard because the
computational capability necessary for
video games is so high and the
interactivity requirements so high that
any little bit of latency would ruin the
experience and so after just so much
refinement so much.we architecture so
much engineering the team has finally
done it ladies and gentlemen we're
announcing today GeForce now for pcs
GeForce now it turns any of your PC's
with the download of a little tiny
client but to essentially your most
powerful gaming PC and it's all in the
cloud just one click away when we take a
look at it ok so what you're looking at
here two computers in the back one is
based on PC one is Mac the thing that
you instantly recognize on the PC side
is all of the major stores and hubs are
now available and they work perfectly
multiplayer works perfectly all of your
sword states your checkpoints all of
your friends all work perfectly all of
the software has been updated every
single game works exactly as it should
so why don't we do this why don't we
launch steam steam is the single most
popular video PC gaming store in the
world and here David's launching steam
on GeForce now and there you are
just a few seconds and you're into steam
and all of the games that you've
purchased or all of the games you
brought into steam are all there it also
works on the Mac it's just one of the
apps on Mac there it is steam on Mac and
you can buy a game right there you can
buy a game right that we won't do so
right now but we could buy a game and
usually when these video games are so
large they're so large because they have
so much content and takes hours and
hours to download it but in the case of
GeForce now it only takes about a minute
now that it's installed let's launch it
and this game doesn't run on a Mac and
this game runs very poorly on integrated
graphics but yet we're going to see it
working here and it's full fidelity and
one of the things that's really cool is
that it just works exactly the way you
would expect an app to work on a
computer it's completely seamless to you
you pretty much forget that it's even in
the cloud and video games are
complicated in the sense that the game
is always being patched there's always
digital download content the drivers
need to be enhanced all the time and we
can do that
all behind your back update it all the
time keep your computer always fresh and
there it is
Tomb Raider
so g-force now will be available in
March it's coming very very soon we're
putting the final touches in there and
it'll be available for $25 for 20 hours
of play
it's basically a GeForce gaming PC on
demand there will be several grades of
performance that are available to you
the higher performance grades you'll
have fewer hours for every 25 dollars of
credits
okay so GeForce now incredible value for
somebody who doesn't have the ability to
access a gaming PC somebody who hasn't
taken the effort to build a gaming PC or
who's somebody who just plays in
frequently but would love to be able to
enjoy a videogame from time to time a
year and a half ago we announced our
partnership with Google to build the
world's first Android TV today we're
announcing the new shield Netflix and
Amazon worked closely with our engineers
and we're announcing that shield will be
the first entertainment platform that is
able to enjoy Netflix and Amazon's
library of content in 4k HDR we have 4k
services from YouTube we have 4k
services from Google Play we know that
tens of millions of PC gamers around the
world would like to take that GeForce
gaming PC and connect it connect their
steam service to our television we
worked with valve and we're making
available for the very first time a
steam app on Android Play a steam app on
shield that connects to your PC and you
can enjoy 4k HDR gaming on your TV while
it's playing on your PC and if you would
like to have access to even more games
we now have a thousand games in the
Nvidia shield game store this is just an
incredible amount of content but we
didn't stop there the two most popular
consumer electronics platforms today one
is the smart television products like
Apple TV and the shield but the other is
the Amazon echo it brings AI into your
home allows you to communicate naturally
with an AI assistant we thought why
have two devices when you can have one
and so we decided to work with Google to
create the world's first Android TV with
the Google assistant now your television
can be controlled through natural
language you can control your content
you can access your content seek content
find content play it stop it
fast-forward it look for photographs ask
it questions you could even control your
home but we didn't stop there we felt
that if you had a Google assistant and
you had an AI agent in your home it
seems to me that you would want to have
it all over your home that you shouldn't
want to have to lean over to the coffee
table and yell commands at it all the
time announcing the Nvidia spot hi how
can I help this little tiny device plugs
directly into the wall and because the
computing is done on shield we could
have a whole bunch of these all over the
house and this little tiny microphone
has far-field
processing has echo cancellation so it
picks up your speech relatively
naturally 20 feet away and if you have
multiple of these devices in a large
room it also does triangulation of where
you are using beamforming and they all
go through one shield over Wi-Fi
brand-new and videospot
okay Google start my day okay I'm
turning up your thermostat and brewing
you a fresh pot of coffee
okay Google plaything news pending home
sales dropped in November to the lowest
level in here okay Google I'm leaving
shutting down your house have a great
day ok Google show me some popular TV
shows just drama only recent ones play
trailer from stranger things you're in
trouble hey that looks like your
grandma's house kind of ok Google show
pictures of grandma Mary's house see
what I say you're right don't watch it
yeah yeah let's watch
ok Google Play stranger things on
Netflix the combination of a smart TV
and an AI assistant in your home that is
completely ambient completely changes
how we interact with our house I think
in the future our our house is going to
become an AI and increasingly the vision
of Jarvis is going to be realized today
with a new shield with the NVIDIA spot
the Google assistant and the integration
with the smartthings hub that connects
to hundreds of consumer electronics
devices smart plugs coffee makers garage
doors locks thermostats smart cameras we
can now integrate all of those consumer
electronics devices and all of the
exciting ones that we announced this
year into the shield experience and all
controlled by Google assistant so ladies
and gentlemen the brand new shield for a
hundred and ninety-nine dollars
the nvidia spot will be available as a
separate perform and we'll announce it
in coming months as we all know
transportation is one of the largest
industries in the world there's a
billion cars on the road just in a
single day 20 million ride shares are
hailed on just DD an uber alone there
are 300 million trucks on the road the
infrastructure of society is made
possible by all of these trucks and
they're carrying things over a trillion
miles a year there's half a million
buses in just large cities alone
helping us with public transportation
this transportation industry is one of
the largest industries in the world it
is also one of the most vital without it
society doesn't move forward without it
we don't have the fundamental
infrastructure to live our lives
everything we enjoy everything we own
everything we eat and nourish our
families with all as a result of
transportation and yet this is also one
of the industries that has the largest
waste the amount of waste that comes
from accidents whether it's damages or
loss of lives emergency room visits
insurance measures and hundreds of
billions of dollars a year there's other
forms of waste as well most of the cars
that we enjoy are mostly parked by and
large everywhere we look there are
parked cars they're parked cars and
beaches and parks they're parked cars in
cities they're parked cars on campuses
by the way this is the Nvidia campus
there are parked cars all over the
streets littered everywhere would it be
amazing if we also reduce that waste and
how can we change the face of our
community how can we reinvent our lives
this is the reason why we've decided
almost a decade ago to start working on
autonomous vehicles and to start
developing the technology necessary for
someday for your car to become
essentially your most personal robot and
it is so intelligent that is able to
perform its function enhance your
mobility keep people safe while of
course do it intelligently and keep
people out of harm's way
the technology necessary to do so is
incredibly hard
until just very recently GPU deep
learning has made it possible for us to
finally imagine realizing this vision in
the next year we can realize this vision
right now now when you think about
self-driving cars at some level it's
relatively easy it's easy because we all
do it it's so easy that we can't even
explain it how do you explain to
somebody on a sheet of paper that has
never driven a car how to drive a car
well the reason for that is because
intelligence that we've come very
naturally with is incredibly hard for
computers and as I mentioned earlier
that would deep learning we can now
perceive the world not just sense the
world sensation is seeing hearing
touching those our senses perception is
accumulating all of those senses and
building a mental model of what it is
that you're perceiving we can also
reason about where the car is where
everything else is around the car and
where everything will be in the near
future so that we can decide whether the
path that we're on or the new path that
we're going to take is going to be safe
we also have the ability now to teach a
car how to drive just by watching us
observing us learning from us a car
could literally learn how to drive
that's supported by HD maps which are
maps in the cloud we can now compare
what we perceive with what we know to be
true in the cloud and determine what to
do we created a car called bb-8 and it's
an AI car and it runs everything that I
just described this brand new operating
system and a whole bunch of AI networks
hey man Starbucks in San Mateo
I wanna live like Mardi Gras with a
fungi in our garage don't need the
harbor knob to make it out like I say
our body living life in the fast lane
may obtain only be shipping so please
come join a good
we all got one think we just wanna have
fun
[Applause]
Oh
one thing goodbyes only I'll just quit
my job
well
we um we just wanna have fun
we get back to it
do some crazy
Wow
disengage autopilot
Oh
BBH riding an East Coast is running the
west coast and it's just incredibly fun
watching VBA zipping around and
meanwhile you can never stop learning
the world is changing all the time it
seems every single week a new road is
being beaten up or fixed something is
being added lanes are being added roads
are being shifted we have to
continuously map continuously relearn
the environment all of that requires AI
computing it is one of the reasons why
we dedicated ourselves to building a
fundamental new computer that would go
into a car that has the ability to do
all of this deep learning processing and
to do it at very high rates we call an
AI car supercomputer and we're building
a new one is called Xavier this is what
an AI supercomputer looks like for your
future self driving car it's a little
tiny computer like this and it has
sensor information that comes in and can
information can information controls the
accelerator the brakes the steering and
all of the other things that you want to
control inside the car this runs a new
operating system we call drive works
that takes multiple sensors in fuses it
recognized and perceive localized reason
drive and it does so while connecting to
the HD map and comparing ourselves
relative to the information that we get
from the HD maps incredibly powerful
eight high-end CPU cores inside this
chip 512 of our next-generation GPU it
is a soldi the computer is a cell D the
chip is a cell C acyl automotive safety
integrity level the quality level the
reliability level of this computer is
bar none and then lastly we do all of
that the performance of a high-end
gaming PC shrunk into a little tiny chip
30 Tara operations
trilliant operations in just 30 watts
little tiny computer we can chain a
whole bunch of these together depending
on the application and this will be the
future of our self driving car strategy
so ladies and gentleman today we're
announcing the AI
pilot and it basically works like this
remember the car has sensors all around
it's got cameras it's got radars it's
got lighter so it it has surround
perception the car also has cameras and
speakers inside the car and so it has
internal car perception it is aware of
its surroundings it's aware of the state
of the driver is aware of the state of
the passenger and just as the invidia
spot gives you an array of speakers and
microphones there's an array of speakers
and microphones inside a car and so this
car has incredible perception capability
if only the AI was running all the time
we believe that the AI is either driving
you or looking out for you when it's not
driving you it is still completely
engaged even though it doesn't have the
confidence to drive because maybe the
mapping is has changed or maybe the road
is too tricky or maybe there's just too
much traffic
too many pedestrians and that it
shouldn't drive it should still be
completely aware and it should continue
to look out for you and so what you're
looking at here is the four camera
series of bicycle on the right 45 feet
ahead so notice there's a four cameras
front rear left and right and the front
camera notice there's a bike right in
front of you about 45 feet ahead and
maybe your eyes aren't looking in that
direction maybe your heads not looking
in that direction and the car realizes
that you might be more cautious and in
this case it detects that there's a
biker and tells you through AI natural
language processing and so by talking to
you very naturally and alerting you of
the condition in front of you here's
another example careful there is a
motorcycle approaching the center lane
and so in this case the idea is
relatively simple incredibly complex to
execute you have to understand what
you're seeing and you have to convert
what you're seeing in to natural
language otherwise known as captioning
and then saying it in a natural way in
the car so that we can understand what
the car would like us to be concerned
about
and so that's environmental awareness so
founding awareness we also would like to
have the capability for inside the car
AI the AI should be paying attention to
you to maybe you're not looking where
you're driving
maybe you're dozing off and so those
capabilities modern AI networks can
absolutely do and so let's take a look
at that okay so this is Janine this is
one of our employees so what the first
network does facial recognition okay and
it does facial recognition incredibly
well this is deep learning facial
recognition networks are among the best
in the world and it's reaching human
level capabilities this next one is head
tracking by looking at Janine with a
camera the artificial intelligence
network is able to determine where her
head is looking the artificial
intelligence network the deep learning
network just by studying her eyes is
able to figure out what direction she's
gazing maybe she's looking at a nut
shouldn't do that this is lip-reading
and so if your car is too noisy and
there are too many people talking and
yet you said something rather important
would it be nice if the your AI car was
able to recognize and read your lips and
determine what it is that you said this
particular this particular capability
was inspired by the researchers at
Oxford working on the lip net and
they've been able to achieve a
lip-reading capability that's 95 percent
accurate a human the best humans about
53 percent accurate and so this gives
you a sense of the state of the art of
artificial intelligence network and you
combine what I just described on the
external surround perception capability
and in car passenger and Driver
perception the combination of that
allows us to always be aware and to keep
the car keep the keep the driver as
alert as possible and always be on the
lookout for us not to mention all of the
things that I described to you earlier
about shield and Google assistant we
will surely have that
capability inside the car so that you
can talk to your car and get whatever
access to any information or enjoy any
content and media you would like this is
the Nvidia AI car platform starting from
the bottom this is the Drive px computer
and on top of it is the drive works
operating system these two things are
probably the most complex computer that
we have ever built and we built some of
the most complex computers the world's
ever known the amount of data that's
coming into this computer the richness
of the artificial intelligence networks
and algorithms that we have to run and
the performance I which we have to run
it because time in the case of a fast
driving car is reaction time and
reaction time is safety and so it is
vital that we do this very very quickly
on top of that are all of these
artificial intelligence networks that I
talked about
there's the autopilot a is driving the
car continuously mapping the car and
exchanging that information with the HD
maps in the cloud the co-pilot deep
neural net so that there's an AI that's
watching out for you all the time we
also need the ability to converse and
interact with our computer in a very
natural way so natural language
processing has to be done inside the car
so that the reaction time the latency
between your spoken words and its
recognition of your speech is as fast as
possible and yet it's connected to the
cloud to an AI assistant this
architecture on top of that is an API
called map-works this is one of the most
important things we do map-works
interacts with all of the mapping
companies in the world and it does
basically four things the first of
course is the surveying car some cars
record all the data and do the
processing on GPUs supercomputers in the
cloud to extract the three-dimensional
data from the video or number two is the
GPU supercomputer that's inside the
cloud for mapping number three the
interface of data the exchange of data
so that our car can always see the HD
map in the nearby sir
of the car and then number four as we
continuously map the world and notice
changes we would update the changes to
the live map in the cloud these four
things is vital to the ability for
self-driving cars to be realized in a
very high confident way and so we've
been working with the world's leading
mapping companies the leading mapping
company in China Baidu incredible
partner of ours working across all four
functionalities that I described the
reason why Baidu was so important is
because China is now the world's largest
car market it is too large to ignore and
yet only a Chinese company can map China
and so partnering with Baidu was a very
logical first we then partnered with
Thanh Tom leading mapping company in
Europe today we're really super excited
to announce that we're partnering with
zenrin the mapping of Japan is quite an
extraordinary task zenrin is an amazing
company and we're working with them to
map Japan and then also today super
excited to announce that we're working
with here to integrate Nvidia technology
into their data centers for mapping
working with them on map algorithms as
well as connecting to all of the Nvidia
AI car computers inside cars as we
synchronize the live maps the AI
algorithms that we're developing are all
first of its kind and yet
the endeavor is such great importance
there are so many companies who could
really help us realize this dream I am
super excited to announce today that ZF
is now a partner in helping us turn this
computer into a production computer for
the automotive industry ZF is the
leading truck and commercial vehicle
supplier in Europe they're also one of
the world's top five suppliers to the
automotive industry this is an
extraordinary company and they are the
first to announce a production drive AI
computer to the market it's available
commercially for sampling and it will
ship into production this
here today we're announcing the
number-one automotive technology
supplier to the automotive industry
Bosch is going to adopt the Nvidia Drive
computer the largest and the fifth
largest automotive supplier in the world
have now adopted the NVIDIA computing
platform so that we can bring AI
computers to the autonomous industry
today we're announcing that Audi and
NVIDIA will partner together to build
the next generation may I car let's
welcome Scott Keough the president of
Aldi America to celebrate this moment
with us our partnership goes back 10
years and if you think about where Audi
was in America we were selling 60,000
cars a year didn't have much of an
impact if you look at this year we sold
a record two hundred and ten thousand
cars and the reason we did it honestly
is the crazy technology that Audi
engineers and your engineers put
together it's a virtual cockpit Google
Earth Google Maps point of interest
incredible stuff so that's the secret
sauce and I think with you we want to
keep the secret sauce moving look if I
think of your entire presentation it's
quite simple to me we want to get to
this nirvana state safer and we want to
get there sooner and really the only way
to go about doing that there's no amount
of programming in the world that's going
to manage the complexity of what happens
in the street environments daily the
only way to get there is with artificial
intelligence well let's make sure none
of our kids ever have to drive a man
this is the largest electronics show in
the world
two hundred thousand attendees it is it
is so fun it is so fun to be in the
middle of the technology industry today
it is so fun to be in the middle of the
computer industry and it's just
incredibly thrilling to be in the middle
of the automotive industry because of
artificial intelligence what used to be
science fiction is going to be reality
in the coming years today I had the
privilege of sharing with you a few
announcements of the things that we've
been working on we want to bring video
games to a billion people who currently
just simply don't have the computers
necessary to enjoy the type of video
games that you see from erin-flynn de
Mass Effect's studio
we believe that your home will engage
you and you will engage your home in
natural simple ways and it will arrange
your life help you find content for you
connect you with people and just make
your life better and of course we would
like to turn your car into an AI that by
applying this technology we could
revolutionize the Ottoman automobile and
bring joy and delight and safety to
millions and millions of people in the
future thank you very much have a great
CES
[Applause]
I wanna live like a smarty Gras with a
fungi on surprise don't need
Title: SHIELD Gaming: GeForce NOW
Publish_date: 2017-01-04
Length: 69
Views: 66549
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/LJXkVx0YgS8/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: LJXkVx0YgS8

--- Transcript ---

I want to play the latest PC games on
max settings takes longer to load games
than I expected
all my multiplayer would be amazing the
Nvidia shield TV built to bring you the
next generation of GeForce now stream
1080p games at 60 frames per second on
demand straight to your TV powered by
Nvidia Pascal GPUs in the cloud GeForce
now gives you performance faster than
next-gen consoles with the massive
library of titles you can string the
biggest hit games around shield
technology gives GeForce now instant
load times letting you dive right into
the action and no need for updates or
patches everything on GeForce now is
game ready play with friends and rule
the leaderboards with online multiplayer
all with the unrivaled graphics Nvidia
is known for you wanted more we built it
for you
with the g-force now an Nvidia shield
the streamer for gamers
Title: NVIDIA Cambridge-1 Inauguration | The UK’s Most Powerful Supercomputer
Publish_date: 2021-07-07
Length: 2535
Views: 274113
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/LKa-kDYZ1A8/hq720.jpg?v=60e5af85
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: LKa-kDYZ1A8

--- Transcript ---

[Music]
i am a healer
bringing clarity to our most important
questions diabetic retinopathy can
happen if blood sugar stays too high
over a long period of time and comfort
in times of uncertainty what procedure
am i having today you are having a
bronchoscopy
shining a light on a path forward
and finding answers
every second counts
i am watching over our frontline heroes
a safer place
to do their life-saving work
[Music]
guiding them
to faster answers
[Music]
and delivering care
wherever it's needed
i am opening new worlds of discovery
and mapping new treatments
for every individual
[Music]
creating a community
to collaborate with thousands
[Music]
and sharing knowledge that can heal
millions
i am a i
brought to life by nvidia and brilliant
healers everywhere
[Music]
nearly 70 years ago francis crick walked
from his lab at the university of
cambridge into the eagle bar and
proclaimed that he had found the secret
of life
with the help of rosalind franklin at
king's college london
crick discovered the double helix
structure of dna
and opened the door to understanding the
operating system of life itself
around the same time in 1950 alan turing
at king's college of cambridge
university did his seminal work in
computing and artificial intelligence
70 years later nvidia's touring gpu
pioneered a computer architecture that
made artificial intelligence a central
method of computation the results of
this approach
have proven to be staggering
in computational molecular dynamics
researchers simulated 300 million atoms
for 100 000 nanoseconds a scale that is
10 million fold larger than what was
possible just 15 years ago
moore's law the exponential law of
computing progress would have achieved
only one thousand fold the fusion of gpu
accelerated computing and ai has started
the age of super exponential advances
today oxford nanopore technologies uses
innovative sensors and ai algorithms
running on nvidia gpu supercomputers to
digitize dna into sequences of billions
of characters ont's technology helped
characterize the coronavirus genome it
is now used to do rapid testing and
identify variants making the uk a leader
in this area
the nearly simultaneous breakthroughs of
ont and cryo-electron microscopy for
atomic scale imaging and ai computing
has set the stage for another scientific
breakthrough
using the same breakthroughs and natural
language understanding we may soon learn
the structure and meaning of biology
itself
with ai we will soon decode and
understand biology
we are at the beginning of the digital
biology revolution
to do this work scientists need a
powerful rocket for their journey today
we're inaugurating cambridge one
named in honor of the birthplace of
computing and genomics and also arms
headquarters which we hope will be our
future home in the uk
nvidia's cambridge one is uk's most
powerful supercomputer the fastest
industrial supercomputer in the world
the greenest and most energy efficient
with a net zero carbon footprint
nvidia cambridge one brings together
decades of our work in accelerated
computing ai and life sciences into a
single computing center consisting of
nvidia's ampere ai gpu inside nvidia's
dgx supercomputers running nvidia's
clara computation platform for life
sciences
cambridge one is our first big bet on
the digital biology revolution
cambridge one and the purchase of arm is
our giant investment in the uk giving us
a platform to work with the amazing
universities companies and over a
thousand ai startups in this community
working with nvidia researchers guys on
st thomas's nhs trust foundation gsk
astrazeneca kings college london and
oxford nanoport our early partners who
will access cambridge one
some of the first projects slated for
the supercomputer
are using ai to sequence
genomics discover novel drugs
and to unlock the mysteries of dementia
by studying mri scans
our partnership with king's college
london has led to extraordinary
capabilities for the world's medical
imaging community with them we pioneered
federated learning the ability to build
ai models collaboratively without
sharing any confidential patient data
and sharing data privacy will be the key
to enabling the medical community to
leverage artificial intelligence to its
full potential
together with kings college london we're
building the world's most popular domain
specific
open
ai application framework for imaging
called moni
an invaluable collaborative tool to
supercharge the development of ai
applications
king's college london guys and st
thomas's nhs trust foundation and nvidia
are building ai to do incredibly
detailed brain imaging
learning from the world's largest
biomedical research data sets
pioneered by the uk biobank we will
train ai models to automatically detect
abnormal from healthy brains
astrazeneca and nvidia will use
cambridge one to train a language model
to understand smiles the language of
chemistry we can even use ai to
synthesize potentially effective
compounds and reduce the cost of drug
development
the pandemic has made the need to
accelerate healthcare discoveries a
paramount social and economic imperative
we need to bring these two superpowers
together now
the super exponential advances of ai
computing and genomics to tackle this
pressing need and in doing this we will
accelerate the digital biology
revolution
these first steps
bring us full circle
the discoveries made on cambridge one
will take shape in the uk
where it all began
where so much talent resides
but the impact will be global
benefiting millions around the world
cambridge won
we have a world to save
make it so
[Music]
hpc ai and gpu accelerated computing are
fundamentally changing the way in which
data centers must be designed and
engineered
at ko data we've witnessed this
evolution first hand we are delighted to
be the host of nvidia's cambridge one
supercomputer
defining a new era of ai enabled
infrastructure within the heart of the
uk innovation corridor
working with nvidia comes with the
expectation that you'll work at a
supercharged pace so hold on tight but
we've built the uk's most powerful
supercomputer amidst the background of
the global pandemic has shown what can
truly be achieved when a vision and
aptitude for collaboration sets a new
bar for innovation for those involved
and for the industry peers watching
waiting cambridge one has exceeded every
one of our expectations supercomputers
in essence perform complex computational
analysis at light speed
but they are generally lethargic in
evolution typically two years
development is far from unusual stifling
restricting and delaying important
research from being undertaken yet in a
record-breaking time of less than 20
weeks cambridge one has gone from
concept to fully operational in respect
of ko data i'm truly proud of how our
team of hpc data center specialists has
worked collaboratively with nvidia to
achieve this
cambridge one raises the bar for hpc
implementations globally it is the very
pinnacle of innovation
it shows the world how nvidia's dgx
superpod system can provide rapid access
to some of the world's fastest
supercomputers all for the betterment of
mankind
i'm delighted to be hosting our founders
panel today as part of cambridge one
inauguration the healthcare
supercomputer based in the uk cambridge
bond brings together a collaboration
between industry academia research and
scientists to advancement of ai in
healthcare
we have nadine
haram clinical innovation lead guys in
st thomas's nhs foundation trust
we have sebastian auslan professor of
healthcare engineering at king's college
london
we have rosemary sinclair dockers vice
president of product and programme
management at oxford danapor
we have steve crossan vice president of
ai and machine learning at gsk
and we have lindsey edwards head of ai
respiratory and immunology at
astrazeneca
in your view what are the biggest
challenges in applying ai and healthcare
today and how will cambridge one assist
with this perhaps i could start with
sebastian i guess when we're talking
about the high in healthcare two major
challenges are coming to the front one
being access to data at scale
and the second one being
access to computing power at scale and
clearly
cambridge one is solving the latter
problems and it needs to be intertwined
with the former problem which is really
where uh working very closely with the
nhs working very closely with
industry
and and all the places where that are
accessible is really essential
what you can do nowadays with uh with
supercomputers is going to be always
limited in terms of the resolution of
your model unless you are able to
actually get
access to such a large capacity of power
so that's pretty much what we believe
cambridge one is going to offer and we
already have been demonstrating this
with our proof of concept uh we really
see actually
that we can generate data we are a
company that can generate a huge amount
of data and
there's that initial processing step uh
that needs to happen but what's
interesting now is that um there's
actually two challenges there's one
challenge which is generating cataloging
uh and curation of data but at the same
time you're performing discovery on that
data so you actually have the challenge
of having to go backwards and forwards
and checking and testing your
assumptions all the time and yet a limit
limit to uh to compute actually really
doesn't help with those with those
challenges from a data perspective i
think there's two ways of thinking about
it one of one of those is the sort of
legacy data the data that we've got and
then there's the data that we'd like i
think going forward one of the most
important things is to start thinking in
terms of producing data data pipelines
and sort of fully engineered end-to-end
data flows that allow us to train some
of these more complicated models
but i think another key thing is that um
if you think of modern machine learning
methods as being kind of
ways of learning
complicated functions or complicated
ways of taking inputs and turning them
into outputs the more complicated the
thing you want your model to do
typically the bigger the model has to be
and that immediately requires greater
computation and as you scale those
things up data and and the sort of size
and complexity of your model then the
the power of your computer limits you
then another really interesting and
important point is if it takes you three
weeks to train a model your ability to
hone that model is really difficult so
what you need to be able to do is
iterate quickly and the more compute
power you have say cambridge one it
becomes possible to to fine-tune really
quite complicated systems and steve
we've also heard a lot about data data
creation
sort of pipelining etc from your
perspective how do you how do you view
this opportunity if i think about this
in the context of uk science as a whole
and uk health healthcare science as a
whole
you know i think there are a number of
enormous assets that the uk has in this
area most notably talents we have a huge
amount of talent both on the ai side and
on the life sciences side and a huge
historical depth of that that was one of
the main reasons why i was very excited
to come to gsk last year because we have
both of those in spades
we also have within the live sciences as
my colleagues have said the ability to
generate now enormous amounts of data um
exponentially more data every year more
so even than we were able to do a couple
of years ago and way more so than we
were able to do um 10 years ago all of
all of those aspects are in place
the one thing where we also need to play
as uk science as a whole is on the
compute side we have more assets in the
form of two camps and we're now with
cambridge one and with other similar
opportunities around the uk we're
starting to
really get those opportunities on the
compute side as well and that's that's
really what's going to make the
difference nadine how do we think about
translating the work done in cambridge
one to bring it to the point of care
i think it's a very important comment i
mean it's been really exciting first
being here but also listening to all the
panelists talking about the enthusiasm
and excitement around the computing
power and the potential of looking at
data discovery data pipelines that
end-to-end solution
but coming at it more from a kind of
frontline clinical aspect or a patient
aspect
the even more exciting thing for me is
how do we start to reimagine healthcare
where we're actually trying to bring
together cutting edge you know
healthcare life sciences innovation and
compute power to ultimately enable us to
do what we're all here trying to do
which is deliver best care to our
patients
try and reshape that access to care
think about how we deliver more
data-driven care but ultimately give
patients that opportunity to have
personalized holistic
data-driven care that's going to
hopefully make sure that every patient
is getting that best care the first time
every time and so with my clinical hat
and coming at it from a hospital point
of view we're very excited as well to be
part of this because we're sitting with
industry leaders we're sitting with
science leaders and the life sciences
and the this amazing compute power
and it's a green field the opportunity i
think is is immense and so
i'm really looking forward to seeing how
this is going to translate into
frontline impact for patients
i know you've got a lot of thoughts in
this area so perhaps maybe we could have
some comments from you yeah no
absolutely i think nadine really makes a
clear point that it has actually a
direct impact to patients those models
what would be really important is the
deployment of those models into the nhs
and so the way we we can see cambridge
one is is an extension of a smart nhs
supercomputer environment
where one will be able
through appropriate governance to gather
all of those data to be trained at scale
across
the capability of all of our nhs within
the uk and trend models which basically
become unbiased which are equitable as
well which
really represent our demographic
represent our population the different
genotypes the different phenotypes and
therefore have a model which is vastly
more performant than any other model
created before and as lindsay mentioned
those models it's not only about
getting access to a large amount of data
it's really the depth to get this model
to a granularity which is not possible
at present is really directly correlated
with your amount of computational power
but as well if you have to wait two
years 10 years 20 years to get your
model to be optimized how are you going
to move to the next stage how are you
going to expand this model further so
clearly the patient will win by being
able to have models which will be far
more personalized because they are
really representative of the entire
population and therefore in fact
interestingly enough if you don't do
those large scale models then there is
no way you can personalize them further
and so it's by combining in-depth
information of this specific patient and
comparing this patient with the entirety
of the population and all of the 20
years 30 years of data that we will be
able to exploit that will be able to do
better precision medicine
so rosemary how do you think about
translating the work that's been done on
cambridge one to the point of care from
your perspective
thanks and it's really really great to
get uh the views of the uh of the team
on the front line um from our side it's
it's interesting because as a technology
provider working
with institutions and with the
healthcare teams but also working with
the industry
means that you can start bringing all of
these three pieces together um and so
we're we're tackling we're tackling it
from our other products right or the
offerings right so that you can
translate all of this
wonderful work that's happening in the
background these model trainings these
insights um
how are they actually going to end up in
in the hands of a clinician uh to make a
difference to someone on their daily uh
on on their daily daily lives and uh and
i guess from the rest of our colleagues
here it will be a similar similar thing
is how can they take the knowledge how
can they develop the uh
healthcare of the future really uh from
our perspective we're interested in how
can we put that in people's hands so
they can make better decisions so we can
bring that data to to the clinician's
hands
very interesting thank you steve from
pharmaceutical perspective how do you
think about this translation from you
know this the work being done on the
supercomputer to actually how does that
translate through the work that you do
ultimately into patient care into real
world patient impact yes well um you
know
i think when i think about what we're
doing um at gsk.ai
um we're fundamentally using ai to
understand mechanism in biology that's
a lot of that evidence that we're using
is genetic evidence genetic and genomic
evidence we know that
medicines which have genomic evidence
behind them are twice as likely to
succeed and result in successful
medicines for for patients um uh at the
end
we are using that understanding that
we're gaining through i in two broad
ways
firstly right at the beginning of the
drug discovery pipeline um at sort of
understanding targets and understand
validating targets um and secondly at
the points of of of care with with the
patients um so really using that
understanding not just to understand a
disease in its generality but to
understand
what is going on with this patient at
this moment at this time and therefore
what is the right treatment or
combination of treatments to give to
this patient at this time in a clinical
setting that's where
this work you know really really
translates into making sure that
the right people that have the highest
likelihood of getting the right
medicines at the right time
very interesting indeed
lindsay what is cambridge one helping
you achieve astrazeneca
yeah we have a couple of really exciting
projects on cambridge one one way to
think about modern machine learning
methods is to think about them as
representation learning algorithms they
learn an encoding of your data that can
be useful to do
other stuff with this has been really
true in computer vision it's been true
in natural language processing where the
model is able to learn the basic rules
of vision and the basic rules of
language so we're using cambridge one to
train a very large model called mega
mole bart to learn the rules of
chemistry so we're trying to learn a
representation of chemical space based
on a huge chemical library and then
we're going to make it available to
everybody and it can be used for things
like new molecule prediction or property
prediction and a host of other
downstream tasks
we're also using cambridge one to
accelerate our digital pathology program
so one of the challenges with digital
pathology is the slides the images are
huge and incredibly the files are very
large and so what we've done in the past
is to break those files up break those
pictures up and train models on those
but that comes with a whole host of
problems uh computation on the scale of
cambridge one allows us to do whole
slide image processing which is pretty
transformational sebastian maybe you'd
like to comment i know you have lots of
thoughts on this area no absolutely
actually in fact we we have been already
starting to do
a few of uh of the important stage to
actually deliver this at scale on
cambridge one what we have been looking
at is is brain disease and how can we
understand the natural progressions of
of a disease
in the context of uh of radiology um
basically using diagnostic imaging in
this particular case mri and mris which
are nowadays acquired in in in the
clinic and which are we tends to be
quite high resolution
in the order of a cubic millimeter one
cubic millimeter of resolution which is
what most people are now using on the mr
scanners and being able to actually
build a synthetic image of any brand and
the whole idea here the whole principle
is can you start to build millions and
millions of for images and by doing this
first you are generating so many image
that you can create a situation where
you will be absolutely certain that you
have the authority of the possibilities
of how a brain will look like if it's a
healthy brain but what you could do as
well is to start to generate the same
amount of data but for specific disease
and then start to understand the
evolution and the progression of a
disease start to be able to better
diagnose better stratify a different
type of of disease which are very hard
to diagnose or even being able to in the
future to prognose and our intention is
to make those database available to the
community as well
that's very interesting the fact you're
looking to share that is is hugely uh
interesting
from your perspective obviously you're
hearing the projects there
how do you view this work and how that
will have an impact on on uh the
frontline healthcare one of the most
exciting things and i think i would
agree with the commentaries making is
really that kind of trifecta of you know
industry life sciences and kind of
patient or healthcare institutions
you know we're seeing this increasing
appetite across the nhs and across kind
of uk health sciences and life sciences
how do we start to bring the power of
computing
technology academia to really materially
change or shift how we do things
part of that of course is changing
behaviors and how we've traditionally
delivered care and that in and of itself
is its own um you know uphill battle but
we're seeing that becoming less
complicated and more you know people are
more willing to drive that forward
and what's going to be really exciting
is to start to think about
by bringing these partnerships together
we're we're educating and driving that
material behavior change across the
industry we're showing patients we're
showing front liners that actually data
can save lives working together in these
partnerships has material impact and so
when we look at it it's really important
about setting the problem first what is
the clinical
application or clinical problem that
you're trying to solve
and if you're coming at it from that
approach as a clinical problem we're
trying to solve and we're going to bring
together the power of computing the
power of science the power of innovation
the power of academia we're going to try
and solve that together i know seb and
the work that they do at the
universities always starts with a
clinical problem he's just described a
disease of the brain but there are many
other models where they've brought
together clinically led
decision-making around how we build
these programs together so i think
that's how this is going to really
materially shift the the front line
adoption and willingness to drive these
innovations forward but equally it's
going to enable us to take the patients
and the public on the journey with us
because ultimately this is about their
health care it's about how do we enable
better access to care for our patients
for the nhs
that is equitable that is inclusive that
has the diversity of data
the power to make the material shift
in particularly chronic diseases but
also some of those
you know more acute diseases that are
difficult to treat where you really need
to aggregate huge amounts of data to be
able to to shift those so
i'm kind of excited looking at it from
that point of view and i think as you've
heard from the panel everyone is looking
at it from that point of view as well
even though we're all coming at it with
different hats it is ultimately landing
at how we shift healthcare um you know
for the future
very interesting indeed and i would
agree
the combination of uh all the founding
partners really brings huge value about
how we collaborate as a healthcare
community uh rosemary from your
perspective from oxford nepal's
perspective how are you thinking about
the use of cambridge one and what it
will achieve well there are a lot of
very very excited people back at base um
looking forward to uh to starting
getting started with cambridge one quite
a few things um you know there are
certain platform development aspects
that will really really accelerate um
with the ability of using cambridge one
these are sort of model developments we
we have a very different signal um and
and we use again sort of speech
recognition type algorithms to um to
deconvolute it and there's a lot of
novel information that's captured in
that signal a lot of base modifications
which
are biological markers that um are not
yet fully understood and so there's an
enormous amount of work to to really
accelerate not in just
understanding what those markers are and
how you call them and how you get them
uh working reproducibly um but also how
they impact health um and this is where
the the work the working together with
the academia with um with with all of
the with all the industry comes in
because um that's where all the samples
are in in the healthcare space uh i
think there's an enormous amount of
discovery to be done uh around novel
markers that are
not known today
um but can be very very rapidly
accelerated with with the uh with the
power of compute
very interesting indeed um lindsay from
your perspective how do you think
cambridge one will impact the healthcare
system as a whole in the uk
what are the wider indications you think
of being able to deliver a super cuter
like this to the healthcare system it's
a terrific question and it's been really
fascinating to hear others views on this
i mean
obviously from a pharmaceutical
perspective
our primary role in all of this is to
supply new medicines to you know our our
colleagues in medicine steve was
referring to i think there's a couple of
different ways where machine learning
already you know we can see it's having
an impact one of those is in the way
that we understand the patients that
we're trying to find medicines for every
discovery of a new medicine starts there
and so things like digital pathology
already are kind of gaining traction as
a way of getting a deeper understanding
a richer understanding of the of the
patients that we're trying to serve and
then i think another key point which
we've sort of touched on a little
is that so much of this has been viewed
over the last century as being an
intensely human somewhat messy process
and i think that we're moving more and
more towards reimagining these things as
being engineering problems rather than
just um human and scientific problems
and in a sort of in the drug discovery
setting that is
looking at everything from early target
discovery through to molecular
generation through to safety screening
through to even clinical trial
management and trying to understand
where in that process we can apply
modern machine learning methods the one
thing that i would add as well is that i
don't think we can train these
spectacular models and dump them in the
existing systems and expect them to have
the effect that we want them to have
like we need to work together and
completely reimagine how these systems
function together for this stuff to have
the impact that we want it to have i
think the thing about cambridge run as
well is that that kind of compute power
allows us to sort of it's a scientific
booster rocket if you'd like to be able
to try things out that we couldn't try
before and then figure out how we apply
those into the new systems as we go
forward
very interesting i would totally agree
the potential is very significant indeed
steve from your perspective how do you
how do you think about the impact on the
overall health system from cambridge one
one of the great strengths in particular
of the uk
system is the uk scientific community is
that there is this
strong
tradition of collaboration between
uh um between pharmaceutical companies
between life sciences companies between
uh companies like oxford and nepal and
with the nhs with academia as well so
there's there's actually a really kind
of big community out there um which
already has established ways of working
together not only from the point of view
of sort of training the next generation
of scientists both on the ai side and on
the life sciences side but actually in
really practical projects which which we
undertake together and i think that sort
of uh symbiosis of the ecosystem is
going to be part of the cut part of the
key to us sort of achieving the promise
of of what we're all talking about today
yeah you know in the future um one uh
example of that is
you know we're running uh currently a
fellows program where we're getting
early stage post docs from academia in
effectively to do a postdoc inside
gsk.ai for a couple of years that
community is still connected with its
academic partners
and we're starting to do
quite significant methods development in
that area taking you know existing work
from the literature and advancing the
frontier uh francesco farino and emma
slade for example have recently
published on
using graph neural networks a class of
neural networks which deal with
data that is in a graphical form where
the important things about the native
data are the the networks
that exist between different elements of
the data and which of course is
incredibly relevant to genetic and
genomic data which we see right now and
which is a class of neural networks
that's very difficult and and very
difficult to train very complex to train
these these models can become very very
large indeed um and they've recently
published some work which has removed
that frontier forward
a little bit um
based on our own resources
and actually we can move it forward
another significant step um by
partnering with nvidia on the cambridge
one so that i think that that's an
example where all of these elements are
coming together absolutely it's about
that collaboration um
yeah if i can just add one point which
perhaps we haven't really touched upon
which is really really important is um
what the what the uk ecosystem i think
as well offering is access to extremely
bright people who are entrepreneurial
and those people at the moment they are
limited by two things they are not
limited by their talent they are limited
by access to data and alex limited by
access to infrastructure
if we can break those silos if we can
buy the creation of a consortium and an
infrastructure like cambridge one offer
to the next set of startups and
spin-outs the opportunity to not be
limited by computer power
then i believe we will actually see
solution coming from all or four
different
genius across the country which at the
moment are limited because the only
thing they can do is use a few data sets
available online
on their own computer at home on a
single graphic card and i think we
should not underestimate this i mean
that's hopefully it's going to unlock
the talent we have in the uk and to
bring two patient
new solutions that at the moment are
basically silent not because they
haven't been invented because only
they've haven't been tested and they
haven't been
created at scale
yeah i think you raised a really
important point which is talent i think
we'd all agree um you know talent is one
of the biggest challenges and i want to
ask a number of you really about how we
think about developing this talent for
the future you know how do we develop
how do we maintain how do we attract ai
talent to the health care system in the
uk maybe nadine you could uh give us
some comments there yeah and i think
what's you know i would echo what sub is
saying i mean it's incredibly exciting
to see the potential opportunity that is
only going to enthuse the earlier kind
of stage more upstream
scientists engineers and others to get
excited about the options that they can
get involved in and how they can
potentially train these models train
these algorithms and move that forward
you know a few months ago we you know we
did a panel actually talking about what
should this look like from a kind of and
this was at the launch of one of the
centers that sub leads is what does this
mean when we think about developing
talent and taking that talent forward
and i really think it's it's it's
something we have to be doing upstream
of course when i think about it more
from a clinical aspect is even in our
programs our nursing programs our
medical education programs our pre-med
or you know intercalated programs should
we be bringing these programs around ai
machine learning
knowledge upstream so that at least at
the very least were versed in this new
language which is a new language it's a
new way of doing things and at best
we're actually contributing we're
contributing part-time as as in
sub-school you know we have a lot of
clinical staff that are part-time
working with the academic institution as
well as on the front line treating
patients and that ability that symbiosis
is that synergy of bringing
that skill set that's spending time in
industry coming back into front-line
care spending time in academia and
learning all these new languages these
new words these new opportunities of how
you translate that amazing compute power
that amazing science
that people like oxford nanopore and
others are doing into clinical practice
i think is important the second thing i
think this transcends even clinical care
it's how do we think about reimagining
workflow in hospitals logistics many
other problems that we often don't think
about because we're so focused on
frontline patient care and so probably
to lindsey's point is we you know when
we reimagine what healthcare would look
like i think we have to reimagine our
systems as well and so i think what
cambridge won and what this partnership
is doing is it's kind of giving us a bit
of a kick to to move forward faster and
make those changes quicker absolutely
rosemary from your perspective how do
you think about talent and the
challenges that we face yeah um i mean
i'd echo a lot of a lot of what our
colleagues have just said from our
perspective we think um development
starts uh it starts young
we one of the most engaging programs we
we run at the education programs where
people start sequencing in classrooms um
and when they start sequencing
classrooms that means that they start
base calling classrooms which means that
they need to start understanding what
gpu is pretty pretty early in life we
have that reach out into in into that
sort of level of real real startup and
and that is the kind of thing that
really excites us is is not just
specializing people in in ai development
or specializing people in molecular
biology but actually getting people to
sort of have a really rich um a really
rich understanding of the whole system
um so people who have to get involved in
in wet lab and people have to get
involved in dry lab um to really just
create a different type of talent a sort
of a multi a multi-faceted one um is the
kind that we're really interested in
driving and the combination of both
traditional and new science so it's
fantastic
lindsay from your perspective how does
astrazeneca think about talent and the
challenge we face attracting that talent
to the uk
modern machine learning methods are
incredibly complicated things and as
steve was talking about sort of method
development at the frontier requires
people that really understand
at depth the techniques that we're using
one of the joys of these modern
techniques is the ability to build
modular architectures that can deal with
lots of different data types
simultaneously so
images for example free text omics data
from patients but those systems become
very complicated very quickly you know
folk in my team currently are working on
these kinds of architectures and you
need to be able to build really quite
complicated effectively software
engineering projects and to take
advantage of compute power like
cambridge one you need to know how to do
that stuff well so it really is there's
a kind of unicorn overlap between people
whose sort of mathematics and
understanding of the of the techniques
is really good but also have fantastic
skills in terms of building large
complex software engineering projects so
that's a difficult thing to find at the
moment absolutely um steve from your
point of view how important is nvidia's
investment in cambridge one to the uk
and what it means for the healthcare
system
i think it's going to be a really
important element in in what we do
within this intersection of of ai and
healthcare ain biology over the next 10
years and really you know the starting
gun has been fired on this decade i
think it's going to be a great decade um
for for that uh for that combination and
the uk is extremely well positioned to
to take advantage of it on the skills
side we need to we need to as lindsey
was saying we need to bring together you
know engineering machine learning but
also the life sciences as well where and
you know we have great strength in the
uk in all three of those areas projects
like this
can kind of inspire people to get
involved who are maybe coming from those
life sciences backgrounds um or those
those other alternative backgrounds
those health and medical backgrounds um
and it's that sort of fusion of talent
that we need in order to realize the
promise i think
fantastic
any thoughts from you in terms of the
impact that this will have overall
i'm first hoping that uh our founders
and our government is actually going to
listen to this panel and
we'll take notice that
this i think is a turning point in the
way that the uk is positioning itself
with respect to
the future of health care i think it's
it's very clear from what you could hear
that
people have been talking much more about
gpus as they've been talking about
molecules and of course at the end it's
all about medicine of course at the end
it's all about intervention it's all
about drugs but rather than doing
what could be considered as a random
work to find your solution now we've got
much better tools
to drive this
and as i said at the scale which is
which was not even possible to even
contemplate
a few years ago so i think it will put
the uk at the forefront and it's really
up to us to grab the opportunity
as one of our very eminent colleagues
from gst its own ceo say this type of
opportunity are really in the situation
where you use it or you lose it and i
think that's what the uk is going to be
in this position
and and we need to help the community to
understand it and to to appreciate it
and to embrace it so first of all thank
you to being founding partners of
cambridge one which i think is a
fantastic opportunity and i can't wait
to see the amazing breakthroughs in
science that come from this
collaboration with as we say with
industry with academia and with research
and science but also the opportunity to
really transform health care in the uk
thank you for joining us today it's been
a very interesting conversation
[Music]
[Music]
[Music]
do
[Music]
[Applause]
[Music]
you
Title: GTC China 2020 Keynote Spotlights NVIDIA Technology and Research
Publish_date: 2020-12-14
Length: 7078
Views: 568165
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/LOjmczTTmtQ/hq720.jpg?v=5fd6fc44
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: LOjmczTTmtQ

--- Transcript ---

[Music]
i
am an explorer
searching for the origins of our
universe
[Music]
and charting a safer path to other
worlds
i am a helper
moving us forward one step at a time
and giving voice to every emotion
i love you
[Music]
i am a healer
modeling the future of medicine
[Music]
and finding a needle in a haystack
when every second counts
i am a visionary
[Music]
uncovering masterpieces lost to the ages
and finding new adventures in a galaxy
far far away
i am a builder
[Music]
driving perfection in everything we
create
i am even the narrator of the story you
are watching
the story you are watching
and the composer of the music
and when the world faces its greatest
challenge
[Applause]
i give us the power to take it on
together
[Music]
i
hello and welcome to gtc china for those
of you who are meeting me for the first
time
i'm bill daley chief scientist and
senior vice president of research at
nvidia
i lead nvidia's research labs and work
with our product groups to transfer the
technology we develop in research
to make our products even better than
they already are today i'm going to tell
you
about some of the great technology we're
developing we build the world's most
high performance computing devices and
focus them on the world's most demanding
computing problems
it all starts with our hardware which
today is ampere
here's an ampere a100 sxm module
this is a tremendous amount of computing
performance i'll go into some of the
details in a few minutes
and we can scale the power that's an
ampere from this module for very
demanding computing problems
all the way down to our jetson line of
products for embedded products
if you want to scale up we can take
eight of these amperes and put them in a
dgx box
the gold box you see on the screen and
we can put a number of those into a rack
with our melanoc switches
to build computers that are among the
most powerful in the world but hardware
by itself doesn't solve the world's
computing problems
it takes software to focus this
tremendous computing power on the
demanding problems
and so we've put a tremendous amount of
effort into developing a software suite
to do this
it all starts with cuda which is an
outgrowth of work we did on stream
processing
back at stanford and since 2006 people
have been using cuda
to harness the tremendous amount of
power you can get out of gpus and it
lets you get at every last bit of that
power
then built on top of cuda to simplify
building applications
we have a whole bunch of libraries if
you're doing things having to do with
linear algebra we have cooblos
and coosparse if you're doing things
with spectral methods we have qfft
we have an adaptive multi-grid package
for deep learning
we have qdn and tensorrt that simplify
the task of getting very high
performance implementations of deep
learning
on top of these we build applications in
a number of verticals
of course graphics is one of our most
important areas and i'll tell you a
little bit about what we're building in
the graphics area we have a tremendous
amount of software to support artificial
intelligence
including software for natural language
processing and recommender systems
in healthcare we have a clara package
that runs the gamut
from our parabrex that does genome
analysis to
image analysis to things that mine
databases of
medical papers we have packages for
intelligent video analytics so you can
take video streams and
draw conclusions from what you're seeing
we have an entire package for autonomous
vehicles
from curating the data sets training in
the data center and deploying in the
vehicle itself
and our isaac package makes it simple to
deploy robotic systems
on top of our jetsons so let's talk a
little bit about ampere it's an amazing
device
not only is it the world's largest seven
nanometer chip with 54 billion
transistors
it has a number of innovations that make
it a lot more powerful than our previous
generation it's our third generation
of tensor cores special hardware we add
to our gpus
to accelerate deep learning and in this
generation we've added support for new
data type tensorflow 32
which solves the problem that in the
past you had to choose between b
float 16 which has a lot of bits of
exponent and it's very high dynamic
range
but very few bits of mantissa and
therefore not enough precision to train
many networks or
fp16 which flips it the other way around
it has lots of bits of
mantissa it has a lot of precision but
limited dynamic range
tensorflow 32 gives you the best of both
worlds it has a lot of dynamic range
a lot of precision and so far it's been
able to train every network that we've
applied it to
so we get tremendous amount of computing
performance using the tf32 data type
one of the things i'm most excited about
is ampere has finally cracked
how to exploit sparsity in neural
networks to get better performance
and i'll have a whole slide on that in a
minute if you need to scale
down an amp here because it's such a
powerful computing device
our mig or multi-instance gpu technology
lets you treat one ampere as seven
separate gpus
so you can run separate tasks on each of
them and if you need to scale up ampere
to solve even larger problems
our third generation of envy link and mv
switch have twice the bandwidth with 600
gigabytes per second of bandwidth off of
the gpu
so here's some details about ampere i'm
not going to go through all the numbers
here but to me what's most important
are its performance on three data types
for how well it does in three different
types of applications
for high performance computing ampere
has double precision tensor cores
and so for fp64 arithmetic we can
sustain
19.5 teraflops on executing a matrix
multiply
for deep learning training with new
tensorflow 32 data type
we have 156 teraflops of performance
it's a tremendous amount of performance
for training neural networks
and then for deep learning inference
with into eight we have 1.25 peta ops if
you can exploit the sparsity that i'll
talk about in a minute
and if you can solve your problem with
n4 which many networks can
you have 2.5 peta ops of inference
performance this is just an astounding
amount of performance
so let me tell you a little bit about
sparsity so it turns out that most
neural networks
can be pruned i published a paper on
this at the neurops conference in 2015
and showed that you could prune these
networks cutting out anywhere from
seventy to ninety percent of the
connections between neurons
what this means is that seventy to
ninety percent of the weights in the
network can be set to zero without
affecting the accuracy
now immediately this gives you gains in
terms of compression
you don't have to store all those zeros
so you can get more capacity out of your
memory and more bandwidth out of your
links
but until now people been unable to
exploit this sparsity
to get better arithmetic performance and
it basically was
something we weren't able to take
advantage of something sitting on the
table waiting for us to exploit
with ampere we now are able to take
advantage of it ampere solves this
problem by exploiting structured
sparsity
by allowing two out of four of weights
to be zero
we have a regular pattern that makes it
easy for the hardware to
exploit the sparsity without the
overhead of irregularity swamping the
gains from the sparsity
so for a matrix multiply instruction
once you sparsify the weights to this
two out of four pattern
you get double the performance and even
over an entire application where matrix
multiplies only a part of the
application
such as for the large inference natural
language processing benchmark
we get 1.5 times the performance this is
a tremendous jump forward in
architecture for deep learning
now we can take these a100s and we can
combine eight of them in a box
along with a lot of ssd for storage a
bunch of ram
and nine of our melanox connectx six
nics and this makes a
great computing platform with eight
times the performance of one of these
gpus
since each of them is close to 20
terafops double precision this is close
to 160 teraflops
we can then put a number of these into a
rack this is the dgx pod
the rack also includes one of our
melanox switches to connect up those
nicks and allow interconnection between
those dgx boxes
what's great about a dgx pod is that
we've already solved the integration
problems we've made sure every piece of
hardware here works together
and everything is configured properly so
if you buy a djx pod
you plug it in and it works on day one
with all the containers we have
of software for you to run on them
there's no debugging no figuring out
what the right configuration is
and you can scale these dgx pods up to
what we call a dgx super pod in fact
we built a very large dgx super pod with
280 of our dgx a100s
that's 2240 individual gpus
and this is our saleen supercomputer
it's number five on the top 500 list
and number five in the green 500 list
it's the fastest
industrial system in the world it's the
number one industrial submission to the
top 500.
what's really great about our ampere
technology is not only is it great at
deep learning
that same technology is great for high
performance computing
and this simplifies the integration of
ai into scientific applications you
don't need to do part of it on one
machine a part of it and the other the
same machine does both parts
in fact in the recent top 500 back at
the supercomputing conference in
november nvidia technology was an eight
of the top ten machines
so lean that i just showed you was
number five in both the top 500
and the green 500. number one in the
great 500 was a different dgx super pod
we were also number one in terms of
academic machines
number one in terms of industrial
machines against celine and number one
in both
the us in china where the taiyu white
has a melanox network and number one in
europe with the
eulek machine of all these accolades the
one i'm actually most proud of is the
number one on the green 500 list
because you can move yourself up on the
top 500 by writing a bigger check
configure more nodes connect them up
with a bigger network and you will get a
higher
high performance linpack score but the
only way to move up in the green 500
list is to be more efficient
it actually takes better technology
better architecture better circuit
design
to move up in the green 500 list and so
i'm very proud that an nvidia machine is
number one in the green 500 because
that's
strictly a measure of our technology not
how big a check somebody wrote to build
the machine
now for the ai applications a lot of
people these days
are building dedicated ai accelerators
and they claim that these will be more
efficient
than general purpose gpus this is not
really the case and this chart explains
why
it shows sort of the evolution in many
ways of our deep learning
architectures back in our kepler
generation the biggest instruction
we had to do deep warning with was a
half precision floating multiply
accumulate
normalizing this to technologies these
are all going to be compared the same
that's about 1.5 picojoules of energy
and fetching and decoding the
instruction all of the overhead
associated with doing that instruction
is about 30 picojoules so here the
overhead swamps
the payload we're spending 20 times as
much energy on overhead
as we are on the payload in our pascal
generation we moved forward and had a
half precision.product instruction
dot product a four vector now we're
doing eight arithmetic operations four
multiplies four
adds six picojoules of energy and the
overhead is only five times as much
still
not really acceptable but better
starting with the
voltage generation we introduced our
tensor cores and what tensor cores
really do
is they provide specialized instructions
for matrix multiply accumulate
so in volta where we had a half
precision matrix multiply accumulate
hmma
we now have the energy going into the
payload actually doing 128 floating
point operations
associated with that one instruction now
completely dominating amortizing out
that overhead
so the overhead is only 22 percent and
with touring when we had added the imma
instruction which is now doing a
thousand twenty four in eight operations
the energy of the payload is 160
picojoules and the overhead is only 16
percent
what this means is that if you build a
dedicated accelerator that didn't have
any programmability
you'd be getting a 16 advantage but
you'd also be getting a huge
disadvantage the programmability of a
gpu
allows you to track advances in neural
networks which happen at an amazing rate
new models are coming out all the time
better training methods
and to exploit these you need a machine
which is very programmable
the gpu offers you a completely
programmable platform and by building
tensor cores specialized instructions
amortize overhead
we're able to offer you that
programmability with a negligible
penalty compared to a dedicated
accelerator
this chart tracks a little bit of the
progress i showed in that table
but on a different scale this shows our
single chip inference performance
from kepler in 2012 up through ampere
a100 in may of this year
what you see is that in this eight year
period we've increased single chip
inference performance by 317x this curve
has come to be known as huang's law
which is that inference performance
doubles every year actually we're more
than doubling it every year
and this is in part because of those
advances in tensor core that i mentioned
on the previous slide
better circuit design better
architecture very little of it is due to
process technology
there are only three generations of
process technology on this graph
28 nanometers at the beginning with
kepler 16 nanometers in the middle
and then most recently with ampere and
seven nanometers and those
jumps in process technology gave us very
little of this 317x probably less than
2x overall most of this is from better
architecture and so with moore's law
going away
it's a good thing that we have huang's
law here to keep pushing up computing
performance because we're going to need
it
for a lot of things we want to do in the
future now if you want to compare how
well people are doing
on deep learning performance the way to
do this is with the benchmark suite
called mlperf
most of the people who are players in
the machine learning hardware space have
gotten together
and you know agreed on a set of fair
rules for comparing the performance
on both inference and training and
periodically there's a
submission where you have to have your
results validated and everybody submits
the results
and you know the results are then posted
to the mlproof website
so here i'm showing you the results from
the most recent ml perf training
benchmarks
what you see is that nvidia sweeps all
of the categories we win
consistently in fact our biggest
competition is our last generation this
is all normalized to one x being a volta
v 100
you'll see that ampere's up to 2.5 x
faster
on deep learning and in particular in
the areas that really matter like some
of these large
natural language models and recommender
systems is where the 2.5x are
most of the competitors don't even show
up there's not a single startup that
showed up the only
two other competitors that even showed
up for a couple of the benchmarks are
google with their tpu v3
and ampere beats them soundly and huawei
showed up only for the image
classification benchmark
and got trounce 2x by an a100 the mmo
perf benchmarks are broken into the
training part
which i just showed you in inference and
again nvidia sweeps all of the
categories
the inference part of ammo perf is
itself broken into a data center
inference and an edge inference category
the data center inference we see as our
biggest competitor is our cells and the
gap here is even larger
so compared to turing t4 which is our
previous solution for ai inference
the a100 is between six times and eight
times faster across the board on all of
these benchmarks
the only competitors that show up here
are intel and xilinx
and in all of these cases they're
actually beaten by the t4
ampere just trounces them none of the
startups we're tracking numerous
startups all claiming to have better
solutions for the inference space
show up here if they had something
better they should show it up an
ml perf but they don't bother to show up
the other side of the ml perv
inference benchmark is the edge
inference benchmark this is for edge
servers and embedded devices
here we're showing our performance
numbers for the a100 for the t4
and also for the jetson agx zavier with
our tegra chip
you can see that again nvidia sweeps all
of the categories
centaur shows up and beats xavier and a
few of them but even against centaur the
t4 and the a100
are quite a bit more powerful so as i
said previously
we have software packages that focus the
power of
nvidia's gpu architecture on demanding
problems of interest
and one of the most important of those
for nvidia is computer graphics
we've been a graphics company for for a
long time now
many of you will watch a motion picture
a feature film
that has portions of it done with
computer graphics with cgi
and you'll be impressed with how photo
realistic that looks
and that offline computer graphics is
typically done in a way
where every frame takes hours of time on
a compute farm
to generate by casting tens of thousands
of rays
per pixel using a technique known as
physically based rendering with path
tracing
we've recently been able to come up with
a number of innovations
that approach that photo realism real
time at 60 frames per second or faster
so watch this movie you'll notice as the
lights come on they cast spots of light
on the floor
and they cast realistic shadows from
objects that are in the pathway
the shadows are soft where it's
appropriate each of these marbles has
reflections
showing things that are reflecting off
of the marble and specular highlights
showing where the lights are that move
around as the camera and the lights move
as these glowing marbles roll down
they're casting light on the floor in a
realistic way
and here we see that the shadows move
properly as the lights in the scene move
around
and there are proper soft shadows with
the edges reflecting the distance from
the object to the surface that it's
casting on this is photorealistic
it's very close to what you would see in
a motion picture but it's rendered at 60
frames per second on a single nvidia gpu
so let's look at how we're able to
accomplish near photorealistic rendering
at 60 frames per second it comes from a
number of techniques that i'm very proud
of because they've been developed in
nvidia's research lab
so the first is something we call rtxdi
for direct illumination
and this image actually shows what
things used to look like what
conventional graphics does
with direct lighting you see here a
number of emitters these little
christmas lights
but they're not casting their light on
the adjacent surfaces they're not
casting shadows it looks wrong
this is incorrect lighting with rtxdi
each light is casting its light onto
adjacent surfaces so the yellow light is
making the adjacent surface appear
yellow the green light green and so on
and where things are between the light
and the surface it's casting realistic
shadows
and we can support up to millions of
lights with this technique it's using a
technology called
the reservoir important sampling which
we call restor
and it was published in siggraph 2020
and it's already now in in our
nvidia graphics products and it produces
wonderful direct lighting
so this is half of the problem the other
half of the problem is indirect lighting
so rtxdi makes the lights extremely
realistic for one bounce from the light
to a surface and then back to your eye
or the camera
but lights bounce numerous times
infinite times and to do that we have a
technology called rtx
gi rtx gi casts
light from one surface to another using
white probes
little spheres that we put in the image
at various points to compute the
indirect lighting as seen at that point
so you can have an infinite number of
bounces a surface will illuminate
another surface which will eliminate
another surface
this is done in a way where there's no
leaks
so you if you have a very bright room
next to a very dark room
some you know little difference in
polygon shapes won't let the light leak
through the wall here it's done in a
watertight way
it's also a great example of multi-rate
rendering because the indirect lighting
doesn't change at a very rapid rate
we can render the scene at 60 frames per
second getting the things like
visibility and shading
that need to be done 60 frames per
second done at that rate but recompute
the indirect lighting at a lower rate
depending on the available computing
resources
this is a really wonderful technology
you can see the upper part of the image
looks very realistic with the indirect
lighting in this case almost all the
lighting is indirect since only a little
bit of light
is coming in from the windows the bottom
of the scene is almost dark because
without the indirect lighting
you don't have much you can even
appreciate it more if we turn off the
textures and get the lighting only here
you can see how
much richer the scene looks with good
indirect lighting
another technology that lets us
accomplish a lot at real time rates
is nvidia's dlss or deep learning super
sampling
this is a technology that has evolved
it's now in dlss 2.0 which offers even
better performance in our original
dlss 1.0 so as illustrated in the
graphic here you start
with a image at a certain rate say 1440
and we feed it into a neural network
that upscales it to 4k
and then to make this work we take this
upscaled image and we compare it
to ground truth actually rendered at a
much higher resolution in this case 16k
and the errors there go into a loss
function that's used to train a neural
network
via one of our dgx super pods and that
over certain iterations over data sets
we train the weights of the network to
produce upscaled images
in a very accurate manner now there are
two tricks to making this work
the first is to make it temporarily
stable it's relatively easy to upscale a
still image
but if you were to upscale just a series
of still images without worrying about
motion
you would get numerous artifacts by
doing things a little bit inconsistently
image to image this would cause
very objectionable things that would pop
out at a viewer things little wiggly
worms and the like
that appear where things are
inconsistent from frame to frame so
we've worked very hard
and have solved that problem of temporal
stability we get very temporally stable
videos from this technology the other
thing which is difficult to do well is
to make it generalize
and we've been able to do that so we can
train one neural network and have it
work on
every level of a game and across
multiple games we don't have to retrain
for each game each level or each scene
wonderable network to rule them all
here's how the results look so on the
left is native 4k and the right is a
1440 image that's been upscaled to 4k
the frame rate is shown in the upper
right hand corner what you can see is
not only are we running at a higher
frame rate in this case 141 frames per
second
but if we zoom in on the suitcase on the
character's back
you can see it's a better image it's
sharper you can see more details
in the dlss image than you can in the
native 4k image so it's a wonderful
technology we get faster frame rate
and higher quality graphics so where are
we ultimately going
in video research we're pursuing an
agenda to push our
rendering to be fully motion picture
quality
and to do that we also want to like the
motion pictures do physically based path
tracing but we want to do it in real
time
this image sort of shows what our vision
of that is we want to be casting
rays from the camera be able to bounce
through some number
of specular reflections and refractions
such as through the beer glasses in the
image
in the upper left of this slide and then
as we come out of some number of those
specular bounces
we'll do a couple bounces where each
bounce we do many light sampling using
the restore algorithm i mentioned and
talking about direct illumination
this will give us wonderful direct
illumination and then after perhaps two
bounces which is maybe all we can afford
depending on what graphics hardware
we're running on we will terminate
into one of these rtx gi light probes to
get very accurate
indirect lighting and there are a number
of technologies making this possible i
talked about the rtx di and gi that give
you good direct lighting and indirect
lighting respectively
what allows you to do these bounces both
the specular ones and then the diffuse
ones
bouncing off of these intermediate paths
is the rt cores that are in our most
recent gpus
that really accelerate ray tracing
making it possible for the first time
to do ray tracing in real time graphics
another technology which is very
important
is doing good denoising because we can't
afford to send ten thousand rays per
pixel the way the motion picture people
do
and we need to get by with somewhere
between one and ten rays per pixel
and that will produce a very speckled
image with a lot of sort of shot noise
in it
as if you shot it at a very high iso but
then by applying deep noising and
particular deep learning denoising we're
able to clean that image up and make it
look really great
so this is sort of our near-term vision
for where computer graphics is going
in the longer term we expect computer
graphics to be generated by ai
the images of people you see on the left
are not in fact images of people
they were generated by a neural network
these people never existed
and because they can generate images of
real people they can also generate
images of stylized people as in the
middle frame
and they can generate images of animals
cars
rooms arbitrary scenes and in the long
run we expect that if you want to
produce good graphics
you'll use ai in particular generative
networks to generate these images
directly without ever having geometry
we're just at the beginning of this
today where we can generate single
things a single person a single car a
single room
but we can't compose them well yet we
can't get the lighting to work well and
the interactions to work well but we'll
get there
and eventually computer graphics will be
generated by ai
so the future of graphics is ai in fact
the future of almost everything is ai
it's affecting almost every aspect
of our lives how we work our
entertainment how we play
let's talk a little bit about how ai
came to be where we are today the
current ai revolution was really created
by gpus if you look at deep neural
networks there's three key pieces that
come together to make them work
the algorithms the deep neural networks
themselves the
training data large data sets that may
be labeled in some cases
and the hardware that it runs on the
algorithms have been around since the
1980s
deep neural networks convolutional
neural networks back propagation
stochastic gradient descent
have all been known since the 1980s the
large data sets have been around since
at least the early 2000s with things
like the imagenet dataset
but it wasn't until 2012 when alex
krasinski
developed alexnet a deep neural network
running on an nvidia gpu
that the current revolution really took
off in that one
year he got a performance improvement on
alex net which was more than the
previous
five years of work on imagenet combined
now
gpus enable deep learning they're also
gating its progress
if you look at this chart over just a
few years moving from alexnet to resnet
the demand for compute performance
increased by more than an order of
magnitude
more recently as shown on the right side
of the chart progress in
natural language processing networks
moving from bert to
gpt-3 has been even more rapid and
causing even larger demands on petaflop
days of training time
and the networks people can build are
very much limited
by the power of gpus they can use to
train them they would like to build
larger models and train them on larger
data sets
but they're limited by what they can
train in a reasonable amount of time on
an available gpu
so i'm going to show you this graph
again of huang's law we're trying to
meet this demand and allow progress in
deep neural networks
by doubling performance every year to
allow people to build more powerful
networks and train them on
more powerful models let's look at one
type of network
this is a generative adversarial network
it was developed by ian goodfellow in
2014
and it's a way of synthesizing images or
actually you can synthesize just about
anything with these
and it really works by taking two
networks and training them together
on the left side we have a generator
network the generator network takes a
random number we'll call this a latent
variable it's basically
sampling a latent space a space of the
distribution that it's being trained on
and it generates an image if it's an
image generating gan
the right side of the figure is the
discriminator network it's a neural
network
that takes as an input and image if the
generator generating images it could be
a sound waveform or something else if
it's generating a different modality
and it has a switch and so as you train
it you flip the switch but the
discriminator doesn't know which way the
switch is flipped
and it's trying to tell just from the
image whether the image is a real image
or whether it's generated by the
generator and by training these two
networks together
the discriminator gets really good at
deciding what a real image is
and to try to fool it the generator gets
really good at generating real images
and it will generate real images
according to whatever training set of
real images you use
to train the pair so it's what it's
really doing is it's learning the
distribution of images in this training
set
now a lot of progress has been made
since in goodfellas original work
and much of it actually by nvidia we've
come up with a number of very important
movements forward on a number of years
ago we developed something called
progressive gan
where by training these networks with
the curriculum where we started with low
resolution networks
originally four by four and then eight
by eight and on up to 10 24 by 10 24
pixels
by learning the low resolution first and
then moving on to the high resolution
we're able to make progress where in the
past before we did progressive gan
people had not been able to produce
sharp high resolution images with gans
it was the breakthrough that enabled
that
more recently we developed something
called stylegan which is shown on the
right side of this figure
where we basically take that latent
variable and feed it into its own neural
network
which then decomposes that variable and
feeds different parts of it
into different levels of the generator
network this allows us to independently
control features at different scales
different sizes
and by doing this it makes it much
easier to disentangle the latent
variable to separate parts of the latent
variable
that control different parts of the
image so that we can for example
control whether somebody is smiling or
not whether they have glasses or not
their hair color
by pulling apart that latent variable
space now one application of
gans is to video conferencing in a
normal video conferencing call
you would simply take an image a video
stream and each image of the video
stream we would motion code
and take all the coded pixels and push
them over the wire this takes a lot of
bandwidth
by using gans we can be much more
efficient one aspect of our maxine
technology we send a single still image
over to the receiver
where that still image is used to prime
a generator network
then for every frame of the video we
extract key points
and send just the key points over to the
receiver which is a very low bandwidth
stream
generator network then combines the key
points with the still image
and generates an animated image so we
get very high quality video
with very low data rate for video
conferencing which particularly during
the pandemic is one of the most key
computer technologies around
now what's neat about this technology is
not only can you
animate your own image but if you choose
one day to be a cartoon character
with blue hair you can do that in this
video
tv reporter charlene chan is speaking
and we're taking the key points
from what she's doing along with her
voice and translating them to a still
image which is of this cartoon character
with blue hair
now just as we can use a gan to generate
great images
a lot of ai today involves speech and
text and language so nvidia has a system
for doing that it's called jarvis
jarvis is a multi-mode conversational ai
service so it basically
takes the full gamut you can speak to it
there's neural networks that will
basically take your audio
and recognize from that audio text we
can then feed that text into natural
language models
to do querying translation question
answering
we then can take the answer and feed it
back through a natural language model
generate text and feed that into text to
speech and produce an audio wave form
out
now what's really neat is what happens
when we combine jarvis
which allows us to interact with our ais
via natural language
with gans in a particular gaugan which
lets you paint by numbers you can
paint where you want grass where you
want mountain where you want water and
then it fills in the details and
produces pretty nice images
let's take a look at what happens when
you do that a peaceful lake surrounded
by trees
[Music]
an open grassland on a cloudy day
[Music]
a country road leading to an open field
a beautiful beach in the sunset
now another thing we can do with our
language models is build systems where
we train the language model on a bunch
of text
and then given prompts we can have it
write sentences
here's an example of of our megatron
large language model
on the prompt case we're going to give
it a prompt and see how it writes
realistic sentences given that prompt
so our megatron natural language
processing model has been trained on a
large corpus of data
and from what it has learned it can
generate sentences given a prompt
here we start with the prompt mail
was bored on a weekend from this prompt
the model suggests keywords decided and
go
we reject these and instead type walk
given this prompt and the keyword walk
megatron generates he decided to go for
a walk on the local beach
it also suggests new keywords saw and
beach here we decide to go with those
just hitting return
megatron comes back with at the beach he
saw a group of surfers
and predicts new keywords decided in
surf which we also accept
megatron comes back with he asked one of
the surfers if he could join him surf
no keywords are predicted and we add the
keyword happy megatron
generates he was happy as he got to surf
for the first time ever
now after gans and language models
another really key piece of ai that's
revolutionizing a lot of aspects of life
are recommenders they're in many ways
perhaps one of the most important pieces
of ai in the data center since they're
the ones that particularly
decide which ad to put up in front of
somebody and that determines how a lot
of revenue is generated
they're also for example if you're
watching movies on netflix we'll select
which movie you're likely to be given
next for e-commerce they'll recommend
what products you might like
they're used on social media to
recommend things and most importantly
ads
and these are a very hard ai problem
because in addition to having the neural
networks they involve embeddings
you may have a million items and you
don't want to represent that as a
million bit vector so instead you take
the items and you run them through an
embedding table
to generate a shorter length bit vector
to then run through your neural networks
these embedding tables are very large
and they're very sparse and the neural
networks themselves will end up being
very sparse
so it's a very challenging aspect of
computing nvidia simplified this for
people with our nvidia merlin package
that basically makes this recommender
technology accessible to pretty much
anybody
and by accelerating with gpus we're able
to take
both the etl the extraction translate
and loading part of reading a large data
set to train one of these networks
as well as the training itself and turn
hours into minutes
accelerating the etl part by factor of
90 in the training part by a factor of
60.
there are two aspects to deep learning
training and inference training is where
you take
curated data set possibly labeled and
iteratively use it
to compute the weights for a model a
network
inference is where you take that train
model apply an input to it and get an
output
it might be putting an imaging getting a
classification
putting speech in getting text out
putting a questioning getting an answer
out
putting a random number in and getting
an image out for a gan
whatever it is inference is where most
of the horsepower in deep warning takes
place
and to provide that horsepower we're
making sure that huang's law is
continuing so we're doubling inference
performance
every year actually more than doubling
but just providing that horsepower isn't
sufficient
there's more than just computation to
handling inference it's a complex
problem
involving many people as illustrated
here there's a data scientist
who will curate the data set select
which model to be used
possibly through trial and error trying
many models and different data
and train that model then a machine
learning engineer will optimize that to
get better performance in the data
center
optimizing the return on investment the
optimized model goes into a model store
and an operations team decides how to
deploy those models pulling them from
the operation store
running them on a particular piece of
hardware to take a query in
and get a result for the end user to
simplify this process deploying
inference nvidia's developed the triton
inference server
it's open source software that supports
multiple different backends for
inference
and makes it very simple to deploy
inference in the data center
it supports a standard query reply
interface to the ai application
with either http or remote procedure
call it takes these queries and batches
them
each application is different in terms
of its latency and throughput
requirements
if it's very latency sensitive you need
to run an individual query itself or a
very small batch
so you don't lose a lot of time waiting
for other queries to show up
in other cases you can afford to be more
efficient by getting a larger batch
that's all handled by triton in the
dynamic batching module
once you have a batch it gets dropped
into a per model scheduler queue
to wait its turn to run and then
eventually it gets deployed on either
gpus or cpus we have multiple backends
and we can handle models that are
written for different backends in a
given data center
you may have some models written in pi
torch others in onyx
others in tensorflow we can handle all
of those and more
so it's very easy to interoperate models
and allocate your gpu resources
appropriately
it all makes it very simple in fact
typically you're not running just a
single model
but you're running an ensemble of models
as in the jarvis examples previously
where you may have one module that does
feature extraction feeding into a neural
network that does speech to text
feeding into a natural language model
that does say question answering and
then back out through speech synthesis
and ultimately the waveform generation
triton can take an ensemble like this
and understand how to schedule all these
different modules
and feed them together simplifying the
deployment of complex inference in the
data center
another key application area for
artificial intelligence is healthcare
nvidia's clara package is a suite of
applications designed to accelerate
healthcare on the gpu
as shown here we can start with genomics
with our parabrix package
and take a genome sequence do assembly
do alignment
do variant calling to help personalize
healthcare for an individual based on
their genome
many pathogens like kovid have structure
and how therapeutics interact with that
structure is important
packages like cryospark help discover
that structure with cryospark one takes
a bunch of images with x-ray
crystallography
and then on the gpu cryospark constructs
the structure of the
of the virus from those images once you
have a structure you can do docking
experiments
with auto dock with our rapids database
system we can take billions of
therapeutic compounds
access them out of the database run
autodoc on define which ones the most
likely to interact with the covid virus
and then
select those for further screening that
further screening would be done in
various molecular dynamics simulations
to find out which ones actually bind to
the particular sites on the virus that
are most interesting and
are therefore suitable for advancing to
do physical tests on
clara also includes a lot of facilities
for imaging it can analyze
x-rays mammograms ultrasounds helping
radiologists
discover things that are important for
their diagnoses whether it's from a
genomic analysis or
image or anywhere else along this
pipeline a doctor may want to see what
the literature has to say about
something
the medical literature is huge with too
many papers for any individual to keep
up on
so we have our biomegatron which is the
natural language processing system
so the doctor could query a particular
thing that they're looking for
by megatron we'll search that medical
literature and suggest which articles
are most relevant
and be able to answer questions about a
particular condition
as one example about how gpus are
accelerating health care
at many different time scales consider
the examples shown here
folding at home is a program where
people with gpus can donate unused
cycles to be used to take
protein sequences and run folding codes
to try to discover structure from
sequence
this takes months of time on many
different gpus but it's 30 times faster
than other methods
moving up to the days scale taking that
x-ray crystallography data these x-ray
images of frozen
virus one can discover the structure of
the virus in 12 days instead of five
months as was previously required
more recently deep mind has released
alpha fold
which is a way of applying reinforcement
learning to learn
structure from sequence which will take
the amino acid sequence of a
protein for example of a gene sequence
of the virus and be able to discover its
structure
using artificial intelligence much
faster than previously possible in
minutes
on the other side once we have some
therapeutic compounds we want to screen
them against a known structure
autodoc has been gpu accelerated to run
33 times faster than previously possible
and the oak ridge national laboratory on
their summit supercomputer based on our
gpus was able to screen two billion
compounds in one day which would have
previously required three months
once those compounds are discovered they
can be simulated
using a code like torch to screen
millions of drugs in eight minutes
instead of being hundreds of days
if further analysis is needed there are
now full computational chemistry
packages
that run on the gpu that can be used for
additional analyses of these drug
candidates
at the beginning of the pipeline we
start with genomics
nvidia's parabrex package accelerates
genomics on the gpu
it includes modules for doing assembly
when you sequence a genome you get a
number of reads
they could be anywhere from a few
hundred bases in length to tens of
thousands of bases in length
but it's like a jigsaw puzzle we need to
assemble all of these reeds a big
one-dimensional jigsaw puzzle
into the full three billion base human
genome
terabrix does that assembly and then
once it's done it will compare the
assembled genome
to a reference genome and call out
variants places where
a particular individual's genome differs
and those variants can be very important
for therapeutics
as shown in the middle plot here there
are many different ways of calling
variants
but parabrix accelerates all of them by
10x to 40x
allowing people to very quickly discover
what's really interesting about a
particular genome
now peribrix can operate on three types
of genomes the germline genome is the
one you were born with
the somatic genome is one that's taken
from a tumor which may be mutating very
rapidly as a cancer advances
an rna sequence is one taken from a cell
that's expressing a particular protein
it's the dynamics of genome expression
the plot at the right shows if we have
this rna sequence data
parabricks has modules for analyzing
letting us visualize what's happening
with gene expression
it takes a very high dimensional space
where there's a dimension for each
protein that might be expressed
and projects it down in a
two-dimensional space in a way that
gives the most insight
and then lets an analyst zoom in on one
part of that space
to really visualize what's happening
that's relevant to what they're trying
to discover
so overall nvidia parabrix accelerates
this process
of taking sequence data and getting
insight from it
the revolution in artificial
intelligence that's been enabled by gpus
has also enabled a revolution of
robotics to date
most robots are very precise positioning
machines
they are programmed to millimeter
accuracy to move an actuator be it a
spray gun a spot welder or some gripper
to do a very repetitive task but with no
interaction with the environment it's
completely open loop
but with deep learning we can build
robots that perceive their environment
interact with their environment and at
nvidia research robotics laboratory
we're doing this work let me show you
some of the interesting things we're
doing
to interact with this human pick up a
block
this robot needs to compute paths and
control its motion in smooth ways that
avoid obstacles
we've developed a new technology for
this called romanian motion policies
that basically are able to express
mathematically this complex motion
problem
in a way that's simple to solve in real
time to make it more difficult than just
stacking these blocks
in simulation we'll create some
artificial obstacles these purple
cylinders
and the robot will very quickly be able
to compute a path through those
cylinders to grip the ball
despite where it moves once we can move
in unknown environments
and avoid obstacles what we want to do
now is to understand how to manipulate
unknown objects
a lot of people have done work on
manipulation but it usually involves
training the robot
for a very particular object and then if
it sees an object it hasn't been trained
on
it can't grip that these videos show
robots learning how to grip objects
they've never seen before we've trained
the gripper using artificial
intelligence in a way that generalizes
so it can produce good grip points for
unknown objects
it can also learn to grip objects in the
presence of obstacles
here we want to pick up the cup but we
see that the sugar box is blocking it so
we'll change our target initially
and command the robot to pick up the
sugar box move it out of the way
again computing the grip on that using
ramani motion policies for the motion
and come back retarget the cup with the
obstacle out of the way it's able to
compute a good grip for the cup
grab the cup and carry on with its task
one thing which makes gpus particularly
applicable to robotics is the fact that
we can train robots
in the simulated world and then have
them take what they've learned in the
simulator world and apply it in the real
world
this video of four legged robots
learning how to walk and deal with
obstacles is a great example of that
we start with the four-legged robot
knowing nothing and using reinforcement
learning
and a curriculum we train it to walk on
progressively more difficult surfaces
we start with a flat surface then we
have stairs of different steepness
surfaces with various obstacles blocks
and and
irregular obstructions and after having
mastered the simple surfaces our robot
moves on to the more difficult ones
it learns how to deal with each of these
and then in the real world
a robot trained in the simulated world
is able to walk over a set of blocks in
its path
it's able to go up and down stairs all
using skills that it learned in the
simulated world training its policy
network
to come up with a particular action in
response to each state that it
encounters
it learns different gates for dealing
with different types of stairs and other
surfaces
so being able to train in the simulated
world and apply in the real world
makes gpus all more applicable because
we're able to train many robots in
parallel
using the parallelism of the gpu in the
simulated world
yet another application of ai is to
autonomous vehicles this is an extremely
complex problem
involving many types of sensors cameras
radars lidars
real-time computation where the vehicle
is moving at high speed and has to
predict the action of the other vehicles
and pedestrians and other actors around
it
but the stakes are high 1.3 million
people are killed each year on the
highway
and by building autonomous vehicles
controlled by gpus running artificial
intelligence that
don't text while they drive don't drive
while impaired can completely focus
and have a tension 360 degrees around
the car at all
points in time we can greatly improve
the safety of
driving on the highways to do this is
not simply a matter of deploying some ai
in a car but it's an end-to-end
problem that starts with data collection
you have to produce a huge data set
of labeled data from all of the sensors
from cameras radars lidars ultrasound
devices
and then take all that data curate it
because not all data is equally
important you don't want to just put all
data into your training sets
you want to select the most relevant
data to train your models and then train
models in the data center on big dgx
super pods
to produce the trained neural network
models that will be deployed in the car
but before deploying them in the car we
want to simulate them with a hardware
hardware-in-the-loop simulation it will
take the actual
ai hardware that will be in the car
running those models
and synthesize what that model will see
will generate synthetic video streams
for the cameras
synthetic lidar data for the lidars
synthetic radar returns for the radars
and validate that those models work
properly in simulation
then the actual software that runs in
the car has a number of components to it
there's drive av which is the autonomous
vehicle part it's the part that drives
the car
and i'll talk about the many different
models that go into making that
but it has components for perception
that use all those sensors to sense the
environment around the car
components for planning that decide
where the car should go whether it
should accelerate or break
and prediction to predict what the other
vehicles pedestrians other objects in
the scene are going to do
so it can plan ahead given likely
courses of action for the other entities
there's also a component for in the car
drive ix that is a camera that monitors
the driver
sees whether gaze is what they're doing
it can monitor their gestures they can
control the car through gesture
rather than having to press buttons or
knobs then there's drive rc which
provides remote control
should something happen where somebody
has to take over remotely we have a
package that allows remote operation
of the vehicle to do this requires
world-class neural networks for
perception
we have to detect obstacles understand
the distance to objects the time to
collision
we have a neural network that finds free
space spaces where the car should go and
conversely spaces the car should not go
we do this not just for cameras but also
with lidar and radar
we have a neural network that projects
paths to help the path planner find the
best
direction to find signs to do mapping
it'll find high beams on the highway
it'll help you park will handle
different characteristics
weather different intersections traffic
lights and the like
it's a huge computational load multiple
cameras
multiple other sensors lidars and radars
each running many models
to pull out many different pieces of
information
needed for the planning process to work
forward to carry out this huge
workload we have a variation of our
ampere architecture that's specialized
for
the edge and in particular specialized
for autonomous vehicles
and we can size this depending on the
workload of a particular autonomous
vehicle
if all we need is driver assist we have
a 10 tire ops
5 watt version of our oren ampere-based
embedded chip
that can handle that task for a level 2
autopilot
we have a 45 watt 200 tops or in
agx that can carry that workload out for
a full
level 5 robo taxi that gives full
autonomy we have a super computer that
goes in the car
it's a pair of orange and a pair of
a100s that provides
two peta ops of performance at 800 watts
and is dual so that there's redundancy
if part of the system fails a subset of
the sensors is processed on the other
part of the system
to continue operating the vehicle at
least until it can be safely stopped
so i'm going to get to the favorite part
my favorite part of this keynote
which is to talk about some things that
are going on in my research laboratories
these are three projects from nvidia
research and i'd like to emphasize at
the beginning
that these are research projects they
may or may not turn into products at
some point in time in the future they
are not
nvidia products the first thing i'm
going to talk about
is how we're going to continue huang's
law and continue doubling inference
performance every year
we've been able to more than double it
each year over the past eight years
going from
kepler to ampere we're working on a
number of research projects
that are looking at different
alternatives any one of which could
continue this doubling for the next
several generations to do this we've
built a bunch of
very efficient deep learning inference
accelerators in the research labs the
photo on the left shows rc18 rc stands
for research chip
this was done in 2018 and it achieves
nine tear
ops per watt and is scalable from point
three tear
offs to a hundred and twenty eight tera
ops it's a
ray of processing elements 16 processing
elements on each of these small
chips then assembled with 36 of the
chips on a multi-chip module
and by building a very efficient
inference engine this is able to achieve
higher inference efficiency than
deployed products today
we learned a lot from building rc-18 and
in particular we learned that a lot of
the energy that goes into inference
doesn't go into the payload arithmetic
operations even with very efficient
tensor cores
it goes to moving data around so we
built a system called magnet
which does design space exploration it
allows us to explore
different organizations of deep learning
accelerators and different
data flows different schedules for
moving the data from
different parts of memory to different
processing elements to carry out the
computation
using magnet we're able to simulate deep
warning accelerators that look basically
like this
we have a global controller controlling
an array of processing elements
that feed data between their local
memories a global buffer that's on the
chip and dram that's off chip
within each of the processing elements
there are buffers to hold weights
and input activations vector multiply
accumulate units max
each of these vector max carries out
multiply cumulate
over a vector that can vary in length
from 8 to 32 and then we have from 8 to
32 lanes of these max we have anywhere
between 64
and 1024 multiply accumulates happening
per cycle
the output of these multiply accumulates
drop into the
output activation unit at the bottom
where they're accumulated in the
appropriate ways and stored
in the output activation buffer waiting
to be written back to the global buffer
the figure on the right shows the
details of a vector mac just a bunch of
multipliers going into a big accumulator
doing essentially a dot product
operation
most of the popular data flows or ways
of organizing the choreography of data
on these chips
involves either holding the weight
stationary while the input activations
run by and the output activations are
sequenced
and as shown in the little pie chart on
the bottom this results in most of the
energy being spent in the accumulation
buffer 55
and less than a third only 29 is
actually in the arithmetic of carrying
out the inference
an alternative then if the problem is
that the outputs are not stationary
you're spending all your time on
accessing that accumulation buffer is to
hold the output stationary
and all this does is push the problem to
the input side and
all the energy is now spent in the
weight buffer because you have to
constantly access new weights
so seeing this problem we innovated and
decided we had to add a level of storage
we call collectors weight collectors and
accumulation collectors
which then adds another level to our
data flow another
level to the nested loop which is
carrying out this convolution in a deep
neural network
and by doing this we're able to move the
the computation
so that rather than only a third of the
energy being in the arithmetic in the
payload as it were of the computation
we're able to get 60 or more of the
energy in the payload of the computation
and this particular unit was able to
carry out inference
at 29 tariffs per watt this was
published at the end of 2019
and we've since advanced this
architecture to the point that while it
hasn't been published yet
we now are achieving 100 tear offs per
watt for inference
so we're continuing this evolution of
wong's law continuing to
more than double inference performance
each year this isn't an announced
product but we expect that these
techniques
will be incorporated into future
versions of tensor cores and future deep
learning accelerators in our embedded
chips for automotive and robotics
now as we scale up we need to connect
multiple gpus together
and we do this with nv link and mv
switch technologies that were developed
in mv research
but as we look ahead we think that it's
going to be increasingly difficult to
continue to double the bandwidth
of the 30s that we use in envy link each
generation
we're currently operating at 50 gigabits
per second per wire pair
we can see our way going to 100 perhaps
to 200 beyond that
the the waters are very murky so we're
looking at an alternative technology
to actually signal out of our gpus in
and out of our mv switches
using light using photonics this shows a
concept
where on the left we show our existing
dgx with electrical signaling
and on the right what an optical dgx
might look like where we package two
gpus
on a vertical card that then get
interconnected with
fiber optic bundles between the cards
the part on the right side of each card
are the light sources that provide
multiple wavelengths of light sort of
the optical power supply as it were
for the optical engines that modulate
this light to signal at very high data
rates
we see our way with silicon photonics to
have many wavelengths per fiber
operating each of those wavelengths
at anywhere between 25 and 50 gigabits
per second for aggregate data rates
between
400 gigabits per second and a few
terabits per second
for fiber and a very conservative number
would be we would do the signaling at
four picojoules per bit
where the electrical signaling is more
like eight picojoules per bit and where
the electrical signaling is limited in
reach
to about a third of a meter about a foot
the optical signaling would have a reach
of 20 to 100 meters so we could connect
much larger systems
with a single hop of envy link rather
than having to constantly repeat as is
it necessary with the electrical
signaling
the way we plan to do this is with a
relatively new optical technology called
dense wavelength division multiplexing
we start with an optical power supply a
comb laser source
shown on the left that produces many
different colors of light these colors
are spaced
very close together perhaps 100
gigahertz
spacing between the different colors we
feed this comb
of laser light onto our optical engine
chip on the transmit side
and it comes along of optical bus where
it gets modulated by ring resonators
each ring resonator is tuned to one of
these wavelengths
and can either pick off that wavelength
or allow it to pass
so we can modulate a bit stream of say
25 gigabits per second
on each of these colors of light and so
if we have say 32 colors of light
at 25 gigabits per second each then that
gives us 800 gigabits per second
aggregate
on the fiber going out at the receiving
side
we again have a set of ring resonators
that are being used to pick off
one collar and feed it into a photo
detector that goes into a
trans-impedance amplifier
to feed out that channel for us to
receive this will be packaged
with a gpu sitting on an interposer that
communicates over an organic package to
a photonic integrated circuit that
contains those ring resonators
and the waveguides to run the light
around an electrical
interface chip that basically receives
the short reach interconnect
from the gpu and controls the ring
resonators
on the pick to modulate the light at the
receiving side again there's an eic and
a pic
but for receiving the part of the eic
that's important are the trans-impedance
amplifiers
and the pic includes the photo detectors
so here is a artist conception of how
dgx systems using this might look in the
future
we can connect a very large gpu tray
with these in this case we have 18 gpus
per row and nine rows for over 160 gpus
connected
and their inputs and outputs all come
out in a big fiber optic bundle
the mb switch tray on the right shows
that we have a number of nb switch
cards each with one envy switch and an
optical power supply
and they're connected to the blue fibers
that are sort of inputs
the orange fibers that are outputs and
the gray fibers are interconnecting the
individual envy switches within this
tray
to form a clone network that allows us
to route data from the input to the
output
so the final research project i'm going
to tell you about is is leg8
one of the key issues that faces us with
gpus is that there are many more
applications
that could benefit from the
accelerations gpus give
then we can take the time to decode in
cuda even using all the libraries we
have
so we're constantly trying to make this
programming process simpler
now a lot of people already do numeric
analysis using python
and particularly the numpy library
within python
so we've developed a package called leg8
which sits on top of a data
aware task scheduling runtime system we
have called legion
and this allows us to take a python
program
and run it transparently from a jetson
nano
all the way up through a dgx super pot
without changing a line of code
all we have to do is take our original
python code
and change the line that says import
numpy as np
to say import legate.numpy as np that
loads the library
and everything from there on is done
automatically so we can run
on a jet to nano on a single a100 on a
dgx a100 as shown here
or a dgx super pod let's look at the
results of what happens when we run this
here's a relatively simple example of
jacobi iteration that the code is shown
at the right side
and to start off with let's look at what
happens when we run on a single gpu
the real relevant mark here is the
orange square
which is our current best library coupe
for running this jacobi iteration and
what you see
is that leg8 running on the gpu actually
runs slightly faster that's the green
line
if we run on das which is an alternative
task scheduler you see as we scale up
the number of
gpus performance falls off pretty
rapidly
whereas if we run leg 8 and this is leg8
running on a cpu since the comparison
was to dash running on a cpu
you see that the performance remains
flat because leg8 is able to do a better
job
of parallelizing the task and keeping
all of the gpus busy
when we run leg8 on the gpu there's a
little bit of a fall off in performance
i should say here that these curves show
what's called weak scaling
every time we increase the number of
gpus we're increasing the problem size
by can measure an amount so perfect
speed up would be a straight line
and here are a straight line out to a
few doublings of gpus
then we fall off toward the end because
leg8 is unable to get perfect
parallelism out to very large number of
gpus
something we're still working on so let
me wrap up
at nvidia you know we're constantly
working to build the world's fastest
computing devices
and then build the software that focuses
those computing devices on the world's
most demanding and important problems
it all starts with our ampere gpu
and we can scale the performance of this
ampere gpu
from this a100 down to our small
embedded uh jetson devices and we can
scale it up
through the dgx the djx superpod up to
some of the world's fastest super
computers
and then we can focus that power at any
scale from the jetson
to the supercomputers onto problems from
graphics
artificial intelligence healthcare video
analytics
autonomous driving and robotics by
having a huge set of libraries and then
vertical stacks built on top of those
i've shown you a little bit of the
glimpse of the future by what we're
doing in our research labs
that is really just the tip of the
iceberg we're doing very many exciting
things
and i think the future is going to be a
very exciting time as we're able to
bring some of these to fruition
build even more powerful computing
devices and apply them to a wider array
of problems to make people's lives
better thank you very much
and have a great day
[Music]
[Applause]
[Music]
i don't need someone to save
[Music]
like a shockwave to my
[Music]
is
[Music]
is
it's a chemical reaction
[Music]
like a shark wave to my
[Music]
is
is
[Music]
[Applause]
you're the one that lights me
[Applause]
[Music]
[Applause]
[Music]
like a shark wave to my system maybe
you're a crazy
[Applause]
[Music]
me
[Music]
just thinking of us too and if you're
really out there it's really you
then what's the road that leads me to
if you're really out there
[Music]
if you're really out there it's really
you
then what's the road that leads me to
if you're really out there
[Applause]
[Music]
i've been waiting for you isn't that
enough i never wait for someone
else but you make me laugh i have been
thinking of you isn't that love i try to
tell myself to stop guess it's not
enough
[Music]
for you
[Music]
[Music]
i've been waiting for you isn't that
enough i never wait for someone
[Music]
is
my lips don't care if i'm wrong
[Music]
don't care
[Music]
hello welcome to gtc china
nvidia executive panel my name is
raymond tay
i'm responsible for nvidia's asia
pacific sales and marketing business
and i'm your moderator for this panel
session
today we will hear from four nvidia
leaders
as they describe the company's latest
breakthroughs
and their relevance for the china
[Music]
ladies and gentlemen please allow me to
introduce our four
speakers for this panel first speaker is
mr jay puri
executive vice president worldwide field
operations
jay is responsible for global sales and
regional marketing for all
nvidia's products and services our
second speaker
is mr greg estes vice president
corporate marketing and developer
programs
he is leading the company's efforts
engaging with more than 2 million
developers
and several thousand ai startups our
third speaker is ms kimberly powell
vice president healthcare responsible
for nvidia's healthcare
business globally and our fourth speaker
is mr ashok pandey
commonly referred in china as pandi
pandi is nvidia's asia pacific vice
president responsible for operations
and partners we have a very diverse
background
and certainly a very a wealth of
experience
we have a lot to cover today so let's
get started
jay if i may let me start with you with
the first question
how important is the china market to
nvidia
well nihao so raymond
uh you know china besides just the size
of the market is extremely important to
him in
nvidia it's so strategic and it's one
that
we make a very significant investment in
nvidia has been in china for more than
20 years
we first got started with our uh
pc gaming platform uh geforce
there are millions of geforce fans in
china
and uh you know it's absolutely
fantastic
we have a terrific ecosystem of uh
partners aic's distribution partners and
so on
game developers some of the most
important trends in pc gaming have
actually come out of china
things like free to play
i cafe esports and so on
so you know one really needs to
understand the china market to be
successful
worldwide so china is extremely
important to nvidia
and i want to thank all the geforce fans
we just introduced our
a new lineup of uh rtx 30
products which are just absolutely
fantastic i hope you guys are going to
try them out uh and then
of course you know 10 to 15 years ago
we realized that moore's law
was reaching its limits and
we pioneered this new model of computing
called accelerated computing
uh started out in high performance
computing and then
for mostly scientific applications
but now over the last few years
uh this platform has been adapted for
artificial intelligence and data
analytics
uh and i have to say i think uh running
apac you you fully understand this
uh china has been at the forefront
of adopting ai to provide a competitive
advantage to the industries
uh you know the universities are doing
some
really leading edge research some of the
most
important researchers on ai in the world
are in china
and the startup ecosystem is so vibrant
so you know that what's really
gratifying is
most of all the work in ai
in china is being done on nvidia's ai
computing platform
so to say that china is important to
nvidia
is a little bit of an understatement
china is extremely
important we try to learn from the china
market to improve our platform so we can
continue to
you know provide
what our partners and customers need
over there
and i would like to take this
opportunity to really thank them
for entrusting uh you know putting your
trust in nvidia and we will continue to
innovate and deliver what you need
thank you jay that was a great uh
response to my question
jay talked about our business uh greg um
but developed gtc is our developer
conference
can you share with us what you're doing
with the developers
well i'd love to and and let me first
also welcome
everybody to gtc china this uh this
kicks off another
amazing event for us for our developers
we're going to have tens of thousands of
people who join us here
at gtc china with more than 200
talks of varying kinds from all
different parts of the marketplace and
from researchers and from core
developers and from students and
and others china is is our in many ways
our most important developer market we
have more than 400
000 registered uh developers in our
program in china which is larger than
any other country
in the world and so uh so china is at
the forefront of what we do
and those registered developers in our
program
of which there's about two and a quarter
million of them
uh right now all around the world uh
they get access to
our sdks you know we put a lot of work
into not only cuda but
a hundred other sdks that we have of
everything from
healthcare which kimberly is going to
talk about to robotics
to financial services to you know really
every market that
that we're in and gtc is where all of
that comes together because
you you get not only the developers
sharing their most
important work and their most important
research
on gpus but you have the rest of the
ecosystem in the industry too right
all of the platform providers and in in
brand new areas
including what the startups are doing
and uh you know jay touched on that a
little bit you know we have hundreds of
of startups in our
startup program inception uh in china
and they're here and well
represented as well and that's kind of
great because
many times the very leading edge
work is coming out of startups right and
it's coming out of the universities and
our connection to
uh the universities and china is better
uh than really almost any country in the
world really any country in the world
and so we're
very proud of that connection and super
glad that everybody is here
and joining us and sharing their work
let me switch to a subject that is
affecting all of us
and i'm talking about the global
pandemic kovit 19.
kimberly from your perspective can you
tell us how
the ai and accelerated computing
contribute to global response to the
kovit-19 pandemic
yeah sure raymond i hope all of you are
staying healthy and safe
and coming back to health and being
stronger than ever
the pandemic is a defining moment in the
global healthcare industry
it marks the biggest threat to the
global health in the last century
and so our race to track test
discover vaccines and therapies has
catalyzed the world really to
use all of the modern technology
available to us
and we know we all share here at gtc
that ai is the biggest technology force
of our lifetime
and covet is really a super charging
moment and i see
the ai healthcare really coming upon us
and it's only enabled by everything greg
and jay and raymond have already touched
on
it's a complete ecosystem approach and
we're so delighted that so many of our
academic partners our startup partners
our
healthcare industry partners in china
have harnessed
every piece of technology to really
fight this fight
and it all starts with genomics
china is an industry leader in genomics
alibaba
and their genomic services offered gpu
accelerated gene comparisons
so that governments and health officials
could really understand the transmission
and evolution of the virus
and then china was very quick to realize
that
medical imaging something nvidia is very
passionate about with our
clara imaging computational platform
that imaging can be an extreme source of
understanding not only the detection of
coronavirus
but also the treatment of coronavirus
leaders like
ping an united imaging startups
infravision and shuken
they put their medical imaging covid ai
technology
into thousands of hospitals across china
so that these tired overworked
front-line workers had
the ai technology at their fingertips to
make
the best choices for their patients and
get their patients back to health as
quickly as possible
so this is an absolute defining moment
the ai healthcare era
is here it was supercharged and 2021 is
the beginning of this new era
it's a tragedy but also a place where we
can
understand how important technology is
and that the global ecosystem
has come together to really fight this
fight
great wow thank you kimberly that is
fantastic to hear
of all the good things that we are doing
with uh from genomics to medical imaging
etc
we all look forward to ai improving
health care
a jay i want to bring you back to uh the
next question
um i have for you nvidia recently
announced plans
to acquire arm can you add some color to
that
what were the reasons to do so yeah
raymond uh be glad to uh
yes i extremely important
development for not just nvidia but for
the entire industry
uh first of all arm is an amazing
company
right i mean they sell more
cpus than any other company in the world
but 22 billion that's with a b
22 billion armed cpus are sold
uh every year and uh
you know they pioneered the sort of uh
ip licensing model uh the
product is just fantastic it's the most
energy efficient core
it's also very high performance so uh
when arm became available of course
we are more than delighted to have them
uh be part of nvidia
but that is only the beginning of the
story really
i think by putting arm and nvidia
together
we are going to be able to make a huge
contribution
to our industry um
you know arm as i said has been
extremely successful
but most of their success has been
in the mobile space and the embedded
space
but there is a lot more opportunity here
one of the most
obvious places where arms should be
playing a role
is in the cloud data center pc
environment
and you know they're starting to
to address that market but frankly
uh it's a really a tough uh
a tough thing to do at this point uh
when they got established in the mobile
space
uh it was basically greenfield right uh
nobody else quite had the energy
efficient technology or the
or the open uh licensing model
and so the whole ecosystem developed
about
around them and they are very successful
the data center pc cloud space is a
completely different story
uh the x86 architecture completely
dominates that space
at this point right and so
uh of course because uh arm
is is great technology we are
seeing success in that space
um amazon has the graviton too for the
hyperscalers that they're using
internally
uh the fastest supercomputer in the
world uh from fujitsu the fugaku ii
number one on the uh you know top 500
uses arm technology and then most
recently apple has said that they're
going to be using the arm
in the m1 family of products for all
their macs and so on
so the technology is fantastic
but when people try to provide it as a
merchant cpu uh
it's being very difficult uh in fact uh
you know qualcomm tried making a uh
made a good sweep you know on cpu on the
general purpose market
centers uh broadcom tried it marvel
tried it
but all of them in the end uh have not
really been successful and
redirected their programs and again it
is not because of
the technology the technology is
fantastic uh it's because there is just
so much ecosystem that is built around
x86 it's hard for another
architecture to come and uh you know
take a significant
position and regardless of the merits of
the technology
now as you know going forward
the most important workloads in the data
center
are around accelerated computing and
artificial intelligence
and so forth and nvidia's platform is
extremely well established in that space
uh and you know we have a full strap
that is available we have all the
partners that are necessary
uh the ecosystem is huge uh you know
more than two million developers
uh and lots of startups lots of industry
research going on university research
going on
so we have the platform that is
what is necessary as to what is required
to be successful in the data center
going forward
and once uh i think uh you know arm is
part of nvidia
and we both put our focus
on making arms successful in the data
center
we're going to get this done and then
you know there will be a viable
alternative to x86
not just in the mobile space but uh for
uh the data center uh and the pc and the
cloud and
you know all of it and that is frankly
uh extremely good for the whole industry
it's good to have competition
you know it drives innovation and
i'm really looking forward to it and in
particular i think it's good for china
so nvidia continue on with arm's ip
licensing model absolutely i mean that
is one of the fundamental reasons why
they have been successful
and so we completely believe in that
we admire the model i think it's a
fantastic
business model and we will 100 percent
uh
continue with that uh in fact
i'm quite hopeful uh that once arm is
part of uh nvidia and we understand
uh you know exactly how they do it we
will be able to take
more of our technologies both in the gpu
and networking space
and make them available to the world
using this licensing model
so jay uh given the current dual
political climate
some chinese companies have expressed
concerns
that arms acquisition by nvidia may
restrict their ability to access
armed technology my question to you to
you is
should they be worried yeah remember
that i'm glad you
you asked me that because
there is a little bit of a
misunderstanding
about how export control laws work
the export control laws work
based on where the technology is
invented
you know the origin of the technology uh
they are not
so focused on who owns the technology so
whether arm is owned by a japanese
company like softbank as it is today
are owned by nvidia in the future that
does not
actually impact the export control
regulations
and so of course all of uh arms key
technology today is invented in
cambridge
uh england uh and uh we are all we've
already committed
that that's going to be the center of
development for future on technology
when they become part of nvidia so
there really will be no change in terms
of
export control regulations uh once uh
arm is part of nvidia
thank you jay that was a great uh answer
and i'm going to ask pandy the next
question in mandarin
pandey how
foreign
we talked a lot about data center and
panties gave a very comprehensive answer
on our csp
i want to switch to another topic which
is very important
graphics and is our heart and soul
uh greg what are the most important new
advancements
for developers in gaming and graphics
well it's it's pretty obvious at this
point that
real-time ray tracing has been you know
already started and is changing the
industry to the point where
i'm not sure that you can you can say
that you're doing serious work
as a technology company in graphics if
if you're
not put a lot of energy into ray tracing
and we've certainly been leading that
over the last few years uh beginning
with the rtx line and the work that we
put in uh
to having specific cores on the gpu
called rt cores
uh to accelerate ray tracing and it's
changed gaming
right it is you're seeing it be adopted
in the biggest games
you know in fortnite justice in
in china uh cyberpunk
2077 coming up you know the the biggest
games taking advantage of our latest
technology is just fantastic and more
and more
almost literally every day and it's
changing more than gaming too by the way
because of course
there are other markets particularly
here in china where architecture
is continues to be such an important
marketplace to be able to to visualize
spaces in in a way that uses the real
reflections and and
light in the way that they physically
act in the world is important for so
many different
marketplaces not just gaming but
particularly gaming right
the second thing that we're seeing is
the adoption of artificial intelligence
for graphics you know one of those
implementations that that we're very
proud of and we think is some of our
most important work is
dlss deep learning super sampling and
for those of you that
aren't familiar with that it's
essentially using ai
to render at one resolution and then use
ai to make that at a much higher
resolution which of course
increases your frame rate and improves
game play
and the way that that works is is that
there are models that are trained
uh to use artificial intelligence to
fill in pieces of information
that would have been there if you had
higher resolution and it gives you an
incredibly
sharp uh view in in your monitor and
extremely good gameplay because again
we're doing the core rendering
at a smaller resolution and and then
doing the super sampling using deep
learning
to do that and again we're seeing more
and more games all the time
uh take advantage of that and together
you see that
the trend here is is to put the highest
level of realism possible that you
can for this and that gives such great
enjoyment to people
who are seeing shadows and reflections
and and these things affect your
gameplay too right you can do things in
gameplay that you can't do
if you don't have this level of physical
reality
and of course that works for training
systems and as i said
architecture and media and entertainment
and everything else so
these core technologies that are being
driven certainly
out of gaming they're affecting other
markets as as well and china has been
one of the leaders
uh in adoption of this as well you know
the the chinese games are
being some of the early adopters here
you know i mentioned justice
is a great example and and we're super
proud of the work that our developers
are doing
many of whom are here presenting their
latest technology at gtc
great wow thank you uh and certainly
this year is a strange year for us and
many of us are staying at home
we are seeing many gamers enjoying the
new features
in ray tracing and dlss i want to
switch back to the pandemic kimberly
can i ask you what is the biggest
challenge or change
in the healthcare industry as a result
of the pandemic
yeah you know raymond we're at a time in
history that has never happened before
not only because of the pandemic but
actually because of all the other
care technologies that have been being
invented over the last decade
let me tell you something that most
people don't know today we can create
more biomedical data than ever in our
history
in one quarter we can create more
biomedical data
than in the 300 year history of most of
the pharmaceutical industries
this is creating with data more data
than we've ever had
ai and data center computing that we
have today
it's absolutely what i call the perfect
storm
for a computational global defense
system
think about it if we can create more
biomedical data today
we have the the technology of artificial
intelligence
and we have the computing capacity of
things like what pandy was mentioning
our dgx super pod we literally have the
perfect storm
and governments all over the world are
realizing this
they're realizing that our
pharmaceutical industry
that is still suffering from too long of
a time
frame to create an a new therapy or
vaccine
sometimes on the order of 10 years it
costing two billion dollars
and still having only a ten percent
success rate
they realize with this pandemic that we
need to
take more investment into the drug
discovery
industry and nvidia has been working on
this problem actually for over a decade
drug discovery really calls upon every
single
computer science domain that there is
and that nvidia is world-class at
it starts it with graphics to be able to
visualize
biology and chemistry it has simulation
so you can understand how biology and
chemistry are interacting
and it has to use artificial
intelligence so that we can
tackle much larger problems and simulate
much longer time scales
and work on much larger data sets and so
we're at this
critical time where we can in fact
create a computational defense system
you can see that in academia
some fantastic work just came out of
qinghuang university and xi jiang
university
where they were able to use
cryo-electron microscopes who can create
terabytes of data in a day they were to
use that technology
to create a complete picture molecular
architecture of the coronavirus and when
you create that data they put it into
a public database so that every other
researcher or
industry researcher can take that and
start the search
for vaccines and uh anti-viral drugs
they publish this work very recently in
cell which is one of the
most prestigious life sciences journals
and that work
and the fact that the ecosystem can
generate that data
create such insightful data digitizing
biology
we've never been able to digitize it at
this accuracy and this
at this um scale before and contributing
back
to the entire industry at large that's
the global defense system and it's
it's a very fortuitous uh circle so
governments realize now
that investing in academia
super computing in their own drug
discovery market
is vital and so nvidia recently
announced our clara discovery platform
as i said we've been working on it for a
decade
everything clara is is domain specific
we have clara parabricks for genomics
we have all of our life sciences
applications in the area of
cryoelectron microscopy molecular
docking
molecular dynamics simulations clara for
imaging
and recently we also published our own
state of the art
natural language models and biomedical
language models
a complete new language is is inherent
in the
healthcare data that we just talked
about and so
we're right at the cusp of this brand
new drug discovery process
that is completely fueled by artificial
intelligence and computing
and so i believe that forever more
changed
is the idea that we have the ability we
have essentially a time machine
data ai and compute is essentially a
time machine
and that's exactly what you need in a
pandemic so that we can accelerate
our discovery of therapies and vaccines
and every government around the world
and all of the
pharmaceutical industries are taking a
new approach
at the process of drug discovery and
investing in the r
d and it with clara discovery and dgx
super pod
we literally have it all packaged up
into
a complete ai driven uh drug discovery
infrastructure
and the world is is taking to it we're
going to build our very first
super computer in cambridge uk that is
dedicated to biomedical
uh research we call it cambridge one
it's going to be uh
the uk's fastest supercomputer
completely dedicated
to this cause because we know we're at
this perfect storm
and that we know we have no more time to
wait for a vaccine and anti-viral drugs
to
to help us in this fight event against
covid19
so the pharmaceutical industry is
forever changed
and i think that's a good thing it's
made us realize that we can now digitize
completely
biology and we can couple that with all
the other
computer science domains out there to
really build this time machine
and this global defense system for all
the future pandemics
thank you kimberly for sharing with us
so many use cases
and also giving us an insight into clara
platform
are we certainly so excited that
nvidia's ai technology can help the
healthcare industry in so many ways
let me switch to one of the most
promising and growing businesses
in china and this is in our consumer
internet space
it is the broadcasting business and let
me switch
foreign
[Music]
so greg i want to ask you the next
question
in china there are many a.i startups
an important part of gtc china is our
work
with these ai startup companies what are
we doing with startups in china well you
think about the companies that are here
at gtc
they're some of the most innovative
companies in the world
baidu alibaba tencent
xiaomi bite dance dd and on and on and
on you know just great
innovative companies and they were all
startups at one time or another right
and that's one of the reasons why we've
put such a strong investment in focusing
on helping startups to grow giving them
the best access that we can to our
technology
and in fact we have more than 800
startups
as part of our startup program which we
call inception
in china right and what do we do for
them well the first thing that we do is
we give them access to our
technical resources right they get a
personal connection with nvidia if you
will
and that's important to startups that
are trying to take advantage of the
absolute best technology and a lot of
times it's much better to talk to a
human than it is to try to get it out of
documentation than others right
the second thing that startups want to
do is they want to be able to scale
right a lot of them start with with gpus
on-premises and then want to scale up
into the cloud
and we have programs to make those gpus
more available to them you know we have
grant programs and discount prices to so
that they can start their company and
get going
on there and then if you think about it
as a startup what's
the other thing where we can be most
helpful to them a lot of times it's
giving them visibility
so we help them with marketing programs
we do success stories with them and we
put nvidia's
marketing muscle uh behind raising their
visibility out to the world
and a lot of times part of that is we
have these special connection
sessions with vcs so we'll take venture
capitalists
and sometimes big companies too and
we'll do the matchmaking to put them
together
so that we're again raising the
visibility of these startups to
other companies or venture capital firms
that could be interested in investing to
them
so there's a technical component to what
we do we try to help their business get
going
we raise their visibility to others
through marketing and we give them a
deep
connection to us through training
through our dli program
and and other technical services that we
have and we think all
all of that together we have one of the
the best and most comprehensive startup
uh programs in technology we have almost
7
000 startups worldwide in our
in our program think about that 7 000 ai
companies uh all working with nvidia
today
it's it's a wonderful program and so for
those of you that may be watching this
that are
are with startups that may be interested
i encourage you to check out
the inception program and if you're not
a startup but would like to get tapped
into
this community that's doing so much
innovation
then contact us as well and we can help
connect you to some of these really
great startups and gtc is the perfect
place to do that
thank you greg we really touched on a
number of topics today we touched from
healthcare to crook
to cloud service providers to startups
to gamers
uh and also um you know to the whole
entire business
uh jay the next my final question goes
to you
our grout to market is through our
partner ecosystem
our china partners are so important for
nvidia
i would like to ask you something about
how we're working to promote ai with
them
one of the recent announcements made by
nvidia is the dgx
a100 and the dgx super port
they are indeed a great ai
supercomputing platform that is becoming
very
popular in china my question to you is
how do you differentiate
you know nvidia's own dgx a100
supercomputer with the many forms of oem
branded gpu computer
servers made in the market and what is
your strategy and positioning between
these oem servers
and nvidia's own branding of dgx
uh sure raymond uh first of all let me
just
say what you just said which is uh
partners are
extremely important and our primary go
to market
for nvidia is through partners there's
absolutely no question about that
uh so let me explain to you why we do
dgx
uh ai is evolving
at a breathtaking rate right i mean
kimberly just told you
what's happening in healthcare and we're
just scratching the surface
um we have just come so far
i mean it's only five years ago we were
you know recognizing cats and feeling
really great about it
and today we are using ai for
um you know natural language
understanding conversational systems
uh recommendation systems i mean
the impact that ai is having and can
have
is just absolutely amazing and so
uh you know the data sets are getting
huge
believe it or not the amount of ai
computing
performance that is required with some
of these new uh
neural networks like vertinol uh you
know is uh
increasing at 2x every couple of months
i mean it's just absolutely
hard to believe uh what's going on and
of course nvidia is the leader in ai
has to uh has to keep up with it in fact
we have to drive this
and uh so we do a lot of
uh innovation
all the way up and down the stack
starting with the gpu and then the
boards around it the systems
all the software going all the way up to
the application frameworks like uh you
know clara
and jarvis and merlin and so forth right
so
we have to innovate everywhere
and now because of the scale at which it
is being done in fact
frankly it's not even possible just to
stay at the server reference
architecture
we have to look at data center scale
computing and that's one of the reasons
why we
bought melanox and now we have uh data
center scale
products uh with uh dgx super pod
and so forth um nvidia
ourselves uh we have celine
has the dgx super pod that
we use which is an ai supercomputer
number five on the top 500 list overall
but definitely the fastest ai
supercomputer
that exists in the world so you know we
are making a huge amount of investment
and we have to to stay stay
you know be the leader in ai
but there is one other thing that
everyone really needs to understand
nvidia is a very
open company and so when we do
all of this r d at all levels
we make it available uh to our partners
at different levels of integration okay
so yes we design bgx and we you know
obviously when we do all this work uh
there are some customers that want to
have a very direct and deep relationship
with the media
and they buy a dgx product or they buy
dgx super pod
but by and large we take what we do and
then we give it to our partners
so that they can build their own systems
uh around this architecture not only do
we provide this reference architecture
to them
but we then work with them to
certify their software stack all the way
up to ngc
and the containers in ngc that have uh
you know all the applications uh such as
those for clara
and other verticals
we look at the help the oems design
these systems
based on our reference architecture uh
we make sure they're fully compliant
make sure the
performance is excellent uh and uh
then we you know brand them as nvidia
certified
okay and the market is huge there's all
kinds of requirements there is no way
nvidia can or
once you fulfill all of these uh you
know
as i said we really expect our oems
and other system builders to deliver the
nvidia's ai platform to the market uh
to uh you know help us all succeed and
to drive
ai forward and i'm happy to say
you know i think we're doing a fantastic
job we have some
uh really great partners
uh in china uh you know inspire and
lenovo the new h3c
networks you know there's a long list of
partners
that are investing in our platform and
are providing solutions to our customers
that they really seem to
appreciate because our business is
growing at a fantastic rate
both for us and for our partners and i'm
really very grateful to them
for you know working with us
thank you jay and ladies and gentlemen
that's all the questions that i have
we tried to cover everything a broad
a white area or business that covers our
partners our customers our industries
our ecosystem etc i hope you find the
session informative
i want to thank jay great kimberly and
pandy for joining me
in providing this session for all of you
and thank you to all our speakers for
providing
the great insights and please do not
forget
to get in touch with any member of the
nvidia
team if you need more information or if
you have any follow-up questions
please also take the time to view the
200 plus
sessions and talks at gtc china given by
experts from the industry
from our customers our partners and from
nvidia
ourselves thank you again and on behalf
of all the speakers on behalf of nvidia
i hope everybody have a great gtc so
that's it for now
thank you and enjoy gtc china
thank you
Title: NVIDIA GTC May 2020 Keynote Pt 8: NVIDIA Ampere Architecture Comes to Orin for Autonomous Vehicles
Publish_date: 2020-05-14
Length: 384
Views: 139152
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/LTDPdp0WS3c/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: LTDPdp0WS3c

--- Transcript ---

One of the most exciting applications for
AI is autonomous AI.
This is essentially a data center on wheels,
and AI is processing at real time with sensors
feeding it, and it has to process so fast
that it can make decisions in a split of a second.
Autonomous vehicles is one of the greatest
computing challenges and will surely be one
of the most impactful.
This is one of the largest industries in the
world.
10 trillion miles are driven each year.
It is because of this that we've created an
architecture that is scalable.
Ladies and gentlemen, today I'm announcing
that the Ampere architecture is coming to
NVIDIA DRIVE.
Because of the scalability of the Ampere architecture
the efficiency and incredible computational
performance, we can now scale our driving
car computer from ADAS behind the windshield
all the way up to robot taxis.
From a low power chip for ADAS driver assistance
to a powerful in-car computer for full self-driving
autopilot all the way up to driverless robot
taxi: one single programmable architecture.
This is basically the entire range of everything
that moves, and it's our belief that everything
that moves will eventually be autonomous.
This will represent cars, passenger cars,
taxis, trucks, shuttles, delivery bots.
We're going to have autonomous vehicles of all kinds.
The Ampere architecture will span this whole range.
We also developed an end-to-end system for creating the AV autopilot application: testing
and simulating it, deploying it into the car,
all the way to operating it with remote control.
This entire system is built on one architecture.
NVIDIA is collecting data, labeling and augmenting the data, training the AI models.
We created a virtual reality driving simulator, testing the entire stack in the car, to an
AI agent that would assist you, all the way
to a virtual reality telepresence system that
allows you to portal into the mind of the
car so that you can remotely control the car.
This end-to-end system and the entire scalable
architecture from ADAS to autopilot to robot
taxi is the DRIVE system.
Now, let me show you our latest update.
Before I show it to you, let me first remind
you everything you're about to see is in virtual virtual.
All of our engineers are safely sheltering
at home.
They could develop their application at home and virtually drive their autopilot in our data centers.
Please enjoy.
Wasn't that great?
The NVIDIA DRIVE platform is an open platform.
We're doing so, so we can understand the
great challenges of this new computing form.
NVIDIA is developing the DRIVE system from
beginning to end.
From the collection of data all the way to
the testing of the cars.
We're developing the entire stack from top
to bottom and we're creating an architecture
that spans ADAS to autopilot to robot taxis.
This entire platform is open.
You can engage the platform however you like.
You could use our libraries, use our tools,
in any part of it to augment your own.
The openness of our platform is the reason why we have so many partners around the world.
Cars, trucks, tier one OEMs, mobility services,
startups, software companies, mapping companies
and simulation companies.
We have partners all over the world using
DRIVE, developing all kinds of different forms
of autonomous transportation.
This end-to-end pipeline, this end-to-end
infrastructure is one of our greatest achievements,
and we've learned so much in doing this about
the future of autonomous vehicles.
Title: NVIDIA SC22 Special Address
Publish_date: 2022-11-14
Length: 2726
Views: 712716
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/m8baYkVrYIw/hq720.jpg?v=636ef933
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: m8baYkVrYIw

--- Transcript ---

[Music]
super Computing is the driving force of
Discovery in every field
scientific
to Industrial
allowing researchers to understand the
behavior of the smallest particles
furthest expanses of the universe
to unlock the meaning of life
[Music]
and with digital twins its giving
Industries superpowers to time travel
letting them explore an infinite number
of futures
[Music]
for different lenses
[Music]
and with million x higher performance
powered by accelerated Computing Data
Center scalability and AI
super Computing will unlock new
opportunities
for assault
[Music]
Computing is the instrument of
scientific discovery the engine of
Industrial Automation
and the factory of AI Computing is the
Bedrock of modern civilization and it is
going through full-scale reinvention
for two decades
from the mid 80s to the mid-2000s CPU
performance scaled with transistors and
order of magnitude every five years
increasing ten thousand times over 20
years but CPU performance scaling has
plateaued incremental performance comes
with disproportional increase in
constant power
meanwhile demand for computing continues
to grow exponentially across Science and
Industry driven by a broad range of
applications from digital biology and
climate science to AI robotics Warehouse
Logistics and consumer internet services
without a new approach Computing costs
and even more urgently computing power
will grow exponentially in the coming
years
data center electricity has already
reached nearly two percent of global
electricity use
exponentially growing demand is coming
up against moratoriums on the energy
consumption of data centers and
corporate commitments to achieve Net
Zero the industry has awakened to the
need to advance Computing in a post
Moore's Law world
a 100x growth in Computing within the
next decade is unsustainable without a
new Computing approach
it is now broadly accepted that
accelerated Computing is that approach
but it takes work
accelerated Computing requires full
stack optimization where software and
Hardware are co-designed we optimize the
entire stack including the chip compute
node networking storage infrastructure
software acceleration libraries and the
application
and this is done one application domain
at a time the work done for molecular
Dynamics differs from that of fluid
dynamics
seismic processing CT reconstruction
quantum chemistry Ray tracing Logistics
optimization data processing and deep
learning
the diversity and combination of
applications algorithms and Computing
infrastructure are daunting
Nvidia is a multi-domain acceleration
platform with full stack optimization
for a wide range of Science and
Industrial applications
nvidious and clouds
super Computing and Enterprise data
centers
PCS industrial Edge devices robots and
cars our dedication to architecture
compatibility has created an installed
base of hundreds of millions of gpus for
researchers and Developers
and nvidia's Rich ecosystem connects
computer makers and cloud service
providers to nearly every domain of
Science and Industry
the results of accelerated Computing are
spectacular
accelerated workloads can see an order
of magnitude reduction and system cost
and energy consumption and with the
speed ups and savings applications from
molecular Dynamics to climate simulation
to deep learning have scaled up one
hundred thousand to a million times in
the last decade
we have dedicated the full force of our
company to advance Computing in this new
era today we will update you on our
latest work
Ian Buck will announce Next Generation
data center platforms
Tim Costa will update you on our
acceleration libraries and Quantum
Computing work
getika Gupta will announce new platforms
for remote sensing and Edge Computing
platforms
and Dion Harris will highlight our work
in applying AI to physics called modulus
physics ML and AI to physical systems or
Omniverse digital twins
I want to take this opportunity to
congratulate Jack dungara for receiving
the touring award Jack's seminal work
and numerical libraries MPI for scale
out distributed computing and standard
benchmarking to objectively measure the
performance of a computer
will continue to be the foundation of
high performance Computing for years to
come
I am very much looking forward to his
touring award lecture have a great super
computing 2022.
thanks Jensen one of the most exciting
things about coming to Super Computing
each year is the amazing work that's
being done by the HPC community
scientific Computing is a critical tool
in solving some of the greatest
challenges facing our world today over
the last year there have been some
amazing breakthroughs powered by HPC a
combined team from Stanford Oxford
nanopore Nvidia UCSC and Google
published a method where the entire
genome was sequenced in just seven hours
currently there are over 7 000 rare
genetic disorders and this new workflow
offers hope for patients with
undiagnosed or rare conditions the
University of Pennsylvania researchers
have used convolutional neural networks
to catalog and classify the shapes of
over 27 million galaxies they use data
from the Sloan digital Sky survey in the
dark energy survey to create a model
that was 97 accurate in detecting even
the faintest galaxies resulting in the
largest catalog of galaxies and their
shapes to date
early detection of covid-19 variants is
Paramount for the proper management of
the ongoing pandemic a team led by
Kareem bagir used a large language model
with diffusion dnns to generate a
theoretical covid-19 genome variant
they then simulated it using openmm and
predicted the Threat Level using Alpha
fold their results showed that this
variant had the potential to escape
immunity this early warning system could
evaluate new variants in minutes and
provide critical information to help
contain the spread of covet 19. all of
these amazing breakthroughs were powered
by a diverse mix of systems across
multiple sites extending to the edge
connected to the cloud and even remote
sensors we see five major workloads that
are powering breakthroughs in HPC at the
center of simulation and the foundation
of HPC and will continue to be the
Bedrock of Science in addition HBC plus
AI can improve our scientific
productivity by several orders of
magnitude over the last five years the
number of research papers published in
AI accelerated simulation has increased
by 50 times supercomputers are being
brought closer to Edge experiments
turning data collection instruments into
real-time interactive research
accelerators digital twins use
simulation AI surrogate models and
observe data to create real-time digital
twins that are revolutionizing
industrial and scientific HPC and
finally Quantum Computing there is a ton
of research being conducted at Super
Computing centers today to emulate the
quantum accelerators of Tomorrow the
modern super computer will leverage all
of these Technologies to solve the grand
challenges of the 21st century
accelerated Computing is a full stack
problem that requires coordinated
Innovation at all layers of the stack it
starts with amazing Hardware our CPU GPU
and dpus are integrated into Hardware
platforms from The Edge to on-premise
into the cloud Nvidia melanox networking
is the connective tissue of the
accelerated data center making the data
center the new unit of compute with sdks
Frameworks and platforms we aim to
provide researchers the technology to
build the software the world needs for
the next Discovery these include Hollow
scan our Edge Computing and AI platform
that captures and analyzes streaming
data for medical devices and scientific
instruments modulus a framework to help
build AI models which learn from
simulation data and physical equations
the Nvidia HPC SDK simplifies the
application development process by
providing GPU optimized acceleration
libraries in ISO standard languages
nvidia's Quantum Computing Technologies
KU Quantum encoda are helping bring
Quantum Computing and Quantum
applications closer to reality and
Omniverse which provides an open
collaborative environment to co-locate
multiple heterogeneous types of data and
visualize it all
as an accelerated data center platform
company we rely on our partners to build
Integrated Solutions that power the
workloads of the modern data center the
Nvidia h100 gpus are in full production
and our OEM partners are unveiling
dozens of new hgx and OBX systems at
Super Computing this year be sure to
stop by their boost to see the
incredible work that we're doing
together
the foundation of the Nvidia stack is
the processors themselves and we
couldn't be more excited about hopper in
the Nvidia h100 earlier this year Nvidia
announced the h100 GPU built with a
custom tsmc four nanometer process it
features five groundbreaking inventions
a faster more powerful tensor core six
times faster than its predecessor and
ampere it's built to accelerate
Transformer networks the most important
deep learning model today the second
generation Mig are multi-instance GPU
partitions the GPU into smaller compute
units that can divide each h100 into
seven separate instances this greatly
boosts the number of GPU clients
available to Data Center users
confidential Computing allows customers
to keep data secure while being
processed and maintain privacy and
integrity from end to end on shared
Computing resources our fourth
generation Envy link allows gpus to
communicate faster than ever before at
900 gigabytes a second of bandwidth
between server nodes and scaling to up
to 256 gpus to solve these massive
workloads in the AI factories of the
future the DPX instructions are
dedicated cores that speed up recursive
optimization problems like Gene
sequencing protein folding route
optimizations up to 40 times faster the
golden Suite is a tool we use inside
Nvidia to measure progress across HPC Ai
and data science workloads It's a
combination of common applications like
Amber gromax AMD Quantum espresso icon
chroma and vasp but it also includes
common AI training benchmarks like Bert
large and Resident 50 and random Forest
data analysis you can see the
performance of cpu-based servers
starting with a dual Broadwell system
common in 16. over the past six years
CPU only servers have only approved less
than about 4X while a p100 equipped
server was already 8X faster than that
Broadwell Baseline today we're very
pleased to show how the flagship of our
product line the Nvidia h100 achieves
nearly 250 times performance since that
original system in 2016. every day more
apps take advantage of GPU acceleration
but many hundreds of Legacy applications
have yet to adopt in addition we're
entering an era where performance and
efficiency must grow together
the cost of energy is more uncertain
than ever we are very excited about what
our First Data Center CPU will enable
the grace Superchip CPU pairs the
highest performance arm Neo versus V2
CPU core with one terabyte a second of
memory bandwidth and it's designed for
the compute intensive and memory bound
applications common in HPC while Grace's
performance excels its focus is on
Energy Efficiency and can provide up to
2.4 x better performance per watt than
today's CPUs
even applications that have adopted
accelerated Computing have large
portions which remain CPU Limited either
because the cost of communication to the
GPU is too high or refactoring the vast
lines of code still running on the CPU
has just hasn't been taken on the Grace
Hopper Superchip offers a first of its
kind NV link chip to chip interconnect
so that both the CPU and GPU have
coherent access to a combined 600
gigabytes of memory this capability
Bridges the gap for legacy CPU
applications and makes accelerating HPC
in ISO standard languages truly possible
compared to the a100 solutions Grace
Hopper will deliver two to five times
more HPC performance just like the gray
CPU the Grace Hopper is very energy
efficient depending on the needs of the
workload Grace Hopper can dynamically
share between the CPU and the GPU to
optimize application performance making
it an excellent choice for energy
efficient HPC centers assuming a one
megawatt data center with 20 of the
power allocated for CPU and 80 towards
the accelerator portion using grace and
Grace Hopper data centers can get 1.8 x
more work done for the same power budget
compared to the traditional x86
deployment Grace and Grace Hopper is the
path forward to maximize performance and
save on energy costs
in the future every company will have ai
factories super computers need to become
Cloud native the Nvidia cloud native
super Computing platform brings several
Technologies to provide performance
storage and Security One accelerated
performance with programmable in-network
computing two security and isolation
including job isolation and performance
isolation three computational storage
functions including compression and file
system management and finally four
enhanced Telemetry for smart scheduling
to improve utilization of the super
Computing resources
the combination of nvidia's Quantum 2
400 gigabit a second infiniband switch
and Bluefield 3 dpu brings amazing
capabilities of in-network computing the
Nvidia Quantum 2 switch includes
integrated Hardware engines for data
reduction operations covering both small
and large messages all at 400 gigabits a
second speed Bluefield 3 includes
Hardware engines for MPI tag matching
and all the wall Communications and of
course the armed cores and the datapath
accelerator
this combination enables MPI and nickel
from the host to the network and the new
dpu Computing platform to increase the
performance of AI in scientific
simulations as a result we're
accelerating a variety of applications
by 20 or more
Microsoft ventured very early into
machine learning and artificial
intelligence with Nvidia but also has
been a long time Pioneer in traditional
HPC and leveraged the performance and
flexibility of infiniband across their
HPC instances at Super Computing
Microsoft is announcing two new
instances with the Nvidia Quantum 2 400
gigabit a second infiniband networking
that will turbocharge HPC workloads from
cfd to FEA to molecular Dynamics and
weather simulation
we have already entered the era of
exascale AI in the last year five new
exascale AI systems based on the h100
gpus Grace Hopper and Grace CPU
Superchips have been announced in 2021
we announced Alps at cscs and earlier
this year at ISC we highlighted venado
Atlanta the most recently announced AI
supercomputers are the mar nostrum 5 at
the Barcelona supercomputing Center
powered by h100 gpus and the Shaheen 3
at the Kaus supercomputing Center
powered by Grace Hopper Superchips we
look forward to seeing how researchers
take advantage of these new exascale AI
systems next up we have Tim Costa to
share more about our accelerated
libraries and the work we're doing in
Quantum computing
thanks Ian
today's Grand challenges require
simulation to occur at unprecedented
scales
simultaneously domain scientists are
focused on reducing time to science
rather than on increasingly complex
computer science enabling developers to
address this scale and Technology with
velocity requires a complete
bottom-to-top ecosystem of software
libraries tools Frameworks and
applications with Innovative features
for data center scale accelerated
Computing enabling developers to focus
on science starts with an investment in
accelerated libraries from linear
algebra to Signal processing Quantum
simulation communication data analytics
Ai and much more Nvidia libraries
provide a foundation upon which
scientists can build applications that
get the best performance from The
Accelerated data center while
transcending Hardware Generations all
with drop in ease of use
and the scope of Nvidia libraries
continues to grow with new libraries
that enable novel compute applications
and groundbreaking features like
multi-node multi-gpu Support over the
past 20 years nvidia's investment in
enabling scientists has transformed the
field of programming languages the
launch of Cuda in 2007 ushered in a
revolution in the accessibility of
accelerated Computing and in 2020 we
announced another breakthrough with the
support of standard languages running on
gpus natively with the Nvidia HPC SDK C
plus Fortran and python developers can
now write parallel first code directly
in their language of choice and benefit
automatically from GPU acceleration but
the true measure of these software
Investments is an application impact
Magnum io's nickel optimizes Collective
communication patterns for multi-node
multi-gpu vasp is one of the most widely
used HPC applications in the world we've
worked with the vast developers to
integrate nickel into the application
resulting in never before seen
scalability
ffts are an essential component in many
scientific domains the coup fft library
now supports multi-node multi-gpu
execution enabling large ffts to scale
to full data center scale sizes when
integrated into gromax this results in a
5x performance improvement over the
existing code as well as the ability to
scale out to significantly larger
problems than before
the roadmap for studpar or standard
language parallelism is rich
the C plus committee is now working on
adopting a new model for asynchrony
called senders which gives C plus
programmers a way to express asynchrony
and concurrency this feature is expected
in C plus 26 but we believe it is too
important to wait and I've created an
implementation of this proposal that's
available in our HPC SDK with our
sender's implementation the palibos
application for lattice boltzmann
simulation has been ported to stimpar
using senders we were able to strong
scale palibos to 512 gpus with near
perfect scaling and remember this is
pure standard C plus plus no additional
programming model for the GPU for the
node or for multi-node communication
this is just the start of what's
possible for developers adopting
standard parallel languages in their
applications
Quantum Computing is a fundamentally new
model of computing with many unique
challenges and opportunities to impact a
broad range of applications
the progress made by the community over
the past decade has been impressive from
one to two qubit devices at Institutes
of higher education a decade ago to
systems with tens to hundreds of qubits
available for General users in public
clouds today
this is remarkable progress
however to get to the point of useful
Quantum Computing there remains
tremendous work to do on the hardware
side Quantum processor Builders need to
continue improving qubit scale
infidelity it's generally understood
that quantum computers will be ready to
act as accelerators for some important
applications when they reach the point
of fault tolerant Quantum Computing with
thousands to millions of qubits error
corrected to hundreds or thousands of
logical fault tolerant qubits just as
important is the work to be done in
software applications and algorithms
with improved classical Quantum
integration and Innovation and
algorithms and applications the scale
and Fidelity required for useful Quantum
Computing can be reduced accelerating
the path towards useful Quantum
computing
to meet that challenge and prepare for a
Quantum accelerated future governments
institutes for Higher Education and
Research as well as industry are
investing heavily across the board in
Hardware software and algorithm
development
GPU super Computing is essential for
Quantum Computing in two areas the first
is quantum circuit simulation with
Quantum circuit simulation on the Nvidia
platform researchers can develop
algorithms at the scale of valuable
Quantum Computing long before the
hardware is ready
on the Nvidia platform we're already
simulating Quantum algorithms with tens
of thousands of perfect qubits
representing the future state of fault
tolerant Quantum Computing the second
area where GPU super Computing is
essential is hybrid Quantum classical
Computing as we move past basic
algorithm r d and work on building full
Quantum applications with tight
classical Quantum integration a platform
for hybrid Quantum classical Computing
with emulated Quantum Resources is an
essential research platform
long term all useful applications of
quantum Computing will be hybrid with
quantum computers acting as an
accelerator for key kernels alongside
GPU super computing
in 2021 we introduced KU Quantum an SDK
for accelerating Quantum circuit
simulation
kuanum is built to accelerate all
circuit simulation Frameworks and is
integrated into Circ kisket Penny Lane
Orchestra and more
with KU Quantum researchers can simulate
ideal or noisy qubits with a scale and
performance not possible on today's
Quantum Hardware or with unaccelerated
simulators KU Quantum has been adopted
by a broad range of groups spanning the
entire Quantum ecosystem including
supercomputing centers academic groups
Quantum startups and some of the largest
companies in the world BMW is leveraging
coup Quantum to optimize pathfinding and
routing for robots large consultancies
like Deloitte and softsurf are
developing applications in Quantum
machine learning for both materials and
Drug Discovery to address their
customers most pressing problems
Fujifilm is leveraging quantum to
explore tensor Network methods for
Material Science simulation with
thousands of qubits the coup Quantum
Appliance is a container consisting of
leading Community Frameworks accelerated
by KU Quantum and optimized for the
Nvidia platform
coming in Q4 of this year the KU Quantum
Appliance will provide native multi-node
multi-gpu Quantum simulation this means
scientists can leverage an entire
accelerated supercomputer as a single
Quantum resource through a software
container with the same familiar
interfaces they are using for the
quantum work today
recently researchers leveraged the coup
Quantum Appliance to participate in the
ABCI Grand Challenge
a variety of problems including Quantum
volume Quantum phase estimation and the
quantum approximate optimization
algorithm were run across 64 nodes on up
to 512 gpus the performance achieved was
up to 80 times better than alternative
multi-node Quantum circuit simulation
solutions enabling problems with scales
that would otherwise be time prohibitive
we're really excited about the results
our partners are seeing leveraging Coupe
Quantum to accelerate their work and I'd
like to point out a few recent
highlights we recently partnered with
xanadute to integrate to Quantum into
Penny Lane the leading framework for
Quantum machine learning and with AWS to
make that available to customers through
their bracket service combining these
tools AWS saw speed up of over 900 times
on simulating Quantum machine learning
workloads along with a three and a half
times reduction in cost for their users
Xanadu is also leveraging coup Quantum
for Research into novel Quantum
algorithms at supercomputing scale in
general the ability to simulate Quantum
circuits is limited by system memory
with the world's largest supercomputers
limited to qubit scales in the mid to
high 40s using a novel circuit cutting
technique they were able to accurately
simulate the quantum approximate
optimization algorithm with up to 129
cubits running on perlmotter through
nurse Quantum information science
initiative and Johnson Johnson who
recently spoke about this result at our
GTC conference has seen a 100 time speed
up from KU Quantum for their work
applying a variational Quantum
eigensolver to the seven mer protein
folding problem in a collaboration with
strangeworks
a critical consideration as we look
towards the quantum accelerated
applications is that they will not run
exclusively on a Quantum resource but
will be hybrid Quantum and classical in
nature in order to transition from
algorithm development by Quantum
physicists to application development by
domain scientists we need a development
platform built for hybrid Quantum
classical Computing that delivers high
performance interoperates with today's
applications and programming paradigms
and is familiar and approachable to
domain scientists to address this
challenge we recently announced Nvidia
Quantum optimized device architecture or
coda coda is the platform for hybrid
Quantum classical Computing built to
address the challenges facing
application developers and domain
scientists looking to incorporate
Quantum acceleration into their
applications Coda is open and qpu
agnostic we're partnering with Quantum
Hardware companies across a broad range
of qubit modalities to ensure it
provides a unified platform that enables
all hybrid Quantum classical systems
Coda integrates with today's high
performance applications and is
interoperable with leading parallel
programming techniques and software it
allows the domain scientist to quickly
and easily move between running all or
parts of their applications on CPUs and
gpus simulated Quantum processors and
physical Quantum processors
and now I am really excited to show our
first proof Point running Coda with both
an emulated Quantum resource leveraged
in Q Quantum and on a physical qpu
continuums h12 processor in this
experiment we're running the variational
quantum eigensolver vqe is a hybrid
algorithm for computing the ground state
of a hamiltonian and is key to both
quantum chemistry and condensed matter
physics the plot on the left is
simulated with Coda compiled to a cool
Quantum back end running on an a100 GPU
representing what you expect from
perfect qubits the plot on the right is
the exact same code of code now compiled
to execute the quantum kernels on the
Continuum processor simply by altering a
compiler flag this is a simple example
but demonstrates how easy it is to use
both Quantum and classical Computing
Resources with coda and this is just the
start we will continue to work to enable
all Quantum processors to be accessible
through Coda ensuring that researchers
can leverage the best resources for
classical Computing simulating Quantum
Computing and physical Quantum computing
and now guedica will tell us about new
platforms for remote sensing and Edge
computing
thanks Jim scientists use instruments
like electron microscopes telescopes
x-ray light sources and particle
accelerators to further their knowledge
and understanding for all kinds of
matter for example researchers at
Argonne National Lab are experimenting
with enzymes to tackle plastic waste
using AI models trained on 19 000
protein structures they shortlisted
specimens to examine and confirm their
hypothesis using an x-ray beam line at
the advanced Photon Source these
instruments that are playing detective
at a nanoscale level are often as big as
a football field they're spread out all
over the world from Australia to UK to
South Africa millions and billions of
dollars of investment is taking place
right now to upgrade them after the
upgrade these instruments and sensors
will produce up to 500 times brighter
raise generating crisp clear clear and
high resolution images
this huge jump in the brightness
translates to thousand times more data
easily running into petabytes and
exabytes this poses a data analysis and
data migration problem the streaming
data software pipeline should be
multimodal easy to use scalable flexible
and include Ai and ml techniques to
filter the most relevant bits of
information in seconds instead of days
let's first look at how these experiment
sites or labs are laid out be it Oak
Ridge National Lab or a university like
Purdue the campus typically they have a
center for biosciences Material Science
nanotech robotics or Advanced
manufacturing with a small number of
workstations and servers connected to
the main Computing Center the site may
also be receiving data from other sister
sites and remote sensors out in the
field
for such a multi-disciplinary campus
investments in compute storage and
connectivity are key to support open
science projects
Infinity band that can run end-to-end
data center to Data Center enables
researchers spread within or across
sites to collaborate data coming from an
experiment at one end of the campus can
feed into another instrument sitting in
a separate building in minutes instead
of days or months
today we are announcing multiple
products where Nvidia is investing to
enable scientific discovery that spans
from The Edge to the data center
Nvidia holoscan is an SDK that can be
used by data scientists and domain
experts to build GPU accelerated
pipelines for sensors that are streaming
data the developers can use C plus plus
or python apis to build modular blocks
that are flexible and reusable for added
performance they can also include Jacks
machine learning or AI
holliscan sits on top of gxf that
manages the memory allocation to ensure
zero copy data exchanges so developers
can focus on the workflow logic and not
worry about file and memory i o the new
features in holoscan will be available
to the HPC developers in mid-December
2022 the second piece of the solution is
NVIDIA Metro X3 or long-haul infiniband
infiniband is synonymous with server
rack standing side by side but Metro X3
extends the reach of infinity band
Network to up to 25 miles or 40
kilometers now taking advantage of
native RDMA users can easily migrate
data and compute jobs from one
infiniband based mini cluster to the
main data center or combine
geographically dispersed compute
clusters for higher overall performance
and scalability Metro X3 systems are
managed by the Nvidia unified fabric
manager or UFM UFM enables data center
operators to efficiently provision
Monitor and operate all the infiniband
data center networks Metro X3 systems
are going to be available end of
November 2022. the third piece is
Bluefield 3dpu using intelligent storage
offloads for data migration at
supercomputing ZR is demonstrating their
data migration solution with Bluefield 3
for infiniband connected data centers
and remote HPC sites where an x86 based
solution needs 13 U of Rackspace with
Bluefield 3 it can be accomplished with
just 4u of Rackspace
UT Southwestern is one of the customer
evaluating Metro X3 to connect their
Healthcare researchers who are using a
dispersed compute infrastructure as part
of the initial research project they are
connecting the UT Southwestern West
Campus with a cluster that is about 10
miles away
Metro X3 customers can also take
advantage of nvidia's full stack
architecture this includes native RDMA
in network computing GPU direct and ease
of management with UFM to operate the
compute nodes that are miles apart as
one large supercomputer scientific
discovery in this decade will come from
an end-to-end converged workflows
comprising the data coming from various
experiments at the edge feeding into
simulation and training AI models
running in the cloud or data center for
HPC at the edge holoscan SDK can be used
to develop data streaming pipelines to
convert raw data into actionable
insights in seconds instead of days or
weeks by analyzing data as it's being
generated the researcher can avoid
errors retries and false starts
added benefit is that the researcher can
steer and control the experiment as the
data is being collected make decisions
and discoveries on the spot
filtered and compressed data can be sent
over metrox to the main data center on
campus or into the cloud not only for
archiving but for a lot of
interconnected applications
observational data can be used to
enhance simulations train or refine AI
surrogate models or can be fed into a
digital twin offline simulations Ai
surrogates and what-if scenarios from
digital twins can recommend parameters
for the next experiment
to enable the converged workflow for
scientific discovery and videos
investing in holoscan gxf and UCF UCF
stands for unified compute framework
that enables developers to combine
optimized and accelerated sensor
processing pipelines as microservices
from the cloud
supporting the new era of scientific
discovery will need workflow management
systems that can orchestrate all the
moving parts and are not bottlenecked by
data migration disk copies and file
transfers
HPC community and those working on
adapting the workflow management systems
can pick up holoscan and UCF to maximize
the utilization of all the instruments
run simulations do AI training and even
build interactive digital twins
to elaborate more on AI and digital
twins Dion Harris will walk you through
modulus and Omniverse for HPC
thanks kitika AI is quickly becoming the
fourth pillar of scientific discovery
for centuries science has been built on
the foundation of three pillars
observation experimentation and Theory
more recently HBC has become a critical
tool in scientific research
HBC simulations are important because
they allow us to study things that
happen too slowly or too quickly to
observe in real time AI allows us to
reshape and accelerate the scientific
discovery process by training a model on
observed or simulated data we can create
a physically informed AI model to
predict new scientific outcomes these
predictions are then verified by
experimental observations and the model
continues to evolve over time
the power of AI is that it can automate
the process of doing simulation by
analyzing data this means that we can
take on problems that were too big or
too complex for traditional scientific
methods we can now use AI to help us
find new cures for diseases design new
materials or find new energy sources AI
is not just a tool for science it's a
new way of doing science Nvidia modulus
is an AI and training inference platform
that enables developers to create
physics ml models to augment and
accelerate traditional simulations with
High Fidelity and in near real time
today we are announcing modulus is
available via Nvidia Launchpad and
several major csps modulus accelerates a
wide range of scientific applications
climate and weather simulations are
critically important to develop policies
to help fight climate change Nvidia
collaborated with Lawrence Berkeley
National Labs and researchers from the
University of Michigan rice Purdue and
Caltech to build an AI model called
forecast cast net that performed extreme
weather predictions 45
000 times faster and 12 000 times more
energy efficient than conventional
numerical simulations
to fight climate change consumer
electronic companies can design more
energy efficient products to reduce
their carbon footprint Nvidia research
used modulus to optimize the design of
our workstation GPU heat sinks resulting
in 35 percent higher power densities on
the GPU when compared to Conventional
Vapor chamber and heat pipe combinations
see Ms gamessa is using Nvidia Omniverse
and modulus platforms to improve Wind
Farm simulations enabling them to create
more accurate models of complex
interactions between turbines using a
high fidelity high resolution
simulations that are based on low
resolution inputs this will help to
optimize the performance of its wind
farms and reduce the maintenance costs
finally industrial plants and facilities
are complex to plan build and operate
whether factories warehouses or even
data centers
for the 7 million worldwide data centers
achieving Optimal Performance and Energy
Efficiency is of Paramount importance
let's take a look Nvidia Omniverse can
be used to build digital twins with high
performance data centers to help
optimize every step of planning building
and operating complex super Computing
facilities
in constructing nvidia's latest AI
supercomputer engineering CAD data sets
from tools like SketchUp PTC Creo and
Autodesk Revit are aggregated so
designers and Engineers View and iterate
on the full Fidelity USD based model
together
patch manager can
Flex topology of
support connections bracket node layout
and cabling can be integrated directly
into the live model
next cfd Engineers use Cadence Six Sigma
dcx to simulate thermal designs
Engineers can leverage AI circuits
trained with Nvidia modulus for Real
Time what-if analysis
and with Nvidia air a network simulation
platform connected to Omniverse the
exact Network typology including
protocols monitoring and automation can
be simulated and pre-validated
once construction is complete the
physical data center can be connected to
the digital twin via iot sensors
enabling real-time monitoring of
operations with the perfectly
synchronized digital twin the engineers
can not only simulate common dangers
such as power picking or cooling system
failures but also validate software and
component upgrades for cicd before
deploying to the physical Data Center
with digital Twins and Omniverse data
center designers Builders and operators
can streamline facility design
accelerate time to build and deployment
and optimize ongoing operations and
efficiency
digital twins are evolving NASA used
physical twinning to duplicate
spacecraft on the ground that matches
spacecraft in orbit on April 14 1970
this approach was used to resolve the
Apollo 13 oxygen tank explosion from 200
000 miles away
the term digital twin was coined by John
Vickers of NASA in 2010 when they
created digital simulations of
spacecraft for testing batch simulation
digital twins are built on large amounts
of data generated from complex
simulations that can take days or even
months to complete the advancement of
iot cloud computing and AI has created a
new class of interactive digital twins
Siemens energy built an interactive
digital twin of its heat recovery steam
generator plant to develop new workflows
to reduce frequency of plan shutdowns
while maintaining safety by simulating
and visualizing the flow conditions in
the pipes Siemens energy can understand
and predict the aggregated effects of
corrosion in real time
virtual and physical steering digital
twins are on the horizon
Bill Tang at the Princeton plasma
Physics laboratory and researchers from
Argonne National Laboratory have
developed an AI model called sgtc to
simulate the plasma physics of a fusion
reactor when paired with Nvidia modulus
sgtc will be included as a part of
real-time control system to optimize the
operation of fusion reactor experiments
operated in Nvidia Omniverse
here a researcher generates simulation
data to measure the state of fusion
plasma visualizing it in pair of view
two more researchers are using particle
code simulations and surrogate models to
further simulate and explore the state
of the plasma however scientific
research and simulations don't exist in
a vacuum the infrastructure design and
3D cat elements are critical to
understanding how the system will behave
in full production
today each of these four workflows exist
separately with no way to see the
combined interactive model but now with
Nvidia Omniverse researchers can connect
pair review and 3D CAD data sets to see
their aggregated model in real time
and when ingested in the Omniverse their
visualization workloads are accelerated
by core Omniverse Technologies RTX Ray
tracing index neuro VDB physics and
modulus creating a real-time end-to-end
workflow
admin versus an open development
platform Bridging the physical and
Virtual Worlds so teams building these
virtual simulations can connect them to
physical worlds to build a virtual
physical steering digital twin
the scientific Community is in the early
stages of adopting Ai and digital twin
Technologies Nvidia is committed to
partnering with researchers to advance
science and make these Technologies more
accessible and today we are announcing
Omniverse for HBC Omniverse now supports
batch rendering and synthetic 3D data
generation workloads on Nvidia a100 and
h100 systems with Omniverse nucleus also
supported on a100 and h100 systems the
scientific Community can now take full
advantage of more streamlined
collaborative workflows across multiple
applications multiple teams and even
multiple continents to achieve full
Fidelity interactive digital twin
simulations customers can adopt Nvidia
ovx powered by l40 gpus a Computing
system purpose-built for developing and
operating persistent Virtual Worlds if
you have an Nvidia a100 system you can
get started with Omniverse today
all over the world extreme weather
events are becoming disturbingly common
the last decade has seen some of the
most damaging floods storms and
wildfires and recorded history
about a year ago we announced our Earth
2 initiative to help the scientific
Community tackle climate change one of
the greatest challenges of our time a
challenge of this magnitude will require
a long-term commitment and collaboration
across government researchers and
Industry
today we are announcing that Lockheed
Martin and Nvidia have been selected by
the National Oceanic and Atmospheric
Administration to deliver the
foundational Technologies for the Earth
observation digital twin project when
completed this project will lay the
groundwork for building the Earth
observation digital twin of noaa's Next
Generation Brown enterprise system
we are at an inflection point the
ability to create digital Twins and
physical systems and then use AI to
control and optimize them in real time
is revolutionizing many Industries and
scientific domains we are just
scratching the surface of what's
possible and I'm incredibly excited to
see what the future holds
Ian back to you to wrap things up
nvidia's multi-domain acceleration
platform is at the Forefront of the
accelerated Computing era and the engine
of the modern supercomputer we are
excited to see how h100 Grace Hopper and
Grace CPU Superchips deliver unmatched
performance and most importantly Energy
Efficiency
and our Cloud native supercomputing
platform delivers scalable secure high
performance infrastructure to develop
scientific Computing applications our
HBC and Quantum Computing sdks allow
users to scale performance and
accelerate both traditional and Quantum
computing simulations
super Computing is reaching to the edge
with the help of holoscan while modulus
and Omniverse are accelerating today's
HPC workflows and introducing a new era
of fully live interactive digital twins
for scientific use cases
the future of super Computing is
exciting and I hope to see all of you at
sc22
Title: Getting Started with Omniverse Launcher | NVIDIA Omniverse Tutorials
Publish_date: 2020-12-18
Length: 430
Views: 31676
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/MbsFBukGOZE/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: MbsFBukGOZE

--- Transcript ---

hello
and welcome to this introductory
tutorial on the omniverse launcher
the launcher provides your gateway to
all things omniverse
including apps connectors and related
utilities
as you can see here the launcher
provides a handsome interface for
launching apps
managing versions exploring new tools
and even provides a nucleus
collaboration server perfect for smaller
work groups to collaborate
but before we get ahead of ourselves
let's start at the beginning with
omniverse launcher
on its first run and let's walk through
the setup process
the first option you're going to be
confronted with is the eula
if you agree to the terms of the
agreement just go ahead and say i agree
that'll begin the installation process
the next option you will confront is the
agreement for nvidia to use your
non-personal data
in improving the quality and usability
of omniverse
this is mandatory during the open beta
but may become optional in future
releases
let's go ahead and agree by pressing the
continue button
this next set of options becomes a bit
more critical and requires a small
amount of foresight
each of these paths should be considered
for differing reasons
especially if you have more than one
drive on your computer for myself
i'm going to select my large and fast
ssd drive
which will both benefit my performance
on the omniverse
and allows me the room to grow over time
let's start with this library path
this path allows you to select where the
omniverse launcher will install your
apps
connectors and other omniverse tools
this location benefits from a fast drive
but does not require a large footprint
for most people this location is fine at
the defaults
but i will select a folder which i have
specified for my library
next we have the data path if you decide
to host a collaboration server
this is where your nucleus collaboration
server will store
user content for all connected users
though a fast drive will help here users
should choose a drive with ample space
as this will grow over time even with
small work groups the data footprint
here can grow to terabytes in size
depending on usage
okay last we have the cache path this
path is where all cached files will be
stored
this path especially benefits from speed
though again
a larger drive can be beneficial as
cache grows fairly significantly when
working with an omniverse nucleus
if you are unsure where to choose or you
only have one drive
you can feel safe in using the default
path once installed we can go ahead and
change these paths at any time
once comfortable with your selections go
ahead and select continue to continue
we are well on our way and launcher has
been successfully installed
unfortunately and as we can see we have
nothing populating our launcher
let's go ahead and fix that by visiting
the exchange tab
here we can review apps connectors and
tools to decide what we want to install
let's begin our library with some of the
more utility oriented tools
as virtually all users will want to have
them when working on the omniverse
let's select omniverse drive and then
let's select the install
option on the top right of the interface
excellent omniverse drive is now
installing
next let's select omniverse cache and
follow the same procedure
as you can see installing with omniverse
launcher is as easy as point and click
now take your time to review other
omniverse apps
and install whichever you choose i'll go
ahead and install
all the apps in the exchange so you can
feel free to pause and install at your
leisure
we will move on to installing some
connectors when you are done
okay assuming you have installed the
apps of your choosing
let's take a look at connectors first
select the connectors tab in the
exchange
this will list all available connectors
however you must have the host
application installed for it to function
as expected
i happen to have autodesk maya and ue4
installed on this computer
i would like them to be able to
interconnect with omniverse
so i will select and install these
connectors in the same way we did with
apps
go ahead and peruse this list for host
applications you want to connect
and install the provided connectors once
installed they will be able to interact
with the omniverse via these connectors
okay now that we have our apps and
connectors installed
let's hop over to the collaboration tab
here we will install
our nucleus collaboration services this
will turn
our computer into a nucleus
collaboration server
and allow other users to connect to and
collaborate on your computer
this collaboration server is also useful
for single user workflows where
connecting several apps and connectors
to the same project is wanted
anyhow let's go ahead and install the
nucleus collaboration services now
simply click to install upon
installation
we will be confronted with a choice for
the data path
as this path was carefully selected
during install
we can continue without change great
now let's go ahead and create our
administration account
for our newly installed server to do
this
simply fill in the details this is the
admin account so please be sure to
remember the username and password you
choose
they will be needed when you want to
connect to your server from either apps
or connectors
ok once filled in select continue to
complete the process
excellent we are almost ready to
conclude this video
but first let's go ahead and jump back
to the library tab
you should now see that the apps
connectors and utilities you installed
are listed on the left side of the
interface selecting them will display
all pertinent information
and links to documentation as well as
provide the ability to launch
update and uninstall the app
you may have noticed the messages
showing up in the top right corner
which alerts you to required actions
updates
and other pertinent information just to
the right of that we have the user
settings tab
which allows you to change your path
options we discussed during
install okay that sums up the launcher
and should get you headed in the right
direction with the omniverse
go ahead and fire up those freshly
installed apps and get started
don't forget to check out the
documentation portals linked from each
app
there is a wealth of information there
to help you continue your journey into
the omniverse
thank you so very much for watching have
a great day
you
Title: SHIELD TV: AI in the Living Room and Beyond with SPOT
Publish_date: 2017-01-09
Length: 70
Views: 12938
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/mEi2w1srm4s/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: mEi2w1srm4s

--- Transcript ---

okay Google start my day okay I'm
turning up your thermostat and brewing
you a fresh pot of coffee okay google
plaiting news pending home sales dropped
in November to the lowest level in ok
Google I'm leaving shutting down your
house have a great day
ok Google show me some popular TV shows
just drama only recent ones play trailer
from stranger things hey that looks like
your grandma's house kind of ok Google
show pictures of grandma Mary's house
see what I say you're right
don't watch it yeah yeah let's watch ok
Google Play stranger things on Netflix
Title: SIGGRAPH 2018: NVIDIA Keynote
Publish_date: 2018-08-31
Length: 2894
Views: 116850
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/MH3mG4IqZkk/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: MH3mG4IqZkk

--- Transcript ---

ladies and gentlemen please welcome
nvidia founder and CEO jensen guam
welcome we've been working really hard
and today is a historic moment today is
a special day because this SIGGRAPH is
pixar render man's thirtieth birthday
over the course the progress that they
have made and the relentless pursuit for
image perfection has been nothing short
of amazing 1995 Toy Story premiered
using nerves to generate characters
stochastic sampled AA's motion blur the
imagery were just beautiful it was
processed with 800,000 CPU hours on a
100 megahertz 27 mega flops spark CPU a
hundred megahertz is so cute several
years later with cars they introduced
rate racing so that they have beautiful
reflections beautiful shadows ambient
occlusion in 2006 they use global
illumination for the very first time
with global illumination all of a sudden
light just works as it should the
subtleties of light bouncing all over
the environment picking up the color as
it goes along painted it just an amazing
amazing movie they took it another step
further with finding Dory all of a
sudden
Dory's in the water environment the Rays
are bouncing off of the waves in the
water the reflections and refractions
just exploded the amount of computation
that was absolutely necessary to render
that scene and they invented something
else using a state-of-the-art technology
and deep learning
they created AI based denoising as a
result filling in all of the spots that
the Rays haven't reached yet and as a
result reduced the amount of render time
necessary tremendously but even then
even then the amount of computation that
was required is amazing now of course
you guys know
that for the last several years Moore's
law has really come to an end we can't
get transistors to go any faster without
consuming even more power we've lost the
ability to find any more instruction
level parallelism we're running into
walls left and right as a result CPU
performance has really come to a halt
and you could see it here but yet the
amount of computation the amount of CPU
hours that's consumed to render these
movies continue to go up meanwhile 25
years ago this year we started nvidia
and our pursuit was to generate the most
amazing imagery in a 30th of a second
and at the price point that consumers
would pay for that curve looks like this
our first contribution was the
introduction of GeForce 256 the first
GeForce the first processor that did all
of the geometry transformation lighting
transformation in hardware and then
several years later in 2003 we
introduced GeForce 3 the world's first
programmable shader and then in 2006 we
introduced our single most important GPU
ever the world's first compute GPU the
first CUDA GPU the rate of progress has
been completely astounding and the
reason for that is this we've been able
to operate an optimized performance
across the entire stack by working with
api's and engines and software
developers we've been able to remove
bottlenecks and innovate and create new
ideas that makes it possible for us to
break Moore's law and that's the reason
why the accelerated computing law the
GPU accelerated computing law has moved
so much faster but it requires us to all
work as one team this is the world's
most demanding co.design challenge where
software developers algorithm developers
tools developers are all working
together to create one amazing
experience we've been pursuing the path
to photo-real and there are so much
progress amazing amounts of geometry
that it's going to be in the scene
because the world's got lots of geometry
and some of it includes light and cast
shadows and
Micra structure on the geometry allows
it to pick up the subtleties of the
light cause that nice rim lighting using
cameras with lots and lots of shots to
reconstruct a photorealistic environment
to model and to create photorealistic
textures creating materials that are
physically based metallic or dielectrics
smooth or coarse rough translucency
subsurface scattering all of these
physically based properties are now
possible for us to be modeled simulating
physics soft bodies rigid bodies
particles fluids Springs and strings
because the world moves in a very
predictable way character animation
using motion capture and then blending
shapes and now we're even able to teach
a character with deep learning how to
animate itself facial animation is
incredibly important this is digital
Doug and instead of using markers on
your face to track your your facial
animation now we could use AI to figure
that out all by itself and so making it
easier for us to capture very realistic
facial animation all of this progress is
made possible because the performance of
our capabilities that technology
underneath continues to advance and
working closely as one industry we've
been able to do amazing things there's
been one enormous roadblock that
roadblock is as fundamental as it can
possibly be and it's the simulation of
light we've been able to do all kinds of
amazing things and make these amazing
beautiful video games but we've been
doing it through a lot of hackery we've
been placing lights that are invisible
to you in the environments so that we
can recreate what otherwise would be
global illumination this roadblock has
been the endeavor of computer scientists
and computer graphics engineers since
Turner Whitted
first wrote about it in 1979 Turner
Whitted is now a researcher at Nvidia he
described a really elegant algorithm
called the multi bounce
recursive ray-tracing algorithm it cast
array from your virtual eye through a
virtual screen through that
pixal and it flies towards and
interceder intersects a surface when it
intersects that surface it creates a
whole bunch of shadow rays in the
direction of lights and if the path of
that shadow ray towards the light were
to be occluded meaning intersects
another surface it would be in shadow
otherwise it would be lit but it goes
further in that surface was not diffused
and it was reflective it would generate
a reflection ray it would take into
consideration to normal death surface
and the incoming angle of DeRay and then
again it would create that Ray and it
would follow the original algorithm
create other shadow rays trace its way
to the light and if it turns out to be a
refractive surface it would have aged
Snell's law and it would Bend according
to the medium and generate more rays and
then follow the original algorithm hits
another surface if it intersects another
surface creates a whole bunch of more
shadow rays and then when it's all done
with that it would figure out how to
illuminate that surface by shading it
well it turns out this algorithm as
elegant as it is is incredibly
computationally intensive and the reason
for that is because the billions of rays
and there are millions and millions of
polygons in the scene and so computing
it is extraordinary well at the time
that he wrote the paper he used the VAX
times is probably a million and a half
dollars and it took him one and a half
hours to simulate one frame so he was
basically doing about sixty pixels per
second these days we do 60 frames per
second but he does 60 pixels per second
he described that it would take a Cray
computer behind every single pixel to
generate real-time ray tracing it turns
out his estimates were pretty close
thirty-five years later were able to put
effectively several Cray supercomputers
behind every single pixel and we created
a brand new way of doing rendering and
we introduced it this year at GDC the
Game Developers Conference in March for
the very first time it's a brand new
rendering system we call it the Nvidia
r-tx and it runs on a deskside
supercomputer we called the dgx
station it requires four of our highest
performance GPUs the same GPUs that are
now powering our nation's fastest
supercomputer the fastest supercomputer
in the world called a Tesla V 100 takes
four of those all connected together
working simultaneously generating five
race per pixel to simulate what you're
about to see in real-time let's roll it
[Music]
what's the story would only elevators
lately I heard kylo Ren destroyed the
one over in D sector pianist me who's
ever in charge of this place should be
transferred to hawk
[Music]
you think she hurt us
yeah I think she heard us
please be blended for once well ladies
and gentlemen I have a surprise for you
it turns out it was running on this one
single GPU ladies and gentlemen this is
the world's first ray-tracing GPU now
the concentration in my face is because
the reflections on this graphics card
works perfectly
we call it the Quadro r-tx family there
are several versions of it the
performance is absolutely incredible up
to 10 Giga rays per second 10 Giga rays
per second just to put it in perspective
the fastest CPU on in the world with a
whole bunch of cores in it can probably
do a few hundred thousand a couple to
three hundred thousand rays per second
10 Giga rays per second 10 Giga rays per
second it's even fun to say when was the
first time that anybody ever used the
word Giga rays we use teraflops but
we've never used the word gig race this
is the world's first gig array GPU how
many rays you got Giga rays
not only does it have an enormous amount
of floating-point performance for
shading also this shader this shading
architecture this SM our new compute
architecture has the first integer and
floating-point processing pipeline so
that we could do address calculations
and numerical calculations all at the
same time we perform up to sixteen
teraflops and when was the last time you
heard the word tips sixteen tips and it
comes with this this brand new env link
multi-gpu connector so that the frame
buffer every single GPU the GPU could
talk to every other GPUs frame buffer as
if it sold over this MV link interface
100 gigabytes low latency this
incredible envy link connector here so
so the highest-end Quadro also powered
by this Turing GPU has 500 trillion
tensor operations per second no
processor in the history of processors
has ever commanded this much
computational resource on one chip three
types of processors the SM for compute
and shading a brand new processor called
the RT core with 10 Giga rays per second
from ray tracing and a new processor
called the tensor core for deep learning
and AI 500 trillion operations per
second the neutering GPU is too great
asleep since 2006 when we introduced
CUDA this fundamentally changes how
computer graphics is going to be done
it's a step function in realism these
are our goals to make sure that whatever
images are generated is dramatically
different than what was possible before
that we invent a new computing model
that takes advantage of the efficiencies
of rasterization
but the real realistic simulation of
light for shadows and reflections and
area lights refractions and all of those
trans liu
and season all of those effects that are
so difficult with rasterization to
combine it with compute and also
artificial intelligence to make all of
this work together in an operable way it
has to be amazing at today's
applications but it has to be utterly
awesome for tomorrow's when you buy this
GPU you're gonna enjoy it immediately
and you're gonna be able to enjoy
applications that will be coming very
shortly that I'll show you in just a
second and just to take a look at what's
inside this chip so the Turing sm 16
tariffs lots in 16 tips does concurrent
floating point integer instructions at
the same time has a unified l1 cache has
variable rate shading for fo v8 and
rendering all kinds of new algorithms
are possible motion adaptive shading so
when things are moving you don't have to
dedicate as much shading horsepower to
it for different areas that you might be
able to reduce the amount of shading
horsepower dedicated to it because the
the contents not changing very much so
all kinds of new algorithms are now
possible so that we can conserve the
shading horsepower to create and
dedicate it and assign focus that
computing resource where you might need
a most the Rd core 10 gigabytes per
second it has an accelerator that
accelerates the BVH the bounding volume
hierarchy acceleration structure
basically when you look into a scene
there are all these geometries the
geometries are encapsulated in these
bounding volumes these bounding volumes
are created in real time and this
bounding volume inside has sub parts in
the geometry and though they have sub
bounding volumes when an Ray intersects
one box you can dismiss all of the other
boxes because they're not going to be in
the Ray that allows you to figure out as
quickly as possible tests of all the
rays that you're sending in into the
scene which Ray intersects with which
surface and which primitive that test is
so computationally intensive for the
longest time people thought that it
would never be possible to do on a GPU
until now that's why I took ten years of
research figuring out exactly how to
create this arty core
ray-tracing core that accelerates this
data structure does acceleration
structure to figure out exactly which
one of the primitives that array
intersects and to interoperate with a
shader was a great challenge that was
the amazing invention as a result we
were able to achieve real-time ray
tracing for the very first time the
tensor core has three different
Precision's FP 16
well of course s SP 32 it has 120
teraflops 25 teraflops of f feet 16 250
tops of in 8 and 500 tops of infor if
you need precision use FP 16 10 times
the performance of Pascal if you need
less precision used in for 10 times what
was possible with Pascal incredible the
display is designed completely for HDR
through-and-through
and supports 8 K displace video is
designed to be able to encode 8 k and to
do so with the highest level of quality
reducing your bitrate so if we wanted to
stream graphics or remote graphics from
far away we could do so incredibly well
for the very first time and as this MV
link that combines connects multiple
GPUs so that the frame buffer is
essentially additive if you compare it
to Pascal looks a little bit like this
Pascal was eleven point eight billion
transistors 471 millimeters squared
twenty four gigabytes at ten gigahertz
Turing is 1.6 times bigger nineteen
billion transistors
it's the largest processor the world has
ever made short of just one other and
that one other is called the Volta V 100
that's used to power supercomputers all
over the world touring is 754 millimeter
squared and has the ability to drive 48
plus 48 gigabytes of frame buffer a
giant frame buffer if you compare it
like this 13 teraflops is now 16 plus 16
tips
instead of having to wait between
floating-point or integer we can now do
floating-point and integer at the same
time instead of 50 tops of n8 we now
have a tensor core that can either do
125 teraflops for precision inference or
500 tops of infor and then of course a
brand new processor called RT core this
brand new software stack in order to
bring all of this to life is software
and algorithms and libraries and SDKs
and tools this is the first rendering
architecture that supports this hybrid
approach of rendering interoperability
between rasterization and ray tracing
and compute in AI it supports optics
Vulcan and Microsoft DXR there's a
plug-in architecture for deep learning
so that we can create models for super
resolution for example for denoising for
example for frame interpolation for
example all kinds of new special effects
are going to be possible because of this
architecture and it runs it on the
tensor core the tensor core is so fast
it makes it possible for us to create
these really sophisticated models deep
learning models and run it in real time
today we're also announcing that we're
gonna open source a really really
fantastic piece of work called the
Nvidia MDL material description language
a high-level programming language that
captures the properties of physical
properties of materials and its
reflectance functions its metalness and
dielectric smoothness or roughness all
captured in this thing called a
bi-directional
reflectance scattering function on first
principles capture the properties of
materials and makes it possible for us
to then interchange it between
applications we're also announcing today
that we're working closely with Pixar
and when to support the universal scene
description language so that content
could move in and out of tools computer
graphics reinvented merging
rasterization and ray tracing compute
and AI well our goal of course is to
create amazing image
this is a wonderful way to inspire us
the Cornell box it was designed so that
it's easy for us to compare and
determine the accuracy of a rendered
image to image it that was taken from a
camera let's show you what today's
computer graphics looks like so we're
going to step through this very quickly
just give you a sense of the progress of
computer graphics this is traditional
graphics and this is a point light and
you have different materials on it the
red is leather white is cement and the
green ball there looks like clear coat
car paint green car paint and you can
see that the the point light is testing
shadows and those shadows are well
they'd look rather hard and it's made
out of made our shadow matters and so
let's let's go take it to another step
okay so we can add compute to that we
could do things like camera effects like
depth of field okay let's take it
another step we could do bloom but none
of these look super great okay so now
we're going to turn on our TX so this is
area lights with our TX and notice the
soft shadows some parts of it is well
lit and some part of it is kind of
blurry and nice and soft and that's the
that's the feeling that you get when you
see a really nicely well cast a shadow
let's suppose we had more area lights
okay now with more area lights you would
expect each one of the area lights to be
able to contribute to lighting the scene
and so you could see now the penumbra
and the Umbra right here nice and sharp
and this is nice and soft you could see
the light bleeding on top of the roof
casting that shadows so if you were to
do this with traditional graphics the
way you would do it is to simulate that
area light with a whole bunch of point
lights and those all of those point
lights would have to light the
environment one at a time and you would
integrate that let's suppose we go do
something even harder so this is
reflection
reflections are hard to do using
traditional computer graphics and the
way that they do it is to create an
environment map and that environment map
would now be a texture map that's looked
up ok let's take another step forward
over and this is diffuse reflection
diffuse reflection adding adding
roughness to the ball ok another one any
refraction this is another ray-traced
effect that works incredibly well
however there's something looks wrong
down here and where does that that
doesn't look quite right there's an
effect called caustics as light
accumulates there you go
so that's caustics this is a mixture of
ray tracing and rasterization suppose we
went to path tracing suppose we did go
global illumination Wow ladies and
gentlemen all of this is just working
right out of the pipeline and this is
the benefits okay so we have computer
graphics traditional graphics we
rasterization we showed you depth of
field rasterization with compute we
showed you area light rasterization ray
tracing and compute we showed you rest
reflections rasterization ray tracing
and compute and the compute in each case
is 2d noise ray tracing because the
bound the rays are bouncing all over the
place at some point you can use AI or
some heuristics to figure out what are
the missing dots and how should we fill
it all in now the benefit is this this
is how we started this is traditional
computer graphics rasterization and
shading and wonderful materials we could
fake it we can improve the looks of it
with all kinds of its of trickery and
and we all do that and it makes video
games look amazing however it's so
brittle there's so many things you can
do but you could turn on RT X and it
looks like this everything here is
completely in real time so off on which
one do you guys like better
okay so what we did is we put everything
together into a really short clip what
you're about to see is completely in
real-time
[Music]
[Music]
[Applause]
[Music]
[Applause]
[Music]
[Applause]
[Music]
[Music]
for the longest time real-time computer
graphics has been really used for
interactivity design of CAD use it for
flight simulator use it for video games
the market that relies on visualization
photorealistic visualization is
absolutely gigantic
however it's never been able to use GPUs
it's never been able to have the benefit
of acceleration because the requirements
are photo-real unless you could do ray
tracing and you could do global
illumination with physically based
materials and physically based lighting
until you could do that it's simply
impossible to take advantage of
acceleration for most of these markets
and they're all completely run on CPU
today architectural engineering and
design unless it looks like it's going
to look it's really impossible to use
the image and today's architecture
they're not your large brown buildings
and large structural buildings their
buildings that are basically gigantic
products beautiful gigantic products and
they want you to feel the experience of
being in it and without understanding
the effects of light and how light
responds in that entire environment it's
really really hard to convey the
sensation that you would ultimately get
when you're in the building
visualization every catalog today is
largely rendered the reason why it's
rendered is because it's more flexible
obviously and because it's cheaper and
you guys know of course whether it's
full feature-length animations or
special effects all rendered with large
render farms for the very first time
NVIDIA r-tx is making it possible for us
to bring accelerated workflows and
acceleration to this market it turns out
there's a 70th birthday of Porsche and
they created an amazing trailer I want
to share with you
the speed of light a universal constant
never diminishing never ending
[Music]
70 years of unrelenting progress
the Porsche 911 speedster concept the
speed of light now just imagine in order
to create that trailer the reflections
has to look right the materials have to
be physically based so when light
strikes it it creates the right
appearance the reflections are dynamic
it's flowing over the car there's global
illumination soft shadows the little
tiny creases reflection and refraction
off the windshield this image is so
subtle is simply not possible to
rasterize it well as it turns out what
you saw just now it was not a movie it's
completely in real-time this is to work
again of some amazing engineers and
artists we worked so closely with the
team at an epic and the Unreal Engine
team the Porsche designers that were
part of this project they're so proud of
it I mean this is just utterly shocking
it's Kim there where's Kim Kim what do
you think you know I've been in computer
graphics now for 27 years and it was a
dream to see real-time rate racing come
true and I think over the next decade
we're gonna see such amazing
capabilities of your hard work
powering you know interactive
experiences you know movies and games
they're gonna look the same
photorealism absolute forty realism is
within our grasp now it's amazing and I
think AI for ray-tracing
AI for computer graphics is also going
to rebel eyes that revolutionize the
possibilities so I think we're going to
get movie quality within the within this
new generation of cards I think we're
going to see movie quality in games as
well as more enterprise like
applications like this if you would have
projected from dgx station to where we
would have finally expected to see this
on one graphics card we expected
something along the lines of five to ten
years because of two fundamentally new
technology we were able to bring it to
today and those two technologies is
exactly what Kim was saying earlier
ray-tracing acceleration with the RT
core and then deep learning with the
tensor core by combining those two
technologies with what we did before
rasterization is shading and compute all
of a sudden we gave ourselves with just
an enormous boost and we pulled in ray
tracing somewhere between five to ten
years and everybody is surprised
everybody is shocked all the developers
were working with it's just amazing that
we can basically cast as many shadows
and rays as we want reflection rays and
refraction rays are much much more
expensive but even then we could cast a
whole bunch of them the ability to be
able to do ray tracing at tangu Ray's
per second is really really quite
powerful what you're looking at on top
is essentially what you would get if you
had dgx station the architecture that's
inside dgx ation for computer graphics
is essentially what you're looking at
Pascale okay and with some of it is used
for rasterization some of it's used for
shading and a lot of it is used for
tracing the Rays there's a few
technologies that when in combination in
this RTX platform when you put them
together does this a 6x speed-up in
computer graphics
well the way that it's done is we number
one accelerate the ray tracing part of
it we still have to shade it but the the
ray tracing acceleration is just
incredibly fast the second part is by
using AI we're able to render at a
slightly lower resolution but because we
trained that model off of very high
quality ground truth we're essentially
going to be able to generate the final
image at a much much higher rate and so
we call a DLA a the combination of AI
and ray tracing has made it possible for
us in Unreal Engine 4 with our TRT rate
real-time ray tracing running on top of
microsoft's DX r api this is basically
today's most popular platform we're able
to take computer graphics and improve it
by a factor of six to
is six times Pascal
one of the most important things you'll
see and Kim was alluding to it is that
we're going to use AI in so many
different parts of our graphics pipeline
one of them is anti-aliasing we use a
temporally stable convolutional auto
encoder we teach this auto encoder from
ground truth that was generated from 64
samples jittered samples of a rendering
you rendered the same image 64 times we
generally jittered the samples and then
we use a very high quality filter and we
combine it all together into one and it
becomes one frame of a ground truth we
create a whole bunch of those and we
teach this new network how to take an
image and automatically generate a much
higher quality image as a result we
could start with a lower resolution
image and the result is a higher quality
than a higher resolution image really
really quite amazing the power of deep
learning high-quality motion image
generation we call it the nvidia dl AAA
and that's what you saw in action as a
result the combination of ray tracing
faster shading dla because of the tensor
core these three processors working
together touring was able to accelerate
the rendering time by a factor of six
architectural design needs
photorealistic images these scenes to an
architect has to be photo real so that
they could convey the sense the
sensation of the environment to their
clients to do this is incredibly
challenging and the reason for that is
because in most of these rooms light is
largely indirect it comes in it bounces
around and it picks up the nuances of
the surface it strikes the light is
indirect materials are physically
obviously physical and so they have
different reflectance properties and
that gives you the sensation of a space
so this is the Rosewood in Bangkok so we
created this visualizer and it's
connected to Revit in this case
and SolidWorks between SolidWorks and
Revit you could modify and edit right
there in the tool and will show up
instantly in the viewer time and then
it's rendered on our TX Shawn go and
give it a try that's exactly right so we
can see we have Revit open there we're
looking at the lobby scene and at this
time of the day there's just not enough
natural light coming in so what why
don't we open up some of those skylights
and we can see how that affects the the
illumination now obviously you can't
fake this obviously this isn't baked
into the textures and the reason for
that is because you just placed the
lights and you can modify the lights you
could change the time of day and get a
sense of the feeling of this room look
at that Wow
if we go a little deeper into the scene
we can see you know this kind of Lobby
feature here by the elevators so we can
import an object here from Saltworks and
simply see how changing the materials in
SolidWorks is actually changes the look
and feel of the space this is Jeff Koons
whose love shiny dogs Wow you guys see
what's happening I mean this is just
amazing this is like having a wormhole
between SolidWorks rivets and this
viewer that's beautiful look look at
that look at the reflection of the Sun
right back there it's just incredible
the bloom there okay ladies and
gentlemen using r-tx for architectural
engineering and design we worked with
autodesk Arnall team and they've been
working they've been working with our TX
and they're just completely blown away
in no time ever has it been possible to
render final film final film production
on a GPU and the reason for that is
because it just wasn't fast enough it
wasn't fast enough and and it didn't
have large enough frame buffer to be
able to hold all of the assets it wasn't
fast enough because so much of the time
is spent ray tracing well finally with
our TX we've been able to do so what I'm
about to show you are several images of
a shot every one of these frames were
rendered on the Quadro r-tx final film
quality for the very first time this is
what you get when you have the world's
first rate racing GPU with 48 gigabytes
each and if you connect the two of them
together with this incredible technology
that I'm wearing called MV link you get
96 gigabytes of frame buffer and so for
the very first time we're able to rate
race final film production quality
assets well we created an a server
called the RT X server it's designed for
production rendering with full global
illumination and this production server
is really quite amazing this one server
with eight GPUs will allow you to speed
up your final film rendering and it's
designed to be remoted system and data
center it could be your render farm it
could also be your workstation it could
be your render man workstation it could
be your Maya workstation it could be
your nuke workstation every single
application in the DCC universe is
compatible with the software layer that
we created called quadrille infinity you
could put this in your data center it
becomes your workstation and when you
whenever you're not using it as a
workstation is your render server the
rendering time goes down from hours to
minutes and so let's take a look at what
that means suppose you know most of
these data centers as you know it's not
unusual for production cost to be a
hundred plus million dollars and twenty
percent of that twenty million dollars
of it is very typically assigned to the
render farm I'm showing you here just
two million dollars one tenth of a
render farm this is not quite a farm
this is probably just a garden okay two
hundred and forty dual CPU twelve core
sky lakes a hundred and forty four
thousand watts two million dollars
that's what you're looking at if we
wanted to have the same throughput on
the r-tx server this is what it looks
like
this is a before is after this is your
personal render farm and it's just a
fraction of cost it only cost about 1/4
as much it takes one tenth to space and
it takes one tenth the power each
megawatt of power is a million dollars a
year most data centers have 10 20
megawatts and so you could imagine the
cost as it goes forward saving money is
one reason but as we know from Blinn's
law whatever the budget is the DEP is
gonna want to use that full budget to
maximize their creativity and so let's
take another shot at this suppose you
had a constant budget of $500,000 so you
had one server rack here for 500,000
another one for $500,000 you get four
times the performance the reason why
it's so amazing is because each one of
those DG X's allows you to create a
three-second shot most shots are about
you know call it 2 to 3 seconds about 75
to 125 frames those shots now take one
hour instead of a shot taking five hours
or six hours it now just takes one hour
it's gonna completely change how people
do film the excitement from the DCC the
ISPs have just been incredible adobe
dimension has is working on integrating
r-tx
Autodesk Arnold picture our render man
chaos v-ray Wetty
all integrating RTX all of the DCC tools
CATIA
Siemens NX NSX SolidWorks integrating RT
X the number of software developers that
have come out to embrace this platform
is like nothing I've ever seen and the
reason for that is this everybody is
under the constraint to deliver
photorealistic images global
illumination is now the bar the
quadrille RT X starts at $2,300 RT x
5,000 all of that capability
accelerating rendering for final film
the ability to generate photorealistic
images for architectural engineering
and construction DCC and generating
photo real images
not to mention real-time ray tracing for
video games and interactive content our
TX 5000 16 gigabytes 32 gigabytes with a
env link starts out at $2,300
six gig arrays per second the 6000 is 24
gigabytes and 48 gigabytes
if you use MV link has 10 gig erase per
second $6,300 and then finally the
monster RTX 8048 gigabyte frame buffer
is just gigantic for just $10,000 it's a
steal the important thing to remember is
this the more you buy the more you save
every single world-class global system
maker has jumped on to RTX you're gonna
have workstations you're gonna have
servers of all different sizes quadrille
r-tx is gonna be available all over the
world so this is it nvidia turing
graphics reinvented there's no question
in my mind this is the single greatest
leap that we have ever made in one
generation this is the most important
GPU we've created since nvidia cuda the
quadrille r-tx eight thousand six
thousand five thousand the world's first
ray-tracing GPU the supporting
complicated stack that has been
integrated into ice bees all over the
world the Quadro r-tx systems allows you
to do seven shots in one day seven shots
in one day who doesn't want seven shots
in a day and then for the very first
time after all these years we've been
able to build something that the
visualization industry - 250 billion
dollar visual effects industry we can
enable them to be accelerated so that we
can change their workflow so we enable
them to do more with the same budget
before I say goodbye I want to thank all
of you for coming to our launch we're so
excited about the touring launch I
appreciate the support of literally
every single developer in the DCCC
industry the embrace that you have given
us for NVIDIA r-tx is nothing short of
astounding and the reason for that is I
believe you all see for the very first
time we could take a giant leap and
redefine what computer graphics is
computer graphics will never look the
same again I also want to thank all of
the audience who are the pioneers of
this industry because of the work that
you have done that has allowed us to
stand on your shoulders to create what
you see today so ladies and gentlemen I
want to thank all of you thanks for
coming to our lunch
the team is going to show you something
that is utterly unbelievable
[Music]
[Music]
[Music]
[Music]
[Music]
[Music]
[Music]
[Music]
[Music]
[Music]
Title: GamesCom 2013: NVIDIA SHIELD Debuts In Europe
Publish_date: 2013-08-24
Length: 98
Views: 7871
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/ml4IP7nDYno/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: ml4IP7nDYno

--- Transcript ---

you think guys from the largest gaming
trade show in the world gamescom my name
is Igor stank I'm senior product PR
manager for NVIDIA for in DIA and I'm
here basically talk about our amazing
device Nvidia shield it's a portable
gaming system featuring our latest Tegra
4 SOC which is the world fastest mobile
processor and we are showing your first
time to the European gamers this device
you can stream your pc games directly to
the shield you can play the latest
android games in the best quality and
the best experience thanks to the
full-size control of the Shirley's
offering you can basically play all
these amazing games you are featuring on
a TegraZone and of course you can
consume any other media on the shield
like playing the HD videos in very good
quality thanks to break good screen we
have in the shield and you can as well
experienced the music thanks to great
speakers we were building inside of the
shield so again if you are coming to the
games chrome I could really recommend
you to come and see our consumables and
experience this amazing gaming device
Title: Supercomputing for the Next Era of Digital Twins
Publish_date: 2022-05-30
Length: 91
Views: 21263
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/mLgyo5-SBJk/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: mLgyo5-SBJk

--- Transcript ---

Supercomputing is the driving force of discovery in every field
from scientific
to industrial.
Allowing researchers to understand 
the behavior of the smallest particles
and visit the furthest expanses of the universe
to unlock the meaning of life.
And with digital twins, it's giving industries superpowers to time travel  
letting them explore an infinite number of futures
and decipher the past through different lenses
and with million-X higher performance 
powered by accelerated computing  
data center scalability and AI
supercomputing will unlock new opportunities for us all
Title: Real-Time Video Editing and Faster Rendering in Adobe Premiere Pro with NVIDIA RTX
Publish_date: 2019-11-15
Length: 139
Views: 69237
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/MnVsfZT1wHU/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: MnVsfZT1wHU

--- Transcript ---

Adobe Premiere Pro has been really
tweaked to take advantage of the GPUs
and computers and Nvidia czar TX GPUs
really makes things fast and efficient a
good fast GPU it's important for my
workflow for two things number one I
don't have any more time I need fast and
not only that if I have to wait for
something to render my whole creative
flow gets disrupted the great thing
about using Nvidia r-tx GPUs with Adobe
Premiere Pro is it saves me time with a
lot of the things I'm doing regular
color correction reframing videos
currently I'm cutting a project with a
ballet dancer and she's moving across
the stage so I'm Auto reframing it in
Adobe Premiere and it is so much faster
now with my optimized system a lot of
times we have footage that's shot
widescreen and we need to reframe it for
social media for example make it square
make it nine by sixteen and what we do
is normally can track it you have to
actually crop it and then move that and
what auto reframe does is well it just
does it for you another thing that was
really great is that I had to deliver
this to multiple formats so I can export
for Instagram I can export for Facebook
I can go into any of those without
having to redo my whole thing I teach a
lot of hands-on classes and the great
thing is here at Adobe MAX
we're using r-tx laptops we taught a lab
together using the r-tx laptops and
ultimately everything just flew holy cow
it was fast a really cool thing about
working on an artsy external laptop it
essentially allows me to bring the
post-production studio to my client
instead of the client having to come to
me because there's so much power in it
we can easily edit on site anywhere
anytime and get great results I need
speed and that is what my Nvidia card
gives me for me the key thing is
creativity I don't want to say waiting
around for something to render for me to
watch it if it happens in real time it
allows me to try different things and be
more creative
in the end of the day I like what I've
done better and the people around me
like what I get
you
Title: NVIDIA Inception: Accelerating the World's Leading AI Startups
Publish_date: 2020-04-02
Length: 246
Views: 33819
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/MQnuTES0Xl0/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: MQnuTES0Xl0

--- Transcript ---

I'm Jeff herps vice president of
business development at Nvidia and I
lead Nvidia inception the goal of
inception is to educate nurture and
support the global ecosystem of AI
startups and the venture capitalists
that fund them utilizing deep learning
AI startups are at the heart of a new
era of industrial revolution rapidly
transforming industries across the globe
Nvidia inception provides fuel for these
startups to grow from seed stage through
unicorn status the inception journey
often starts with access to Nvidia's
incredibly successful Developer Program
with the right mix of technology tools
and training a kernel of an idea evolves
into a product or a service and
ultimately a funded startup
the main challenges we had when we
joined the inception program were we
were programming in a vacuum we had good
people but we didn't necessarily have
access to all of the latest resources
that we need
so the dev talk is actually really
useful as a means of connecting with
other developers both experts on the
Nvidia side and also other people
working in the space in terms of of how
we deliver our products using AI a lot
of our developers use the deep learning
Institute to in some cases learn new
skills but also for those people at the
expert end to really get a feel for what
the latest techniques are and also we do
use a number of the SDKs for NVIDIA
offer some of which are currently
available but because we work so closely
with the experts on the Nvidia side
we've actually been asked to preview a
number of the new kits which are coming
through with maturity a startup will
enjoy many additional inception benefits
guidance from Nvidia's world-class
technical experts can often prove
critical at this stage and preferred
pricing on technology and services can
help significantly extend financial
runway
ometimes I have AI techonologies aside
from GMA Nvidia Kim Chi PU dareka tiene
you tsunami Joe toko Nvidia office the
sheer tuna from titanal MVD are you
Jason big up in time
narrator pinned item after someone far
away I'd the modeling multi- REI
technologies software or the tech wasn't
on CW talent Oakland so I see a woman
take I have a subway that's how we're at
Eaton and people summer so she this is
at Hopewell Nvidia the content sushi a
true civilization the tool I have a
linear so continue shall be yoke so the
GPU in power AJ cone and then zinnia
when ready
inception provides multiple
opportunities for go to market and other
forms of strategic support for AI
startups for example Co marketing
customer introductions and even finding
the next round of funding through our
extensive network of BC partners
for both of our product launches both
for sky to r1 and sky do two we've
worked with Nvidia on the launch and
I've had a great collaboration with the
marketing teams they're around social
media PR everything such that the day
that we kind of unveiled the product we
get the nice lift from what Nvidia
brings to the table the other big
benefit that we get through the
inception program is access to Nvidia's
partners so we've been pulled into a
bunch of briefings and events at Nvidia
where we've been introduced to people
that have eventually become customers
bars also through Nvidia and through the
the GTC conference you get introduced to
other companies that are working and
building complementary technologies you
get introduced to investors that are
interested in the space that can invest
in your company so it's really a
fantastic thing to get to be a part of
this with other companies with other
individuals that are on the forefront of
pushing this kind of technology forward
Nvidia inception has already supported
thousands of AI startups across the
globe we hope you will accept our open
invitation to join us on this incredible
journey together we will forever change
the way we live work and interact
you
Title: Maximizing Wind Energy Production Using Wake Optimization
Publish_date: 2022-05-09
Length: 134
Views: 158990
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/mQuvYQmdbtw/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: mQuvYQmdbtw

--- Transcript ---

Total worldwide wind energy capacity 
currently exceeds 750 gigawatts.
And it’s growing faster than ever with an additional 
650 gigawatts to be added in the next five years.
For companies like Siemens Gamesa Renewable Energy, 
optimizing the configuration of each new wind farm
is critical for getting the most out of their investment, 
and reducing costs for consumers.
When designing a wind farm, it's critical
to place each turbine so as to minimize the
effects the turbines have on each other, 
due to the wake that they create.
Accurately modeling the wake requires high-resolution, 
high-fidelity simulation data that is specific
to that windfarm such as the 
geographic location and the terrain.
The "gold standard" for generating this data
is the Large Eddy Simulation, shown here.
But to run just one iteration for a single
turbine can take 40 days on a 100-core CPU.
And with so many iterations needed to develop
an accurate model for a specific site, 
using CPUs is impractical.
Using NVIDIA Modulus and NVIDIA Omniverse,
Siemens Gamesa has been able to reduce that
40 days to just 15 minutes – 
approximately 4000X faster.
This is accomplished by running the model
at a lower resolution and then 
with a physics-ML model trained using Modulus, 
to enhance or "super-resolve" the data.
And the results are functionally equivalent
to having run the model at full resolution.
Let’s take a look at the same data presented
in a way that clearly highlights the coarseness
of the low-resolution simulation – 
here they look like pixelated blobs.
In the super-resolved flow field, 
you can see the finer vortex structures.
Now that we have the simulation data, we can
build an accurate wake model for the site
and optimize the placement of each turbine.
Thousands of iterations, accelerated by NVIDIA GPUs
are run as part of the optimization
to achieve the maximum power output for each
farm while minimizing the cost.
And the impact is huge.
When using NVIDIA Modulus and Omniverse for
a typical 1000-megawatt offshore wind farm,
optimization can provide power to supply up
to 20,000 additional homes—and to do it 10% lower cost
Title: NVIDIA RTX - The Creative Freedom You Imagined
Publish_date: 2019-02-08
Length: 69
Views: 43238
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/MRZiya1DW3c/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: MRZiya1DW3c

--- Transcript ---

beauty accuracy and precision are
already at your fingertips but what
could you create if you had more time
Quadro r-tx breaks through the time
barrier
now you can visualize in real time with
ray-tracing
create interactively with the power of
AI
[Music]
and bring your vision to life from
iteration to final rendering faster than
ever before
this is the creative freedom you've
imagined this is Quadro r-tx graphic
reinvented
[Music]
Title: Teaser: AI with the Heart of a Composer - AIVA | Season 1 Episode 1 | I AM AI Docuseries
Publish_date: 2020-08-20
Length: 15
Views: 11251
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/MUyTjV96MPw/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLB4tWJ3MXaKkfZB3L3bjw8qVEiKnA
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: MUyTjV96MPw

--- Transcript ---

IVA is an artificial intelligence that
composes music for films TV shows games
what we're trying to do is create deep
learning algorithm that creates music
full of emotions
[Music]
Title: Dungeons & Dragons Neverwinter Tech Video
Publish_date: 2013-12-19
Length: 84
Views: 32659
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/MyE84ydeYRY/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: MyE84ydeYRY

--- Transcript ---

hi my name is andrew from nvidia and i'm
here today to talk to you about some
exciting new technology inside
Neverwinter using NVIDIA GeForce enabled
pcs Neverwinter is a free-to-play MMORPG
set in the Dungeons & Dragons Forgotten
Realms universe
the developers over at cryptic Studios
have implemented TxAA or temporal
anti-aliasing technology into
Neverwinter which reduces the flickering
and crawling caused by motion in the
game aliasing is caused when detailed
portions of the game begin to crawl or
flicker such as in these examples
with the help of TxAA the flickering and
crawling is removed giving a much more
appealing scene
TxAA is only supported on nvidia Kepler
GPUs and can be enabled in the game's
graphics options
txa is spectacular Neverwinter and is
truly the way it's meant to be played
Title: Exploring New Looks in AI - ModiFace | Season 1 Episode 4 | I AM AI Docuseries
Publish_date: 2018-01-29
Length: 346
Views: 35242
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/N0ffMLIDvNc/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: N0ffMLIDvNc

--- Transcript ---

[Music]
over centuries we've been taught that
our hair gives us a certain personality
and charm but if you've ever dyed your
hair you've more than likely been
disappointed in the results at some
point then you face the panic of
actually fixing it I heard about a
company that can answer these questions
before you face them but can they really
find the best solution for something as
personal as hair color using something
as technical as AI I'm Arjun from Nvidia
and this is Imai so as soon as a user
will open the application on to their
iOS device and they're instantly able to
see their live image it's like looking
in a camera feed so as you're moving
back and forth we're seeing that you're
moving it's like looking into a mirror
and based on that we can instantly start
tracking your face tracking the hair and
you can apply any color you want we've
been around for over a decade now and it
all initially started back when our CEO
who is also the founder dr. Parmer
Robbie was working on face tracking and
lip detection some of our initial
clients Allergan being the very first
one to ever launch with us had
approached him with the concept and idea
of can you take this facial tracking and
do it in a different way and actually
show what a product could look like for
things such as Botox and Juvederm
injectables but we progressed so much
over time because there's such a need in
the market for the consumers to see what
they could look like with the hair color
applied before buying that product and
doing that themselves are going to the
salon and so being able to utilize that
it is so easy to use now there's no user
input you just look at a screen and even
at the back of my head without my face
necessarily being shown on the screen we
can track that we know where your hair
is we know what the texture really is
we're understanding the placements and
we can color it virtually anything you'd
like as facial tracking lip detection
have become more common uses of AI what
was the natural progression from face to
hair where does the process start the
initial applications were 2d
applications where you would upload a
photo you had to manually outline your
hair or make corrections to what the
detected mass was and then you would see
a coloration result on that 2d photo but
it doesn't give you that live view and
so what we had to do was really
understand
how can you take all of the data and
know-how we have about hair and applied
to understanding and finding 3d hair so
an image find every strand of hair and
know what is hair that's black but what
is not hair that also is black but is
part of the background there are 150,000
hair follicles that are unique to every
human I wondered where you even start
the training how can you teach a network
to look at each strand of hair what we
had to do is manually annotate these
images so we had to make sure you our
users would go and manually define what
is hair and what is not hair to provide
training data for it for the neural nets
and then the neural net was trying to
mimic what humans could do so that first
attempt did well but it wasn't perfect
so we had to actually improve our neural
net architecture
modifies uses a convolutional neural
network or CNN that's been modified to
generate a mask for the hair in an image
typical CNN architectures consist of a
sequence of convolutional layers of
increasing depth and decreasing
resolution together these layers are
called a convolutional encoder they
provide strong feature Maps and deeper
levels of the network but without all
the detail of the source image to
generate a reasonable mask or hair this
process has to be reversed this is done
by decoding the feature maps generated
by the encoder using up sampling and
convolution to improve resolution this
encoder decoder model forms a fully
convolutional network or f CM that can
generate a detailed mask for hair it's
interesting that once this neural net
understands what hair is in one image
when you provided live video it can
build that three-dimensional model of
the video but the basic ingredient of
what it detects is a 2d hair image the
hair color app is so easy to download
and try out I wanted to take it out of
the world I wanted to see how people
responded to it
I was curious to see how hair color
enthusiasts would react as well as those
who would never risk diner
[Music]
NoHo there it is
[Music]
so the haircutter app is a demo of what
our technology can do eventually
customers can use this technology if
they go in salons if they if they're
trying to get on a website if they're
trying to buy a haircolor product or if
they they're just at home they're
wondering what color might be best for
them so there was a lot of hope and a
lot of guessing but now it's all about
knowing precisely what the color will
look like on you and different lighting
conditions and by the time you commit
that this the color you want you know it
is the one you really do bond after
seeing how people responded to the app I
wanted to know how this technology could
evolve what about this is going to be
the game changer in the beauty industry
in terms of where this is going to be
going in the future
I mean AI is really just hitting the
market and I think in the future this is
gonna be more prominent in new locations
but it's also gonna be used more for
that personalization consumers to
understand what's gonna work best for
them know what works best on that
celebrity in the magazine or what works
best on that model but it's all about
them their face their skin tone their
hair colors at what works for who they
are
[Music]
you
Title: Christie Digital Interactive Auto Design
Publish_date: 2013-08-19
Length: 119
Views: 10049
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/n4QbPEfxrf4/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: n4QbPEfxrf4

--- Transcript ---

hi I'm Andrew page one of the product
managers here in Nvidia in front of an
interactive design studio that we've put
together along with Christi one of our
partners doing all the projection and
display systems RTT with their software
generating all the images you see here
and our Quadro GPUs putting all the
image together and giving the image we
see so what you're looking at here is a
car that is actually being projection
map so we have a 3d model of a car that
we printed out and on top of that we are
projecting all the different images we
can come in and do a configuration we
can change the scene of where the car is
we can change the color of the car we
can come in and change the wheels all of
this is being done in real time behind
this demo there are 12 GPUs working in
unison to render all the different views
that you see here Artie T Delta Gen the
software that's standard within the
automotive industry for design and
styling and design reviews providing all
the imagery and then again Christie's
projectors and micro tiles putting all
the different displays together in a new
way but unique and interesting with this
is the notion of projection mapping kind
of the idea of shrink-wrapping the image
of the car onto that car model we showed
earlier
projection mappings used a lot in kind
of architectural design and building
stuff not usually brought down into the
scenario of seeing it on a model the car
in front of you but when you put it all
together you get an impressive system
you can see click choose and interact
with the model right in front of you and
see it all in real time again from
Christie providing the displays RTT
providing the software and NVIDIA Quadro
GPUs powering it all
Title: PlanetSide 2 PhysX Trailer
Publish_date: 2013-03-21
Length: 173
Views: 259236
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/n5qhaEghJ74/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: n5qhaEghJ74

--- Transcript ---

hi my name's andrew from nvidia
planetside 2 is a massive online
first-person shooter with thousands of
gamers battling on maps of planetary
scale
the folks at Sony Online Entertainment
have teamed up with Nvidia engineers to
bring physics to the game now we're
going to take a look at some of the cool
GPU accelerated physics effects in
planetside 2 first we're going to take a
look at physics particles in planetside
2 we use the GPU to simulate the
interaction of particles with the
environment creating a much more
realistic experience now we're going to
take a look at vehicle explosions as you
can see here with physics disabled
destroying this tank is fairly
lackluster but with physics enabled it
creates a pretty epic effect
next we're going to take a look at
ground debris as you can see with
physics disabled there's no debris
whatsoever browse with physics enabled
you can see all sorts of particles are
being created by the weapons interacting
with the environment
finally we're going to take a look at
reactor explosions with physics disabled
there's no sparks no excitement when
you're playing planetside 2 destroying
the reactor is a pretty epic part of the
game when you enable physics it's a
pretty serious explosion
just look at those embers smolder now
we're going to check out Apex turbulence
apex turbulence is an Nvidia only
feature which uses velocity fields to
simulate particle motion the particle
counts on some of these effects can get
upwards of a hundred thousand particles
first we're going to take a look at the
healing grenade when the combat medic
uses this item it creates a turbulence
effect inside the AoE healing area
attracting players to the healing effect
next we're going to take a look at the
medic gun as you can see with this X
enabled you can see it creates a whole
bunch of particles using Apex turbulence
now we're going to look at the
antigravity lift as you can see on the
left with physics disabled there's no
particles whatsoever browse on the right
the weight of the character causes the
particles to swirl around in a realistic
way jump pads are everywhere in
planetside 2 and a great way to travel
across the map as the character launches
through the pad you can see the
particles interact with the air
turbulence and finally teleporters
spawned players onto the map as
turbulence particles follow the player
onto the battlefield as you can see
physics looks awesome in planetside 2
check out planetside 2 for free on
planetside 2 calm now gear up game on
you
Title: Android 6.0 Marshmallow now on NVIDIA SHIELD tablet K1
Publish_date: 2015-12-15
Length: 120
Views: 81769
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/N7DlYc1l9aA/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: N7DlYc1l9aA

--- Transcript ---

I'm will O'Neill and today we're going
to show you Android marshmallow running
on the new Nvidia shield tablet k1 which
is the fastest 8-inch tablet on the
market the shield tablet is the ultimate
tablet for gamers and NVIDIA is always
improving performance and adding new
features we're also committed to keeping
it up to date the latest updates and
releases the shield tablet k1 will be
one of the first tablets to rock Android
marshmallow so let's sit back while we
show you what the experience is like one
of the first really cool features you'll
notice in our marshmallow update is the
new shield camera a brand new camera app
it features enhanced by the Tegra k1
mobile processor another cool change is
on the apps screen all you have to do is
scroll up or down as the apps page is
now a single page also check out the
search bar at the top of the page this
makes finding all of those apps cinch
speaking of apps within you can ensure
that you're safer than ever with apps
permission this tool lets you simply
control which apps have access to its
features posting a microSD slot with
marshmallow you can more easily make
your sd card an extension of your
internal storage and you no longer have
to decide where apps get installed
marshmallow does this automatically
another fun feature is now on tap simply
put you hold down the home button and
you're offered a contextual screen that
lets you pretty much search in whichever
environment you happen to be in of
course where it really shines is in its
complete integration into invidious
gaming ecosystem the tablet is perfect
for gamers who want to play games on the
go or on the couch and we prepare
official controller it's gaming jobs
become even more apparent in addition to
being able to run all the latest Android
games it also benefits from GeForce now
this means you can stream your favorite
triple-eight PC games to the tablet and
play them using the shield controller
the shield tablet k1 is going to be one
of the first tablets with marshmallow
and for the full skin be sure to read
more at shield Nvidia calm
you
Title: NVIDIA Supercomputing 2021 Special Address
Publish_date: 2021-11-15
Length: 3645
Views: 737216
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/n9_qtjqZXvQ/hq720.jpg?v=61928bed
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: n9_qtjqZXvQ

--- Transcript ---

good afternoon good evening good morning
depending on where you are in the world
uh my name is john josefakis i'm the
vice president of high performance
computing sales at nvidia
excited to have you join us for this
sc21 special address
it's been another exciting year with
very with many announcements following
our gtc event and now with sc21 it
promises to be a very exciting time uh
for all of us in the high performance
computing community it is the first
event actually after a long time where
nvidia is participating both in a
virtual way but we also have a small
presence at sc21 to meet our customers
our partners and of course our hpc
friends over the last year we have had
many announcements around our technology
on the hardware platforms with our grace
arm cpu with our a100 and different
options there and also with our ndr
networking devices but in addition to
that we have announced a lot of many
software initiatives and different areas
of research
in addition to that we continuously
invest in research computing in hpc and
how data sciences is becoming part of
our modeling and simulation
in all areas we see that data sciences
are making a big difference in ability
of people to solve larger problems with
larger systems
in areas such as edge computing quantum
computing weather and climate and
alternative energy just to name a few
areas of focus and work
today we'll have the opportunity to hear
from a number of nvidians about some of
the advancements that we're making and
in addition we'll have the opportunity
to talk about specific technical
sessions that will be addressed at sc21
and one week ago at gtc we had a lot of
events that please feel free to go and
watch and see but even more we'll have
more sessions around supercomputing to
anyone that you are able to watch
catch the catches also on the floor or
at our meetings and we'll be glad to
discuss with you either face to face or
a one-on-one call as we go through this
a few next few days and weeks
but with that i'll pass it on to mark
hamilton and he will take it from here
and go into our presentations thank you
so much and again look forward to a very
exciting sc21 thank you
thank you john for that great intro and
let's go ahead and
head straight into the talk
i'm mark hamilton and today i'm going to
talk four quick chapters before we open
it up for q a
if you have questions please go ahead
and place your questions in the q a box
our moderators will be listening to them
and we'll get to them after these four
sessions
i'll jump right in and talk about full
stack accelerated computing the
technology nvidia pioneered over 15
years ago
we'll then move quickly to talk about
cloud native supercomputing
million x acceleration in science and
finally
cap it off talking about omniverse and
how omniverse plays in the super
computing space
there's a new computing loss that
started
moore's laws come to an end
and we need new innovation in high
performance computing
nvidia's been leading the way in
accelerated computing for 15 years
and we've been here super computing
after super computing talking about
advances every year
in what we've been driving remains the
same
we continue to be the leading
accelerated computing company and
continue to drive
increases in performance
but there's two new things that are
happening
the data center is really becoming the
new unit of compute
supercomputing today isn't just about a
chip
or an accelerator
supercomputing isn't about building the
fastest node
supercomputing is about scaling out at
the data center level
and that requires of course the compute
nodes the storage the networking to all
work together and work together with
millions and millions of lines as
software code
so between accelerated computing
amplified by scaling out and up across
the data center
on top of that we've now added machine
learning
in the early days machine learning was
just about data science in data
analytics and there's many advances
going on in that space but today we see
machine learning applied not only to
data science but to traditional
scientific computing
we'll talk about some of the advances in
that space
nvidia today truly is a full stack
computing company
it's not just about one or two or three
amazing chips we're excited about all of
our chip families the gpu
our new armbase cpu
and of course the dpu for networking
but that's just the start
we have over 150
sdks
i'll get to talk about only a few of
those today but our sdks today serve
multiple industries
ai cyber security
5g robotics
telecommunications in all of these
industries now are applying
supercomputing in high performance
computing
just in the last year since sc20 we've
introduced over 65 new or updated sdks
we're now up to three million cuda
developers in over 30 million cuda
downloads
in over 2
500 accelerated applications running on
the nvidia platform
nvidia is all about our partners
and we're so proud to be here at sc21
with many of our partners and even many
more joining virtually
i'll talk throughout the talk about how
we're working together with partners
to make our solutions even more powerful
but let's talk about real performance
not just on one application but on a
basket of over a dozen applications
representing data science
high performance computing
machine learning and deep learning
the chart on the left is a geometric
mean of the speed ups
we've delivered 16x performance
improvement in six years over three
generations of gpus from the p100 to the
v100 to our latest a100 tensor core gpus
if you look closely
especially at the v100 and a100 you'll
see that eurovir on the same exact gpu
we continue to increase and improve the
performance
what other platform today can you buy
and on the same hardware have improved
performance every year
today
nvidia really is the world's leading hpc
in ai platform
in the ai space there's been a new
benchmark that's emerged over the last
several years
ml perf
and nvidia has come in number one in all
of the ml perf
benchmarks not only for training where
you would expect nvidia
to be number one but also an inference
and we'll talk about continued
improvements and increases in
performance in inference
and finally this morning
as every year at sc we celebrated the
latest version of the top 500 list
nvidia today has more systems on the top
500 than ever
71 percent of the top 500 systems use
nvidia technology in 23 out of the top
25 green 500 systems
we see other new benchmarks
coming out as well
and we're very proud to be participating
in all of those
nvidia has announced more exascale ai
supercomputers than any other company
some of these like summit still place
very high on the top 500 list and others
were just installed this year or will be
installed soon
around the world from europe
to the u.s
in asia
customers are choosing nvidia for their
exascale ai computing
and finally research breakthroughs
four of this year's gordon bell
finalists as well as the nobel prize in
physics or one on top of nvidia
technology
here's an announcement we just made last
week
one of the most commonly used commercial
hpc applications insist fluent
now available on a new gpu optimized
version
it's an amazing value prop
using the new gpu optimized version
running on a single dgx a100 with eight
a100 tensor core gpus
compares to two racks or 30
dual socket cpu nodes
that provides 10 times more energy
efficiency in five times better cost of
ownership
ansys is seeing great reception to the
beta of this product and we really look
forward to seeing it adopted by ansys's
thousands of customers around the world
one of the hottest topics in super
computing are super computers of the
future quantum computers
but of course
much work is still left to be done
before we actually develop a real
quantum computer that's commercially
viable at scale
today we're very proud to announce the
release of the quantum public beta
integrated with leading quantum
frameworks and simulators from ibm and
from google
in more
more quantum frameworks coming soon
we've been working with all of the
leading
industry and research community partners
this is a space where we're partnering
more than anywhere
let's talk about how coup quantum is
accelerating quantum circuit simulations
that will be needed
to build
new quantum computers
we introduced the djx quantum appliance
this runs on the dgx a100 and similar
hgx a100 servers from our oem partners
the speed ups are truly transformational
here we show some benchmarks reducing
the time of compute from minutes to
seconds
but real quantum simulations today that
take months or more to run
now be run in days
this will enable
much faster progress towards future
quantum computers
out of the box or dgx quantum appliance
is optimized for google circus system
and many more
simulations planned to be optimized for
this in the coming months
in video researchers though haven't been
standing still
we set a new world record simulating
eight times
a larger qubit system one thousand six
hundred and eighty eight qubits that was
ever had an exact quantum simulation
before using quantum running on our djx
quantum appliance
this was done using the popular
simulation of the max cut algorithm we
we were able to simulate over 3 000
vertices
these are the types of breakthroughs in
quantum simulation that will really move
forward the development of future
quantum computers
this system will be available starting
in q1 of next europe
switching gears a little bit
we'll talk later
how
our hpc sdk of course supports industry
standard languages such as c and fortran
that so much hpc code is written in
but you know there's many new
programmers in the world and if you went
to university in the last five years
you're probably writing your scientific
computing code not in fortran or c but
in python
and i get a lot of laughs sometimes when
i talk to a fortran programmer about
writing fast python code it wasn't
possible before
but let's let
let's talk about what nvidia has done
with numpy one of the most popular
packages in in python for writing
scientific code
we've introduced the coo numeric sdk
numeric automatically accelerates python
code and is a drop-in replacement for
numpy
we've scaled
we've tested this running it over 2000
gpus
this will open up a whole new world of
gpu acceleration for a new generation
of scientific
programmers writing in python today
the automatic parallelism for this not
only supports multi-gpu parallelism on a
single server but also supports multi
multi-node acceleration across multiple
gpu servers
and of course it not only uses the gpu
but automatically runs the code across
all the processing elements of the
server
the cpu
the gpu
and even the dpu cores
now of course
back to fortran in c
i wanted to give an update
three different ways of course to
program the nvidia platform
15 years ago
we started with nvidia cuda the parallel
api that is still the most widely used
in the world
for many years
nvidia supported open acc in openmp
directive style
languages or enhancements to languages
but today
for programmers starting out in standard
languages we believe
the standard parallel extensions in
fortran and c
and there's more of these going into
every new version of the standard or one
of the best ways to pro
to program accelerated computing
we don't believe there's any need for
yet another
parallel programming api
when parallelism is now based in the
languages themselves
anyhow all of these models will continue
to be supported by nvidia moving forward
one of the most common
one of the most common data structures
in scientific computing is the graph
and data scientists can now use graph
neural networks to learn both the
structure or to process both structured
and unstructured data
and of course graphs
when representing and when represented
when taking structured or unstructured
data and representing that as a graph is
an inherently sparse
representation
a perfect
a perfect problem to run on our tensor
core a100 gpu with sparsity
there's a number of things that we were
accelerating in here we're using dgl
deep graph library coming from another
one of our partners amazon
we've accelerated it with the cuda x
framework and provided a number of
different breakthroughs
finally let's talk about nemo megatron
nemo megatron is a great example
of a new class of ai problems and yet a
new performance curve
if you look at all ai applications that
have been popularized over the last 10
years we see about a 25
x a year increase
or doubling 25 x in performance needs
every two years
however a new class of natural language
processing in transformer based
applications or seen a doubling or every
two years or seen an increase of 275
percent
gpt-3 from open ai is a great example of
that
and of course the latest megatron turing
natural language model released by
microsoft and nvidia together is the
first
public model that runs on over
representing over
billion parameters
an nvidia selene supercomputer with 500
dgx systems
we we can tr we can train gpt-3 in 11
days
in megatron turing can be
can be trained from scratch
in six weeks
now of course our researchers want to
train those frameworks even faster it's
a great snapshot of where we are today
and with nemo megatron
you don't have to train from scratch
with the models we provide
in using
you can go through and add your own data
in retrain even quicker and develop your
own state-of-the-art language models
now just as the size of models you need
to train
increase
so too
to the size of models that you need to
inference
trans triton is nvidia's distributed
inference engine
it's a great tool to get started on
running inference even if you don't have
any gpus it was one of our first pieces
of software our first sdks that we
ported and ran in cpu only
many of our customers start using train
on cpu-only nodes and then add gpus is
their model grows or is there a number
of users and number of inferences that
they need to run grow
train of course started supporting
single gpu performance again if you look
at inference on the full gpt3 model it
needs at least 350 gigabytes of gpu
memory
more than any single gpu will have for
years to come
so multi-gpu inference in train was a
feature we needed to add
finally to run the full megatron turing
we needed over 530 or we needed over one
terabyte of gpu memory
and while megatron turing is a huge
model researchers tell us that they just
see that curve going up and up every
year
and so with one terabyte of memory we
can run inference across
three dgx a100 servers with eight gpus
each
so this is amazing using triton you can
take these giant models and not only
spread them across multiple gpus in a
server but across multiple servers
taking advantage of all of nvidia's
networking technology nv link within the
server between the gpus
infiniband in
gpu direct rdma over ethernet outside of
the server
one of our amazing products that's
catching on across more and more
customers
we've talked a lot about our partners so
far
and today very proud to announce
with one of our most important oem
partners atos a new nvidia plus atos
excellence lab in ai
together in this virtual lab we're going
to focus on advancing five different
parts of supercomputing technology one
we'll combine
we'll combine the atos and nvidia
research in quantum computing to make it
further advances in that field
we'll work on cyber security taking
atos's expertise providing cyber
security solutions to commercial
customers around the world with new
nvidia products in the cyber space
climate
something's important to all of us
etos of course runs some of the largest
climate systems in the world
and we'll cover that in our
collaboration
in genomics we'll also be working with
etos and finally on computer vision it
is so important in areas from smart
cities
to video analytics
i'm very excited about this new ato slab
and working closely with atos not only
in europe but around the world
and of course we'll be working together
with all of nvidia's
exascale class technologies bringing
them to the bulls iguana supercomputer
line
our arm-based grace cpu nvidia's next
generation gpus
in atos's bxi exascale interconnect
working together also with nvidia's
quantum 2 infiniband technology
so that was just chapter one we're going
to speed it up a little bit here and
talk about cloud native super computing
hpc and ai are going everywhere
not just in the traditional data center
where the supercomputer was but out to
large edge instruments and out to the
cloud
but of course
this requires your supercomputer to now
take on new cloud-native supercomputing
types of capabilities it's an entirely
new platform that nvidia is introducing
nvidia quantum
quantum two isn't just about increasing
infinite band speeds to 400 gig
it's about so much more
multi-tenant bare metal secure isolation
performance
congestion control
sharp gen 3
providing 32x the ai processing
capability right inside the switch
and precision timing down to the
nanosecond level in enabling new levels
of performance in applications like
distributed databases as well as
enabling the world's cloud and other
large data centers to become part of the
5g revolution with software defined 5g
networks
three parts to quantum two
one is of course our quantum two
switches
up to 64 ports
of 400 gigabit or 128 ports of 200
gigabit in each quantum two switch chip
this allows us to bring a tremendous
amount of speed and performance and
reduces the number of switches in a
large infiniband network
reducing complexity
increasing performance saving costs
we'll have our connectx 7 infiniband
network interface controller
bringing new levels of in-network
acceleration
and finally
bluefield 3 infiniband building on top
of connectx 7 with a new 16 core arm
processor and a number of new
acceleration functions
here's the challenge solved by quantum
two
the supercomputer expects supercomputer
customers expect peak performance
for specific problems
but when you run that in a multi-tenant
cloud
you sometimes have other tenants get in
the way and your execution time spreads
out
and this is unsatisfactory for
turnaround time on your problem
now with the cloud native supercomputer
performance is isolated to each
individual tenant and it lets you reach
that bare metal performance in a cloud
environment using all of the quantum
software and hardware features
we've already had our first customers
wins for quantum two
mississippi state university is using
quantum two for their latest cluster
this will be the fourth infiniband
cluster
running at the same time on campus and
one of the great features of quantum two
like all the infiniband generations
behind before it is backward compatible
so mississippi state will be able to
connect all four clusters without having
to upgrade or change the networking in
their older systems texas a m another
one of the early adopters of quantum two
switches
now another feature in cloud native is
you need to have zero trust in your data
center
we've all heard about cyber break-ins
across all sorts of industries and even
at supercomputer centers it's no longer
enough to trust a firewall you need to
isolate and accelerate every segment
every host in the network requiring a
zero trust solution
we've been expanding our bluefield
ecosystem with a whole range of cyber
security partners with just a few of
them listed here
now let's go through and talk about
million x science
how is nvidia accelerating applications
a million x now and in the future and
what are the sorts of applications that
really need this million x acceleration
again the way we're going to provide
million x acceleration is not simply by
building a faster processor
that's just the start
we'll need to scale out and up across
the data center
and we'll need to add machine learning
and ai
we really need to take a full stack
approach and work with our partners
work with thousands of software
developers around the world
let's jump in and talk about some
examples we're working on today
life sciences is a field that's been
revolutionized just in the last few
years like no other
life science of course has been a long
time user of high performance computing
but there's a number of examples here
where
life sciences customers are going
through and combining hpc acceleration
with machine learning and ai
one of my favorite favorites is mega
mobart
the name alone is interesting mega
mobart was gen was developed by
astrazeneca and nvidia just this year
working on our djx super pod
and also running on the cambridge one
supercomputer of nvidia's
with mega mobart
the system was able to generate new
molecules that don't exist in in any
textbook don't exist in any of the
thousands of molecules that have have
already been identified and tracked
there's 10 to the 60th possibilities of
of how you can generate a molecule
this is an amazing advance in this
science
alpha fold
now can be used to predict protein
structures
this is a key part of drug discovery
and of course
everyone on earth can benefit from new
drugs
in drug discovery very small molecules
of potential drugs need to be tested
against hundreds of thousands of protein
structures to decide whether they're
actually going to cure the disease the
condition
nvidia's been working around the world
with different researchers in this area
one of my favorite is entos entos is a
company that was just founded a few
years ago 2019
and
what better to tell it than this short
video from intos themselves they've
worked with us to provide solutions one
thousand times faster
entos founded in 2019 in san diego
california is using nvidia gpus and
nvidia clara discovery to accelerate the
development of tomorrow's drugs and
therapeutics let's take a closer look
entos has developed orb net a graph
neural network that models chemistry at
the atomic level where quantum mechanics
governs how atoms interact with each
other orbna is allowing entos to
discover covalent inhibitors for example
against the protein hsp90 shown here
orbnet is able to predict energies and
forces to drive molecular simulations
intertwining quantum mechanics and deep
learning modeling the electronic
structure of molecules allows entos to
simulate how a potential drug will
interact with a protein
here we can watch as chemical bonds
break and form as a candidate covalent
inhibitor reacts with the lysine residue
of hsp90 on nvidia gpus simulations with
orbnet are a thousand times faster than
dft
[Music]
it's amazing entos has done in just a
few years
what before them researchers spent their
lifetime trying to solve and wasn't
possible
climate simulation
and climate certainly been in the news
with cop26 the last two years
is an area that's being revolutionized
by deep learning
there's a number of different projects
around the world on earth digital twins
on super resolution for climate models
in other areas to go through and improve
climate science
and of course why is climate science
why is accelerating climate science a
million x so important
let's look at some of the scales where
we're at today
thomas schulte from
uh cscs
swiss super computer center has done
extensive research in climate science
has predicted that
this year
or by 2023
that they need to get to one kilometer
resolution to resolve convection and
remove parameterization to improve model
accuracy and they're working uh they'll
be working on their new grace
supercomputer in 2023 to provide just
that
but that isn't enough
a second projection
says that 100 meter resolution is
required to resolve storms and predict
extreme weather
we've all seen examples over the last
year where unpredicted extreme weather
has caused havoc
destroyed property and destroyed lives
and finally
looking out farther into the future
even
even more even even more resolution
one meter resolution is required to go
through and simulate stratocumulus low
clouds to reduce uncertainties in
climate projections
this requires literally 100 billion more
compute than the 2023
uh one kilometer model
we believe that the world can't wait to
2060 to reach these sorts
of accelerations
that's why this fall we introduced
nvidia modulus
modulus is a framework for developing
physics-inspired
ml models a machine learning model
that's been trained by real physics
with modulus
scientists around the world will be able
to create real digital twins to better
understand large climate systems like
never before
it's reshaping the world
let's go ahead and take a look
at some of the things we might be able
to accomplish
this is an earth digital twin in
omniverse
this slide here describes what's going
on taking in different sorts of sensor
data
it running modulus and then displaying
the results in omniverse
let's go ahead and roll the demo
the furrier neural operator physics ml
model predicts the precise path of
hurricanes a full four days in advance
and can provide near real-time updates
as conditions evolve
here we're looking at the massive 2016
hurricane matthew the red line shows the
observed track of the hurricane the
contours show the ai prediction the
black dots show how closely the actual
path of the hurricane followed that
prediction the white cones show the noaa
forecast
being able to accurately predict the
behavior of complex weather systems
around the world is a key step towards
building a digital twin of the earth
you know in in silicon valley where i
live we've experienced nearby fires
across the state
started off the season with more
rainfall than ever
though we're in a euro drought
these sorts of advances can't come soon
enough no matter where in the world you
live
finally the last chapter before i go
into this one last reminder
there's a q a window and if you have any
questions please post them in the q a
window and we'll get to them shortly but
first a few minutes about omniverse
there's been a lot of talk in the press
about on on omniverse and i thought i
would take just a few slides to talk
about what is it and how does it apply
to super computing
many of you today
were just infants or in school when the
world wide web was founded
and you don't remember before
but before the world wide web the
internet existed
but it was just point-to-point
communications things like email or ftp
for file transfer
it was with the discovery of the web
browser and the world wide web that you
could connect to any company's business
site
any consumer site and get in effect 2d
data
in a standard web browser without having
to write a lot of custom code
it just simply worked
html is what made the world wide web
possible
but today of course
the world isn't just about 2d data
there's so much 3d data available
various sorts of 3d design tools
used in architecture and engineering
used by anyone who has a camera in their
cell phone and does some simple photo
editing and all sorts of different tools
and today they don't interoperate
so the first thing omniverse is it's a
tool for digital content creators
using pixar's universal scene descriptor
language
omniverse brings in 3d data from any
tool with the appropriate connector and
lets users interact with and manipulate
the data in their tool of choice
instantly
showing
the changes in one tool to a different
user using a different tool connected by
omniverse with the same connector the
same way the world wide web let two
people
look at two different websites and
perhaps type in a document and see the
changes in the document at the same time
only this time it's 3d data
the second part of omniverse is of
course the ai and the science behind it
in the early days of the internet
you had the app server in the app server
provided simple transactional database
or other logic that's set behind the
data
but of course in today's world of ai
it's not just simple structured data
we're running robotic simulations
developing self-driving cars
running complex digital twins of
factories
all overlaid with that 3d data in
bringing it all together
omniverse isn't just a concept that we
dreamt up to last week
omniverse is something we've been
working on our lifetime is a company
and
very specifically is a product over the
last four or five years
so let's take a look at one of our
partners siemens energy and how they're
using omniverse today
let's roll the demo
this massive heat recovery steam
generator uses hot gas exhaust to
convert water in the pipes into steam
for the turbines
predicting corrosion to avoid downtime
is challenging
reduced order models aren't extremely
accurate and full simulation takes
expertise and time
siemens energy is developing a digital
twin with nvidia modulus for the
multi-phase turbulent flow
with point cloud data to train a
physics-based ai model they can infer
high-fidelity flow shown by the
streamlines in seconds
this could reduce downtime by 70 percent
saving the industry 1.7 billion dollars
a year
so that's just one example of multiple
customers were
we are working with today in omniverse
this will be a huge use of super
computing going forward and letting the
results of super computing centers being
put to new uses and shared not just in
the data center where they're created
but across the world by anyone
today we already have many many partners
connecting virtual worlds
and here's some of the the stats and
some of the names of some of our
partners today
finally as jensen announced last week
we're going to build the world's largest
digital twin
i know many of you have heard about our
cambridge one supercomputer some of our
partners like astrazeneca gsk kings
college london and others have been
working with us and collaborating on
cambridge one
earth 2
will be a similar supercomputer but for
collaboration not in life sciences but
collaboration around
our own planet building the world's
largest digital twin
we're very excited about this and
it will be designed to use all of our
latest technologies including of course
modulus created ai physics models in
omniverse
so quick wrap up of what we talked about
today
nvidia is that continues to be the
number one accelerated computing
platform
it's not just about a fast chip
although we have three very fast
families of chips
it's about the thousands of applications
in data center scale acceleration
the data center is the new unit of
compute
in million x speedups from where we are
today are needed
finally
our omniverse platform powering digital
twins will connect supercomputer centers
and help them share
their outputs like never before
so let's go ahead
and bring up our panelists and i see
there's a few questions here
the first one we'll uh we'll give to a
gita to answer akiraka how do you plan
to serve big models like gpt-3 into a
service that can run at the edge
okay can people hear me now
okay yes oh wow this is a great question
i mean um so the question is how do we
plan to use uh or serve big models like
gpt3 into service that can run at the
edge so first of all uh gpt3 is a
model from open ai that is used for
generating new language
and answering questions
and edge computing when we talk about
edge computing edge computing is very
anywhere where a computer or
any device is interacting with the human
world
so
i would say that when it comes to edge
computing um
definitely wherever there is speech
required there is
interaction with
with with a human being who's answering
or who's asking for questions or looking
for information
that's where
gpt3 can be definitely used
for
for resisting
you know answering questions to the
customer
now these can be used cases either in
customer service these could be use
cases in in retail
so there are various places where this
model could be used at the edge
great thank you
next one for tim i think tim
here's the question can any huge matrix
solving compute problem be optimized by
cuda
sure um i mean pretty quick answer and
then i'll give a medium length answer
the short one is yes
um you're really only bounded by the
total system memory uh you can write an
optimized cuda code to solve the problem
uh even if it's out of core um
of course i would be remiss if i didn't
mention that you know nvidia has a huge
set of libraries we built over you know
15 years providing highly optimized
performance for a huge huge range of
types of solvers so i would always of
course recommend checking out what we
have there
uh first um sometimes people are
surprised by the kinds of solvers that
we already have implemented
you know for example
we have mixed precision solvers that can
take and return fp64 accurate data uh
but as a black box internally leverage
reduced and mixed precision um in some
ways we want to talk about huge problems
hplai that benchmark is actually really
an at-scale demo of that capability
which we have available for users in our
libraries
tim i think that's a great answer and
just resounding on some of the themes
from earlier right
you know just doing matrix math in a
chip is is not that hard right but
really doing it at scale
in some with so many different types of
software applications in doing it
effectively all the way from a three or
four watt nvidia gpu embedded at the
edge all the way up to the world
currently the world's sixth fastest
supercomputer selene number one leader
in ml perf really requires that full
stack approach to acceleration
here's one for dion dion
is modulus a simulator
um
no short short answer is no um basically
modulus
is i think as you perfectly described
is a
way of providing physics informed or
physics ml neural networks that can then
be used to inform a digital twin
or just be used for surrogate
approximation of simulation practices so
it's really meant to help you accelerate
some of your your traditional
stimulation practices using a physics
and form model it allows you to ingest
things like pdes or or
tnos fnos et cetera lots of
physics-based based
algorithms that can then be used to
train a neural network and give you
uh outputs that obey the laws of physics
you know i i heard one of our
researchers uh describe it this way they
said look uh for for decades simulators
have been built to simulate the laws of
physics and the laws of physics of
course are not changing
right but
but the laws of physics are sometimes
very very expensive to calculate and to
simulate so he said the way i'm using
modulus is when i have to i'm actually
simulating the laws of physics using all
of your different hpc sdks but when i
don't have to
simulate the whole problem i can
actually use data science or ai
and go through and solve part of the
problem that way in save on
computational costs
galad here's one for you i i know i sort
of glossed over some of the cloud native
quickly
if i'm processing in the cloud and i
want to use or my cloud offers hpc bare
metal instances would i still benefit
from telemetry based congestion control
well the answer the answer is is
definitely yes that's the short answer
um and in regardless if you're running
on a cloud and getting bare metal
performance or running on a super
computer and getting a bare metal
performance
you you are sharing that infrastructure
with other users
for example let's say that i'm running a
cf a cfd code on the system and my
friend
pasha is doing a massive vault all
operations and his friend mark
is actually running massive storage uh
applications and and his friend gitika
is doing a massive data that comes from
the edge all the way to the main system
those applications share the same
infrastructure and the for one
application can impact the other
application performance
and with the telemetry based congestion
control we are bringing performance
isolation
into bare metal cloud or bare metal
supercomputer which means that one
application will not harm another
application performance and every time
you're running your application you're
gonna get the same predictive
performance
so the answer is definitely yes
uh and we're bringing great capabilities
with the quantum two platform to enable
new new levels of performance relations
thanks
and you know i love to talk about our
partners and i forgot to mention that
both the texas and the mississippi state
systems are being delivered by our
partner dell so uh thanks to dell for
being a great supporter infiniband over
the years and for being first to market
for shipping ndr
uh
nicola here's one for you
how do you see hpc plus ai use cases
evolving for health care and life
sciences
uh well very good question mark it's
indeed a very exciting time to work in
the field of healthcare and life science
and it's evolving rapidly so we have
seen major breakthroughs in medical
imaging and genomics and many other
fields in the last years that have led
to new opportunities but one of the most
exciting developments that we have seen
the recent breakthroughs happen
simultaneously so in both biology and
chemistry so you also address this in
your your your presentation
and this may really reshape the way of
drug discovery will be done so it will
be computational
and every area from target
identification to protein engineering
are leveraging ai so some examples you
mentioned where
orpnet alpha fault and rosetta fault for
example
but
believe me this field of drug discovery
is extremely complex and the models
tackling these problems are similarly
complex so some of the successful
approaches
are for example are based on
transformers like the se3 transformer
for property prediction
or your favorite amiga mobile for
molecular generation or the rosetta
fault for protein structure prediction
developing this kind of models and
simulations require a system that can
handle complex workloads efficient
communication within the system and
secure access so if you have to keep in
mind that healthcare data life science
data is usually a very sensitive data so
it's not only using a component as i
said before but the entire software and
hardware stack that really needs to work
together to train efficiently and safely
so it does make a difference and my team
for example has the pleasure of working
with key partners with like astrazeneca
king's college as you mentioned before
on cambridge one so the supercomputer
dedicated for healthcare so really
really exciting field i see a lot of
development
uh thank you nicola i need to give you a
special shout out you know this is a
hybrid show and and some of us like
galad and jack tim and i are here
at the show in st louis others are back
at our headquarters in in california and
i know you're calling in from your
kitchen in germany and so it's it's
middle of the night there and it's not
quite as fancy as jensen's kitchen but
but thank you very much for for being a
part in dialing
um
jack here's a question for you uh what
are some of the different ai techniques
that you see the hpc community adopting
so thanks for the question mark
the combination of ai and simulation is
really taking off
for years and years simulation science
was working away
with steady progress
making comparisons to experiments after
the fact
but now
if you will we're taking
our
left hand out of our pocket and having
it work together with our simulation
right hand and ai and simulation is
making tremendous progress
this is happening in
diverse fields
where the data is available or it can be
generated by simulation
to be sure
material science is out in front as
you've mentioned and biologies out in
front but also in classically very hard
areas like computational fluid dynamics
people are tackling the turbulence
problem the last great unsolved problem
of classical physics people are
tackling this turbulence problem using
ai
to learn from data
what the physics
is requiring um of the of the solutions
so the the
the areas that are going to be explored
are just going to be unbounded and even
if we step away from the areas that have
been dominated by pdes and science into
those areas
like social science and biology and the
humanities
all these areas are going to have
abundant data and where data is
available we're going to see ai applied
and
as our ability to collect data through
sensors
is
is just continuing to progress
this is really a wave that is going to
be at the forefront of science and
research and engineering for several
years to come
uh you know jack
that's exactly right i know i was
speaking to uh some of our researchers
at university of florida that of course
has
their gate gatortron system there of one
of the largest
dji super pods in higher ed and they
just hired over a hundred new ai faculty
members and those faculty members spread
across 50 different fields
the most interesting one was forestry
right who would have thought someone
from forestry needed a supercomputer
before but now with ai and think about
it there's an awful lot of unstructured
data out there on forestry
another question for tim you know i can
see we've got quite a few questions on
quantum here and and that sort of echoes
the interest in quantum in the industry
why tim why is simulating quantum
circuits important versus just testing
them on quantum processors
yeah that's a great question and and
mark you talked a little bit about this
but i'll just reemphasize some points
that i think have been made
there is just a lot of work ahead of us
from here to bring you know hardware
software
algorithms to the point that useful work
can be done on a quantum processor and
that's exactly why nvidia is investing
in this area
you know to help the community
research the
computers of tomorrow on the fastest
computers of today
you know quantum computers today are
small uh they're error prone so too
error prone to demonstrate an advantage
on on useful problems so simulating
quantum circuits with classical systems
allows researchers to study algorithms
at a performance and scale far beyond
what's possible on quantum processors
today
the result that we saw last week at gtc
when jensen talked about it and then it
was also shown in this special address
here
is a great example of this so
we're using quantum tensor network
library on a dgx super pod and we can
simulate a quantum solution to max cut
optimization problem
on thousands of qubits that's multiple
orders of magnitude above what has been
done on a quantum processor
that capability is vital to effectively
research quantum algorithms and
applications to understand where and how
quantum advantage is going to is going
to come
great thanks tim um galad here's one for
uh for you we had a listener asked they
said i invested a lot buying a cpu
cluster last year with uh hdr infiniband
i want to add a gpu cluster now with
quantum two can i connect my cpu cluster
running hdr to my quantum two
gpu cluster
so i'm getting i'm getting all the yes
uh yes questions i guess and so the
short answer is yes
um
long answer is that um
by by choosing the the nvidia quantum
infiniband platform
you're not just getting the best
networking performance the best
in-network computing
capabilities and so forth but you also
protect your investment
infiniband enables you connect a
previous generation to current
generation to future generation it's
completely backward compatible and
future compatible and therefore you can
connect your
cpu hdr system to your gpu ndr system
and if you have a storage system that
might even be on a different generation
you can connect all of those together
and not just that you can do it but we
have many of our users that are doing
the same things we have actually
multiple end users that having multiple
generation of infinite men in their
system or they're having a different uh
compute structures connected to a
different storage structures and
everything works together with the same
software with the same architecture
so the answer is yes and definitely that
sound that's that's a great benefit of
infinite
thanks galad and unfortunate now we're
out of time but thank you galad jack
giraca tim
nicola and dion for joining and thanks
to all of our listeners who joined in to
listen to our special
address at sc21
we'll see you next year
thank you very much everybody for
attending today's sc21 special address i
want to thank all the speakers for their
presentations and the discussions and we
look forward to connecting with you over
the next few days at supercomputing and
over the next
weeks after that and months to come
and hope everybody has an enjoyable sc21
and talk to you soon thanks
you
Title: GPU Technology Conference (GTC) Keynote Oct 2020, Part 8: "Everything that Moves Will be Autonomous"
Publish_date: 2020-10-05
Length: 448
Views: 227793
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/ncAW5Bdq8BE/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: ncAW5Bdq8BE

--- Transcript ---

Everything that moves will be autonomous someday whether partially or fully.
Breakthroughs in AI software has made all kinds of robots possible.
And we are working with companies and enthusiasts all over the world to build these amazing machines
ones that fly, walk, or swim;
with two, four, or more wheels;
underwater or in space;
to deliver pizza, move inventory, farm lettuce and strawberries,
or just being a helping hand.
AI software was the big breakthrough.
But it is the Jetson AI computer that is democratizing robotics.
Jetson is a robotics computer -- it's mighty, yet tiny, energy-efficient, and affordable.
Jetson is an Arm SoC that is designed from the ground up for robotics -- the sensor processors, the CUDA GPU and Tensor Cores, and, most
importantly, the richness of AI software that runs on Jetson.
Today, we are announcing that the Jetson Nano 2GB will be $59. Anyone can now build a robot.
Developing robotics software is like all AI -- a large computer like DGX writes the software
and a small computer like Jetson runs the software.
The AI researchers at NVIDIA have trained many models using our DGX SuperPOD infrastructure
and those pre-trained models are available on NGC.
Using NVIDIA Transfer Learning Tool, you can adapt them to your domain.
But how do we collect the vast diversity of objects, surroundings, and conditions data to train Isaac?
And how do we validate Isaac's functionality in the infinite variability of environments and scenarios that it will operate?
This is where the Isaac simulation comes in.
Isaac Sim is built on Omniverse.
It is a realistic world where we design and train robots.
Isaac Sim imports files using USD Universal Scene Description and URDF,
Unified Robot Description Format, that describe robot models, sensors, and scenes.
This simulation is from the work we're doing with BMW to create a platform to design, simulate, and operate a factory of the future.
The robots are running the actual Isaac stack and interacting with a real-time simulation of the factory and other agents.
This next example is the NVIDIA DRIVE autonomous vehicle system.
Again, the starting point of AI is the DGX training system. This supercomputer learns from data and trains the AI.
The AI software is then OTA'd into the car, powered by the NVIDIA DRIVE AV computer.
Before road testing, our engineers simulate a virtual car in DRIVE Sim, built on Omniverse.
The realism of Omniverse reduces the Sim-to-Real gap and allow our engineers to iterate rapidly,
as if they can instantly update a test car to try their new software build.
Let's take a look:
Hey Mercedes, please pick me up.
Hey Mercedes, please take me for a ride.
Okay, let's start auto-pilot.
Hey Mercedes, please park the car.
Okay, I will go ahead and park the car.
This is work that we are doing for our Mercedes partnership.
Starting with 2024, the entire fleet will be powered by NVIDIA DRIVE AV.
On the left is the RTX server that generates the world view for each sensor.
On the right is the DRIVE AV computer.
The two are connected.
The world views in DRIVE Sim are sent to the DRIVE AV computer, which perceives, localizes, plans,
and actuates the virtual car in DRIVE Sim
This in turn generates new views of the world.
DRIVE AV is running the actual self-driving car stack - all of it - and in theory has no clue it is in a virtual world.
Someday, neither the AI or us will be able to tell whether we are in the virtual or physical world.
DRIVE Sim is an open platform with plug-ins for excellent third-party engines.
Title: VR Content Showcase Winner: Nurulize
Publish_date: 2017-06-01
Length: 78
Views: 13530
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/ncP3oMdSSR8/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: ncP3oMdSSR8

--- Transcript ---

I'm the chief creative officer at
neutralizer a VR software company and
regi QC showcasing our new technology
called
atom to a quick cloud technology it's
completely agnostic sort of like
volumetric editing utility so Adam view
doesn't care what the source of the data
is whatsoever so you can bring in
datasets from 3d renders from real-world
Scan locations performance captures and
then you can integrate into your own VR
application so we have a plug-in that's
right now available for unreal and then
will be soon releasing one for unity
we've been big fans of GTC for a long
time there was a showcase that we
thought we'd submit for will
competitions good and there is a lot of
really excellent examples of software so
I was actually pretty thrilled and blown
the way that they're actually takes
we're going to be showing more content
and atom view in action at SIGGRAPH 2017
you
Title: NVIDIA Studio | Create Stunning Motion Graphics and VFX in Adobe After Effects With NVIDIA RTX
Publish_date: 2019-11-08
Length: 129
Views: 32060
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/ngAb5SFbe78/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: ngAb5SFbe78

--- Transcript ---

After Effects is a motion graphics and
visual effects application created by
Adobe and I use it to create motion
graphics animations and I also use it as
kind of a Swiss Army knife
I've been training here at Adobe MAX in
their labs and the lab specifically has
the Nvidia r-tx graphics cards and we
had 100 computers with 100 students and
honestly the lab was seamless primarily
we use After Effects with element 3d and
so having a fast GPU makes a big
difference
recently did a project for thx which
pretty much had every different feature
used in it and we've developed some
technology called nebula 3d which works
in After Effects and it's a real-time
volumetric renderer that runs on the GPU
and so to be able to use real-time cloud
effects in After Effects is something
that was never really possible for us
most of our tools are GPU optimized
which is great for me personally as an
artist if I had to wait on just the CPU
process then I would probably go a
little bit nuts I'm a high-energy person
which means I like things done fast and
the thing about GPU and leveraging the
GPU is it's a lot faster than just
waiting on the CPUs and NVIDIA r-tx GPU
is quintessential to my workflow because
I push a lot of pixels and I do quite a
lot of processing when it comes to
actually creating complex composites for
me it's been a game changer as far as
our removes go because vocopro moves are
now GPU accelerated and when you're
waiting for an automated process you
don't always want to wait up a computer
like it's a good excuse to go get coffee
but you just want it done immediately
especially if somebody's waiting on you
I have a laptop with our TX 2070 in it
and what's nice about it is when I take
it on the road and do some of the
presentations I show off some of the
work that we've done on desktop machines
and you know really big complicated
scenes and I'm able to open them up and
actually load them and demonstrate them
and the performance is great anything
that requires heavy processing I really
like to push to the GPU as much as
possible and rgx is really what makes it
happen
you
Title: NVIDIA Research Achieves AI Training Breakthrough Using Limited Datasets
Publish_date: 2020-12-07
Length: 100
Views: 35671
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/nh9oiz3F9ZA/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: nh9oiz3F9ZA

--- Transcript ---

researchers and artists around the world
have been using style game 2 to do all
kinds of interesting work
at nvidia we've been pushing the
frontier of using gans to synthesize
extremely realistic images
the catch is that you need to train them
on extremely large quantities of data
you might need fifty thousand a hundred
thousand images to produce a modern
high quality high resolution gan and
often that
quantity of images is simply not
available
so to tackle this challenge we've
invented a new approach for style game
two that we call adaptive discriminator
augmentation
or ada ada is a training protocol the
idea is to augment training images
with random distortions so that the
network never sees the same exact image
twice
instead it sees one that's been flipped
or rotated or moved or has the colors
adjusted
so by applying these augmentations
adaptively the gan learns not to
synthesize
those distortions into the image but
instead to synthesize the original
images
this lets us reduce the number of
training images by a factor of 10 a
factor of 20 or more
while still getting great results this
means gans can tackle tougher problems
but the vast quantities of data that we
needed before are either too expensive
or simply not possible to obtain
so for example artists can recreate the
style of rare works
where you don't need tens of thousands
of images by a particular artist or a
particular medium to train on
medical experts could produce all kinds
of diverse data to build computer models
to train
others to accelerate the diagnosis of
rare pathologies
in general i think the developers and
researchers around the world are going
to find that this allows them to pursue
breakthroughs by using this gann
technology
in all kinds of settings that they
couldn't apply it before
Title: NVIDIA Spectrum-X Platform | World’s first Ethernet fabric built for AI
Publish_date: 2023-05-28
Length: 136
Views: 31143
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/nKqfi3q4S5I/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: nKqfi3q4S5I

--- Transcript ---

In order to build new AI clouds, a separate AI 
fabric is necessary. AI workloads are unique  
and generate large network flows that 
lead to increased impact on the network.
Running these workloads on existing cloud 
infrastructure built on traditional Ethernet  
introduces significant congestion, increased 
latency, and bandwidth unfairness. This causes  
performance loss, and the inability to 
effectively utilize the system’s GPUs.
The NVIDIA Spectrum-X networking 
platform is the world’s first  
Ethernet fabric built specifically 
for AI workloads – delivering AI  
cluster performance up to double 
that of traditional Ethernet.
This end-to-end NVIDIA solution 
starts with the Spectrum-4 switch  
featuring the impressive Spectrum-4 
ASIC. Powered by NVIDIA innovation,  
this ASIC provides numerous groundbreaking 
features that puts it in a class by itself.
The Spectrum-4 switch works in tight 
coordination with the BlueField-3 DPU  
to ensure optimal resource utilization and 
efficient data transfer within AI clusters.  
It comes with enhanced RoCE features such 
as adaptive routing, performance isolation,  
and congestion control – all combined 
in an optimized full-stack solution
The Spectrum-X Networking Platform 
delivers acceleration technologies  
over standard Ethernet protocol, 
achieving the highest effective  
bandwidth and delivering low jitter and 
short tail to maximize AI performance.
In this real-world example, Spectrum-X 
delivers 1.7X higher performance in terms  
of performance per GPU, power efficiency, 
training time, and total cost of ownership.
Additionally, AI models utilizing NCCL AllReduce 
achieve 2.5x higher effective bandwidth,  
enabling faster and more predictable training.
Continuous optimizations across 
the software stack, libraries,  
and operating systems ensure optimal performance 
and interoperability of the AI infrastructure.
The NVIDIA Spectrum-X Networking Platform 
powered by Spectrum-4 and BlueField-3  
accelerates performance and time-to-market as 
the first purpose-built Ethernet fabric for AI.
Title: NVIDIA DGX SuperPOD
Publish_date: 2019-11-15
Length: 198
Views: 38496
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/NoCdoBl9vPw/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: NoCdoBl9vPw

--- Transcript ---

[Music]
people often say that AI is the next
Industrial Revolution but we're just at
the beginning researchers all over the
world are inventing the future through
AI right now
AI gives us new tools to help people get
that work done
this means that AI is transforming every
industry from telecommunications to
agriculture to healthcare and beyond
we made the dgx of Avadh so that we can
power on the research and development we
needed a big AI infrastructure to solve
the biggest AI challenges of humanity
usually when supercomputers are built
people view these things taking months
to come online and we wanted something
that could be built and employed in
weeks the way that you approach doing
big iron AI is massive models is you
have to build one of the best HPC
supercomputers in the world for our
cluster that's the number of 22 super
here in the world that actually shocked
a lot of people the fact that we could
take available equipment and not even
that many nodes pile them together and
end up with a number 22 entries still
surprises people
we made dgx super pod because people
need to Train huge models on huge data
sets to get the best possible AI some of
the biggest models being used right now
are in language modeling for example the
bird model which is being used for a lot
of tasks like question answering those
models are very large and require a lot
of resources train setting up all that
infrastructure can be difficult deject
super pod makes it a lot simpler dgx
super pod is comprised of 64 DG X 2's
represents 1,024 Nvidia v110 Sequoia
GPUs which is good for 128 petaflop sub
AI performance it's all interconnected
with a terabit Mellanox InfiniBand
network it runs standard n GC containers
using slurm to issue those containers
and it's the fastest AI system that you
can buy to get really good performance
and AI applications requires not just
great hardware like a dgx super pod but
also amazing software when Vidya has
several thousand employees that are
working to develop libraries to support
all of the major frameworks and a very
fine-tuned the inner workings of our AI
software so we get amazing performance
on all of these ai benchmarks and what's
more is our performance
proves overtime over a seven-month
period on exactly the same hardware we
improve the performance of the
heavyweight object detection benchmark
mask our CNN by 80% so when you get a
dgx super pod you're not only getting
great hardware you're getting amazing
software and eating performance that
will continue to improve as our software
gets better we systemize the
supercomputer for AI and Baisakhi it's
not a science project anymore
anybody can get one and be running in
weeks it's a top 25 class supercomputer
so any researcher has performance at
their fingertips so you have an AI
infrastructure for the AI powered world
we've seen that the biggest models on
the biggest datasets lead to the best
results over the past 10 years of AI
that's always been true and because of
this companies that are inventing the
future of AI view computing as a
strategic resource it's necessary in
order to invent the best possible AI
[Music]
Title: GTC Europe 2016 - Keynote
Publish_date: 2016-09-29
Length: 7326
Views: 31989
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/npzRyTimcZo/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: npzRyTimcZo

--- Transcript ---

from a single origin
a unique point in space and time
this is the spark of innovation fueling
your most amazing breakthroughs with the
power of AI
a passion for discovery that unveiled
the Genesis of all that exists in the
universe
today deep learning is helping Farmers
Feed the World
and marine biologists save our most
precious resources
by analyzing in one month
what used to take 10 years
everyday devices translate even the most
complex languages from voice
images into words Xavier is now present
helping the visually impaired recognize
an old friend or letting a blind woman
read to her child for the first
time
[Music]
autonomous vehicles give us the freedom
to reimagine our city streets
and deliver relief to those who need it
most even under the harshest conditions
[Music]
robots tap into the power of deep
learning to separate trash from Treasure
give us amazing new ways to explore
other planets
[Music]
today at 2500 year old game meets its
match as a computer competes with one of
the greatest human champions of all time
and wins
GPU deep learning is the Breakthrough
that sparked this AI Revolution
and fuels your most amazing discoveries
yet to come
[Music]
[Music]
ladies and Gentlemen please welcome
Nvidia co-founder and CEO Jensen Huang
[Applause]
thank you welcome to GTC
GTC is about GPU computing a form of
computing we invented 10 years ago
GPU Computing has come a long ways in
just the last 10 years it has enabled
amazing new applications
solve problems that were impossible
before
and now it's in the process of
completely revolutionizing Industries
GPU Computing is a specialized form of
computing
it solves problems and it can do things
that normal forms of computing simply
cannot we've got some pretty exciting
things to show you today so let's get
started
well GPU Computing this thing that we've
been working on for 10 years is at the
beginning of something very very
important
a brand new
a brand new Revolution
what people call the AI Revolution
the beginning of the fourth Industrial
Revolution however you describe it
we think something really really big is
around the corner
about 20 years ago 25 years ago 1995 the
PC internet Revolution started
several things came together that made
the PC internet era really really
exciting the availability of a
microprocessor the CPU a standardized
operating system and a standard document
exchange system that made it possible
for us to share information all over the
world the PC Revolution the PC internet
era
has put computers in the hands of a
billion people
Ten Years Later
in 2006
two simultaneous things happened
the mobile Revolution a mobile Cloud
Revolution was started by the iPhone and
the Amazon AWS
for some reason it happened about 10
years after the PC internet era
all of the combined and cumulative
amount of innovation that was built into
the industry made it possible for us to
put Computing technology
in the hands of nearly 3 billion people
and made Computing capability available
to you wherever you are
from every computer in every home
to a computer in every hand
well it's 10 years after that
it is now 10 years after that
and we're in the beginning of something
very very big
we call it the AI Revolution
in this new in this new era of computing
something pretty amazing happens
software
writes software
machines learn
and soon machines will
build machines
in this new era of computing the type of
software that's written by the computer
is impossible for humans to write
and is therefore able to solve problems
that we've never imagined before
in each era of computing a new Computing
platform
was developed
a new Computing platform made it
possible for these new capabilities to
emerge
the CPU
and the standardized operating system
called windows
the arm low power SOC
and Android operating system
and the cloud platform made the second
era possible and now the Third
a brand new type of processor is
necessary to make this type of software
development possible
and it put us
put Nvidia put GPU Computing Square in
the center
of this revolution
it happened
in 2012.
the Big Bang
the Big Bang of GPU Computing or deep
learn GPU deep learning was 2012. even
though even though great work had been
done in deep learning before that even
though in fact
Jurgen schmidhuber's lab in Swiss ai's
laboratory had already started to work
with gpus with deep learning it wasn't
until 2012 that happened and as part
serendipity
part destiny
the serendipitous part of course is that
the researchers of the Swiss AI in this
case the researchers of NYU
Alex karshevsky under the laboratory of
Jeff Hinton
was trying to develop a new type of deep
Learning Network that was incredibly
deep neuronet as you know is informed is
inspired by the human brain and it has
the ability to learn features
from very complicated data
by itself and it does it in the case of
deep learning hierarchically meaning
that if you were trying to recognize a
human it might detect edges first from
the image it might detect
after that small features it could be
eyebrows eyelashes
pupils nose
ear and from that
it learns this is a human head and from
that it recognizes this is a human and
it has the ability to generalized
incredibly well
generalized meaning that although it
learns from just a few examples
those few examples could be thousands of
examples it's able to generalize that
all of you are humans
and so the ability to learn features
hierarchically and to generalize
representation learning
was a very powerful idea it had one
enormous handicap
it had one enormous handicap
this idea has been around for several
decades in fact but the one enormous
handicap that it had is it required a
large number of examples to learn from
it required an enormous amount of data
to learn to write the software
that handicap
of being computationally exhausting
that handicap of requiring massive
computers for the software to be written
so that it could be useful lasted two
decades
a handicap that lasted two decades and
then one day
because we had caused our GPU to become
general purpose Alex krachevsky
was able to discover our GPU
and developed a deep neural net on that
GPU
Serendipity met Destiny and in 2012 he
wrote a paper
a milestone paper and this Milestone
paper
chronicled and described his deep neural
network
and he submitted it for a competition
and this young man who has no experience
in or little experience in computer
vision created a neural net that was
able to recognize a large quantity a
large-scale amount of images
learning from one and a half million
images
and putting into a contest of
recognizing over a hundred thousand
images
it won
this deep neural net that was written by
software learned by itself on gpus won
the contest
and it he beat
every computer vision expert
and every hand engineered computer
vision algorithm
over decades
one young man's paper
one neural network called now the famous
alexnet
beat everybody
unbelievable results
the results of his achievement rang
through the industry
we had the benefit I have the benefit of
working with companies all over the
world and Industry leaders and
scientists all over the world the
results of that paper the results of
that singular achievement is probably
the most
exciting moment
in computer science
that we've experienced in the last 25 30
years
and the reason for that is the
achievement in itself is significant but
the extrapolation of the achievement is
daunting
the extrapolation of the achievement
what it means now
what it means to computer science what
it means to computer programming what it
means to the computer industry what it
means to all the problems that we're
trying to solve how is it possible that
a software piece of software learns by
itself and creates such amazing results
and beat every human engineered
algorithm that has ever been developed
well the stage for the AI revolution has
been set
since then
in just the last four years there's not
one week that goes by
where some deep learning paper has been
produced
some lab has new groundbreaking results
in deep learning some company has been
founded some new breakthrough
has been achieved
there are three very important
milestones in the last four years that I
want to highlight
the first
is a collaboration between us and
Stanford research our Nvidia research
and the AI lab of Stanford Andrew Eng a
world famous pioneering AI researcher
worked with our laboratory to create
essentially
a large-scale large-scale
GPU deep Learning System that has the
ability to simulate enormously large
brains
we can now create we can now allow
computers to write software for very
large problems and the reason for that
the reason why that's important is
because we want AI not to be a toy
we wanted to solve real problems and
real problems are large and we need
large computers that are incredibly
scalable so that we can train enormous
models
enormous models
that singular breakthrough that paper
that paper has put gpus in the hands of
literally every serious researcher and
every serious
software company to solve very serious
problems
big breakthrough in the year 2012 and a
turbocharged literally everything since
then
some other achievements
this is imagenet this is the competition
that happens every single year and it
just it was just completed yet again in
2012 2016 and the new winner is deep
learning again based on gpus since 2012.
notice first of all the discontinuity in
the black dot the last black dot and the
first Green Dot the last black dot is
human engineered expert engineered
computer vision algorithm Developers
expert Engineer Expert engineered it was
able to achieve 74 and it stayed around
low 70s for quite a long time
in fact
if it wasn't because and then of course
deep learning came along Alex came along
and we took a big jump since then since
then the models get larger
the architectures of these networks
become more complicated the
computational intensity of the networks
continues to grow
and one day last year
we achieved super human levels
I am pretty certain not one of us in the
audience today has the ability to beat
this deep neural net in large scale
image recognition
in large scale image recognition and
it's very very likely that even all of
us getting together
working together as one team we cannot
beat this network in large-scale image
recognition image recognition using deep
neural Nets has achieved super human
levels
it was because of deep learning that has
inspired us on Nvidia to apply this
technology to many of the things that I
want to talk to you guys about
and in fact one of the areas that's of
great importance is of course autonomous
vehicles self-driving cars
it is inconceivable to us anyways that
we could achieve
the level of safety and the level
of capability of self-driving cars using
traditional computer vision approaches
to object detection and finally now we
have if you will Thor's hammer this
incredible Magical Hammer that fell from
the sky to help us solve this great
challenge image recognition at
superhuman levels
just a couple of weeks ago
our friends of Microsoft XD Huang XD
Huang is uh Microsoft's speech Chief
scientist
speech recognition as you guys know is
one of the most
most researched area in artificial
intelligence and the reason for that is
because if we can understand speech we
can read and we can if we can read and
understand language we can learn the
ability
to understand speech
won't will not will not only change how
people interact with computers it will
also change what computers can do deep
learning has recently made enormous
achievements this is from a paper from
XD
that he published several years ago and
the work that's done of speech
recognition are some of the finest
artificial intelligence researchers that
we know in the world Jeff Hinton has
made enormous contributions in this area
uh Deng Lee over at Microsoft XD Huang
of course and and here in Europe uh one
of the one of the one of the most
significant AI researchers and has made
enormous contributions to deep learning
use for speech recognition Jurgen
Schmidt Hoover's laboratory in
Switzerland has done amazing pioneering
work they were they were really quite
frankly the first to use deep learning
with long short-term memory to force for
understanding to improving speech
recognition just a few weeks ago
Microsoft
announced that after all of these
decades they have achieved quite a
significant breakthrough of 6.3 percent
6.3 percent word error rate while speech
recognition is really hard
speech recognition is really hard for
for some very obvious reasons for
example uh vocabulary the larger the
vocabulary the the lower the the lower
the accuracy the higher the word error
rate
speech versus
spoken versus red speech you know when
we're when we're talking we have lots of
Oz and Oz and ums speech is hard because
everybody's talks in different ways and
there's a whole bunch of as it turns out
ease you know the bcds
they sound very similar to computers the
EVPs
you know the
pvzs
a whole bunch of E's so the English the
English language as it turns out is
relatively hard for computers to
understand it's fairly hard for us to
understand and and so so speech
recognition is something of great
difficulty not to mention surrounding
environments where you're inside a car
inside a bar inside a train station
inside uh inside uh inside of inside a
pub here in Amsterdam where everybody's
talking
um and so all kinds of different
environments creates enormous
complexities for speech recognition we
now have achieved 6.3 percent by the way
humans don't achieve zero percent and
that suggests that these computers with
deep learning has achieved quite
significant
levels of capabilities
well these three achievements I
highlighted
for a particular reason
we now have the ability to simulate very
large brains
we now have the ability to recognize
images that is computer
site
and we now have the ability to recognize
speech that is
computerized under a computer's ability
to understand what we say
Sight and Sound
and the ability to learn
the ability to perceive and the ability
to learn is the foundations of
artificial intelligence
that is the reason why
the world has become so excited about AI
we now have the three pillars
the three pillars necessary to solve
very large scale artificial intelligence
lots of research is happening in this
area
and it has really it's really shaped
shaped the industry the industry that we
know today and it surely has shaped us
Nvidia as you know is a GPU company we
invented the GPU and 10 years ago we
invented GPU computing
we are basically in three fields of
endeavor
that is unified by one concept
our three fields of endeavor first of
all is high performance computing
we created GPU Computing to solve it's a
specialized form of computing and it
solves problems that normal forms of
community can't
if you wanted to simulate weather if you
want to create virtual reality if you
want to simulate the brain this is
a style of computing a way of computing
that has made that possible
almost every supercomputer in the world
brand new super computers in the world
are accelerated we started that Trend we
represent about 70 percent of the
world's high performance computers that
are accelerated I think that in the
future every single high performance
computer will be accelerated the second
area of endeavor for US computer
graphics
the simulation of virtual reality
the simulation of virtual reality
this is obviously a very exciting area
for us we love computer Graphics it has
field enormous amounts of innovation and
and an r d effort for our company
and in this area of visual Computing
this is a little bit like
computing
human Intel Computing human imagination
we take what's in your mind
and we translate it to
computer Graphics so that all can enjoy
and this new field
that we found ourselves in
and that we have been propelling in the
last six years
artificial intelligence or otherwise
Computing human intelligence some people
have said that we've become the AI
Computing company
whether we're Computing human
imagination or Computing human
intelligence we've become the AI
Computing company but I kind of still
think that we are the
fund Computing company
we get to solve all of the world's fun
problems
and we're if you will
the Computing of the future company
in fact most of the work that we are
doing is leading to a pretty exciting
future
in a moving Iron Man Tony Stark is
interacting with holographic computer
Graphics floating in front of him
he has Jarvis in the ambient
Jarvis is helping him fetch information
in fact Jarvis as you know is rendering
that computer graphics
Jarvis is talking to him answering
questions
collaborating with Tony Stark
and one of my favorite Parts was when
Tony Stark puts his hands into
the Iron Man suit that he was designing
and interacting with for the first time
using merging simulation
virtual reality
augmented reality
and of course powered by artificial
intelligence
if you will that one scene
captures what Nvidia is working on
this is the future that we're trying to
create and we're super excited about it
and if we come to GTC every year we're
going to take giant leaps towards this
future
this is a pretty exciting time
I think all of the pieces are starting
to come together and GPU Computing is at
its core
GPU Computing is what GTC is about
and if there's any
any doubt whatsoever
that GPU Computing is being more
important than ever and becoming more
Central to one industry after another
this chart
would surely change your minds the
number of attendees in gtcs were
requested to go to just about every
country in the world these days
we are on a world GTC tour the first
time in the history of our company after
10 years
developers all over the world
has asked us to go to literally every
single country because GPU Computing is
now used in every single country every
GPU Computing is touching every single
industry there's not one software
company that I know of today that's not
using GPU Computing either a little or
just a ton
GPU Computing is at the core of
computing as we know it today
the number of developers has grown
tremendously
and it's grown three times not in 10
years it has grown three times in two
years
three times in two years I believe
that's exponential growth
it's incredible a hundred and twenty
thousand to four hundred thousand but
this one is just shocking
the number of deep learning developers
has grown 25 times in two years
it's probably doubled since I
started traveling
it's absolutely incredible the number of
deep learning developers and they're
touching just about every single
industry
and so the question is
the question is why
why has AI researchers all over the
world
discovered
the GPU why has it now I'm not going to
offer you a scientific reason for it
this is a little bit of a cartoon reason
but I think it might inspire you it
might give us maybe a little bit of the
understanding for why it is that AI
researchers all over the world have
adopted the GPU
suppose I were to say
I'm going to ask the audience to think
I would like to now ask you to think
I would like to ask you to think
about an iconic image in Europe
I would like you to think about the
Eiffel Tower
I think it's a fairly good choice for
everybody
let's think about the Eiffel Tower
well it turns out when I ask you to
think when I ask you to think about the
Eiffel Tower when I ask you to think
about the Eiffel Tower it's very likely
that most of you
did a mental image of the Eiffel Tower
your brain performed computer graphics
your brain performed computer graphics
if I said think about Ferrari
it is very likely your brain performed
computer graphics
some probably chose the 458 maybe some
chose the 430 maybe some chose
the LaFerrari
who knows but it's probably red
your brain
performs computer graphics and your
brain performs computer Graphics in
color
when you think
it's also very likely that our gpus
because our GPU is designed like a brain
your brain as you know is not one super
processor your brain is a whole bunch of
neurons billions of neurons connected by
tens of thousands of synapses each
and each one of these neurons don't
perform much work
but together
it's able to
think together it's able to
achieve something that only we can
achieve
something with trillions of dollars of r
d the computer industry has not yet
accomplished
the GPU is maybe a little bit like a
brain we have a whole bunch of
processors thousands of processors that
are working in parallel to solve a
problem thousands and thousands of
processors in the case of a
supercomputer the largest supercomputer
in the United States is powered by
Nvidia Tesla and it has 16 or so almost
18
000 processors
excuse me gpus
those gpus have thousands of processors
inside all together about 36 million
processors are working together to solve
a problem
our GPU Computing approach is a little
bit like a brain and so maybe those two
reasons
inspires us
give us some evidence of why it is that
researchers all over the world has
jumped on to GPU Computing as the
fundamental processing approach for AI
advancement
while GPU Computing is a GPU deep
learning is a new Computing model
now before I go off and tell you about
the products that we're going to
announce today and the new initiatives
and our new partners let me first
describe why this new approach is
different
whereas Computing in the past is
Engineers sitting in front of computers
using visual C plus
developing essentially
recipes
incredibly complicated recipes that are
followed step by step by step
and they're written by engineers
and what the engineers wrote is what it
does what the engineers wrote is what it
does
what was written is what it does and
when you're done you compile it you test
it to make sure that it performs
according to your expectations and then
you release it to the world for
functionality for application
software Engineers write the software
QA Engineers test the software
and we released the software into
production and the software does exactly
what we expect what we wrote it to do
if there's a bug we eventually find it
we fix it
we fix it we test it we release it we
find a bug we fix it
back into that Loop GPU deep learning is
a little bit different
there are several different elements of
GPU deep learning the first part is
training
it's about the Steep neural net learning
from an enormous amount of data it's
learning from digital experience which
is what data is
because the world has an abundance of
data today we have an abundance amount
of experience to train these neural net
with this is a computationally intensive
part of deep learning incredible and
I'll illustrate some in a second the
output of that is a deep neural net and
then
you infer you now apply that Network to
infer and then you have intelligent
devices now let me go around this Loop
one more time so in the case of training
in the case of training we have billions
of trillions of operations billions of
trillions of operations billions of
trillions of operations that's a fairly
large number and that's one of the
reasons why it takes so long to train a
network
but what you have done is you train
large models and your goal is to
accelerate your time to Market
you've created a network this network is
a neural net with hundreds of what is
called hidden layers
meaning layers on layers on layers on
layers and as a result we can generalize
we can learn and generalize
representations
that are based on hierarchies of
features that there could be edges
eyebrows
eyes
head
body
human
that all of this data underneath
could eventually be abstracted
and be represented with a vector a
vector a piece of information that says
human
lots and lots of raw data
human
the ultimate form of compression
lots and lots of images output feature
representation human
result
our brain has the ability to do that
to take raw information
and somehow extract it from it the
essential pieces of data
the essential nuances so that we can
abstract that data into a higher level
representation called human
we put that Network and data centers all
over the world
these are data centers that are
populating all over the world hyperscale
data centers so every time you make a
query you say
find human image of a human it is very
very likely well it's certainty now that
it goes through an artificial
intelligence Network
and it searches its entire library of
images and it detects finds the images
that best represent your queries as
human when you do a voice query
a voice command OK Google it goes
through the same similar type of process
goes into the network and the network in
first from you what you need to do this
is going to be a huge market and the
reason for that is this almost every
single query in the future is going to
be AI based every single time you touch
your phone every single time you use the
internet it will be
routed through an AI Network
billions and billions of queries they
could be video they could be voice they
could be music they could be text they
could be requests for commands they
could be things like help me book a trip
help me book a trip
to Monaco
GPU inference makes this response time
incredibly fast and as a result of that
it improves the throughput of your data
center IE reduces cost
we're going to put these networks on
devices as well
this is the era of the intelligent
device your vacuum cleaner is already
relatively intelligent it has the
ability to be much more intelligent
your toaster your coffee maker your
house
the cameras that watch the outside of
your house
a little microphone that's connected to
a speaker otherwise known as Amazon Echo
these devices are going to be infused
with artificial intelligence so that
they can be much much more intelligent
deep learning and AI is the fuel it's
the technology for iot
this is going to be pretty exciting
well this is the so what we just went
around is basically how GPU deep
learning works it's this new model of
computing and notice very little coding
a lot of computation
very little coding an enormous amount of
computation
very little coding and enormous amount
of computation
and the amount of computation is going
to grow
I show you I'm showing you three pieces
of work from three very important AI
research organizations in the world
the first one is Google this is Jeff
Dean's comes it comes out right out of
Jeff Dean's uh PowerPoint slides and he
basically says the important property of
neural net the important property of
neural net is that the results get
better when there's more data
when they're bigger models I.E bigger
brains and as a result you need more
computation
a bigger brain with more experience lots
and lots of opportunity to learn it with
computation makes it for better results
higher quality Network
second I'm showing you here Microsoft's
progress
in their image recognition network this
is alexnet
and it's just tiny by today's standards
it's just tiny by today's standards only
four years old it's eight layers
it performs 1.4 billion operations
okay and it achieved an error rate of 16
percent
and literally three years later
Microsoft announced the resnet the Deep
learning deep neural net super deep
Network 152 layers and recently recently
since time
announced that they broke this record
with a network that is four times deeper
several hundred layers
deep neural network these networks are
getting larger and larger and larger and
as they get larger
they can recognize more and more and
more subtle details as a result of that
their accuracy goes up
baidu's deep speech one and deep speech
two went from 80 gigaflops of total
processing to train that Network to
something that was
10 times larger
unbelievable
advances and computational demands in
deep learning as you can see these
numbers are far far
far
higher than Moore's Law
and that
is the handicap of deep learning at some
level
this new Computing approach
this new Computing approach
if it were to advance
if it were to advance needs the
conviction
needs the conviction of an industry or
at least the conviction of a company
to push Computing technology at a pace
that is so much greater than Moore's Law
well we thought
why not us
we think deep learning is amazing
it is
this incredible Hammer that fell from
the sky
it has the ability
to turbocharge AI
it could be the foundation of the next
generation of computing and it can solve
problems that we only dreamed of solving
our whole lives
and for me personally
I want to do it before I get to retire
and so I've been doing I've been doing
my job for 20 almost 25 years
and I want to dedicate my next 40 years
I want to dedicate my next 40 years to
this endeavor and so I better get going
well we invented this we decided after
we started to understand about deep
learning that in fact the rate of change
has to grow not diminish
we recently we recently
um rolled out Pascal and the thing that
it was incredible was this
the first customer of Pascal the first
customer of this incredible new
processor that is 65 times the
performance of what we were able to
achieve four years ago 65 times I mean
it's kind of I love every GPU we've ever
built
you know a parent has to love every
child
but what is that about
that is the picture of underachievement
and yet yet Kepler Kepler this GPU was
the GPU that Alex found
this was the GPU
that accelerated deep learning by a
factor of 40 over the CPU
and look at it
a picture of underachievement
a picture of underachievement so Kepler
to Maxwell to Pascal you could see we
are incredibly serious about the
advancement of this field
the first customer
the first customer of
dgx1 is an open laboratory called open
Ai and their mission their mission is to
democratize to advance this field and to
and it has gathered some of the world's
finest researchers in AI to democratize
this technology
it's an open industry laboratory and
they were the first
email that I received as soon as I
announced this product by the time I
walked off stage they've asked with a
great deal of urgency they need a
machine like this to advance their
science
so nvidia's dgx1 is the system that
embodies the Pascal processor that is 65
times faster than what we were able to
achieve just four years ago
well the thing that's really great is
that that our platform is so accessible
you could get it in a gaming PC you can
get it in a laptop
you can get it in a server you can get
it in a super computer you can get it in
clouds you can get in dgx once
you could build it yourself you could
buy it you could rent it
you could get nvidia's GPU Computing
platform literally every country
everywhere as a result every single
framework
that has been developed for AI has been
optimized for
the Nvidia GPU platform
if you are an AI researcher this is your
platform
and we're committed to continue to
advance it at a rate that is
incomparable
to the rate of computing Advance frankly
in the last 30 years
all of the curves that we have seen
and the progress of Moore's law has to
be broken
we can't slow down we've got to
hypercharge it
this is our first
example of hyper charging Moore's Law
well
to bring this capability to the world we
need a whole lot of Partners as well and
I'm super super pleased and super proud
to announce that IBM is a great partner
of ours in this new area of AI computing
as you guys have heard IBM talk about
cognitive computing
cognitive Computing is the future of
their company
cognitive Computing has the ability to
solve some very very large problems and
underneath that cognitive computing
Services stack called Watson needs to be
a supercomputer
and that supercomputer needs to have
super capabilities super capabilities
for artificial intelligence we've been
working with the IBM team it was
announced I guess a couple of years ago
we work together to create a technology
called mvlink
the power eight which is the fastest
microprocessor in the world today is
connected to our gpus directly through
the fastest single interconnect that
Humanity has ever created
between Power 8 and the Nvidia Tesla GPU
is this interconnect called mvlink when
you connect all of them together
you have this network of fast processors
fast C fast CPUs and fast gpus and it
can be dedicated to solve AI problems
partnership with IBM
well today we're really excited to
announce a new partner
I'm going to show you some some uh
amazing applications of of AI and some
amazing applications of GPU deep
learning and the breadth
and the reach of our platform
in just about every industry but there's
there's one one area
of research that uh is is of of great
importance to us and I I think this is
an area that we can really move the
needle for society
and it's in applying AI for the work of
companies all over the world
to apply AI for the work of companies
all over the world and today we're
announcing that sap and ourselves
are working together
to make the world's largest
one of the world's largest enterprise
software companies to integrate it with
the Nvidia dgx1 and our GPU deep
learning platform so that we can bring
AI capability
to Enterprises all over the world
we're partnering with their Germany and
Israel team
amazing team working on this now and
when we're successful
hundreds of thousands of customers of
sap will have the benefit of AI
Computing so that they can turbocharge
their business
really exciting let's give a round of
applause to sap please
dgx1 dgx1 is an instrument of AI
just like the Large Hadron Collider
it's an instrument of particle physics
without that scientific instrument you
can't do reasonable part of the particle
physics advance
the r d budget of dgx1 was 2 billion
dollars
this is the most expensive
most ambitious modern Computing Endeavor
in recent history
10 000 engineering manures went into it
we're now
shipping dgx1
this incredible and this important
instrument of AI research
should be put into the hands of the
world's best AI scientists
we put in the hands of open AI
the laboratory at Stanford
Peter Beals laboratory at Berkeley
yahshua's yahshua bengios
laboratory in Toronto or Montreal excuse
me
Jan lacun's laboratory in NYU
all of the world's most important AI
Laboratories
must have access
to the most capable instrument of AI
research that the world has ever known
dgx1 and so today we're super proud
super excited to announce that
the German lab German research
Center for artificial intelligence and
the Swiss AI lab where Jurgens located
will be the two designated research
centers of the Nvidia AI lab here in
Europe
don't have access to our dgx1
supercomputer
they'll have access to our resources so
that we can advance important areas of
research and AI that otherwise wouldn't
move along as fast and of course
and of course we have all kinds of
abilities to collaborate to move AI into
society
in a good way
okay so let's let's recognize let's
recognize these two research centers
here in Europe they are truly Pioneers
in AI
and I'm so delighted to partner with
them thank you very much
[Applause]
so AI training
GPU deep learning training now let's
talk about data center inferencing this
is a massive Market you train you train
the software you train the software you
train the network so that this network
could be as as great as possible you
train it with enormous amount of data it
takes billions of trillions of
operations it takes months and months
and months this is what researchers now
do in the software development cycle of
their new services and new software
when the network is complete or it's
ready ready to be tried or ready to be
enjoyed you put it into a large-scale
hyperscale data center there are
millions there are millions tens of
Millions
of servers in the world today that
support cloud computing and all of the
internet services that we enjoy
tens of millions of nodes
of hyperscale data centers this is a
brand new market for us
and now that these networks now that
these networks are trained
they're ready to be deployed into
production
and if we were to design if we were to
design
the exactly right accelerator
the exactly right GPU
we can make it possible possible for
these networks to be inferenced
meaning when you ask it a question what
is this image what is this song what did
I say
when you ask it of a query that it would
respond instantaneously and when
billions of us when billions of us were
to make queries simultaneously and some
of the queries are extraordinary
extraordinary queries when we're able to
make these queries and have it respond
instantaneously and for these data
centers to be able to support literally
a million times more workload
without having data center costs go up
by a million times
and energy consumption to go up by a
million times
we need a special and new accelerator we
call it the Tesla P4 and P40 they're two
brand new accelerators
um one of them
is for our large-scale processing but
the second one the P4 is designed thank
you very much
so this is designed for GPU servers
so this is designed for GPU servers and
this cute little thing
if you can ever call it GPU cute
this is the cutest GPU that has ever
been invented
the Mercedes S600
the Mini Cooper
this little thing P4 fits into a open CP
hyperscale
server that's 1u
it consumes anywhere depending on your
configuration whether you want to be 50
Watts or 75 Watts
the thing that's really amazing is this
you have 50 Watts here you have 250
Watts here
This is 40 times 40 times faster than
the fastest CPU
at AI Computing at GPU at Deep learning
so you plug one of these things in and
you replace 40 nodes
plug one in replace 40 nodes
40 nodes is basically
three or four racks of servers
replace it with one of these
incredible amounts more performance
incredible savings this you plug it into
this little tiny server and it's 40
times the Energy Efficiency of a CPU 40
times
what used to be a thousand watt CPU node
would be 40 times less
incredible okay so Tesla P4 and Tesla
P40
thank you
40 times the Energy Efficiency and 40
times performance performance well
that's just the GPU we've opted we've
optimized the GPU we've optimized this
new generation of gpus with new new
architectures for deep learning and new
instruction and new numerical formats
that are optimized for deep learning as
a result we get a huge boost in
performance but on top of that we need
every new architecture essentially needs
this new optimizing compiler and we're
announcing for the very first time a
runtime called tensor RT
tensor RT what tensor RT is is a
performance optimizing inferencing
engine it's a software that goes along
with P40 and P4
and when you put the software on top
when you run the software and because it
supports all these different numerical
formats that our GPU support and it has
the ability to smartly fuse operations
that are vertically in the network or
horizontally across a layer of the
network reducing eliminating work
fusing operations together so that you
could do it in one cycle and it does a
whole bunch of Auto tuning so that the
network that you trained on the GPU
Computing platform
is now optimized for runtime on our GPU
Computing platform we already support
vgg Google net resnet alexnet and these
networks and all of the custom layers
that you guys want to do in between
we're going to support in the future of
course all the networks okay so tensor
RT really really really important
innovation I'm super excited about it
congratulate the engineers that worked
on this it's available today go to our
website to download it
well let's take a look at um let's take
a look at all of this
so what I'm going to show you is this
imagine you are a hyperscale Data Center
and you've got videos that are streaming
that are being loaded up and you know
that live video today is one of the most
frequently shared
forms of social content
and the thing about live video
is it's not recorded
and so if you don't enjoy it it's gone
and if there's live video that you would
like to share with your friends
it would be nice
if as you're uploading the live video
it already knows which one of your
friends or which one of your family
members or your relatives would want to
enjoy that live video
and so what we need to do is to make it
possible for the data center to
literally look at every single live
video stream that's going by
every single mobile user every everybody
who's loading the live video in the
future and we're going to be loading a
lot of live video in the future
every single one of those videos every
single moment of it we were going to
apply artificial intelligence to figure
out is there something of importance in
here what is being shown what is being
shown and who would be interested in
seeing it what is being shown and who'd
be interested in seeing it so when we uh
why don't we do this in 90 videos of
streaming into our server into one node
of our server Edward yeah let's go ahead
and run the video first let's show
people this is the video for example
one stream at a time there are 90
different streams they're all running at
720p and you're up you know we're we're
uh we're loading these for example into
into YouTube live it could be Facebook
live it could be a periscope
and these are all live videos
and what we need to do is we number one
is we need to figure out what is it that
we're looking at now for all of us for
all of us we probably can tell what
we're looking at that that's probably
somebody doing push-ups
okay
that's probably somebody playing
a guitar
that's probably two people making dinner
and this is somebody playing a
formidable
ping pong opponent
it just always seems to come back
okay so let's go ahead can you go ahead
and label it let's go figure out what
the computer receives so the computer
things that we're playing table tennis
that this is sumo wrestling that there's
somebody on a swing that's their rope
climbing that they're rowing and Tai Chi
so the computer was able to learn from
looking at all the videos that it was
taught with and now when we load new
videos into it it's able to recognize it
and and suppose suppose um I I have a
service and I said look you know what
I'm only interested in people playing
music
and so it's got to be smart enough to
recognize that these are people playing
music what if it's people playing sports
for example
okay so it would recognize people
playing sports and suppose I just wanted
water sports
okay and so so the the amazing thing is
this in the future as we stream video
artificial intelligence Network that's
able to recognize images and artificial
intelligence
networks that can recognize
um the meaning of the image what is
known as semantics what is the context
by understanding the meaning of the
image we can now have more information
by which we can filter search or to
recommend to people
this is something that's kind of cool
so the next thing I want to show you
thank you Edward that was great
[Applause]
How Could you teach so we now we've we
can we can teach a neural network how to
recognize things we could teach a neural
network how to recognize things but how
do we teach
something that we've always thought
is the domain of humans
which is creativity
artistic capability
something that defines what a human is
okay and so is it possible for us to
teach a neural network artistic
capability artistic Flair and so what we
did is this we took a whole bunch of
videos a whole bunch of a whole bunch of
different arts arts by different artists
for example Picasso and others and we
train a network to recognize the style
of Picasso
or I recognize the style of a
traditional
pencil artist in Asia
we teach them these styles
and then what we're going to do is we're
going to show it an image in this case
an image this is where
where am I
this is London right the flag of
London's yeah and so this is London uh
and we show it an image and it would
repaint it
not filter it but repaint it
we take this image and we say paint it
again with a different style with the
artistic style it could be Monet it
could be Picasso it could be Van Gogh
okay and so take an image and repaint it
and suppose we could do it so fast
suppose we could do it so fast with our
gpus we could do it on live video
and so let's let's uh take a look at
some examples so let's first show some
videos so these are
some beautiful places in in Europe
[Music]
okay so we have some live video footage
we recognize that
suppose let's take these live videos and
now let's redraw it with an artistic
sensibility for every single frame this
neural network this artificial
intelligence network will redraw it
let's go ahead and
it's not a filter I guess it could be a
filter you can think of it as a filter
but we're redrawing every single frame
every single frame is being redrawn
one frame at a time
isn't that beautiful
this is what Picasso would have done
to a movie
[Music]
incredible
[Music]
a neural net artist
repainting
all these different frames turning it
into video
we've now we've now seen all kinds of
interesting neural networks we know that
we could take we could take the the Arts
of certain Styles or a certain time
frame and we can learn it into a neural
network and that neural network can
actually generate new art completely
generate Newark and and images that that
looks like it was done by the artist but
has never been has never been painted
before
okay this is one example doing in real
time thank you very much Edward
what you were looking at was the Tesla
P40
with a network that was trained
with everything I was describing this
new artistic Flair
trained to be an artist video was coming
in
and it was regenerating the video into
this new art form okay and then
previously you were looking at a stream
large stream large number of streams 90
different streams of video
and we're recognizing what's in the
video
the semantics of the video what's
happening not just there's a person but
the semantic what they're actually doing
and we're labeling we're detecting those
videos based on those semantics
well the applications for deep learning
GPU deep learning is really Broad
and the the reach the reach in the last
several years of our GPU deep learning
platform is really quite daunting and
quite amazing and I'm just the the reach
and and the new services that are being
used on our gpus are growing every day
in fact if you look at look at Jeff
Dean's presentation at Google just
literally three years ago they have some
20 different applications internally at
Google that were using that was using
deep learning it has now grown to nearly
three thousand
that's an exponential growth in just a
couple two three years Facebook talks
about deep learning literally all the
time we have great Partnerships with
Baidu Yelp uses deep learning for
recommendation and recognizing which one
of the images are best uh to show their
customers Microsoft uses deep learning
for Cortana uses deep learning for their
speech recognition Netflix uses deep
learning for movie recommendations
um
the list goes on the list goes on
Pinterest uses deep learning so that you
can now take a an image that you like
and you want to know where to buy it and
so it recognizes what's inside that
image and it recommend recommends uh
things that you can buy that are similar
not the same necessarily because you
might not be able to buy that but
similar and point you to those to those
websites the number of AI powered GPU
deep learning powered consumer services
is literally everywhere in the world I
frankly don't know of a one yet
a one deep learning one consumer
services that doesn't rely on it
nvidia's GPU
our platform is also available on
Services it's available on Alibaba it's
available on Amazon it's available on
Microsoft cloud it's available on IBM
cloud
if you're an Enterprise customer
we have Enterprise Partners who
configure servers that are ready for
deep learning GPU deep learning and so
it could be Dell it could be HP it could
be IBM it could be Cisco it could be
Lenovo we are able to literally reach
every corner of the world with GPU
servers designed and configured for deep
learning and now with the partnership of
sap we will soon have applications that
are running on these servers to serve
the world's largest Enterprises if you
want to build
your deep learning
super computer and have a special need
or special configurations you would like
to build we also have odm Partners GPU
server makers GPU server builders in
Taiwan and all over the world that could
have them ready for you
tens and tons of configurations just
about every single version from one unit
to you to three year to 4u from one to
two to four to eight gpus whatever
configuration size you would like to
have every single version of our gpus
are all supported
and so as you can see
whether it's services that uses Nvidia
gpus to cloud services that rents the
platform to you to server companies that
can offer you servers designed and
optimized for GPU deep learning to odms
and server builders that can help you
build it
however you would like to have it
nvidia's GPU deep learning platforms
available to you
that is one of the most important works
that we've done in the last several
years so that we can put this platform
democratize it and make it available to
literally everybody
well as a result startups are cropping
up all over the world
we now know of 1500
of 1500 startups around the world that
are deep learning based startup
companies they're used deep learning to
solve some very important problems in
the case of deep Instinct they're using
it for cyber security
if we can recognize very very subtle
differences in how somebody
were to rummage through our files
that subtle differences would be an
intrusion and so they can use Ai and use
deep learning to identify the subtlest
of differences and subtlest of intrusion
patterns deep learning for genomics if
we can if we can read and understand the
human genome and we can do it fast
enough we have 20 000 in our body
but that's not the hard part the hard
part is first of all understand what
those 20 000 are and then as they mutate
to get in front of it before
it spreads so that we can get ahead of
it
deep learning for self-driving cars a
really tough problem and we're going to
come back to that deep learning for
advertising
this company nerve
has the ability to recognize a logo a
brand a trademark in live video and they
could literally in a second in a
fraction of a second recognize it for an
hour of movies in a large and a large
scale GPU server so as a result as a
result they can show the advertisers all
of the places where their brand has been
exposed and their brand has been
presented to customers great for
companies who are advertising companies
AI startups here in Europe some really
amazing Stories benevolent AI
there are 30 million
medical stories medical papers in the
world
every 30 seconds
a new medical breakthrough is happening
and a new piece of a new medical paper
is being published every 30 seconds
now I don't know what how much time
doctors have to spend in Reading papers
but it's just impossible to stay on top
of the torrent of new Breakthrough
Medical Research and the papers that are
being written there are over a hundred
million chemical compounds that our
bodies react to positively or otherwise
there are tens of millions of patients
of various
forms of disease
they would like to use deep learning
to sort through rummage through and to
understand the process all of that
unstructured data to discover insights
to advise doctors
on how best to discover and how best to
invent the next cure the next drug
well they were using
an Amazon cloud service to process all
of that data and they estimated that it
would have taken way longer than a year
to process the data that I just
described
and so here in Europe
here in Europe benevolent AI is the
first customer
of dgx1
with the dgx1
their researchers can help doctors now
process through that enormous volume of
data in about a week
from over a year
to a week
a year is basically impractical nobody's
going to use that approach
nobody's going to use that approach but
now we can with dgx1 this instrument of
AI this AI supercomputer we can now do
that processing in basically a week or
two
smile art has the ability to recognize
faces and they're used in Iva
surveillance systems so they can look
for people who are lost or look for
people who are wanted
deep learning for facial recognition
what's really special is this most of
the time when you're looking at looking
at faces it's not like it's not like
getting a driver's license or a passport
photo it's very unlikely that the person
you're trying to find is looking at you
like this
it's very likely they're either somehow
looking away they might be occluded they
might be in Shadow their hair might be
down they might be wearing a hat they
may have aged a little bit
gained a few pounds
okay got a tan
they might have changed just a little
bit and smile art has the ability to
recognize that in just a fraction of a
second intelligent voice insurance
companies
companies that have call centers it
recognizes and it follows everybody's it
basically recognizes the speech of
everybody on the phone and not only that
it detects their emotions and so that
you could figure out a way to understand
whether the person who's on the phone
with you may be deceiving you for
example for an insurance claim or a
financial trade or something like that
intelligent voice really interesting
this one's really cool
Sadako has been trained to recognize
this AI network has been trained to
recognize what is plastic versus what's
trash
it has as a result
automatically automatically picked up
60 ton 60 60
000 tons
of plastic
by itself
and now that plastic could be used to be
transformed into something else instead
of being put into landfill sadaka really
really exciting and of course that work
is something that is very complicated as
you can imagine because all the objects
that are coming through are all very
different
inferencing in data centers
Pascal P40 and P4 tensor RT data centers
all over the world brand new market for
us now that these networks are being
developed and they're ready for
production we can now put it on these
servers and put it into the hands of
customers all over the world service
providers startups all over the world I
want to talk about intelligent devices
what some people call iot
but if we Infuse it with artificial
intelligence these devices can be rather
rather interesting and be used to solve
all kinds of interesting problems
you know whereas the PC era introduced
computers to a billion people and then
mobile Cloud introduced computers to
three billion people
I believe the AI era
would put tens of billions
of intelligent devices connected to the
internet
and these devices and these machines
these autonomous machines could be of
all kinds of interesting size and shapes
only our imagination only our
imagination limits us whether it's a
camera that only records when something
interesting is happening
or a little tiny camera and display and
microphone and speaker is a little tiny
agent called gebo
and it's just talking to you and it
knows who you are might tell you a story
might turn into a video conferencing
system if I call Mom
tell you what the weather is like today
it might be something like Echo
essentially artificial intelligence
Network connected also to an artificial
intelligent Cloud it might be a drone
that's completely autopiloted
and it flies around looking for looking
for people to save or delivering drugs
to medicine to somebody who's in harm
it could be something as simple as a
grocery delivery robot that delivers
groceries to you from around the
neighborhood or delivers pizza
these Internet connected artificial
intelligent machines are going to start
cropping up and what they need is a AI
supercomputer they need a computer
that's battery powered they need a
computer that has the capability of AI
and has the ability to be
instantaneously responding to the
circumstances around it and so we
created this embedded AI supercomputer
called the Jetson tx1
running tensor RT
the network that we were talking direct
the inferencing
optimizing software that I talked about
earlier this little tiny computer has
the ability to recognize images and
sound and learn
and do amazing and wonderful things
little tiny AI supercomputer
well having the system is one thing but
one of the greatest challenges right now
is what is AI Computing mean to software
developers and how do software
developers take advantage of this
amazing capability called Deep learning
and so we've created the platform for it
if you come to nvidia.com our SDK
is rich rich with all kinds of software
and algorithms for your AI
supercomputers but something that we're
doing that I'm super excited about is
we're starting an Institute
to teach apply deep learning
how do you take the problems that
you would like to solve
and what kind of tools do you have
available to you
and how do you use those tools to create
essentially an embedded system or a
service that you can deploy this network
into in a really optimized way we call
that the Nvidia deep learning Institute
and people are so excited about it I
think there's um today I think there's
300 people who are going to be attending
it this deep learning Institute that
we've we've rolled out has been offered
uh all over the United States it's been
offered in Japan it's been offered in
China it's been offered in Taiwan and
it's sold out every single time and so
as a result we decided to partner with
three very large
an incredibly successful digital
education platforms Coursera Microsoft
and Udacity
so we're super excited to partner with
them and we'd like to spread this new
way of doing Computing all over the
world we want to democratize deep
learning we want to democratize AI
computing so Nvidia embedded AI
Computing platform
I want to talk about something that's
really really important
and this this uh this industry is not
only large the the problems the
challenge the technical challenge of
creating autonomous vehicles and to
bring AI to this industry is not only
extraordinary and Technical challenge
but it's also extraordinary in society
benefits fewer accidents
more utilities for the vehicles that we
have
lower cost access to Mobility putting
Mobility making Mobility accessible to
people that otherwise wouldn't have it
and maybe even a complete redesign of
how our cities are
AI for transportation is a very very
large industry and that's one of the
reasons why so many people are focused
on it
the thing that that that
um
we've mentioned we've talked about for
some time and it's becoming more obvious
to everybody is that
autonomous vehicles is not about
smart sensors only it needs smart
sensors it needs lots and lots of them
but it is if you will a robotics problem
it is a AI Computing problem
you have to first number one perceive
what is happening so if I were to show
you this particular scene and you you
look at this the first thing that you
have to do is you have to perceive what
is happening
what are the things in it and what's
happening the semantics of what's
Happening Here
that there's cones there's the there's
workers or cars there's it's obviously a
construction site
and this construction site wouldn't have
been obvious to you if all we did was
detect the cones and lanes and cars and
signs
that this particular scene is different
that somehow a school bus
that's driving down the road and a
school bus that's parked by the sidewalk
is a very different condition that we
have to think about it very differently
number one perception we have to
perceive the world around us sense the
world around us number two we have to
reason
ing is reasoning is one of the most
important things that we do as humans
and that's one of the most important
things we have to do as AI Computing we
have to reason and then number three we
have to plan what do I do now well as it
turns out
in this particular case
the plan is not to stop
it's got red all over it
the plan is simply to drive more
cautiously
but to drive
stopping is simply the wrong answer
we're going to create all kinds of
congestions and so let the people work
and we should drive through it we have
to reason and we have to plan and we
have to drive accordingly
at the foundation of all of this is
learning and that's one of the reasons
why deep learning has an opportunity to
help us solve all of these challenges
that we've been waiting a long time to
solve and that's one of the reasons why
Nvidia has jumped in with both feet
to work with the automotive industry to
create a scalable platform
for self-driving cars so that together
as an industry together we can help
revolutionize transportation
our platform is called Drive px2
and
and it came with some amount it came
with some amount of questions about why
is drive px2 this way
well it turns out that autonomous
autonomous vehicles comes in all kinds
of sizes and shapes and is now becoming
clear that in fact having a scalable
platform with one architecture is really
the best way to go
and the reason for that is this
different car companies different
segments of the industry
different
applications and different countries
have a different vision
or different time
scale of their vision for autonomous
vehicles
it could range all the way from somebody
who would like to create an auto Cruise
Highway cruising capability that is
incredibly safe incredibly safe Highway
cruising capability but it's not just
about sensing it's sensing it's
localizing it's reasoning it's planning
it's acting
it's AI computing
to somebody who would like to be able to
say take me home and it actually takes
you
onto the highway off the highway
destination to destination Auto
chauffeur obviously require a different
amount of computational capability and
then number three somebody who would
like to build a fully autonomous vehicle
where there are no drivers if there are
no drivers
you have to be
99.999999
accurate and the reason for that is
because if something happened and the
car doesn't know what to do it stops
it remained stopped
for a long time
maybe forever
and so that car if you have a fleet of
these cars they're eventually going to
find something they don't recognize
you're going to have a fleet of cars
that are all stopped
just all over the world
and so full autonomy requires even more
ability to detect the very corner of
corners of corners of conditions and so
all of these platforms are designed so
that we can address different segments
of the
autonomous vehicle
Spectrum
and everybody's different visions
we call it dry px2
it allows us to do perception to do
reasoning to do driving and it consists
of basically three parts
the AI Computing part which is the
processors the Computing system
the operating system that takes Sensor
Fusion in
does all of the AI processing
connects it to all the AI algorithms and
do it so fast
that the car can actually respond to
adverse conditions in time to take the
appropriate action you have to do that
fast enough so performance matters and
scalable
well today
we launched when we launched drive px2
we showed everybody the large version of
it because our earliest customers wanted
everything our earliest customers wanted
everything we're working with some 70 80
different partners around the world
startup companies
taxis as service companies
shuttle companies
trucking companies
branded car companies
mapping companies
the num this 10 trillion dollar industry
is quite large as you can imagine
is 10 trillion dollar industry is quite
large and there are players important
players in the ecosystem all over and so
we started with the highest performing
configuration
and um uh at CES and there are all kinds
of people who are using it today this is
the smallest configuration of it this is
drivepx Cruise
this little tiny computer
this is this little tiny computer
connects to a couple of front front
cameras
and has the ability to recognize what's
in front of you
to localize where you are
to connect with a high definition map
and to update that high definition map
because it can recognize the
surroundings they're going to do slam
and it could update the hype definition
map
and it's nice and small fully integrated
computer
okay so this is the drive px2 Cruise
we recently announced that this computer
in order to create an autonomous system
needs to include not just this computer
and the algorithms inside but also
connected all the way to the cloud
the proper
autonomous vehicle platform is a cloud
to car platform it's a cloud to car
platform from hdmap
all the way down to the AI supercomputer
hdmap all the way down to the AI
supercomputer and all the software in
between
and we announced in China a partnership
with Baidu
that Baidu has selected the Nvidia
platform for their mapping cars for
their self-driving Vehicles as
for for
that would be used for oems as well as
their self-driving taxis
that want this one Computing platform
will be used in those variety of use
cases and it would be the same
architecture
dry px2 the operating system is called
driveworks and it's connected to the
Baidu cloud
well today
today I'm super excited to announce
that we're partnering with one of the
world's largest
and most pervasive mapping Partners
TomTom
that TomTom has selected the Nvidia
drive px2 to be incorporated into their
mapping cars and that together we're
going to create
together we're going to create
a cloud to car platform
for the Western Market
TomTom has mapped a very large part of
the world as you know
one of the world's great mapping
companies their service is used by
nearly everybody
and we're going to work on basically
three things
as TomTom Maps the world they're
collecting video
that video
has to be processed and turned into a
high definition map that processing is
enormous
it is the Grand Challenge of super
Computing to literally record the world
and turn it into an HD map it is a
computational challenge of extraordinary
proportions recognizing Lanes
recognizing objects recognizing
structures recognizing
what is car and don't
and reject it turned the whole thing
into a three-dimensional map register it
fuse it
so that is coherent and it has to be
within a few centimeters
all of that processing is done in their
Cloud the first thing we're going to do
is we're going to work with them to
accelerate that processing so that it
could be absolutely super real time so
that we can collect videos as fast as we
want in the future and we can
continuously crunch it and turn it into
HD Maps the second thing is the drive
px2
the dry px2
will now be their in-car HD mapping
system
not only will it continuously collect
and update differences into the map to
be fused with the HD map and then
thirdly together
we will have a cloud to car platform
HD map in the cloud
AI algorithms localization algorithms
and an AI supercomputer
for the car
ladies and Gentlemen let's welcome
Alinda Tay who is the board member of
TomTom the head of HD mapping Alan come
on up
thank you hi hey thanks
thanks for organizing the conference
here in in Amsterdam thank you literally
50 meters away from the TomTom
headquarters well that's exactly where
we should do it yeah we we chose it for
that very reason so that we could
announce our partnership so so first of
all first of all I think it'll be great
if the audience had the ability to
appreciate
the magnitude of the problem of mapping
the world yeah I know it sounds it
sounds nice when when when people just
HD mapping is such a short phrase yeah
but the magnitude of the problem tell us
about a little bit about that how much
of the world have you mapped well at
this moment is uh 47.1 million
kilometers of roads in our map database
but only uh 120 well actually tomorrow
on the Paris motor show we will announce
a bit more but I can't speak about that
120 000 kilometers is mapped in HD so
that that kind of gives you okay so 47
million kilometers of which only a
hundred thousand HDs in HD 120
000. so you're saying we're almost there
yeah
It's a Small industry we're almost there
now out of that so 47 million out of how
much of society does that represent
47.1 million is about 70 70 of society
yeah develops a side note and out of
that that 47 million miles you've you've
surely driven more than a hundred
thousand miles absolutely yeah so we we
have basic information available that's
by the way one of the reasons why we uh
use this platform we have basic
information to go much faster right so
the whole problem of making an HD map is
people believe just as they believed in
the old days that making a navigable map
was unaffordable now people believe HD
Maps which is very detailed very very
accurate is unaffordable it's not
unaffordable you need to be clever about
it and you need to use Ai and AI
platforms to automatically create and
maintain because it's an even bigger
problem because the world's changing
absolutely the world's changing
differences we send it up to your Cloud
you've got to find a way
to filter out all the junk yeah figure
out what the major differences are and
fuse it with the new map isn't that
right and we get images in we get traces
in just to give you an idea I think we
get over 7 billion traces in a day
that's a lot of information and that's
why we need platforms like that so
that's also the same with localization
if you want to localize your car with a
with a centimeter accuracy you don't
want to miss 20 centimeters when you
have a self-driving car yeah then you
need that information which we have as
Road DNA and that does the localization
also on your platform and so between the
two of us we are going to Endeavor to
map
47 million known
civilization roads that's drivable
that's made at 60 then we have the whole
60 million miles we are 100
000 miles into it
120. right that's 20 more okay and so
and tomorrow even more and so so what we
need to do of course it sounds like a
lot but the fact is is this once we get
going and we need a Computing platforms
process at super real time we need to
process that super dual time and so what
that means is if we collected a video
for a day we need to process that video
in an hour yeah and so once we can do
that once we can do that and fuse it
into an HD map we'll get through 60
million miles in a hurry I go even a
step further
no no that's that's another guy
but uh if we put that in a mobile
mapping fans we can think about putting
it in all kinds of commercial vehicles
so you can think about
putting it in cars and then actually
maintain the HD map in the car itself
that's kind of the that's Holy Grail
that's right incredible well the future
of autonomous vehicles the future of
self-driving cars requires a very high
quality HD map we're all counting on you
we're going to build it thank you thank
you congratulations thank you
okay let me um let me show you something
new
thank you Olan
every time I hear Alan to Tay's name I I
actually think of Alan ducas as well
are there only engineers in the audience
there are no there are no nobody in the
audience who enjoys food
yeah that's right Alan ducas okay
thank you sorry about that
this is
15 000 Watts that I'm standing in front
of
you know I'm getting a 10 in the back of
my back
okay so so I want to show you guys
something new you guys know that that in
order to in order to create a
self-driving car there's the Computing
platform has the process information
fast enough and the fusion the sensor
information is coming in from all over
you've got you've got of course the
simple stuff like GPS and the IMU and
the and the rotation of your tires and
your steering wheel you also have the
cameras that are coming in all around
you you have lidars you have Radars and
then now talk as you heard from from
Milan we also have hdmap all of this
information is fused together as the
input information into your car's
operating system and this information is
being changed and updated and ingested
in real time this is the ultimate this
is the ultimate real time High
throughput super Computing problem
and you've got to get that job done
you've got to get that job done best
effort is not enough
it is not enough that when you hit that
you hit enter it says you know I tried
I just wasn't able to detect it in time
next time when a car is not traveling so
fast I will be able to detect that in
time you got it has to be high
throughput it's got to be Mission
critical the algorithm for basically
driving self-driving cars the the basic
functionality I described there's
sensing there's localization there's
planning and there's the action taking
what I want to show you today is this
this is our operating system the
operating system is much much more than
this but I want to show you the
algorithms part of the the the the
operating system and I want to give you
an up show you an update of where we are
basically the way it works is this the
sensor information is coming in on the
left we have three artificial
intelligence networks running on this
car three deep learning algorithms the
first one is just detecting things we
detect cars we detect Lanes we detect
signs we detect cones we detect things
we detect things okay it's called Drive
net we detect things
as it turns out
none of us drives like that
none of us drive
by
going
no Cal no dog no person
no car
no cone
no tree
no Lake
no house
no truck no water
the list could be pretty long
it would be a dehabilitating way to
drive
we detect these things for two reasons
we detect these things for two reasons
one we want to continuously update our
HD map in the cloud and there are
several different markers that we see
that can help us figure out where we are
number two
as a backup
just in case
just in case as a backup but the real
way of driving is this
you detect what's safe to drive
when you're driving you're detecting ah
look at that it's a road it's safe look
it's open I don't
there's nothing inside
the absence of things is open road
there's a network that is doing
segmentation and figuring out where is
it safe to drive
not what not to hit but where is it safe
to drive
however
it turns out
it turns out there's even a third way
that we
a third network and really the way that
we drive
when we're driving we're not thinking at
all unfortunately
when we're driving it's just a behavior
like playing tennis
it's a behavior you simply grab the
steering wheel you start driving
all of the sensory information comes in
your brain is thinking it's working and
it's doing completely by reflex and we
just drive
we just drive and that's one of the
reasons why we can listen to a book
while we drive that's one of the reasons
why we can have a conversation while we
drive
you're when we're driving it is a
behavior we have a third neural network
called pilot net and is simply a
Behavior Network
okay not a detection Network a Behavior
Network so the first thing is this the
first section is we detect everything
around our car we create one of the most
important data structures of
self-driving cars called the occupancy
grid the occupancy grid is a
three-dimensional grid that's being
updated completely in real time with all
of the sensor information all the
detection networks and it's creating
this three-dimensional mesh
of Your World
it's creating a virtual world
this is the virtual mathematical world
based on all of the sensor information
that it has accumulated it's called the
occupancy grid that occupancy grid is if
you will the primary asset the database
of the self-driving car
that occupancy grid then
is tested against what the car would
like to do the car is driving by itself
using pilotnet
it's going to go and start driving based
on just what it sees we're going to tell
it go in that direction it's going to go
it's going to start driving
based on everything that we taught it
based on everything that we taught it
and it's going to be tested against the
future prediction of everything around
the car
and if
we were to hit something or the path
would lead to a collision
in the future in the near future we
would of course override it
okay so and then what we do is because
we would like to test
we would like to test that our
understanding of the world is consistent
with the understanding of the car
that our understanding of what we see is
consistent with the understanding of the
car we visualize the occupancy grid in a
virtual reality if you will computer
Graphics of what the car sees the
artificial intelligence car sees and so
let me first show you let me first show
you the upper left hand upper upper left
hand corner just detecting things Justin
please
this is in California this we're
detecting things now when we detect this
is our latest Network we're going to
detect um we're going to detect
three-dimensional objects
because a car is not a car as you know a
car as you know is not a flat image so
we have to detect what we think
is the volume around the car
okay and all of that information now
gets go gets included into our occupancy
grid we would also like to know where
the lanes are
okay we'd like to know where the lanes
are we would also that's those are our
easy things we would also like to know
where is it safe to drive
now this is this is really cool look at
this it's looking at the scene and it's
saying you know what you see the yellow
all the yellows the red Passenger not
safe yellow not safe red not safe
blue safe
you see this
okay
red and recognize this the past it
recognizes pedestrians
not safe please do not go there
does that make sense
you see that red is pedestrians yellow
are cars
okay and it's simply looking at every
single
image and saying where is it okay to
drive where is it okay to drive I don't
necessarily drive there but where is it
okay to drive
first of all where is it okay to drive
and when I'm of course driving this is
this is what I would see now of course I
would never go into the other lane
because I know the rules of the road
but otherwise I can pretty much Drive in
the areas that is safe to drive now
let's put it all together Justin
and so we got detecting the cars
detecting the lanes it's running on the
drive PX computer the operating system
called driveworks
and it's detecting these things okay now
these things are all getting put into
the occupancy grid as I mentioned
earlier let's go back to the slides
please
okay so you saw all these all these uh
Network detections
we take what I'm going to show you now
is the occupancy grid however before you
can have the occupancy grid you have to
have the HD map in this case from TomTom
and it tells us where the lanes are it
tells us where the signs are and based
on our ego motion which is the motion of
the car
we figure out where we are on the road
it's called localization
okay it's a very simple version of
reasoning the car's reasoning about
where am I in the world and so let's
show people what the car sees
okay so this is our digital dashboard
this comes off of dry px2
as you know one of the things that we do
very well is computer graphics
I've had people look at this and they go
wow it's amazing the graphics you guys
can do
and I said I know thank you we learned
it recently
it turns out that the computer Graphics
you're seeing here is actually quite
remarkable this this team of young
Engineers over here Justin wants you
come on guys stand up Take a Bow I know
you haven't slept
thanks guys
it's amazing what you could do on
caffeine alone
it is
you know where whereas whereas deep
learning is the is the feel of
artificial intelligence caffeine is the
fuel of deep learning engineers
and so what you're looking at this is
the occupancy grid this is the mind of
the computer this is literally the data
structure of the computer
okay this is the data structure of the
computer and so figured out where it is
in the lane these are the cars next to
it can you guys see this and we're
tracking we're detecting the cars and
tracking cars in 360.
Justin does it make sense to show the
front video did you want to do that
sure yeah uh the video or
we can also enter into sort of oh wow
Auto Cruise mode where the car is
driving itself like that okay so when we
are in we're in Auto Cruise mode the the
the the the the digital dash changes
pretty dramatically to let us know that
it is an auto Cruise mode yeah it's got
to be a big change yeah yeah absolutely
so now these are all the cars that we
detect notice the cars in front of us
we're detecting these cars
okay
so you see the cars
and so what's happening here is we're
detecting we're localizing connecting to
the HD map we create an occupancy grid
and this is the visualization of the
occupancy grid when you look out the
window and it's inconsistent with this
then you know that something is wrong
I would go back to manual mode
file a bug with Nvidia
okay and we'll get on it right away yeah
and it's it's so much more sophisticated
and difficult to represent this full 3d
space rather than just some bounding
boxes so this really represents really
Advanced deep learning to be able to
identify the exact locations yeah right
yeah
by recognizing the Bounty boxes we know
the extent that's right of that car in
the occupancy grid it could be a little
tiny Mini Cooper it could be a super
super long truck it's
right okay so visualization of the
occupancy grid
now let's come back to the slides please
all right so so what you saw earlier is
detection and then you saw us you didn't
see the occupancy grid but you saw the
visualization of the occupancy grid
which is in this ux which I think is
going to be pretty important to the
future of self-driving cars actually you
want the driver you want the you want
the part you want the the passengers to
know that the car has the situation in
hand and you want to be able to
calibrate against where it it wants to
drive and where you think it out of
Drive
um now I want to talk to you about
pilotnet
um the fact of the matter is when we're
driving we don't do any Newtonian
physics
when we're driving we're not doing any
calculus when we're driving we just
drive and so the question is how do we
get a car to just drive and what we're
going to show you is this this is BBA
this is the latest version of our of our
Network and we've been teaching it how
to drive now remember this
bb-8 has no detection networks inside
when bb-8 is driving it's doing we
didn't tell it detect the cone
detect the lane
detect a post
detect a fence
detect cars detect the absence of Roads
we didn't tell bb-8 anything
we just drove
we drove and drove and drove and drove
and drove and drove and bb-8 imitating
us
so what bb-8 is going to do is imitate
us
so the question is this when we're
driving what do we see and why did we do
it it turns out
we don't have to describe it using
algorithms we don't have to describe it
using equations we only have to do it
over and over and over again until bb-8
generalizes if detected the features and
it generalizes what is driving behavior
ladies and gentlemen bb8
oh yeah good the universal sign for Otto
[Music]
there are no Lanes here ladies and
gentlemen
thank you
that's barely a road
[Music]
[Music]
[Music]
that is incredibly strange
oh cool
[Music]
and they're on top
[Music]
just imitation
[Music]
and to learn how to drive in the dark
[Music]
got it
what do you guys think
autonomous machines the thing that's
really cool is I want to show you what
BB what's in BBA H mine is that okay
the question is what did it see
the question is what did it see and so
we just started driving and eventually
it learned how to drive just like us
okay and learned how to drive like like
us the question is what did it see what
are the things that it saw that it
learned and generalized and eventually
say these things matter to driving
these things matter to driving I think
you'll be kind of surprised
those sparkles
are
bb-8s AI neurons
that's what's firing on this Vision on
this video
and look
it's kind of interesting
it's looking at that corner because that
particular car is kind of closer to us
and every now and then it checks the car
in front
and it checks the card to the right
every now and then
it's always looking at the lanes making
sure I'm staying inside it
everything else that's not lit wherever
there's no sparkles
non-information to bb-8
these are the things that it figured out
that were important to its driving
behavior and then as a result it just
stayed in the middle of the lane all by
itself did no calculations
because it knew that largely when we
drove we stayed in the middle of the
lanes
it knew that when we drove when there
are no Lanes we stayed in the middle of
the road and it recognized to recognize
how to discern a mud Road a dirt road a
dirt road in the dark next to bushes
that we we don't seem to drive over
bushes
we seem to drive over to Dirt Road okay
ladies and gentlemen bb-8
So today we're announcing we're
announcing that drive works that we
talked about in the beginning of the
year has made enormous progress and
we're in a process of packaging all up
our strategy with driveworks is this
drive PX and driveworks is an open
platform
it's an open platform it's an open
platform the tier one uh oems and T1
odms and Tier 1 oems and card companies
have the ability to pick off the pieces
that they would like to use or the
pieces they would like to replace
and together we will work together as an
industry to move autonomous driving
forward
Drive works as an open platform we have
reached Alpha One
as you know that this is going to be an
area of research and development for
years to come and even though we're
going to see some self-driving cars get
on the road here in the next next year
or make no no more than next year and no
more than the next couple of years we're
going to continue to enhance the
software today we're announcing that
drive out drive drive Works Alpha One
will be released to our early Partners
in October
after that
of which they could use pieces that they
like pieces that they would like to
replace and after that we will update it
every two months
just as we continue to learn as humans
our car will continue to to learn and
the network will get better and better
and better considering what we've
achieved in just one year's time and
imagine what we would be in another
couple two three years
driveworks Alpha One
well the number of autonomous vehicles
on the road are increasing and we're
working with some 80 companies 80
Partners around the world one after
another after another over the next
several months
the coming year are going to be revealed
I think that the vision of having an AI
Computing platform an AI Computing
platform by which cars could be built on
top of that has the ability to do
perception and sensing
localization and reasoning
planning and driving the ability to have
all of that on top of a deep learning
platform is really
quite the right answer at this point and
we're seeing just really really rapid
developments across all of our partners
well the work that you've seen so far is
really at this intersection
you know whether it's whether it's the
the company slide that I showed you or
all of the different examples that I've
created that I've shown you so far they
share one thing in common
that the work that we are doing that
Nvidia is doing is at this intersection
between visual computing
which in our brain consumes the vast
majority of our neural cortex Ai and
high performance Computing the work that
we do have to be achieved very very
quickly this area of computing AI we
call AI Computing would enable the
future of intelligent machines and we're
super excited about this area so much so
that several years ago we decided
that the world needs a processor that is
designed specifically for this
intersection
we started working on a project
internally called project Xavier and
ladies today I'm announcing project
Xavier to you project Xavier
is basically an AI supercomputer SOC
7 billion transistors reason we put 7
billion transistors into into context
for you seven billion transistor
transistors
is equivalent to the largest
CPU the world's ever made
the highest performance server CPU the
largest number of cores you can find is
just about seven billion transistors
this is the largest
processor Endeavor that I know of that
we have ever done
not only is it large it is also
multifunctional
not only is it multifunctional the
throughput requirement necessary is just
really quite tremendous
the ability to support
HD cameras all over all over you
lidars and Radars the ability to do
three fundamental things
three fundamental things
deep learning
computer vision
and high performance computing
these three fundamental elements of
computing we think is going to be super
super exciting area for us to innovate
and we've taken an enormous chance to
create Xavier and I'm just so incredibly
excited about it seven billion
transistors eight high performance CPU
cores inside
512 of our next Generation GPU cores
has a brand new computer vision
accelerator
moving video processing to 8K
process it in full HDR and the reason
for that for in the case of autonomous
driving
we need a very very precise Black Box
inside the car and it should be
recording things all the time and
recording at an HDR and it's designed
for asyl C functional safety
this is
the greatest SOC Endeavor I have ever
known and we have been building chips
for a very long time and so this is
project and project Xavier
our next Generation SOC
we'll have samples end of next year
well let me tell you what it looks like
when you have Xavier
this is Drive px2
the drive px2 motherboard includes two
latest generation Tegra slcs and two
discrete gpus
okay the drive px2 and its full
configuration is two Parkers and two
Pascal gpus
it performs 20
trillion operations per second 20
trillion operations per second of deep
learning capability and has 128 spec 120
spec-ins all in about 80 Watts okay so
this configuration 120 tops and
120 uh 20 tops 120 seconds is 80 Watts
well that's approximately equal to
150 MacBooks
150 MacBook Pros all in a little tiny
computer that sits inside a car well
Xavier
is exactly the same architecture
it's 20 tops it's 160 seconds and it's
20 watts
and a little tiny board like this
okay Xavier and so just imagine what
autonomous vehicle could do in the near
future with Xavier so super excited
about Xavier we have plenty of time
before
next year and I'll give you more details
as we go
let me quickly summarize our
announcements today
we have introduced an end-to-end deep
learning platform
I introduced the new P40 and P4 today
it opens up a brand new market for GPU
deep learning we have great partners for
Enterprise IBM and now really excited to
announce sap
we now have the ability to take GPU deep
learning from training
all the way to inferencing in data
centers from training the network
to applying the networks to queries the
incredible number of queries that
consumer applications have from training
the networks to actually finding insight
for your business
okay so first thing is announcement of
P40 and P4 the second set of
announcements has to do with our
self-driving car initiative
we are today announcing that driveworks
Alpha One
all of the capabilities that we talked
about will be released to our partners
first release in October and then after
that every two months
we announced
a partnership with TomTom that together
we will enable a autonomous driving
platform from Cloud to car
and then lastly the future of AI
computing
this processor we called Xavier one of
the greatest Endeavors of our company
well I think you could you can get a
sense now
of why I think AI is going to be so
important for the future of the computer
industry and frankly for the future of
the world there are so many so many
ideas now of the applications of AI new
problems that we're able to solve that
we weren't able to solve before whether
it's AI for transportation
a 10 trillion dollar industry that we
can we have an opportunity to make a big
difference in AI to revolutionize
medicine and then AI of course
to completely utterly utterly
revolutionize Society with intelligent
machines they're Among Us helping us do
things that are mundane helping us do
things that are dangerous or even
helping us do things that we simply have
no possibility of doing ourselves
this is a great New Era of computing I
want to welcome all of you to GTC and I
look forward to seeing all of you today
[Applause]
Title: U.S. Building Two Flagship Supercomputers to Accelerate Scientific Discovery
Publish_date: 2014-11-14
Length: 212
Views: 80391
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/nqERLsNTnXk/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: nqERLsNTnXk

--- Transcript ---

from climate research to medical science
to alternative energy supercomputers are
the key to unlocking the answer to some
of life's greatest challenges what
supercomputing gives is the ability to
manage the complexity of the real world
some of my favorite projects involve
modeling the plasma inside a fusion
reactor this is like putting the energy
of the Sun in a magnetic bottle inside a
room there many times where we can't
observe a system at the scales that we
need to and so if we can establish a
mathematical framework for that system
and then that becomes the virtual world
we can explore using high-end
high-performance computing today to be
excellent in scientific research is
really to be excellent in scientific
computing to further accelerate the
goals of scientists the US Department of
Energy has announced plans to build two
new GPU accelerated supercomputers ciara
at Lawrence Livermore and summit at
oakridge home of the titan supercomputer
so summit will perform about five to ten
times faster on applications then
tighten does today with the introduction
of summit is going to allow that
frontier that boundary of what you can
do to be moved significantly and when we
move that those boundaries we expect
that the quality of the science that
we're doing the knowledge that we're
generating is going to improve
accordingly these enormous improvements
are made possible by new technologies
from Nvidia and IBM summits architecture
is made of thousands of compute nodes in
these compute nodes are heterogeneous
they have different kinds of processors
it has the IBM power processor for
latency optimized tast it has the NVIDIA
GPU for throughput optimized tasks and
has the envy link technology faster
higher bandwidth communications
technologies to integrate these into a
powerful large compute node that is
summit envy link is really one of the
most critical technologies for the
summit system
envy link is invidious high-performance
interconnection between the nvidia gpus
and the IBM power processors so that you
can move data from one to the other at
very very high speeds this is really
terribly important for our users because
the the speed that you can move the data
between the GPU and the cpu really god
provides an upper bound on how much
performance you can get out of the
system with these powerful new
supercomputers the Department of Energy
is taking the lead in the pursuit of
exascale computing there's almost no end
to the scientific problems where
supercomputing gets deployed in areas
like weather simulation whether you're
talking about extreme locality of
forecast to long-range climate modeling
medical research trying to find new
drugs or understand disease any
dimension of science where there's new
theory new exploration these are areas
where supercomputing is increasingly
getting deployed as a third pillar of
scientific discovery along with theory
and experimentation so they provide a
tremendous benefit to us and to the
entire country
Title: GeForce Garage: Antec 900 Series, Video 3 – How To Prep and Paint Your Case
Publish_date: 2015-09-14
Length: 745
Views: 148847
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/nSqRpGHMDtw/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: nSqRpGHMDtw

--- Transcript ---

hi I'm Andrew with Nvidia and you're
watching GeForce garage there are a
million ways to customize your PC case
to make it exactly how you want it but
today we're going to show you the
biggest bang for your buck mod with the
weekend and a couple of cans of spray
paint you can personalize your PC case
and turn into something really cool
later on in this series of GeForce
garage we're going to show you how to
finalize the paint with some really cool
detailed airbrushing but even if you
want to go full three Wolf Moon you
still need a nice base coat to match
your feet here with me in the garage
studio is Dwayne Carol legendary mater
from modders Inc comm to show us the
ropes on painting our kids painting a
case is not all just aim and spray paint
cans at it and hoping that it turns out
right there's a method to this madness
right if you follow certain steps and do
certain things and if you hit those
steps and do them correctly you're going
to come out with a great finish on your
on your case okay that's what I'm going
to show you how to do awesome and of
course we're not going to be painting in
a closed environment like the g-force
through our studio right that's correct
you need to work in a well-ventilated
area so for the demonstrations and stuff
we're going to head outside
like any project you need to make sure
you have the right tools to do your job
of course safety equipment classes
gloves and masks for when you're doing
the spray paint because the paint could
actually damage your lungs any of your
sand papers you have different types
different sizes front grits so the
higher the number the smoother it's
going to be this is a 220 grit like a
sanding block you're also going to have
what they call wet sandpaper it's a lot
smoother you're gonna be polishing so
that's when you start doing some of your
finish work you've got the different
types of tapes stay away from duct tape
because really really super tacky
do not use masking tape and a scant a
bleep glue behind do you want to use the
painters tapes also some papers that you
can use the mask off large areas what
you're going to want to do is make sure
that everything is nice and clean before
you do any spray painting so you want to
make sure you have rags and you can use
a sponge because you want to be able to
go around edges and I like to use what
it's called a tack cloth it's kind of
like a cheese cloth that has like wax on
it that way you can take off the fine
particles that might be left behind get
it into your spray paint so the first
thing that you would lay down would be a
primer then the paints of choice this
one's going to be metallic but you can
do just regular enamels or the flat this
particular one is in it adhesion
promoter you use this on plastic and
it'll dissolve into the plastic lid but
it gives a better bond next when you
want to get to the finer detail you're
going to start getting into your rubbing
compounds and your waxes or a protective
coat and get that extra shine so we're
actually going to start doing the work
on the case now so don't forget put on
your your safety equipment so here we go
we're just going to start scraping the
old paint off you
we're doing the same thing on the sides
once a while I want to make sure you
clean it off like I noticed I missed a
little bit here and then we'll be ready
to put some tape on here but what we
want to do is make sure that we're
covering the area that we don't want to
get paint on so let's say we want to get
underneath this lip here you want to go
right up to the edge it's underneath
here make sure it's nice and flat I'll
tell you right now that paint will find
its way into any space
the smaller tape do a little crease put
it underneath so when I crease it I'm
actually getting the tape to stick on
this flat side and then it's the flat
side of the tab so I'm doing two things
in one step I do want to paint this side
because when it's flight in a case
you're going to see probably sometimes a
cracked you might as well paint all the
way around but we don't want to do the
actual back side panel stable little
time and save a lot of tape to get the
paper that I had out before lay it down
where you want it you can cut it to size
and you can do the same thing for the
next one cut it a little short down here
you can see that I didn't tape this edge
of it this part of the handle is going
to be sticking out I just want to paint
the back side of the handle
I'm going to trim it to size
and then I'm going to take my razor
knife this is one of those small fine
details that really set your case apart
because when you look on the side of the
case you're going to see this part of
the handle we get it all taped off and
now I want to make sure that the panel
is nice and clean take the panels and
wash them it's a little bit of soap and
water just kind of wipe off as much
water as you can and let it dry gotta be
dry because the primer if it finds any
water it's just not going to stick you
can get bubbles you're going to get bad
results take a micro rag I'm going to go
over all the edges
it's pretty clean we'll see too much
under next step is the tech cloth this
is going to pick up any small
imperfections you litter particles
anything that might be flowing around
and as you can see we thought it was
clean look at all the stuff that we left
over these are the little dust particles
that will show in your paint your base
has to be good if it's nice and smooth
your finish coats going to look nice and
smooth
okay so now we're going to get ready to
go ahead and start putting our first
coat of primer on I like to use a self
etching primer you can use any kind of
primer with primer you're not going to
put on heavy coats you're going to put
on a nice layer then we're going to
stand it because like you want to try
and get that nice glass looking piece
one tipped on any spray can there's a
little black dot on the top you'll line
up the nozzle to the black dot because
when you're holding the can to spray it
the pickup tube inside the paint is
actually curved so when you're holding
it this way you're always constantly in
the paint if it went the other direction
you can be going rain along and all of a
sudden you're spitting out nothing but
junk make sure that you shake and mix it
very well we're ready to lay some primer
down the coat hanger
I like to make sure that the product is
hanging is that when you're spraying
that your primer or even your paints
sometimes you'll get some spinning out
of this but if some paint will actually
collect on the nozzle sometimes will
spit out and put a little droplets out
if you're holding it in the horizontal
position and you're spraying it it's
going to spray it onto the surface you
don't want that to happen
that's why make sure that's in a hanging
position so that if it were to happen
it's going to fall before it gets the
product you want to make sure that you
put your primer and your paints on the
hardest and places that you're going to
shoot first we don't want any overspray
that happen on the front side give it a
couple of test shots making sure that
the nozzles not clogged up looking good
and now we'll start laying it down you
want to do nice even strokes
you
once you lay this down you're going to
want to let it dry I would say at least
a couple of hours in minimum if you
really want to make sure that this dries
nice and even and hard so we're going to
let this dry and we'll come back and
I'll show you how to work on the primer
and get it ready for paint
so we've applied our primer and we have
letting it dry and then what we're going
to be doing is doing a wet sanding this
is going to be 1500 grit and what we're
gonna be doing is knocking down any of
it the edges that are on the surface
we're almost going to be more polishing
it for this demonstration we're just
going to use regular tap water making
sure that your surface is nice and
coated with water make sure that your
wet sandpaper is also nice and wet it
just makes it a little more pliable and
I like using the sponge because when a
sponge gets wet you can actually bend
the paper around it and you can go all
kinds of different ways
don't use the seam side go get on even
sanding so you want to use the flat side
so just lightly go over it
be careful to the edges maybe sand too
much too hard along the edges will
actually go through the primer and you
go back down to the metal or you go back
to original paint that was underneath
there again you're not trying to take
this layer of primer off what you're
trying to do is just smooth it out give
it a nice glass looking for parents so
we've knocked it down a little bit there
we're going to go ahead and just kind of
go along the edges and this is where
this comes in nice I can curve it a
little bit and I can get it into where
the handle of that and you're done put
some more water on there do a little bit
more sanding like we did before when
we're taking the paint off I want to
make sure that won't take the water off
so I can see make sure that hit all the
spots you're going to see a difference
in the color
I think we did a pretty good job on that
for the final one I want to do one more
and I want to do it all in the same
direction that way they can't be any
chances of being like swirl marks or
anything else when you put the final
coat of paint on I'm just going to go
nice smooth even strokes all the way
across
clean it up look across the surface you
can see if you see any scratches see any
imperfections or new yellow places you
can go back in sand and as you can see
it's already drying you can see the
color changes that's are already
starting to happen so we can see that
we've done a really good job of sanding
this down so now we have the panel it's
all been nice and dry what we're going
to do is start applying the paint's what
we're going to use in is going to be a
metallic paint one of the things you get
to remember is it's got those little
pieces of metal floating around in too
and it gives it a metallic look just
make sure that you mix the paint up
really really good you might even want
to flip it over
clean out the nozzle you'll get a much
better finish out of it that way you
wanna get a nice even stroke when you're
going across it we don't want to go real
heavy with it just want to make sure
that we get a nice coat and now I'm
going to go ahead and do the front side
okay so we got one coat on what I'm
going to do is I'm going to let this dry
for about eight to ten minutes but you
don't want to completely dry yet because
when you put your second coat on like to
go across it in the other direction and
it gives it a more uniform look so we've
already gone through and we have put two
coats of paint on on this and as you can
see it's got a pretty good coverage what
we're going to do now is going to put
our clear coat on there but first is
going to make sure that the surface is
nice and clean and we are ready to start
laying down that clear coat always try
to stay with the same brand if you're
using a different brand it could have a
reaction with it it bubbles up or even
worse it starts cracking and just
destroys all your paint you're starting
all over
can we get our first coat on like so
we're gonna do a little heavier on the
second one and then we'll do the
unveiling in the garage all right
so here we are with the finished panel
that thing looks amazing I think it does
I don't you turned out really really
well compared to the original can see
with just a couple cans of simple spray
paint you can get a really nice finished
panel if you follow all the right steps
you're going to get a finish like this
no problem
awesome well Dwayne thanks so much for
coming in really appreciate your
expertise thank you if you want to check
out more guides by Dwayne you can go to
modders inc.com there's a lot of really
cool information on his site so don't
forget to check that out on the next
GeForce garage episode we're going to
show you some really cool techniques to
take your cable management to the next
level thanks for watching GeForce garage
the ultimate resource center for
designing building and customizing your
GeForce GTX PC
if you're looking for more awesome GPA
Raj videos be sure to check the links
right here right here and of course if
you haven't subscribed to the GeForce
comm channel click the subscribe button
on the bottom right hand corner of your
screen also known as on the left of the
video corner of the screen no big deal
and if you haven't liked us on Facebook
o click right there boop-boop boop-boop
Title: NVIDIA & VMware - A New Partnership. A New Data Center.
Publish_date: 2020-09-29
Length: 118
Views: 13526
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/O1t5mmvLThE/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: O1t5mmvLThE

--- Transcript ---

as the world began
accelerating to the cloud
a digital foundation was built to power
change and redefine the data center
turning homes into hubs of innovation
and children's bedrooms into high-tech
[Music]
classrooms
transforming the backbone of commerce
and engaging every customer with new
experiences
safeguarding livelihoods
and delivering greater peace of mind
[Music]
then ai ushered in a new era
inspiring endless possibilities
to help the world face its greatest
challenges
[Music]
clara hello how may i help you what
procedure am i having today
you are having a bronchoscopy
supporting our health care workers and
protecting them
from harm
finding insight in vast expanses of data
[Music]
and deciphering the building blocks of
life
[Music]
accelerating the way we build
while navigating us to a brighter
tomorrow
[Music]
now we're bringing these two giants of
tech together
to create the next stage of computing
optimizing the data center for ai
Title: Amazing Grace - an ARM CPU for Giant-Scale AI and HPC (NVIDIA GTC Spring 2021 Keynote Part 6)
Publish_date: 2021-04-13
Length: 509
Views: 22635
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/ObYjo-epyYQ/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: ObYjo-epyYQ

--- Transcript ---

[Music]
modern data centers host diverse
applications that require varying system
architectures
enterprise servers are optimized for a
balance of strong single threaded
performance
in a nominal number of hyperscale
servers
optimized for microservice containers
are designed for a high number of cores
low cost and great energy efficiency
storage servers are optimized for a
large number of cores and high i o
throughput
deep learning training servers are built
like super computers
with the largest number of fast cpu
cores the fastest memory
the fastest i o and high speed links to
connected gpus
deep learning inference servers are
optimized for energy efficiency
and best ability to process a large
number of models concurrently
and there are so many more converged
telco industrial edge the genius of the
x86 server architecture
is the ability to do a good job using
varying configurations
of cpu memory pci express
and peripherals to serve all of these
applications
yet processing large amounts of data
remains a challenge for computer systems
today this is particularly true for ai
models
like transformers and recommender
systems let me illustrate the bottleneck
with half of a dgx each ampere gpu is
connected to 80 gigabytes
of super fast memory running at 2
terabytes per second
together the 4 amperes process 320
gigabytes
at 8 terabytes per second contrast that
with cpu memory
which is one terabyte large but only 0.2
terabytes per second as opposed to 8
terabytes per second the cpu memory is 3
times larger
but 40 times slower than the gpu
we would love to utilize the full 1 320
gigabytes of memory in this node
to train ai models so why not something
like this
make faster cpu memories connect four
channels to the cpu
a dedicated channel to feed each gpu
even if a package can be made
pci express is now the bottleneck we can
surely use
mvlink mvlink is fast enough but no x86
cpu
has mv link not to mention four mv links
today we're announcing our first data
center cpu
project grace named after grace hopper
a computer scientist and u.s navy rear
admiral who in the 50s pioneered
computer programming
grace is arm-based and purpose-built for
accelerated computing applications
of large amounts of data such as ai
grace highlights the beauty of arm
their ip model allowed us to create the
optimal cpu for this application
which achieves x factor speed up the arm
core in grace
is a next generation off the shelf ip
for servers
each cpu will deliver 300 spec int
with a total of over 2 400 spec in rate
cpu performance for an 8 gpu dgx
for comparison today's dgx
the highest performance computer in the
world is 450 spec in rate
2 400 spec in rate with grace
versus 450 spec in rate today
so look at this again before
after before
after amazing increase
in system and memory bandwidth today
we're introducing a new kind of computer
the basic building block of the modern
data center
here it is
[Music]
what i'm about to show you brings
together the latest gpu accelerated
computing
melanox high performance networking and
something brand new
the final piece of the puzzle
[Music]
the world's first cpu designed for
terabyte scale accelerated computing
her secret code name grace
this powerful arm-based cpu gives us the
third foundational technology for
computing
and the ability to re-architect every
aspect of the data center for ai
we're thrilled to announce the swiss
national supercomputing center
we'll build a supercomputer powered by
grace and our next generation gpu
the new supercomputer called alps will
be
20 exa flops for ai 10 times
faster 10 times faster than the world's
fastest supercomputer today
alps will be used to do whole earth
scale weather and climate simulation
quantum chemistry and quantum physics
for the large hadron collider
alps were built by hp enterprise and
come online
in 2023 we're thrilled by the enthusiasm
of the super computing community
welcoming us to make arm a top
scientific computing platform
our data center roadmap is now a rhythm
consisting of three chips
cpu gpu and dpu
each chip architecture has a two
two-year rhythm
with likely a kicker in between one year
we'll focus on
xc86 platforms one year we'll focus on
arm platforms
every year we'll see new exciting
products from us
the nvidia architecture and platforms
will support
x86 and arm whatever customers and
markets prefer
three chips yearly leaps one
architecture
arm is the most popular cpu in the world
for good reason
it's super energy efficient its open
licensing model
inspires a world of innovators to create
products around it
arm is used broadly in mobile and
embedded today
for other markets like the cloud
enterprise
and edged data centers super computing
and pcs
arm is just starting and has great
growth opportunities
each market has different applications
and has unique systems
software peripherals and ecosystems
for the markets we serve we can
accelerate arms adoption
let's start with the big one cloud one
of the earliest designers of armed cpus
for data center is aws
its graviton cpus are extremely
impressive
today we're announcing nvidia and aws
are partnering to bring graviton 2 and
nvidia gpus together
this partnership brings arm into the
most demanding cloud workloads
ai and cloud gaming mobile gaming is
growing fast
and is the primary form of gaming in
some markets
with aws design graviton 2 users can
stream arm-based applications and
android games
straight from aws it's expected later
this year
we're announcing a partnership with
ampere computing to create a scientific
and cloud computing sdk and reference
system
ampere computing's ultra cpu is
excellent
80 course 285 spec in 17
right up there with the highest
performance x86
we're seeing excellent reception at
super computing centers around the world
and at android cloud gaming services
we're also announcing a partnership with
marvel
to create an edge and enterprise
computing sdk
and reference system marvel action
excels at i o
storage and 5g processing this system is
ideal for hyper-converged edge servers
we're announcing a partnership with
mediatek to create a reference system
and sdk for chrome os and linux pcs
mediatek is the world's largest soc
maker
combining nvidia gpus and mediatek socs
will make excellent pcs and notebooks
you
Title: AI in Medical Imaging Made Easy with Clara Toolkits
Publish_date: 2019-08-29
Length: 171
Views: 7361
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/OdSovH0B2qg/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: OdSovH0B2qg

--- Transcript ---

AI and medical imaging is booming and has the 
potential to transform radiology as we know it.  
But for this to happen we believe that data 
scientists and radiologists need to collaborate. 
Radiologists should be able to bring their 
expertise to the process of AI development.  
And data scientists should be able to translate 
radiologist inputs into AI-powered applications.  
NVIDIA  Clara makes it easy to get started with 
AI by providing tools that make data annotation,  
training and deployment seamless 
for medical imaging applications.  
Let's start with the first key element: annotated data.
Creating it can be  time-consuming but high quality annotated 
data sets are essential for training AI algorithms.  
The Clara Train SDK includes APIs to add AI
assisted annotation to any medical viewer.  
You can download the SDK container from NGC, 
NVIDIA's hub for GPU optimized software.  
The next step is to create an AI model. 
You can either train one from scratch  
or adapt an existing one with techniques like transfer learning. 
Clara Train SDK can do both.  
The SDK includes 24 pre-trained AI models and a 
Python-based library with APIs that make it simple  
to adapt and train models to new data sets. 
Using tensorboard you can monitor as the model 
adapts to the newly annotated data.
Transfer learning is a great way to accelerate the creation of  
AI models using less time and less training data. 
In fact, we were able to achieve target accuracy  
for this model with 1/4 the data and time 
required as compared to training from scratch.  
We're now ready to export the model and make it 
inference-ready. The Clara Deploy SDK streamlines  
this process of integration into your current 
medical imaging system so that it can seamlessly  
communicate with your PACS environment. 
It does so with containers and built-in support for 
DICOM communication, services that streamline 
workflow orchestration, pipeline definition  
language to define custom workflows, and tools 
that simplify the setup of Kubernetes clusters.
You can monitor the processing of different 
stages of the pipeline using the Argo dashboard.
Using any standard medical viewer that can read 
DICOM you can visualize the AI inference results.  
And this is how you can take your AI algorithms 
into the radiologist's world.  All of the Clara  
AI SDKs are supported for on-prem and in-cloud 
deployment, and because these SDKs are modular  
in nature, users can customize some of the features 
and capabilities or even create new workflows.  
The NVIDIA Clara AI toolkit lowers the 
barriers to AI adoption and medical  imaging
And getting started with AI has never been easier.
Download the SDKs now to see for yourself.
We'd love to hear your feedback!
Title: Top 10 Skills You’ll Need in Rise of the Tomb Raider
Publish_date: 2016-02-29
Length: 251
Views: 235202
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/Oe9nn7sAMvA/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: Oe9nn7sAMvA

--- Transcript ---

rise of the Tomb Raider is finding out
on PC and it looks stunning
but beyond the picturesque snowy peaks
and luscious green forest lies an
action-adventure game with fantastic
depth and variety a skill upgrade system
alone has over 50 unlockable perks for
Lara to enjoy so to help you spend your
hard-earned XP wisely here's our guide -
the first 10 skills you should unlock
Lara's gills are arranged into three
distinct categories brawler which covers
hand-to-hand combat and health
regeneration hunter for archery and
other ranged weapons and harvesting the
squishy bits from local fauna and
survivor covering crafting and
environment deception individual skills
can be unlocked at a campfire once
you've earned enough XP but early in the
game it pays to make very careful
choices as these heavily impact the way
you can play the game the first one to
bag is added learner which gives you an
extra XP bonus every time you examine a
document relic survival cash or complete
challenge this in turn means that for
the rest of the game you will earn XP
and unlock more skills faster next up is
breath control if you've watched our 7
things you need to know video then
you'll know that becoming an Olympic
gold medalist in archery by shooting
them in the head with sticks is an
absolute must therefore master this
skill to turn Lara's initially shaky
hold on her makeshift bow into an iron
grip giving you more time to line up the
killer shot arrow retrieval is next on
the list as this gives Croft e a chance
to reach a bloodied murder pole back
from the cold lifeless forms of her
downed enemies you won't get one back
every time but it still helps keep your
quiver well-stocked when you're on the
move next make sure you unlock eye for
detail a perk that bolsters Lara's
survival instinct
the world is bursting with useful
resources relics and items that drive
the story forward - details specifically
improves Crofts ability to spot
challenged objects by now you'll be
constantly in need of animal hides and
feathers enabling you to craft items and
clothing and to keep your arrows
fletched and flying straight and true
the animal instinct skill is very useful
here as it adds animals to your survival
Institute mode highlighting them in
yellow when you hit Q it also highlights
the footprints and blood trails left by
larger prey making them much easier to
track
slice and dice by this point in the game
you should also have acquired at least a
couple of additional skills from
completing challenge tombs meaning you
should now have access to the second
tier of unlockable skills the first of
these to pick up is Dead Eye which adds
a small red circle to a crosshair when
lining up headshots helping to ensure
you can smoothly and stealthily one-shot
your way through the next group of goons
adding nerves of steel next will further
increase the amount of time that lara
can keep the bow drawn before having an
attack of the wobbles giving you that
extra heartbeat to make certain a card
shot next it is well worth investing in
the finesse skill which gives you yet
more delicious XP by chaining headshots
together there's no time limit or
penalty for any missed shots so long as
the coup de Gras is always on the noggin
or from a stealth kill to complete your
metamorphosis and reach your final form
as kevin costner pick up the double shot
skill when zoomed in on enemies you can
now lock them by drawing or bow and
carefully hovering your crosshair over
them and release two arrows
simultaneously cutting their bovine
conversation there's more two of them
and their lives a little short once
you've opened up to hear three of the
skill tree you can actually up this two
triple shot again well worth doing later
down the line with the air thick with
deadly whistling willow you'll by now be
munching through your materials at a
rate of not therefore it's a good idea
to pick up the naturalist skill which
increases the amount of natural resource
you pick up from each source making it
easier to keep your pockets stuffed with
useful materials and that completes our
top ten skills to unlock first if you've
gone for a less silent and deadly
approach then please let us know in the
comments below how you've got about rise
of the Tomb Raider is out now on PC
Title: NVIDIA RTX Server Transforms Content Rendering
Publish_date: 2019-05-28
Length: 80
Views: 19461
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/OhQfu3VFuIg/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: OhQfu3VFuIg

--- Transcript ---

the new r-tx server coupled with
gpu-accelerated renderers like Autodesk
Arnold allows artists to get the full
benefit of GPU accelerated rendering
both for their interactive desktop and
batch rendering when they need to
deliver frames overnight the r-tx server
is built on top of Quadra r-tx 6,000 or
Quadra r-tx 8000 GPUs the artists work
in a quadrille virtual design
workstation so all their work is done in
the r-tx server and the results are
streamed to their desktop over the
network and a smooth seamless experience
they see their changes right in front of
them all the correct lighting and shadow
behavior reducing guesswork increasing
productivity and increasing creativity
when they're done with their shots and
they submit it to the batch render the
r-tx server will also process in GPU
accelerate those batch renders as well
so each individual frame can come back
up to six times faster on Autodesk
Arnold so where a single CPU node may
process one frame and our check server
will be processing four frames at the
same time so the r-tx server coupled
with an artsy X accelerated render like
Autodesk Arnold allows artists to stay
creative allow studios to save costs and
deliver the products their clients need
you
[Music]
Title: Vans Footwear Real-Time Design Visualization
Publish_date: 2014-08-26
Length: 134
Views: 10997
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/oK52ZFGuwNo/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: oK52ZFGuwNo

--- Transcript ---

we're in the process of integrating
virtualization as a design tool that is
going to allow designers to interact
with virtual samples rather than
physical samples in the past the design
process was pretty much two-dimensional
and by dimensional we started with a
sketch e-learning paper on Photoshop
with the tablet we would then create a
drawing which we call the tech pack
which would send to the factories wait
maybe two or three weeks to get a sample
back review the sample validated or not
send another tech pack and then with
another two or three weeks this could go
on three or four times until we get the
product that we were actually happy with
now with the with the new process we're
able to make a lot of design design
decisions on the spot we don't have to
wait two three weeks we make better
design decisions and we make them faster
the equipment we currently use involves
NVIDIA Quadro 4,000 cars we've used this
this new set up to mainly interact with
one shoe and to have real-time
visualization of a single product which
has been great we're in the process of
investing in a new display technology
involving a 4k 84 inch LCD display which
will allow us to to display a whole
collection not just one - we're planning
on using that for LAN reviews which
involves larger teams of Salesforce and
self people to review the product so in
that regard using the new card like the
K 6000 is almost a must we tested that
card and we saw some of the results and
and just the amount of flexibility it
offers is amazing it allows us to do
things that we weren't able to do before
in terms of showing the footwear in its
environment showing all collections as
opposed to just one
besides the savings the obvious savings
and in development costs and
productivity and transport cost the the
new graphics technologies have really
enabled designers to to focus on the
product and focus on the design rather
than focus on the process and the
technology behind it
Title: The Making of the NVIDIA DGX Station
Publish_date: 2018-06-04
Length: 203
Views: 178420
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/oMqmgxnLuhk/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: oMqmgxnLuhk

--- Transcript ---

[Music]
one of the most exciting things is that
AI is truly everywhere
I mean we're interacting with AI
everyday in our personal lives maybe
people don't realize all the different
industries it's touching one of the
reasons AI has become so successful
recently is that we have a lot more data
and a lot more compute to train models
on that data NVIDIA has been playing an
important role in this with the GPU as
we looked around we saw that it is
sometimes difficult for people to access
the compute power they need to get their
work done that led to the creation of
our dgx products the djx family products
began with our DG x1 which start in the
data center and we knew that the
researchers and deep learning developers
out there needed a supercomputer that
they could easily get going set up and
run what we were really trying to do
with the station was that the server was
doing really well and people were using
it not all the developers could get on
to the server so the idea there was
let's give them a supercomputer that
they can put at their desk and do the
development of their algorithms and then
when it was working they would run it on
the server often the data center
the goals were to make it plug-and-play
to make it transportable between office
spaces to be able to plug it in to any
power outlet at an office at a home
office and because of that it has to be
really quiet so air-cooling we tried it
we tried some baffling we tried some
ducting to try to push the air through
and it just it was just loud read it out
actually we wanted to do not be heard at
all so we turned to water cooling and it
keeps everything you know 60 to 63
degrees C djx station is the only
workstation in the market that has 40
pews in it that are fully connected by a
env link so what we've done is basically
a fully connected quad each bridge ties
two of the pair to another pair so all
of them can talk to each other at much
higher speeds than they can over PCI
Express I mean it just it just came out
beautiful good job by the ID team and
mechanical and thermal all came together
to come up with a very clean system that
only has what it needs and doesn't have
anything extra the breadth of deep
learning software that people are
writing is really incredible there's new
kinds of neural networks all the time
there are different frameworks that have
different programming models that allow
people to try out new ideas and new ways
all of this creates a vastly complex
environment deject station brings all
that software together so that
researchers can get started immediately
with the full power of the hardware
whatever works on the server runs on the
station and they can run the station and
just port that color over and it
automatically run on the server it's the
same software and at some point if they
need additional resources they can still
go out to one of our CSP partners and
pull down the exact same framework from
the NVIDIA GPU cloud and they'd be up
and running in minutes every time we
ship a TGX station to our customers it
makes us really proud to think about
what they actually gonna do with
I'm really excited about the future of
AI because I think AI is going to make
our society better and our world a
better place to live and giving
researchers tools that they need to go
and build that future is really exciting
Title: NVIDIA GTC May 2020 Keynote Pt6: NVIDIA A100 Data Center GPU Based on NVIDIA Ampere Architecture
Publish_date: 2020-05-14
Length: 1426
Views: 248132
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/onbnb_D1wC8/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: onbnb_D1wC8

--- Transcript ---

modern data centers are complex the
number of workloads that has to run in a
modern data center is growing incredibly
from scale up applications training and
data analytics and data processing to
scale out applications inference and
conversational AI
to public cloud applications
high-performance computing cloud gaming
or even remote workstations the
architecture of modern data centers is
complex it first started with the
disaggregation of cpu servers and
storage servers and Mellanox is
high-speed networking makes that
possible the second thing that happened
was the acceleration of workloads and by
accelerating it the throughput increases
by orders of magnitude reducing the cost
however the type of accelerators we've
offered to data centers over the last
several years have been optimized for
different tasks one particular
accelerators designed for scale up we
call that V 100's XM's with envy link
another type of data center is really
designed for scale out T for
accelerators are used for inference and
some of it used for flexible clouds V
100 and PCI Express allows public cloud
users to be able to use it for training
or inference or high performance
computing and whatever application they
would choose it's designed really to be
flexible and yet the data centers really
want to be high performance as well as
flexible at the same time with all these
different types of configurations of
servers the ability to predict exactly
the amount of capacity you need for each
configuration is difficult meanwhile
utilization ultimately drives TCO and
there's an insatiable demand for
increasing the capacity of workloads as
well as driving the cost down in the
cloud datacenters clouds now represent
nearly a hundred billion dollar industry
growing at about 40 percent per year
into an IT infrastructure industry that
represents about a trillion dollars this
is the largest growth opportunity of the
computer industry cloud computing and so
it stands to reason that there's so much
focus to advance public clouds and cloud
data centers would it be amazing if we
could create an accelerator that Inc
the throughput of scale-up applications
as well as scale out applications and
yet completely fungible completely
flexible in one server architecture so
that independent of what workload comes
that one server architecture is able to
serve it the ability to create a
flexible high throughput acceleration
architecture is something we've been
pursuing for some time well ladies and
gentlemen I have something new that I
like to share with you it's called the
Nvidia a 100-hour brand new data center
GPU the Nvidia a 100 is based on an
architecture we call ampere and there's
several amazing breakthroughs that make
this possible the first is we're using
TSM C's 7 nanometer process that's been
optimized for Nvidia using a packaging
technology called COAS chip on wafer on
substrate 3d packaging technology which
puts the memory and the chip on the same
substrate which allows it to
interoperate incredibly fast and we're
connected to HBM to memory that now
provides for 1.5 terabytes of frame
buffer bandwidth this is the first
processor in history that comfortably
delivers over a terabyte per second of
bandwidth the second breakthrough of the
ampere GPU is the new tensor core
architecture and it has a new numerical
format tensor float32 and as the range
of F P 32 the precision of F P 16 you
input an F P 32 it process it with
tensorflow
32 and accumulates it in FP 32 as a
result no code change is necessary when
you train now for some people who are
ninjas it is possible to optimize for FP
16 but our experience is the vast
majority of the world simply trains in
FP 32 today and so with this new format
TF 32 and no code change all of a sudden
we can accelerate training tremendously
let me show you here on the left is V
100 Voltas FP 32 matrix operations and
on the right is a 110 sir core
accelerated TF 32 the speed-up is
extraordinary am pure has a new tensor
core acceleration for Sparsit
it takes advantage of the fact that most
neural networks are very heavily sparse
so it starts with a dense Network the
original network and in zeros L the
weights that are small or close to zero
and then it retrains that network as a
result this network can be compressed
two to one and using the same data
structure using the same pipeline we can
now effectively accelerate processing by
a factor of two let me show you the
performance here I'm going to show you
the performance of Volta which is today
stated the yard GPU this is the highest
performance GPU in the world today this
is the industry standard of deep
learning
the black is Volta the gray on top of
the black is the peak and the solid is
measured in a case of Volta V 100 FP 64
is 8 teraflops FP 32 is 16 teraflops
peak FP 16 125 and in 8 is 60 and the
gold will be the new a 100 and pure GPU
look at that a 100 FP 64 is 20 teraflops
a 100 TF 32 is a hundred and sixty
teraflops 10 times in the case of FP 16
310 in the case of an 8
625 peak now this is without sparsity
with sparsity you get another boost look
at the factor of 2 a 100 sparse TF 32 is
now 310 peak a 100 sparse FP 16 625 peak
and for n 8 the world's first processor
to achieve over 1 peda ops this is now
1.25 pops 1250 terra operations per
second the inference performance of a
100 is incredible now when we compare it
as an x-factor to Volta and remind you
this is the most advanced processor in
the world today the Volta numbers are
all normalized to basically 1 X the
training peak of a 100 sparse TF 32
nearly 20 times the performance of Volta
in the
of inference 20 times the peak of Volta
this is the greatest generation leap we
have ever experienced let me show it to
you oh boy ladies and gentlemen the
world's largest the world's largest
graphics card this is the a100 processor
board it is 50 pounds eight GPUs
connected by mblink 600 gigabytes per
second 6 mb switches 1 million drill
holes 1 kilometer of traces connecting
all of this over 30,000 components 50
pounds and dgx it moves 700 cubic feet
per minute the most transistors on one
computer the world has ever made ladies
and gentlemen the Nvidia a 100 system
board
that is just the technology marvel but
there's more ampere has a new
architecture we call MIG it stands for
multi instance GPU it's the ability to
turn one GPU into many you could have
one GPU you could have up to seven
independent GPUs or some combination in
between in the past we would have a
rocket ship with a very large payload
and while the payload is being filled up
the rocket ship is waiting but as soon
as the payload is filled up the rocket
ship flies the space like you couldn't
believe well with a 100 you have to
Billy to run it as one gigantic rocket
ship with a very large payload or you
could have seven independent rockets
each with a smaller payload but they
could take off as soon as you're ready
and so for inference or public cloud
instead of having one person
use a GPU fractionalize it create seven
different instances so that each one of
the customers could rent a smaller
computer you now have the flexibility to
do that
meg is going to have a profound impact
on how we architect data centers when I
talked earlier about how we would like
to have a universal unified server
architecture that allows us to scale up
as well as scale out and to be able to
configure it as the workload needs this
is exactly what we mean ampere is not
only incredibly fast for training not
only is it incredibly fast for inference
it also has the ability to fractionalize
and partition itself up into a large GPU
for scale-up applications or a whole
bunch of small GPUs to maximize scale
out whether it's for inference or public
clouds you now have the ability to have
one data center architecture for
acceleration that is flexible high
throughput and enables higher
utilization the performance is
incredible right out of the box this is
vert one of the largest models and most
important models today Bert training and
bird inference compared to Volta a
platform that has been refined and
optimized for three years now right out
of the box a 100 it's six times the
performance in training six times
transistor budget only increased about
70 percent now ampere is the largest
most complex processor the world has
ever made thousands of engineers worked
on it for several years and it came
together in this one incredible chip 70
percent more transistors with great
architecture delivered six times more
performance out of the box in a case of
inference it is seven times that
performs a Volta and much more over 12
times the performance of T fours now let
me show you ampere in a demo this demo
is a natural language understanding
model that includes speech recognition
not speech recognition of a human speech
recognition of a bird the question that
it has to answer is this what is the
native region of the bird that I'm
hearing first it has to understand the
question
it has to understand that
question has something to do with
hearing the sound understand the sound
classify the sound and then figure out
from what region is that bird located
and respond as quickly as possible in
the case of v100 this is the result
first of all it's a miracle in itself
that we have an application that can do
it at all the breakthrough of artificial
intelligence is pretty astounding this
next demo is a 100 with one MIG just one
of the seven MiG's could achieve the
same performance as an entire Volta the
world's state of the art GPU now what
happens if we put all the Minx to work
all seven MiG's
Wow a 100 with all seven MiG's could do
over 500 queries per second with Volta
we could do about 80 so ampere has seven
times the inference performance of Volta
this is pretty extraordinary ladies and
gentlemen invidious brand-new data
center GPU the a 100 DG X is our third
generation system it's the world's first
fully integrated AI system it was
designed to be the ultimate instrument
of AI researchers it's fully optimized
and you simply take it out of the box
plug it in and you have a
state-of-the-art development system for
AI now the previous generation of DG X's
were really optimized for training the
DG x a 100 our third generation this is
the first one that's unified in the
sense that you can use it for data
analytics you could use it for training
and you could also use it for inference
you could also split up this DG x and
share it among 56 different users at one
time each one of them could have the
benefit of an equivalent performance of
a Volta it is elastic for scale up or
scale out computing inside this machine
is nine Mellanox CX 6 virtual protocol
interconnect each one of the nexus 200
gigabits per second of network
capability do all 64 core AMD Rome CPU
with one terabyte of memory 8 Nvidia a
100 GPUs 6 nvidia MV switches the reason
for that is this we want every one of
the GPUs to be able to communicate with
each other simultaneously without
blocking and with the new MV links which
is 600 gigabytes per second the
cross-sectional bandwidth is about 4.8
terabytes per second it's like a
high-end switch integrated into dgx so
that all the GPUs could communicate with
each other simultaneously and it comes
with 15 terabytes of PCIe gen four mdme
solid-state drive this is the first
computer ever made that in onenote
exceeds 5 petaflop s-- of computing
capability dgx is a marvelous machine
and our creative team
using omniverse and the amazing
rendering capability of r-tx created a
short movie for you everything that
you're about to see is based off of the
original CAD design and its render photo
realistically using Nvidia's RTX let me
show it to you it's really quite amazing
[Music]
[Music]
the performance of this machine is
incredible in 810 peda ops peak FP 16 5
petaflop speak TF 32 for training 2.5
petaflop speak FP 64 for scientific
computing a hundred and fifty-six
teraflop speak this is an amazing level
of performance if you compare it to the
highest end servers $10,000 server dgx a
100 is a hundred and fifty times its
peak performance forty times the memory
bandwidth forty times the i/o bandwidth
a hundred and fifty high-end servers
that's a million and a half dollars
ladies and gentlemen the nvidia dgx a
100 is in full production and it's
available today at an amazing price of a
hundred and ninety nine thousand dollars
incredible performance incredible value
you could also buy it in the forum for
hyper scalars nvidia is an open
computing platform company we developed
systems so that we can fully integrate
new categories of products and engineer
the highest performance components we
also offer the components disaggregated
for all of our partners around the world
and so if you would like to build your
own hyper scale data center using nvidia
s-- hgx a 100 the carrier boards the
motherboards are available separately
let me show you what it's like when you
put a 100 year data center here we're
showing you a modern typical AI data
center there's a lot of DG X's inside
with vultus running in parallel for
training and for data analytics running
spark for example or inference they
typically run on CPUs and so this
particular data center has 50 DG x once
and 600 CPU systems for ai inference and
data processing 11 million dollar 630
kilowatts is approximately the going
price for
a state-of-the-art AI data center with
the a100 this is what it looks like boom
unbelievable this is the benefit of the
new architecture the combination of the
high throughput the MiG instances and
the ability to do data processing deep
learning and inference all in one
computing platform and the acceleration
software we develop from spark to
training all the way to inference we can
now combine all of those different
server architectures into one and by
doing so we reduced an 11 million dollar
datacenter into a 1 million dollar rack
28 kilowatts instead of 630 kilowatts a
tenth of the cost and one twentieth of
the power the more you buy the more you
save let's take a look at that one more
time this is before 11 million dollars
50 dgx once 600 CPU servers modern AI
data center ladies and gentlemen now
before and now incredible achievement
now let's take a look at another
algorithm the famous PageRank algorithm
there are hundreds of billions of web
pages and trillions of links the
PageRank algorithm crawls the Internet
and creates a gigantic graph of
hyperlinks and websites and analyzing it
to determine which one of the websites
is the most relevant we've taken a
publicly available database called the
common crawl data set this is only 2.6
terabytes of data it's a hundred twenty
eight billion edges the internet has
several hundred billion websites and
trillions of edges so this is a very
small fraction of that and to do that it
takes three thousand servers and 105
racks and these three thousand servers
can analyze the relevance of web pages
using the PageRank algorithm and deliver
fifty two billion edges per second this
is a state-of-the-art fifty two billion
edges per second now what you're about
to see is for dgx
a 100 s connected through MV linked into
essentially one giant dgx this giant dgx
has 32 GPUs effectively the performance
is incredible ladies and gentlemen this
is what it looks like before and this is
what it looks like now a reduction of 75
times in cost six hundred and eighty
eight billion edges per second it's
simultaneously 13 times the performance
and 175th the cost you got to see that
one more time before after ladies and
gentlemen the more you buy the more you
save the dgx super pod connects a
hundred and forty dgx a 100 systems
creating a 1120 a 100 computer it has a
hundred and seventy Mellanox quantum
InfiniBand switch to have the low as
possible
latency each one of the ports is 200
gigabits per second and together the
Mellanox InfiniBand Network fabric
sustains 280 terabytes per second 15
kilometers of optical cables nearly 10
miles which is one of the reasons why we
pre configure the networking for our
customers it's 700 petaflop s' of AI
performance the fastest supercomputer in
the world is about 300 pedophiles this
one pod delivers for AI performance
nearly twice that and this can be built
in just three weeks one of the first
deliveries of the D GX is to Nvidia and
we extended our Saturn 5 AI
supercomputer with dgx a 100 s the
Saturn 5 supercomputer is used by all of
our researchers to advance for example D
LSS 2.0 to do pre train models for
Jarvis or metropolis or Clara to do
collaboration research with researchers
and scientists around the world in
healthcare for example and we use it to
train models for our self-driving cars
and robots our supercomputer is super
important to us and nearly all of the
software platforms we do today has some
neural network component to it
investing in our own supercomputers
toward business Saturn 5 today has 1.8
EXA flops of performance the fastest
supercomputer in the world is about 300
pedo flops and so we have several
supercomputers inside the company that
we've been using to develop all of these
advances in AI and now with just this
extension with dgx a 100 Nvidia Saturn 5
will have 4.6 EXA flops of total AI
capacity that is just a gigantic jump
with this extension our researchers are
super excited about it the brand new
Nvidia a 100 datacenter GPU and the dgx
a 100 integrated AI supercomputer and of
course all of the software stacks that
go along with it it's available today at
a hundred and ninety nine thousand
dollars
Title: What Is Computer Vision & Why Does It Matter?
Publish_date: 2022-03-21
Length: 116
Views: 20520
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/OnTgbN3uXvw/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: OnTgbN3uXvw

--- Transcript ---

[Music]
computer vision is seeing an increasing
number of industry applications
monitoring and managing crop yield in
agriculture annotating and pinpointing
anomalies in health care facilitating
warehouse logistics and manufacturing
and safety and navigation for autonomous
vehicles
but what is computer vision and why does
it matter
simply put computer vision enables
devices to use human-like vision
capabilities broadly speaking computer
vision enables devices to perceive
objects for patterns in images or video
frames and uses that information for
further analysis or decision making
using techniques like detection
classification and segmentation
computer vision can also help synthesize
and generate new images and video
classify helmets for safety
segment parts of a road scene as
drivable and non-drivable space
and even morph and adjust highly
accurate high resolution images
computer vision uses data trained on
mathematical algorithms to analyze
interpret and process images and video
let's take classification for example
computer vision enabled devices take
images like this one of fido and analyze
the photo's pixels to extract features
and characterize the image to see if
there's a dog in the photo or not
and if it is actually fido himself or
not
based on images previously given
computer vision software helps people
solve the most difficult challenges and
supports everyday tasks including
identifying where a farmer needs to
water his crops and protect against
weeds
pinpointing where firefighters deploy to
extinguish a fire
and identifying bottlenecks and how to
improve production and packaging on an
assembly line
computer vision software is the enabling
technology to solve many industry
challenges whether you are new to the
field of computer vision or an expert
developer nvidia has the computer vision
software for you to start from scratch
integrate and customize into your
workflow and skill for deployment
for more on computer vision software
check out nvidia's computer vision
solutions landing page in the link below
Title: GeForce Garage: Antec 900 Series, Video 4 – How To Hand Lace Your Cables
Publish_date: 2015-10-23
Length: 771
Views: 66644
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/oP8WxllE9UM/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: oP8WxllE9UM

--- Transcript ---

hi I'm Andrew with Nvidia and you're
watching GeForce garage everyone who has
ever even attempted to build a PC has
had issues with cable management it's a
huge part of cleaning up the inside of
your case and today we have a special
guest John hands from nurdrage pros.com
and he's going to show us how to do some
awesome cable management so you're an
electrician by trade which means you
have a lot of experience with cables and
wires and all that kind of stuff
why don't you tell us about it well we
call it cable lacing it's a process
where you sew all the cables together
using a series of knots to basically
pull wires closer and closer together
and then you're able to tie several
cable assemblies to other tables not
only does it improve air flow to it but
it improves the aesthetics of the case
that sounds really cool let's get
weaving all right sounds awesome
boom
the difference between cable combs and
lacing cable combs look nice
lacing looks a little bit nicer there's
a difference between the two and more
than just the looks it also tends to to
hold a form a little bit better you know
where as you move these ones and go hold
a form but sometimes wires will pop out
and not stay exactly where you want when
you do cable lacing everything stays
exactly where you put it so here's a
basically all you need to do your own
set of lacing I stick with some
electrician scissors to wind up cutting
the wire on the ends after I'm done this
is probably one of my favorite tools is
small pair of gig or sewing scissors I
mainly use it for cutting of the string
and use some medical clamps to hold the
wire while you sew it it's like an extra
set of hands I usually have two because
when you're doing a 24 pin cable you
just need that many extra hands there's
actually another handy tool the
paperclip so what you want to use a
paperclip for is just going in between
the wires
it'll help grab some of the string and
pull it back through for you and it's
just that much easier you don't have to
try to feed it through and worry about
the string getting getting stuck we're
using a beetle on wildfire it is a bead
string you can use pretty much any wax
based string I choose this for its small
diameter and the fact that it holds a
very tight knot and that's pretty much
it as far as the tools go so let's get
started so the first step that we're
going to wind up doing is you want to
get all your cables pre sleeved you want
to leave the ends long to go where you
need to go as you bend around corners
your wire will actually start losing a
little bit on the outside and you'll
actually gain a little bit on the inside
so by the time you get down to the
connector some may have a lot of slacks
that may have a little bit of slack some
might not have any it might be too tight
so we leave these long and we do them
all at the end after we have the cables
formed sewn and put into the case the
next step is to figure out the
configuration that we wanted the cables
we've chosen to go with having it come
and spiral out of the case and lay flat
down and cover the SATA connectors as
they go into the motherboard we've
chosen with the graphics card to have
these ones
lay over the top flat and then come down
and into the power supply so the first
step is is we're going to take the
motherboard and your graphics card and
everything needs to be all set up and
ready we'll kind of plug them into place
and figure out where we want to be and
we're going to use some of these clamps
to hold the wires in place while we sew
the cables so your first stitch you can
do probably without it being in place
and then as you roll it you got to keep
in mind that this is going to roll so
we'll probably have a stitch that's
going to be coming across the top right
here and then when we roll it under
we'll probably have a stitch coming
across here so the stitches and usually
it's best to do them in a straight line
if you can you can actually use a piece
of wire or paperclip to pull the string
through you can use that as a gauge to
say okay I want to want to sew some here
I'm gonna come down I want to sew some
here so you hold up your gauge to go
down to the next one so on and so forth
all the way down and then you have a
nice even pattern all the way down so
now we have a cable we figured out the
layout you can always alter that as you
go as well once you get your first
stitch in there you may see something
you didn't see before and you'll be able
to delay them around how you see fit so
to start out we're gonna grab some
string we have a 24 pin it takes a
little bit more you can start out taking
as much as you want so here's the first
knot you're going to start with so you
make a loop at the end about the same
size here this is the quick way to do it
you can take two fingers and roll them
around put them together and there is
your first knot right there you're going
to wind up using so another way you can
do it and it'll accomplish the same knot
if you're not able to do that motion or
you're having trouble with anything is
you can take the string like this make
two even ends put the two ends here
through the other end so that it looks
like the other not as well now you also
may notice when I do my cables that a
lot of 24 pins there's actually a wire
missing out of there I'll actually throw
a dummy wire into here and that way you
can keep all the cables across keep them
nice and neat and uniform otherwise
you're going to be stuck sewing one wire
and that becomes a little bit more
difficult
so you're going to start out with your
first knot so you slide it through the
first two cables then you take these two
ends and you're going to slide them
through that knot that you just made and
then you're going to pull it tight and
you're going to pull it straight out you
can use two fingers here to hold it
tight to hold the wires and you want to
pull it out and away from where you're
going and since we're going to be sewing
it it's going to be rolling out this way
we want to pull it out toward us and
that's your first knot to cinch it down
you're going to do one more just regular
knot and pull it tight and now that knot
won't back out so now is what you want
to do is you want to roll the string
onto the back side are your cables
because that's where the majority of the
string is going to be that goes in
between all the different knots and
what's going to be seen is going to be
out front so we're going to pull the
string that was around the back we're
going to pull it around the next two
sets of wires now another key is you
want to try to keep the string from
crossing where you're going across it if
the string rotates you will notice it if
it's laid side by side you won't notice
it as much so you can kind of see how
the knot comes across right here and we
want to keep it flat and we want to keep
which one's on the top which one's on
the bottom step two is the next knot so
now we know which string is on the top
which one's on the bottom we're going to
call the one toward you being the top
and the one toward me being the bottom
you're going to take that string and
you're actually going to be feeding it
through the middle section in between
the wires so then we pull it from that
middle to the front of the cable again
pull that a little bit get a little bit
of the slack out and always pull away
from where you're tying to if you pull
this way it'll wind up loosening the
knot and this is the way that you want
to keep the knot tight and it's
basically just a series of knots that
all add up and tighten each other so we
do the same thing with the other one but
we go up so we took the one that was on
top here and we went down through the
middle between the first two wires here
we pulled it to the front side we took
the one from the bottom put it up
through and pulled it to the front now
we're going to take them both together
take our fingers here and pull them like
that
so now we started our first one so we
want to make you know get a nice tight
pole there now we want to take that top
wire again how they're separated top and
bottom and we want to feed it between
the two sets of wires that we just did
so we pull that one through and we pull
this one through to the back and now
we're going to pull that tight and again
using your fingers to hold it in place
and give it a nice firm pull and then
separating the string again turn the top
and the bottom and that step right there
repeats until you get to the end
so now we're on the last stitch so as
you saw me do with all the other
stitches I keep pulling it toward the
other stitches to keep it in a straight
line you want to kind of figure out
where you're going to be you want to
figure out where you're going to end and
this one is not too critical because
we're about ready to spiral two wires
and you won't even notice if there's any
slight deviation you can also use your
paperclip to move some of these around
now you won't have a lot of play and you
can actually pick some of this color off
of it so you got to be careful as you're
doing that and we have the last knot now
we want to cinch everything off double
knot it here it's small enough you'll
never see it especially if you're
putting it all on the backside then cut
it off flush and there's your first
stitch
and that's what we're going to do all
the way down the cable
so now we got the GPU cables all
stitched up now we want to sew the two
cables together just keep them all
uniformed
you just do that first not that I showed
you and you can actually take the tail
end this time to help pull it through
now you're going to pull it back down
and then use that first knot to cinch
everything together what we're doing
here is we're just we're tying in
between these first two sets of cables
on the top and the bottom and we're just
tying these together that same basic
knot which I actually use instead of zip
ties most mobility you won't see any zip
ties in you'll see the cables with one
of those knots so you do that first knot
that I showed you and you pull it tight
and then they don't go anywhere they
stay together I do one on this side then
I do one on this side and that's pretty
much all you need then you do the knot
to keep other knot from backing out
this will keep anything from moving
around on you at all it will help them
flow more and it gives them a stacked
look look so what you'll see after you
finish the other knot
pinch it together and from one you can
see they look more stacked we'll do that
with the other one too and that'll hold
any bundle of cables that you want as
well so when you roll them around just
take all the whole bundle get a nice and
tight all the way up to the power supply
after that it's pretty much a matter of
cutting these ends to length and putting
connectors on it so we're going to do
that and then we're going to see how
this looks in the case
Wow John that is super cool not only
does it look really clean but it's also
going to improve the airflow and the
aesthetic of our case so thanks so much
yeah no problem
normally we do this inside the case
makes it a little bit easier but for
demonstration purposes worked out well
well you definitely demonstrated it well
and thanks so much for coming on the
show thank you and don't forget guys to
check out nurdrage pros.com for john and
his fellow modders you can see their
work there on the next episode we're
going to use some cnc milling to create
some cool custom side badges for our
case and don't forget to check out all
the other cool videos and guides at
g-force comm slash garage you're
watching g-force garage the ultimate
resource center for designing building
and customizing your GTX PC if you're
looking for more awesome GTA videos be
sure to check the links right here right
here and of course if you haven't
subscribed to the g-force comm channel
click the subscribe button on the bottom
right hand corner of your screen also
known as on the left of the video corner
of the screen no big deal
and if you haven't liked us on Facebook
Boop click right there boop-boop
boop-boop
Title: Watch Dogs 2: PC Trailer – NVIDIA GameWorks
Publish_date: 2016-11-22
Length: 141
Views: 146154
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/orBxZ72nPBg/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: orBxZ72nPBg

--- Transcript ---

hi I'm Dominic gay senior producer on
watchdogs 2 for the PC version of the
game one of our priorities was to extend
our rendering technology to additional
graphic presets and features optimized
for both older and newer hardware we
also have been collaborating with Nvidia
when ansar recreation of San Francisco
through H Bao plus ambient occlusion
shading and rendering techniques add
depth to a scene immersive lighting and
details and shadows add realism to the
Bay Area
Nvidia's hybrid frustum traced shadows
technology allows for more realistic
shadows with a smooth transition from
hard to soft edges subtle and detailed
real-time shadows are also added to the
scene with the implementation of TxAA
aliasing is diminished and edges sharpen
san Francisco's most famous landmarks
are rendered in high detail
with n soul and videos Game Capture tool
you can even become a game photographer
and share 360 HD captures of your
favorite moments
we'll also get to experience watchdogs
too with various PC exclusive features
all available at launch no FPS cap high
fidelity 4k textures ray marched
bolometric plug multi-monitor support F
will be slider and many extra visual
enhancements the flop with multi GPU
support thanks for watching and see you
in San Francisco on November 29th
watchdogs - available on PC November
29th
Title: Einride and NVIDIA
Publish_date: 2018-03-29
Length: 93
Views: 11578
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/oREWhvCZAdc/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: oREWhvCZAdc

--- Transcript ---

Android is a transportation company and
my role in the company is to be in
charge of the self-driving technology in
the vehicles the tpod is a battery
electric freight vehicle which is
designed to be part of the system that
we offer to our customers the
self-driving is based on a number of
different sensors we're using radar
sliders in a bunch of cameras all these
sensors are needed to create a good view
of the surroundings on the vehicle but
they also generate tons of data so much
data then you need really powerful
processing onboard in order to cope with
all of that data and we're benefiting
from using the Nvidia drive platform in
order to in a safe way create a
perception system that's capable of
detecting all objects around the tpod
the Nvidia Drive platform is the only
platform out there that's both capable
processing the immense amounts of data
that our sensor generate but also do
that in a safe way the platform contains
sub processors which are designed
specifically for safety so when stuff
fails these are there to back you up
these can help you out of a tricky
situation
[Music]
creating a safe transport system which
is also sustainable is what I see as an
obligation that we have and that is
really the core of enron Decorah what we
do we're trying to create a better world
for the people tomorrow
Title: Predicting the Future with RNNs - NVIDIA DRIVE Labs Ep. 4
Publish_date: 2019-05-22
Length: 87
Views: 25864
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/OT_MxopvfQ0/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLCHfk3mMfusEMHUe754yu5Wq8WdBQ
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: OT_MxopvfQ0

--- Transcript ---

Today's DRIVE mission is about predicting
the future, specifically predicting the future
position and the velocity of dynamic obstacles
on the scene such as cars and pedestrians
using recurrent neural networks.
In this clip, the white boxes represent current
object positions, while the yellow boxes are
the recurrent neural network's predictions
on where these objects will be about half
a second in the future.
And to simplify visualization, predictions
are refreshed every half second.
We can see that as time passes, the white
boxes move towards the yellow ones in 2D image
space, which shows that the network is correctly
predicting future object motion.
And this information would help the car to
anticipate and adjust its trajectory as needed.
Here the network is predicting time to collision
information or TTC between the ego car and
objects on the scene.
The red boxes represent cars that are getting
closer to us and would have lower TTC or equivalently
higher urgency to avoid a collision, while
the white boxes represent cars that are moving
forward and about the same speed as us.
The green boxes represent those that are moving
further away.
And the number at the top of each box represents
the computed urgency, which mathematically,
is the inverse of the time to collision.
And here we see the same computation performed
for a scene with pedestrians.
And that was DRIVE mission 32 which we affectionately
refer to around here as project back to the
future.
Title: Customer Service Kiosk with NVIDIA Omniverse ACE for Project Tokkio
Publish_date: 2021-11-10
Length: 129
Views: 30279
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/ox2Cc88I-Os/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLAiCcy8kSYFL9znmnSowR7CB7ikog
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: ox2Cc88I-Os

--- Transcript ---

hi welcome to shannon's cafe how can i
help you
can i have a cheeseburger with a side of
onion rings please
would you be interested in our popular
toppings for cheeseburger like bacon and
fried onions yeah let's do it
done added cheeseburger with bacon and
fried onions what else
what protein options do you have
we have multiple options i recommend the
double protein burger would you like
that
do you have any vegetarian options
i can get you a black bean patty instead
would you like that
sure
okay i have added double protein burger
with black bean patty to your cart what
else
do you have any sides
we have these three vegetarian items
french fries onion rings and fried
pickles what would you like i'll have
the french fries instead of the onion
rings please
sure would you like some garlic on it
yes please
okay i have added french fries with
garlic to your cart what else
what salads do you offer
we have market salad cobb salad fruit
salad and side salad side salad is
popular with our cheeseburger
i'll do the side salad
all right it is sunny out there would
you like to add some drinks i recommend
our lemonade
sure i'll have a cola
we have two options
diet cola and regular cola which one
would you like i'll take the ladder
please
adding regular cola
anything else
that's it thank you
okay your total bill is 24.46
your food will be available shortly
thank you thank you thank you bye
Title: GTC 2017: NVIDIA News Summary
Publish_date: 2017-05-10
Length: 149
Views: 21497
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/OXM8rXDzSp0/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLDGyjILJ2jHgbZSf1KGoYR1J8G8Xg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: OXM8rXDzSp0

--- Transcript ---

hi I'm Greg Estes and I'm here at GTCC
and we're with 7,000 of the world's best
developers at GPC we've got more than
600 talks 300 of them on artificial
intelligence we've got all of these
sessions we've got posters we've got
works from some of the best minds in the
industry we've got all kinds of exhibits
many of things that haven't ever been
seen in the world before and of course
as with every GTC great news from our
keynote
[Music]
our biggest announcement is around Volta
our new architecture both it is built
from the ground up for deep learning
both for training and inferencing you'll
find Volta in the cloud you'll find
Volta in the data center with a Volta
version of our dgx one supercomputing
platform you'll even find it in a desk
side version with a new dgx station
Volta is going to be the essential tool
for deep learning developers going
forward we also had a lot of fun showing
project Isaac it's a virtual environment
as a robot simulator if you're going to
train robots using artificial
intelligence they're going to have to do
a lot of work to learn their tasks let's
say that we're putting a card together
you don't want a bunch of robots
breaking cars as they're trying to
figure out how to put the pieces
together correctly so what this does is
it creates a virtual environment and it
turns out robots can learn very
effectively in this virtual environment
then you take what they've learned and
you can put it into a real robot in the
real world so that when they're doing
the inference to put everything together
it works in the real world the way they
learn to do in the simulated world one
of the biggest announcements was in the
automotive sector where exploit on one
of the largest companies in the world
announced that they have selected our
Drive px platform as the basis for their
self-driving cars which will be coming
out in the next few years we also talked
about how we're taking Xavier which is
the module that goes inside our Drive px
and we're taking the deep learning
accelerator from Mac and we're open
sourcing it we've made our best
technology available for anyone which is
better for everybody that's just today
from what's been an amazing and huge GTC
for us already and there's more to come
so stay tuned
[Music]
Title: GPU Technology Conference Keynote Oct 2020 | Part 2: "Exploring Our World, Creating New Worlds"
Publish_date: 2020-10-05
Length: 305
Views: 242553
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/o_XeGyg2NIo/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: o_XeGyg2NIo

--- Transcript ---

Let's start with graphics.
At its core, computer graphics is about simulating
the physics of light and matter  to simulate reality.
Simulating reality is a great computer science challenge and computationally unbounded.
This is the driving force of our company.
20 years ago, NVIDIA introduced the programmable shading GPU.
The GPU revolutionized computer graphics.
Hundreds of millions of insatiable gamers
made the GPU the most advanced chip in the world.
NVIDIA GPU processing has increased a stunning 100,000-fold.
We recently launched Ampere.
It is the greatest generational leap ever.
We expected and prepared for huge demand
and the response was still staggering.
I know many of you are anxiously waiting for your new Ampere GPU.
It's totally understandable and I assure you that it is worth the wait.
Ampere is the fastest ramp in our history.
Everything we ship is instantly sold out.
We're working around the clock.
Your new GPU is coming.
You're going to love the Ampere GPU.
Ampere is the 2nd generation NVIDIA RTX.
Fusing programmable shading, ray tracing, and artificial intelligence - giving us photorealistic graphics
and the highest frame rates at the same time.
Once the holy grail of computer graphics,
ray tracing can now be standard.
If the last 20 years was amazing,
the next 20 will seem nothing short of science fiction.
The Metaverse is coming.
Metaverse, was first coined by Neal Stephenson in his 1992 science fiction novel, Snow Crash.
Humans, as avatars, and software agents interact in a 3D space.
A virtual reality successor to the internet.
With worlds like Minecraft and Fortnite,
we're already seeing the beginnings of the Metaverse. Though they seem like games today,
gamer inhabitants of these early Metaverses are building cities, gathering for concerts and events
and connecting with friends.
Future worlds will be photorealistic, obey
the laws of physics, or not, and inhabited
by human avatars and AI beings.
The metaverse is not only a place to game, we will create the future in these Metaverses
before actually downloading the blueprints
to be fab'd in the physical world.
Omniverse is NVIDIA's platform for simulation and collaboration.
Omniverse is built from the ground up for
the future.
Cloud-native, multi-GPU, photorealistic
with path tracing and material simulation,
and physically based, obeying the laws of physics
Omniverse is designed to connect many worlds.
Worlds like Epic's Unreal, Maya, Blender,
Adobe, Autodesk and others.
Omniverse allows designers, artists, creators, and even AIs using different tools,
in different worlds, to connect in a common world,
to collaborate,
to create a world together.
Omniverse is even a world where AI agents are created and learned.
A simulator for robots.
In Omniverse, everything is physically simulated.
It's a place where robots can learn how
to be robots,
just like they would in the real world.
Ladies and Gentlemen, NVIDIA Omniverse.
An open platform for collaboration and simulation.
NVIDIA Omniverse is in Open Beta now.
Title: Gears of War: Ultimate Edition Developer Interview at E3 2015
Publish_date: 2015-06-18
Length: 102
Views: 16639
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/P1z4nb_Ajlw/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: P1z4nb_Ajlw

--- Transcript ---

what are you doing here here's a warn on
PC is basically that gives you our
ultimate edition but taking up a notch
and make it even more over that what
we've done is taking this game that's
now almost ten years old and we've
rebuilt it from the ground up we've
replaced every yard asset we've redone
all the cinematics we've modernized in a
way that we'd wanted to be done if we
had the time in the power of like of the
Xbox one and now of the PC to bring to
that is that story going from the
console to PC is just harnessing the
power of dx12 I mean it's been amazing
you know the guy was back there in 2005
when we first working on the game and
then in 2007 we released the PC version
of years or one and really it was a port
that's not the case for Ultimate Edition
it's not a straight port it's actually
taking all that power using the power of
the X well getting 4k resolution which
I've seen which looks like you seem to
see the game at 4k we're just really
happy to have 60 frames per second on
the console which is what people expect
but now to be able unlock that and it's
having on the limited refresh rate I
just thought you to top into the machine
that you have mouse and keyboard support
be able to use the wireless controller
the wireless elite controller on PC and
then we're tapping into all the features
that Windows 10 has like xbox live game
DVR and all those sorts of features
okay let's roast this thing we'd like to
say it's the first at its best you know
it's this idea that this is speaking
that original story the thing that
launched a franchise and we're bringing
it back at its best
you
Title: The deadmau5 Project: Episode 1
Publish_date: 2016-07-18
Length: 374
Views: 129988
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/p2k2aZzX57E/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: p2k2aZzX57E

--- Transcript ---

Joel: Because i like computer games and
computers and playing around with graphics -
and technology in general -
it seemed natural that i would go into
the way of electronic music production.
On my days off from making music, I'm still at the same computer playing a video game.
You know, it's just this like all-encompassing thing.
I would spend my free time there; I would do my work there, and I would play games on it.
So, you know, computers have just been at the very core of my life. [Laughs]
Jules: The PC design project is centered around innovation in technology,
and what we can get our computers to do.
We're looking to partner with all kinds of creative innovators -
that can take what we make and infuse it in their projects to push it to the next level.
Kris: I've always been a fan of deadmau5 and his music,
and one day I was browsing through Twitter
and I came across a photo of Joel playing Project CARS on this little 24-inch screen and a small steering wheel.
I looked at that and I thought, "He and we can do way better."
So I reached out to Joel via Twitter and we started talking through this project,
which was initially just to build him this cool computer so that he could livestream for his fans,
and this quickly evolved into something much more.
Jules: deadmau5 from the beginning has been a hardcore gamer.
You can see by the tattoos on his arms.
Joel: If you like popping bubble wrap,
picking at scabs,
or the smell of gas,
you will love Diablo.
Right now I'm working my way up to a greater rift level 60.
Doing that is not going to pay for the game.
It's not going to give me back the 7,000 hours I've put into this game since the start of [Laughs] the launch of the game
and that's a true story.
Jules: That was the primary start of our relationship,
was you love gaming; we love gaming.
We're both heavy into pushing technology
and letting it drive our creativity.
Kris: When you go see a deadmau5 show, you're not just hearing his music.
You're seeing visuals that he's created and put together,
and that love for design is translating right over to this project.
Joel: Art -- art -- I like art.
I like toys that are artful -- arty toys.
This is a suit of armor.
It's actually wearable.
I really want to get in it one day, but that's like a whole day.
Kris: The goal of this project is to create an amazing, five-PC,
livestreaming studio that he can use to create content for his fans.
Joel: If you're going to have five boxes for a stream lab,
and you're going to play games on them and stuff like that,
really the most sensible thing to do is just get five
Labello Blanco boxes and just put them down there,
Have some monitors and stuff like that.
But we plus-oned it, to the point
"Hey, let's have some fun with it and come up with some really wacky designs, and like showcase them."
Kris: Joel's a huge automotive enthusiast,
and over the last couple of years he's been collecting a range of custom vehicles and super cars.
Joel: I'm doing a bit of training. I'm a bit of a car nut, as you may or may not know.
There's a track nearby, about an hour away, that's a pretty popular track,
so I kind of use this contraption to hone my lines and breakpoints, and shifting and all that stuff
as not to destroy a really good car on a track, much less kill myself.
Jules: So we wanted to take the personality of his cars and turn them into PCs.
Joel: I've got a lot of kooky cars, and I like standing out.
Kris: We decided that we wanted to create five beautiful PCs.
Four of them are themed after his cars, and one of them is just going to be crazy.
The art director on this project, Jules, has done an awesome job of pulling really key elements from each
vehicle and bringing them together into a really cohesive design.
Jules: Well, we wanted to give him multiple range, because he has so many cars.
We picked his four most iconic cars.
One is this new Rubicon.
There were two final elements that were really going to set this PC off.
One was this huge LED light, which spans the entire top of the roof.
It's also got this really cool red leather interior which offsets the black and white and gray camo.
So the idea was to make the inside of the PC the red leather, the outside the digital camo.
Joel: So I wake up and get down -- I'm like, Whoa, where did my computer go? [Laughter]
Yeah, I'm sold on that one.
Jules: Cool
Joel: This is a no-brainer - this one, eh?
Jules: Well [Laughter]
Another great one is the Lamborghini Puracán.
In itself it's an iconic car, iconic design, has great, really angular, aggressive lines.
All right, the Mono -
So going into the design phase we definitely wanted to make sure we hit all the details
and we were a little nervous, because we know he knows cars.
He knows the details. He's spec'd them. He's designed them. And it went great.
He had a lot of good feedback. He even called us on a few things that we didn't quite have right.
Joel: Yeah, that's a Lexus LFA, that's the rear end of one but yeah, yeah.
Jules: He knew exactly "Oh, that tailpipe is from a Lexus, not a BAC Mono,"
and, you know, of course we counter-corrected for that.
All right, so the last one we've got is the McLaren,
this design ended up to be a little bit more curvy and roundy, because this car's got those shapes.
Joel: Kind of what I was thinking though was like
if we could have each machine kind of like the same form factor,
so that way when you're putting them side-by-side,
you have this like same product, different color kind of vibe.
Jules: Should we put them all on wheels?
Joel: No, I wouldn't. [Laughter]
Jules: Give them rims and everything. [Laughter]
Joel: Rims /Facepalm
With Spinners, right?
Jules: Yeah, right. Give you a remote control.
Joel: So when you push it, it looks like it's going a whole lot faster than it actually is.
That's amazing.
Jules: And then the fifth case is the NVIDIA case that we're going to show you next time,
so that's going to incorporate --
Joel: The MCP?
Jules: That's right. [Laughter]
We wanted to really,
that's going to be a beacon of both what NVIDIA can do combined with,
you know, the kind of horsepower and what you want to use it for for your system at home.
Kris: We had some great meetings with Joel.
We have final designs in hand,
and we're really excited to see how this project takes form.
It's time to make some computers.
Title: NVIDIA Press Event at CES 2018 with NVIDIA CEO Jensen Huang
Publish_date: 2018-01-08
Length: 5573
Views: 143459
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/P3BjB5-Y4JM/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: P3BjB5-Y4JM

--- Transcript ---

I am a visionary seeing cars people
roads and the world around me
constantly aware of my surroundings in
the dark the rain and the snow I am a
learner
giving robots the skills to perform
human tasks
and mastering the art of driving by
watching others
I am a protector watching out for your
safety
caution just a moment all clear even
when others are not
I am a helper giving you a hand when
yours are full hello
and assisting the disabled in their
homes I am a navigator
planning a safer path
and showing the road I'm taking
I'm a creator
powering robots that make the things we
need
that create original works of art in the
style of the Masters I am even the
composer of the music you're hearing
I am a I brought to life by Nvidia deep
learning and brilliant minds everywhere
ladies and gentlemen please welcome
nvidia founder and CEO Jensen Huang
happy happy here welcome to CES it is
always exciting to kick off the new year
just wondering what we're gonna see at
CES and this year just like always we're
gonna see some amazing things and this
was just in the news five big things to
watch for at CES 2018 it's the year's
biggest tech tradeshow and this should
be jam-packed with announcements related
self-driving cars gaming pcs VR headsets
and much more well let's get started
because we're going to talk about all of
that and much more at our keynote
tonight so welcome everybody has
dedicated ourselves to a form of
computing called GPU computing 25 years
in the making we have dedicated our
entire company in developing one of the
most complex and most powerful forms of
computing because of GPU computing we're
able to synthesize virtual reality and
virtual worlds and with recent
breakthroughs and artificial
intelligence we're able to understand
these physical worlds our physical
worlds we have three primary business
drivers the first is gaming 100 billion
dollar industry it is larger than ever
more and more gamers coming in the types
of game you're you're able to play more
more genres and people are gaming in
more ways this is just really exciting
in a vibrant industry the second growth
driver for us is artificial intelligence
recent breakthroughs and deep learning
has made it possible for the very first
time for software to write software and
the software using deep learning
technology is able to write software
that no human is able to develop and as
a result we're able to now solve what
was previously unsolvable problems this
is one of the most exciting developments
in technology it's going to influence
every single industry that we know and
it is also one of our fastest growth
drivers by applying the technology and
with recent breakthroughs and deep
learning and developments in algorithm
we're finally able to apply deep
learning and AI to one of the most
difficult challenges of all
self-driving cars for a car to be able
to navigate through a really super
complicated world and do it very safely
it's just an incredible and a daunting
challenge as we will see today we're
making enormous progress this is one of
the largest industries in the world the
moving of people the moving of goods
including cars taxis mobile at mobility
as a service and trucking it represents
10 trillion dollars the single largest
and most impactful important industry on
the planet we now have an opportunity to
revolutionize that three fantastic
opportunities for us to apply our work
to revolutionize the industry and three
incredibly large and growing
opportunities GeForce is the world's
largest gaming platform 200 and 200
million gamers in every country in the
world it is the largest gaming platform
in the world it is also the most vibrant
and active gaming platform in the world
because it's a PC because it's an open
system it has the ability to continue to
be vibrant and evolve and adopt new
technologies we believe in a couple more
years the experience of VR is going to
get even better thinner smaller more
elegant head-mounted displays wireless
inside out tracking all of those
technologies are going to make VR even
more exciting Gaming is no longer just
about gaming it's a sport in fact it is
now the largest sport in the world it's
kind of amazing but yet it's perfectly
obvious virtual reality should be the
largest sport because virtual reality
can be any sport it could be a racing
game it could be a basketball game it
could be a football game any game you
could imagine virtual reality can make
it happen
600 million people more people than
watch and
fell football now enjoy eSports it is
the single largest sport in the world
and it's also the single largest social
platform in the world more people share
their experiences on YouTube on Twitch
by playing video games than any other
forum in just the time that we've made
possible GeForce experience for you to
be able to capture and share your
winning moments 1 billion people have
shared it on YouTube 1 billion people
unbelievable and then of course video
games is not just on your PC and in
front of your television video game is
now also mobile the fastest growing game
console in the US the fastest growing
game console in the US after 30 years of
making game consoles the Nintendo switch
was the fastest growing and the reason
for that by working so closely together
we were able to take advantage of our
incredible technology to make it
possible for them to create a game
console that is amazing and mobile at
the same time the Nvidia gaming platform
well this at this show we're gonna
announce 10 desktop new desktop gaming
platforms 3 new laptops and 3 amazing
displays we call it the big format
gaming display the BFG d big format
gaming display be sure to go look for it
today here
BFG d short pause the FG d the FG d big
format gaming display making it possible
for you Nana out and joy PC gaming and
enormous displays that are incredibly
fluid HDR g-sync silky-smooth gaming
experience like you've never had before
well one of the things that we've been
doing that's really fantastically
successful is called max Q we applied
all of our know-how and all of our
expertise
to take this incredibly powerful
processor and putting it into a super
super energy efficient form factor and
the result is called max Q and it
basically we left no technology and no
science out of this thing this is how a
gaming notebook this is what a gaming
notebook hey Marshall come on ladies and
gentlemen Marshall so this is what I
this is what I I think I was breathless
in anticipation of holding this this is
this is twenty pounds this is what a
high-end nvidia gaming notebook looks
like last year okay so this is 20 this
is excuse me 60 millimeters 10 pounds 60
millimeters and 10 pounds 10 pounds at
my age it feels like 50 all right so
this is this is a 60 millimeters 60
millimeters 10 pounds and this is 20
millimeters and five pounds okay so
before after before after okay can you
guys see this this four times the
performance of a macbook pro twice the
performance of the highest performance
game console no wires guys look at this
thing
max Q technology new architecture design
process engineering design engineering
system software thermal management power
supply design we tuned and tweaked and
optimized every little drop of power out
of this thing as a result inside you
have one of the most powerful GPUs in
the world in this little tiny thin
laptop and you have something that is
more powerful than that most powerful
game console right there in your hands
isn't amazing unbelievable so that's the
max Q + video max Q thank you
all right much a good job guys Marshall
gaming the reason why it continues to
grow is because production value
continues to grow and every time
production value grows you need more
technology to be able to synthesize
those virtual worlds more gamers are
coming into this market place if you
think about it in the generation before
me almost very few are gamers I was the
first generation gamer in the planet my
generation started a Nintendo revolution
however my children's children
generation everyone will be a gamer and
so in just three generations time no
gamers - everybody will be a gamer
someday every human will be a gamer this
is going to continue to be a very large
industry does it continue to grow and
will continue to evolve and people enjoy
in different ways it's now sports
it is also art it's a way to share your
passions with friends and people that
you know so Nvidia gaming large and
continuously growing market the second
growth driver for us is AI from
synthesizing virtual worlds - now the
breakthrough of deep learning able able
to understand our physical world
interpret and gain insight from it and
programs automatically that allows us to
take the necessary prediction or the
necessary action is truly a breakthrough
we were really fortunate to have started
focusing on deep learning years ago and
this last year the progress was
incredible
this last year we announced the single
most complex processor the world has
ever made this is called the Nvidia the
Tesla V 100 Volta 21 billion transistors
the largest chip the world has ever made
12 nanometer FinFET 3d stacked memory
using HBM - the fastest memories in the
world in this incredible module it runs
at a voltage of less than 1 volt and
right here 250 amps flow through that's
connectors these two connectors in the
back called MV link connects it to the
other eight processors and together it
makes the most compact supercomputer the
world's ever known
the Nvidia Volta it also includes a
brand new type of deep learning
instruction set called tensor core a
hundred and twenty teraflops of
performance a hundred and twenty-five
teraflops what that basically says is
that one single box with eight
processors delivers for the very first
time one petaflop of performance now one
petaflop sub performance basically puts
you way up there on the world's top 500
supercomputers it would take racks and
racks of supercomputers to be able to
deliver this level of performance using
traditional CPUs it's one of the reasons
why we've been so successful with our
GPUs and deep learning the second
contribution this year is called NVIDIA
GPU cloud the software stack of deep
learning is incredibly complicated it is
basically a supercomputing software
stack you have to use the software
framework to learn from data terabytes
and terabytes and terabytes of
to essentially learn what is the
essential and the critical insight in
that data for the software to write
software by itself they call them
frameworks there's a whole bunch of
different types of frameworks tensorflow
and theano and pi torch and and paddle
paddle
MX net there's a whole bunch of
different types of frameworks in the
world and these different frameworks are
developed by different people and also
developed for different types of network
development and for different phases of
deep learning development we've
containerized all of these software
making it possible for people to just go
to the cloud download the entire stack
which has been fully optimized fully
tuned fully test it and just run it in
the cloud you could download it from ng
see the NVIDIA GPU cloud and you can
download it into all of the cloud
service providers who have designed in
the Nvidia Tesla into their cloud
datacenter as it turns out every single
cloud datacenter in the world from
Google to Amazon to Microsoft Facebook
Baidu $0.10 Alibaba every single cloud
service provider in the world has
adopted this processor the very first
time in history one single architecture
has been adopted everywhere it has also
been adopted by every single data center
server manufacturer where there's a
Lenovo or IBM or Cisco or Dell or HP
every single server maker on the planet
can now make it available to you in your
data center go to NGC you decide which
one of the frameworks you'd like to use
register for one of the cloud service
providers by yourself a server with
Nvidia Tesla in it you download you
click downloads and off you're going
developing deep learning your networks
incredibly simple now if you would like
not
use one of the cloud service providers
and would like to own your own data
center but you don't have the technical
capability to do so or don't have the
time to do so we also containerized the
entire supercomputer into one server we
call it the DG x DG X is the most
complex one computer that has been built
in recent history this is the dgx
supercomputer eight processors one
petaflop s' connect it with MV link in a
3u server it comes right out of the box
all of the integrated software is ready
to go you pull it out of the box you
plug it in you have a supercomputer
ready for deep learning and if you would
like one on this side we call a dgx
station with four GPUs completely liquid
cooled completely quiet and so the
processor n GC software stack all of the
cloud service providers and all the
computer makers and the dgx station
makes it possible for you to do deep
learning for any cloud in any data
center and for any type of computer now
once you're done with the deep learning
development of the neural network you
have to optimize it for the computing
runtime we call inference of your
computer depending on whether you want
to do it in the cloud in your data
center for deep different types or it
may be embedded into a car that neural
network has to be optimized and the
optimizing compiler is called tensor RT
the performance improvement running the
output of that code the computational
graph that comes out of the framework
into tensor RT the optimization is
incredible another two four six times of
performance and when you're done with
that if you would like to do deep
learning on your own PC we have a brand
new computer brand new processor called
the Titan V and it is the most powerful
PC GPU in the world this is the time V
it was available just a few weeks ago
demand is fantastic
$3,000 the most powerful GPU for your PC
ever made and it replaces basically
supercomputers okay at the time V and so
thanks Paul
end-to-end development of deep learning
from the processor to the software stack
cloud service providers data centers
workstations inference optimizing
compilers into your PC from end to end
every framework anywhere in the world
everywhere in the world cloud data
center on Prem computers from training
to inference that was our achievement
last year the team did an enormous
amount of work as a result deep learning
researchers all over the world can now
get their hands on this supercomputer so
they can accelerate their development
well the reason why we're going so fast
is because the value proposition is
absolutely incredible
and this is give you an example of what
it takes to do something like inference
so this is 45,000 images per second it
takes four racks a hundred and sixty
CPUs in order to do 45,000 images per
second of resident 50 and you could do
so with a hundred and sixty CPUs which
consume 65,000 watts 65,000 watts now if
you were to use the platform that I just
described then it looks like this
forty five forty five thousand images
four racks a hundred sixty CPUs
sixty-five thousand watts or 45 thousand
images per second one single node with
eight GPUs and three thousand watts one
six the cost one twentieth the power
four racks reduced into one box that's
the reason why people are jumping on the
NVIDIA GPU for deep learning before
after before after and so the takeaway
is simple to take away simple the more
you buy the more money you save and so
if there's one thing that you remember
just just if you could just all of the
technical jargon is just nonsense you
just just remember the more you buy and
just go back and tell everybody the more
you buy from Jensen the more money you
save incredible savings I mean it's just
shocking
actually it's shocking um and so so uh
so that's what it looks like but let's
see what it feels like let's see what it
feels like and so we're gonna give you a
demonstration you know this is the
amazing thing about deep learning deep
learning we're gonna use this this this
program that's been trained to recognize
flowers and it's been right it's been
trained to recognize images now I just
want you to imagine this imagine there's
a whole bunch of binary numbers that
comes into your program and your job is
to figure out what kind of flower it is
what kind of flower it is and several
years I demonstrated for the very first
time to a good friend of mine this this
deep neural network for recognizing dogs
and it didn't do a very good job but but
I would like to to redeem myself today
by using it to recognize flowers and
the beautiful thing is that we're gonna
recognize flowers that you don't even
recognize so you have no clue whether
I'm right or not and so so this is so
this is a demonstration of recognition
where the audience has no idea whether
I'm going to be right or not
so but here's the amazing thing here's a
software that has been written that has
trained learned by itself how to
recognize these amazing species of
flowers from the ones and zeros that
comes into this network if you were to
figure out how to write this by software
with your own hands I have no idea where
to start I have no idea where to start
and so let's go ahead and Matt are you
back there let's let's let's give them a
demonstration so this is this is what
you're looking at here is called
inference to infer using this artificial
intelligence network and the name of
this network is called resident 50 to
infer what is the flower let's stop
somewhere so that we can actually see
can you um you know so that's a foxglove
that's a rose okay so far so good
so dililili i giant Arum Lily that one
petunia that one's easy
Frank imp Annie those words never came
out of my mouth before hell Argonia okay
Windflower
and so sword Lily I mean the the truth
is that that I could have put anything
up there you guys probably wouldn't know
the difference but but it's amazing it's
able to recognize with superhuman levels
of capabilities all of these different
deep different types of flowers what's
happening here is that this is running
on a CPU is the latest generation CPU
skylake has been trained on a framework
called tensorflow
this is using the latest generation CNN
network called ResNet 152 152 layers of
mathematics a hundred and fifty two
layers of mathematics and then it infers
that that is a common dandelion and it's
running at an image performance of
images of five images per second and so
basically five people on their cell
phone could have taken a picture of the
flower sends it up into the cloud and
within one sec
it's time one one thousand it comes back
one seconds time
okay billions and billions of cycles
trillions of cycles of electronic cycles
later it comes back and it says that's a
foxglove this obviously if we would have
shown this to you ten years ago or even
five years ago even three years ago this
would have been a miracle and quite
frankly it's still kind of isn't a
miracle but let's show you what it looks
like if we were to run it on the stack
that I told you about okay so this is on
so what's happening here we're taking
the exact same network we're running it
on one of these okay do you see that so
instead of one CPU we're running on on
one V 100 Tesla voltage GPU and it's
running at 911 images per second 911 11
images per second quite quite a bit
faster almost 200 times faster okay so
if we were now to put these eight into
one node math can you show that to us
all right
and so 7,000 of us up that we have
hundreds in this room so we invited all
of our friends and we took a picture of
a flower we sent it up to the cloud
instantly it would come back and tell us
what that what that flower is now the
amazing thing is this let's do some
simple math so we now have seven
thousand images per second to one node
of CPUs of servers is two CPUs so be
three thousand so if I divided by that
by two so one GPU node is now three
thousand five hundred times and if there
are 40 nodes per rack it would be close
to what is that 20 racks close to 20
racks and so what that would that
basically says is that this thing here
would take 20 racks okay so a server
with 40 rack observer with 40 different
servers would take 20 racks one two
three four five six seven eight nine ten
before I run out of breath okay right
here that entire stage would be filled
with computers and it could be replaced
with this that is the power of GPU for
deep learning so the message is very
very clear it's abundantly clear the
more you buy the more you say okay thank
you we accept all credit cards all right
the thing that's really amazing is this
obviously for image recognition for
speech recognition we've now achieved
the superhuman levels superhuman levels
and Vidya has been doing a lot of
research in artificial intelligence our
first mission our first mission is to
create the enabling platforms for every
developer on the planet whether they
want to do it in cloud and a data center
on Prem in a workstation and PC to be
able to advance deep learning we think
that this work is so incredibly
important and the type of problems we're
going to be able to solve the unsolvable
problems are so powerful that we have to
enable everybody everywhere well
meanwhile we also want to advance to
research ourselves and there's the
several these are already on the web and
so you can go look them up this is using
it for ray tracing ray tracing here all
those black dots are where the beams of
light haven't reached yet we could
actually use artificial intelligence to
guess to predict what is likely the
color that it would have produced it
would have produced and as a result we
sped up ray tracing by a factor of 10 we
taught a neural network to watch how
faces animate when it's talking as a
result we now have a self animating face
all you have to do is type into the
census you wouldn't have spoken and
comes out the other side a
virtual-reality character that's
animating perfectly we have the ability
to take a computer vision
computer-generated image and
resynthesize it into a photorealistic
image that looks like a photorealistic
city cloud image of a city and yet what
it started with was computer graphics
video game graphics we have the ability
to use this great technology called
generative adversarial networks to
networks one trying to fool the other
network by synthesizing and synthesizing
a virtual human the other network
determining whether it's real or not as
a result of the two of them battling
with each other synthesizing
synthesizing more and more images of
humans while the other one rejects it
both of them become better and better
and better at it to the point where
we're generating images of faces that
don't exist this is a virtual reality
generated image based on artificial
intelligence we can even write music we
recently launched a to celebrate the
latest Star Wars movie we disney and us
partnered together to create a series of
graphics cards based on the Star Wars
themes and to celebrate it we decided to
teach a new network how to compose music
like John Williams this neural network
has learned the music of conductors and
composers of history the Box the
Beethoven's and then now on top of it we
layered on top of it John Williams's
style and this is what they produced
is that amazing composed by an
artificial intelligence to mimic John
Williams
well you've now seen artificial
intelligence with the latest
breakthrough from deep learning is able
to achieve superhuman levels of computer
vision you already know that with all of
the assistance that you use from ok
Google the Siri it's now achieving
superhuman levels of speech recognition
we can now generate virtual characters
synthesize and teach them how to speak
and animate and in fact even generate
music we're now able to use software to
write software by itself in ways that we
can never imagine and achieve results
that you're now starting to see AI is
going to revolutionize so many
industries one of the most important
industries is going to revolutionize
this transportation Av autonomous
vehicles powered by deep learning in AI
is going to revolutionize driving no
doubt there are 82 million accidents a
year around the world that results in
fatalities excess of a million costs
Society half a trillion dollars a year
in damages we spend almost an hour a day
and commute if we just assume that an
average person makes 80 thousand dollars
a year one hour out of their eight-hour
working day represents a fair amount of
lost productivity not to mention if you
decide that you don't want to work could
you imagine having an hour free time to
be able to catch up so that when you
finally get home that you can spend
quality time with your families a V can
save millions of lives and frankly
trillions of dollars a V is potentially
the greatest revolution that will come
to this industry in over a hundred years
a V will also revolutionize mobility
services we already know there's plenty
of traffic jams
but the amazing thing is in another 10
plus years a little bit more than 10
years from now there'll be another
billion people that come into society a
billion people we're gonna is estimated
that we're gonna increase traffic by a
factor of three three times from the 15
trillion miles traveled each year to
over 40 trillion miles per year well if
every single car that we have Society
builds two or three parking lots in
anticipation of us it's not possible for
us to sustain a billion more people and
this is where the change and the
technology revolution that's happening
and social behavior changing with paying
as you go it's going to be incredibly
important we believe that a V can
eventually drive the cost per mile of
autonomous vehicles to essentially the
same level if not below that of owner
owned cars self-driving cars and so when
that happens it's possible we believe
that AV could revolutionize mobility
services not to mention just the
imagination of what it could mean
you know what would a car be like if
it's not something you get into to get
to your destination but it is the
destination if I had a car that looks
like that I would love to spend an hour
a day in it if not more amy is going to
revolutionize trucking three and a half
million truckers each year transport ten
and a half tons seven hundred and fifty
billion dollars with the goods across
the United States they're allowed to
drive 11 hours a day with ten hours off
eleven now eleven on ten off recently
with electronic logging devices mandated
by law the day that it was turned on all
of a sudden the productivity of the
trucking industry decrease by a factor
of ten to twelve percent that basically
says
because there aren't enough truckers and
because we're experiencing the
e-commerce explosion the Amazon effect
where you used to go and pick up your
Goods at the stores because you went to
buy it now you bought it on your phone
and it's automatically comes to your
house somebody had to bring those goods
to your house because of those changes
in how we consume the amount truck
drivers around the world is simply
simply limiting tend to fit 10 to 12
percent loss in productivity
could you imagine multiplying that by
750 billion dollars per year and so
maybe here with autonomous vehicles and
level 4 capability for several hours on
the road we could make it possible for
the truck drivers to take a rest and
extending their range keeping up with
the enormous demand of the Amazon effect
trucking could be revolutionized by
autonomous vehicles now at the core of
all this is all the technology that
we're all talking about and there's
there's a in order to make autonomous
vehicles possible you have to solve this
computing problem this incredible
computing problem the largest scale
computing problem is kind from the
bottom all the way to the top this is a
processor architecture problem a
architect and algorithm problem a
systems problem a cloud computing
problem a car system problem a sensor
problem the number of challenges
necessary to solve in order for the
automotive industry to bring a V to the
world is just utterly daunting now we've
built PCs laptops game consoles
supercomputers and I can tell you with
that exception building a computer for
autonomous vehicles is of a level of
complexity the world has never known
this computer is on all the time
monitoring all of the sensors that's
coming at it it can never fail it can
never fail because lives are at stake
and it has to make the right decision
running software the world has never
known how to write
incredible complexity well with
everything like that because the
opportunity is so great because the
contribution we can make the society is
so enormous we just have to tear down
one step at a time and so the first step
is we've got to build the fundamental
processor and so today we're announcing
that the world's first autonomous
machine processor we call the drive
Xavier the silicon is back it is working
incredibly well and we're ready to
sample it in q1 it's the world's first
autonomous machine processor now whereas
this chip here is the largest R&D effort
and the largest chip the world has ever
made called Volta 512 of those
processors fit into another monster chip
in my other pocket I am carrying in my
back pockets ladies and gentlemen
several billion dollars of R&D
investment one here at one here look at
this chip look how big it is this is the
largest SOC the world has ever made look
at that thing the largest SOC the
world's ever made in total 8000
engineering years to build this SOC let
me tell you a little bit about it so
this SOC yeah we could we could turn
that off real quick for a second this
SOC the most complex SOC ever made nine
billion transistors 350 millimeters
squared made from 12 nanometer FinFET
8,000 engineering years as I mentioned
but the important thing is this because
of functional safety necessaries we
wanted to make sure that we have the
ability to have redundancy and diversity
and almost everything that we do
redundancy and diversity and almost
everything that we do one of the things
that's really important about functional
safety safety is that if a fault
is discovered inside your inside your
car it will continue to operate
incredibly well and so how is it
possible to build a computer that when a
fault is detected it continues to
operate well well the way to do that of
course we done and see diversity an
enormous amount of other technologies
that are mentioned now inside this chip
let me just very quickly mention it
there eight CPU cores based on 64-bit
arm completely custom 10 wide super
superscalar 2,700 spec in basically
approximately a server on a chip all
with ECC all with parity it also has
this mode called dual execution it runs
everything twice it runs everything
twice without consuming twice the
horsepower using this clever technique
called deal execution voltage GPU our
latest generation GPU has the ability to
process information in 32-bit
floating-point 16-bit flowing point an
8-bit integer so that the deep neural
networks depending on the type of
detection you want to do the type of
deep learning network it will all just
run one of the things that we know for
certain is because every single deep
learning that we're going to planet that
we know of today is being developed on
architecture we know that it will run on
our architecture there is no question
whatsoever every network that I know of
will run on this architecture 512 CUDA
cores which with this 20 tensor core
tops-20
ter ops I'll show you what that means in
just a second we also have computer
vision processor that has the ability to
do stereo disparity basically do the
processing of the parallax between two
cameras to calculate things like depth
optical flow how fast our feature is
moving in what direction are they moving
it's called optical flow and it does
image processing we have the ability to
encode video for every single camera 6
plus 2 plus inside the cabin for every
single camera and high dynamic range if
something were to happen okay so
basically your black box except every
single camera is captured we have the
ability to process in full HD are from
every single camera from our ISP 1.5
per second Giga pixels per second a
camera has 24 megapixels think of that
1.5 gigapixels and then we have a brand
new accelerator called a deep learning
accelerator for the functionalities that
we're very very confident about and we
just want to have the lowest possible
but lowest possible power or the highest
possible energy efficiency and we have
tenth air ops of that all supported by
256 bit lpddr4 let me show you what the
computer looks like hey Paul which side
are we on
okay all right well this is a record
this is a rigorous job lot of caring all
right so this is our Drive px - this is
our Drive px - not this this is our
Drive px - this is the computer that all
of the developers have been developing
on the drive px 2 is for chips -
Parker's and - Pascal GPUs for chips
okay for chips 24 ter ops 24 ter ops at
the time that we delivered this to our
developers this was the fastest thing
you could put in a form factor like this
this is 30 tops this thing versus this
thing 300 Watts 30 watts a lot of chips
one chip really heavy barely feel it
and this is what you get for a few
billion dollars of Rd well worth it the
world's first processor for a ton of
species this let me put that on my this
is the way you carry this and this goes
into your car okay so this power your
future level four and level three
autonomous vehicles one single processor
you know what would be amazing so the
silicon came back two weeks ago the
silicon came back two weeks ago and all
of the developers including ourselves
have been developing our software on top
of this box the beautiful thing about an
architectural II compatible strategy the
architecture here is CUDA the
architecture here is CUDA the
architecture here is based completely on
NVIDIA the architecture here is based on
Nvidia
well this we've we've been developing on
for almost two years maybe three and
this came back two weeks ago would you
like to see this running well it's it's
only been two weeks so anything could
happen
all right so mark Gary you guys want to
show it to them
anything could happen all right any
where am I what's going on we don't
rehearse that's what happens okay am i
standing in front of something ladies
and gentlemen that right there is the
drive xavier the world's first
autonomous machine processor running the
entire software stack cuda tensor RT the
entire drive work stack and the entire
drive AV stack of perception and
localization and ego motion and planning
all running there in just two weeks guys
that's the power of architectural
compatibility so ladies and gentlemen
and video drive Xavier and we will be
sampling it in q1 and we should be in
production before the end of the year
okay
the processor is just the first step on
top the reason why we know so much about
what is necessary inside that processor
it's because we've been developing the
entire software stack of a self-driving
car all along hundreds of people have
been working on the self-driving stack
from Eagle perception Eagle motion
localization against all of the HD maps
using lidar using computer vision using
radar path planning end-to-end stack
they've been working on it and it's just
an enormous amount of software it's just
an enormous amount of software a whole
bunch of technology that's never been
development for a whole bunch of
software the world the world's never
written before and over the holidays
over the holidays Jenny Chen one of our
engineers got into the car and drove
eight miles without touching the
steering wheel Holmdel New Jersey
December 19th he went eight miles 23
intersections 23 intersections look at
this no helmet
23 intersections 8 hard turns to stop
signs you guys want to see this alright
let's take a look
guys no hands
look mom no hands that's that's so great
and so the computer the processor the
system all of the operating system the
system software the never done before
algorithms all the way from perception
the ego motion a localization the path
planning integration into the car this
is a giant step for autonomous vehicles
we're announcing today that by doing ZF
Baidu as you know is the internet giant
of China and Zetas have one of the
world's leading Tier one OMS has
selected the Nvidia Drive Xavier for
their AV computing platform development
in China this is this is a as you know
by doing making incredible efforts and
into incredible progress in China every
single car made has to be China
compatible it is the largest market in
the world and yet developing in China is
very different than developing in the
West we created an open platform which
makes it possible for us to develop the
driving stack for the west and a partner
with Baidu
to develop the driving stack for China
as a result every car that is
manufactured with the nvidia dr xavier
and a drive AV stack has an opportunity
to be able to ship with excellent
driving capability autonomous driving
capability for both sides of the ocean
in the west as well as in china super
excited about the Baidu announcement by
doing Zeff thank you
level-4 driving is incredibly hard
however if there are places where the
car goes that the car doesn't know how
to drive by itself you still have a
passenger to drive the car however for a
taxi service for mobility as a service
obviously without a driver in the car
that option is not there and so every
single possibility has to be considered
well the way to improve your odds for n
statistics for places you can go in
different conditions you can experience
and drive and navigate the more
computation capability you have the more
sensor backups you have the more sensor
redundancy you have and sensor types
that you have the better off you're
going to be and so whereas the level 4
could benefit from can be powered by one
and video drive in order to do it for a
robot taxi we're going to need a lot
more performance and so what we've
created for the level 5 market and are
all of our level 5 robot taxi partners
is the Pegasus the Nvidia drive Pegasus
it is powered by these two Xavier that
I've already mentioned and look at these
powerful GPUs that are on it ok so this
is a magical 400 watts just 400 watts
and with two either one or two you
should be able to power a robot taxi a
level five driverless car level five
driverless car now the amazing thing is
this this we're going to be sampling
this year and hopefully sampling very
shortly this this box right here one
year ago one year ago this is now one
petaflop s' but one year ago using our
Pascal P 100 it was 300 tops 300 Terra
op operations for deep learning this is
320 it replaced an entire supercomputer
in just one motherboard and 400 watts
the incredible progress and the
breakthrough of our tensor core
instruction and the voltage GPU so this
is the Nvidia drive Pegasus I'm super
excited to announce that one of the most
exciting companies in the startup
industry a company that was started by
Chris Urmson and founders that worked on
the Google self-driving car and the
DARPA challenge they started a company
called Aurora they recently announced
that they're partnering with VW and
partnering with Hyundai to build their
next generation autonomous vehicles and
they have selected Nvidia's technology
and we're partnering together to build
the autonomous vehicle computing
platform so let's put around I'm also
delighted to announce that uber the
company that put mobility as a service
on the map has chosen Nvidia technology
and we're partnered together to create
future autonomous self-driving boobers
self-driving Ebers guys
sensors all over the place lidar all
over the place these cars eventually
will be able to drive without without a
driver at all and hopefully reduce the
cost of mobility below that of owning a
car completely well these these
platforms these platforms are not only
powerful as you can see they're also
open we developed the entire stack of
autonomous vehicle software so that we
can offer it to people who would like
not to build their own software or we
offer it to all of our partners in whole
or in pieces so that they could take
advantage of everything we have and
differentiate into places where they
would like where they feel that they
could do an even better job and and with
the world being so large ten trillion
dollar industry having a whole lot of
people who are developing on the
platform so that we can serve every nook
and cranny of the transportation
industry and enable autonomous vehicle
capabilities is a fantastic thing and so
we created this open platform and
working with partners all over the world
last year at this time we had just a
little over 200 software partners
working with us 200 development partners
working with us today we have over 300
over 300 partners are now developing
their car truck mobility service
computers for as a supplier their
mapping systems their sensor Suites
maybe they're starting a company or
they're doing fundamental research
whether it's Berkeley MIT Stanford
companies all over the world developers
all over the world are jumping on top of
the drive platform because it's open and
they could develop and create the future
of autonomous systems and so I want to
thank all of the development partners
for embracing the drive platform thank
you
functionality is plenty challenging the
performance of these computers the
algorithms that have never been done
before the large-scale system
integration with all of the different
configurations of sensors is so complex
it is the most complex development we
have ever done and yet that's just the
tip of the iceberg ultimately the most
important feature of a self-driving car
is not that it drives by itself the most
important feature is actually safety how
can you create a computer that in the
event of a fault continues to operate
safely for all the passengers involved
that technology methodology culture
tools software system design that entire
system in order to enable functional
safety extraordinarily complex
no system that has been functionally
safe to the level of a Saudi has ever
been this complex before people who are
in the airline business and people who
make airplanes are starting to notice
the complexity of the work that we're
doing and starting to knock on our door
because they would like to take
advantage of this incredible technology
as we develop it high-performance system
navigating through a very complex world
and yet completely functional safe it
requires a holistic system approach from
culture to technology to tools our goal
is to achieve ISO level 26262
a cell D capability basically the
highest level functionality of function
of functional safety I'm going to show
you the system in just a second it
includes everything from processor
design every single every single core
every single IP in our chip has been
touched by functional
safety requirements it's one of the
reasons why it's so powerful that we
design every single core ourselves from
our CPU core full-custom our GPU core of
course full custom computer vision core
deep learning accelerator our ISP our
video encoder our video decoder all of
our image processing systems the entire
SOC we have the ability to achieve
traceability for as long as we shall
live if something were to happen we can
trace it all the way back to its source
and continue to improve and mitigate for
the future there are two companies in
the system software area when we partner
with we've achieved levels of acyl D
capability blackberry with Q and X a
real-time operating system that has
already been certified ISO 26262 and
t-t-tech a time trigger instead of event
event driven operating system framework
for applications to keep applications
from stepping on top of each other TG
Tech and Q and X the two of them has
worked with us to integrate their
software on top of ours and with ours to
achieve
Auto sorry now building the computer
that in the event of a fault could
mitigate and control itself is one thing
but we want to make sure that the
computer is designed properly that is
designed to perform properly for its
intended function well the way to do
that of course is to test a car but
unfortunately the number of miles that
the diversity of experiences the
complexity of all the different worlds
the weather conditions it is simply
impossible to experience and put your
car on the road to be able to test every
single condition necessary on the other
hand we would like to be able to control
and have a repeatable environment so
that every single time we develop
software and improve software we can
test it against our entire regression
suite the only answer is really
simulation but how do you simulate a
self-driving car without a virtual
reality environment well it turns out we
know something about virtual reality it
turns out we know
something about computer graphics and so
we've turned we developed one of the
most complex simulation environments
we've ever done which basically allows
us to either with software models of our
entire driving stack that you just saw
or a hardware model of our entire
software of our stack and run it inside
a virtual reality simulator we call it
auto sim well I'm about to show you is
this this is auto sim a virtual reality
world simulator for your car it runs on
that computer that supercomputer the
goal is we would run it at super
real-time we would run this car in this
world so fast that we can cover billions
and billions and billions of miles over
time and we would like to be able to run
it against two types of models either
running it on the dgx that again with a
software model or run it on a drive px
excuse me Paul this is where you go like
this or you can run it on a Nvidia Drive
Nvidia Drive for example like the Nvidia
drive ok connected running the entire
software stack connected to that machine
running a virtual world can you guys see
that okay so this is hardware in the
loop and so why don't we take a look at
that mark hi Jenson hey Mark this is
Mark Daly guys all right so why don't
you take it away so ok take it away yeah
well so this is a little configuration
of our virtual environment we want to
show you that we can change some of the
parameters of our world and some of the
parameters of our car so let's play with
a world a little bit first I'm just
gonna just do a few fun little changes
and move the Sun let's bring it down
make it a little closer to a sunset get
a nice little glare off the road and one
of the
I love to do we can't do on planet Earth
but we can do in our simulator let's
move the Sun this way we can cast
shadows across the road that plays a lot
of havoc with sensor sometimes or cast
them the other way or put a nice big
glare in the driving eyes a nice
specular highlight coming off the road
and as you as you know one of the things
that lidar Zin cameras are most scared
of is direct sunlight okay alright next
okay so that's our environment let's
let's jump over and configure our car
now we're looking out if you look on the
right side of the screen we got four
cameras hooked up to our car and we're
looking out the front camera right now
let me run over and play around with
that front camera a little bit so first
of all I could simply move it anywhere I
want to in the environment over to the
driver side over the passenger side
maybe I want it toward the front of the
grill a car and maybe maybe you're using
exactly the same chassis in the same
computing platform but to design the
industrial design of the car is a little
different and so the placement of the
camera is a little different and before
you go and build that car you would like
to be able to configure two cameras
accordingly and run quick simulations to
make sure that against the billions of
miles that you already collected up it's
gonna work that's right and let's move
over here and let's configure what are
the other sensors here's here's my
right-hand camera okay one second right
now it's focused almost directly to the
side of the car but I might want to look
a little further toward that blind spot
toward the rear of the car I can simply
rotate that camera and boom I'm looking
at the rear of the car okay that's cool
let's run it okay now that's our
environment let's run the simulator okay
remember this is the simulator and that
is the Nvidia Drive now remember this
because the architecture of Nvidia Drive
and the architecture of that
supercomputer is identical it's binary
exactly the same so we could take the
entire software stack that runs on this
and run it inside that software into
loop
does that make sense okay this is one of
the great things that we can do as a
result of using an architectural
thoughtful approach to developing the
self-driving car all right go all right
on this side is the simulator on that
side it's the car driving by itself
inside the simulator the entire stack is
exactly the same it detects lines the
text cars detects lanes car signs
basically exactly the same software
stack okay mark and once again one of
the beauties of the simulated world is
we can set up some kind of dangerous
situations you do not want to do on the
on the real road we got our friend
merging here we can see that our auto
driver hit the brakes as this guy made a
knot to kind of move in a simulated
world does that software in the loop can
you guys do hardware in the loop of
course we can exactly the same software
tool exactly the same software stack
we're now simply going to replace the
software module and slide in this
hardware module okay so let's switch
over to to see it okay so now we're
actually running on a drive px 2 as the
hardware in the loop and the simulator
is showing this virtual world we found a
new driver so actually interactively
inside of the simulator driving another
car manually driving it okay now you can
see that this is RGB the reason why the
reason why hey Curtis the reason the
reason why the reason why you see it on
on this screen in orange is because the
color space is different and and that's
the beautiful thing the fact that you
saw it differently says that's exactly
the way that drive was seeing it and the
whole drive AV stack was seeing it we
change the colors color spectrum
that we can extend our low-light
capability
okay so there's some colors that just
simply look different but as a result we
have much better high dynamic range and
so Kurtis is basically driving the car
in the simulator and dr is responding to
it over here do you see every time
Kurtis gets in front our automated
driving system applies the brakes
meanwhile we continue to stay in the
lane and mind our own business and
continuing to drive while Kurtis enjoys
himself and so what you're seeing here
what you see here is something that we
could of course pre record and we can
capture capture these different
scenarios and put it away and and
whenever we have new software stacks we
can run it against this entire scenario
in super real-time and bam we know that
whatever improvements we've made has no
regressions okay one of the most
important things we're working on is
functional safety functional safety
includes processor designs all that we
done see the resilience capability
associated with it from ECC to
redundancy backbones and clock checks
and voltage checks and all built in
self-test and the chip is basically a
tester and test by itself every one of
our software stack is going to be ISO
26262 fide and one of the things that
that's never been done before is to use
a two iso certify a deep learning
compiler and a deep learning runtime a
neural net has never been ISO 26262 five
before and we're going down that process
to make sure that it's completely ISO
26262 fide as a result we have a
computer that could detect faults
whether its software hard and mitigate
however in order for us to be truly
functional safe we have to have a robust
testing and validation strategy and
that's why the simulator comes involved
and the type of simulators you want to
create for designing something like a
mechanical system is what you're seeing
here a virtual reality simulator that
has a software in loop as well as
hardware
the loop in our case completely binary
compatible so that we don't have to
change the code before we test it if you
have to change the code before you test
it and something were to happen where
did you insert default was it in the
code or from the things that you test
have you changed and so the ability to
be able to software run it in the
software binary way and to know that the
results can be exactly the same way it's
one of the fundamental advantages of our
approach okay
so functional safety thank you very much
mark
thank you Jenson this computer this
computer does three basic things sensor
input sensor input from that sensor
input and sensor fusion it's able to
perceive the environment reason about
what to do and take action path plan
okay so perceive action take action a
reason and then take action
another way of saying in self-driving
car speak perception localization and
path planning well it turns out that
those three steps are the fundamental
steps of almost any autonomous machine
and one of the things that we would like
to do is we'd like to turn the whole car
into an autonomous machine the whole car
your car in the future should
essentially become an AI and the reason
for that is because it has sensors all
round it has cameras all rounded
ultrasonics all around its kind of light
are all around it
it's got cameras all around the inside
of it and you need the AI to be in the
car
because it's not possible for the cloud
to understand the context around you the
cloud is not going to see all the
cameras the cloud is not going to see
all the passengers in the car the cloud
is not going to be able to see where's
my eyes looking where's my head pointed
what's the traffic around the car who's
walking up to the car what face is
looking at the car and so those kind of
contextual awareness capability has to
be resident in the car and as a result
we have to turn the car into a eye
now in the future every car is gonna be
self-driving there will be millions
a hundred million cars built each year
because of the number of miles that we
have to deliver for mobility as a
service there will be millions millions
of robot taxis built and several hundred
thousand trucks built each year every
single one of them will have autonomous
capability both to either drive by
itself without a driver or to augment
the capability of the driver
well that baseline capability is the
future of Av but on top of it we believe
what's going to really define your
driving experience is the AI that the
car companies create for the car your
driving experience is going to be
defined by the industrial design the
wonderful interiors that they create and
the AIS that is written and so what
we've done is we created an SDK we
created a platform that allows
artificial intelligence capabilities to
be exposed to application developers
it has surround perception speech
recognition speech synthesis in the
future you should have a conversation
really just basically at human rates be
able to cut each other off and the car
will still be able to understand exactly
what you're talking about
they'll have natural language
understanding understand what you meant
open the window it understands which
window to open turn it up it'll
understand that it probably meant you
probably meant the radio eye tracking
know where you're looking and compared
to what's happening outside to make sure
that you're looking in harm's way head
pose making sure that between the head
pose and and where your eyes are looking
that you're looking in the direction of
the points of interest gesture
recognition you may decide just oh hold
it down you know just like that and
it'll know exactly what you meant all of
that technology all of that capability
powered by deep learning in AI are going
to be exposed because this capability is
already in the in the system this
computer is able to perform the deep
learning capability necessary for all of
these all of these incredible AI
abilities and so this is part
our system called drive IX again exactly
the same computer exactly the same
computer you can run it on the same
computer or you can run it on another
processor another Drive xavier and the
second drive xavier could be the backup
for the first driving computer and that
runs to drive IX well if i had all of
this capability all of the surround
capability and sensors and we have
computer vision capability and i know
where i am
i've localized myself i can track
everything around me and i've got
amazing computer graphics you would
think that I could take the way that we
engage cars to the next level in the
future we know that the way we engage
computers in the future will be based on
voice based on gesture based on facial
recognition and of course it will
communicate to us based on augmented
reality and so today we're announcing a
brand new platform called the Nvidia
drive a our it's a software stack that
runs on exactly the same computer we
know that augmented reality is going to
define the future of computer interfaces
it's going to be if you had a car that
you ship in five years time you will
simply not understand if it can't have a
conversation with you in five years time
you will simply not understand if it
doesn't have a mental reality capability
you should be able to look right out
either a head mount display or the
infotainment system and graphics that
highlights the things that it sees and
the points of interest are perfectly
registered to it it almost looks like
the computer graphics is right there in
the environment well we're putting the
whole system together you're gonna have
to take my word for it that the next
time I show it to you is gonna be a lot
better that was a joke it's pretty good
now and so why don't we show it to you
hey Justin you were supposed to get
ready
well I'll get ready now well you get
tired you get ready you get ready okay
so this is what we're gonna do we're
gonna show you something that's really
cool
I'm gonna show you something that's
really cool justin is going to get into
a virtual reality environment he's gonna
get into a virtual reality environment
the environment is called holodeck okay
and this holodeck wait don't don't go in
yet don't go in now that you know you're
getting ready okay this holodeck is
really amazing it's gonna have a car
inside and this car this car basically
runs this entire stack okay it's gonna
see it's gonna have surround perception
it's gonna be able to detect the objects
in front of it and it's gonna recognize
localize itself track all the objects
and register computer graphics on top of
it using augmented reality all right go
in there all right yeah so it turns out
that explaining and just remember
remember you're the only person in VR
the rest of us are watching you and now
I'm gonna go okay remember slowly now
slowly slowly
so creating concepts for user experience
in automotive is really difficult and so
we created this virtual lab to explore
it around each on my side you can see
the six input cameras that are the
sensors that come in to allow the car to
drive
okay if I go over closer to the vehicle
I look down inside
you can see a concept for an augmented
reality cluster and rear view mirror
digital review mirror so I'll climb
inside
okay guys see this alright so that then
that digital dashboard that digital
dashboard is running the drive Network
and it recognizes the path that it has
to drive the objects around it and if
you look behind you nice and slowly
look at that so he's inside this
augmented reality environment and we're
basically taking the simulator we just
talked about do you guys know what I'm
talking about we're taking the simulator
we took we took the videos that we we
captured while we're driving our test
car and we can now test that video
against a virtual reality car running
inside a holodeck running our entire
software stack
pretty amazing huh okay so so we imagine
that in the future we imagine that in
the future when you look down into your
your infotainment system it will detect
all of the things that are important so
that you have direct feedback from the
artificial intelligence self-driving car
what it sees in the environment around
it so that you have confidence that it
recognizes the important things and you
have confidence that it's going to drive
carefully it's also going to be able to
augment on top of it points of interest
and so you could say very naturally
where's the local local McDonald's and
it will literally navigate your way
there render the computer graphics path
that is about to take and then when it
gets close
McDonald's shows up okay and and some
computer graphics will be sitting on top
of it so all of the the fundamental
technology is now made possible by this
SDK the rest of it is application
development all right Justin thank you
very much good job
okay I want to share one more thing with
you guys is something that's really
really exciting as you know as you know
I that this car that we demonstrated it
in is not likely the the most likely car
to bring artificial intelligence to the
whole world okay this is the best way to
bring artificial intelligence to 100
people in the world and and what we want
to do what we want to do is we would
like to revolutionize how people drive
how people move from place to place how
goods are moved but also revolutionized
the way that that we interact with our
car and I can't imagine I can't imagine
a more important company and I can't
imagine a more proper company to take
this technology and change everything
that we all know about cars and so today
we're announcing that we're partnering
with the largest car company in the
world Volkswagen to revolutionize the
future of cars and to infuse AI into the
future line of cars so that we could
bring joy and delight and just
incredible surprise to people who who
enjoy cars in the future and to he'll
help us talk about this and the impact
of this I'd like to welcome dr. Herbert
DS to stage the co-hosts wagon
you're the CEO of one of the most
storied one of the largest car companies
in the world and now we're seeing a time
when when technology transformation is
happening to the car industry like like
nothing that we could have possibly seen
all at the same time and so when you're
in the middle of this
if this incredible opportunity what does
it look like to you
what do you see I think it will be more
exciting than today
cars all will be cars will become even
more important more exciting sexier than
today the industry will change a lot I
think driving will be totally different
to today the car today is already we
left cars on most of us loved cars we
left the brands early we left designs
will have the performance of the car if
you drive in a Volkswagen it's fun to
drive but it has negatives and you
mentioned so it's accidents and it's
also pollution
you know co2 other exhausts and this
will change a lot so the car will lose a
lot of its negatives and going much more
positive things it will be much more
comfortable it will be safer and it will
be environmentally friendly so putting
all those positives to the car I think
the future of the car is really very
exciting brilliant so I have I have a
surprise for you
ok we have the buzz for you in virtual
reality and so you stand over there hey
guys hey Jenson how's it going hey Sean
let's show
let's show dr. DS let's show Herbert his
baby here let's this is the future the
future
Volkswagen go ahead go ahead let's bring
it up
oh it's exactly the same one right one
right it looks exactly the same after
all these years I love it it's fantastic
why this brings back memories
except I think the why not I traveled in
actually had a bed in the back I think
I've been in ones with bed in the back
kitchens in the back some of you even
had chairs in the back all right
so but but let's uh let's uh let's
modernize it shall we
yeah I think we have a newer one that we
can look at too so everything Harvard
everything is virtual reality we're
gonna put this on our in our booth so
that everybody can come and see it we're
gonna make this available for your your
team so that you guys can put it in the
cloud and we can we could stream it to
millions and millions of people for them
to enjoy and let them start to see the
amazing design that you guys came up
with Wow okay show us around quickly
yeah let's let's open it up and take a
peek look at that it said when you're a
teacher no sitting sitting face to face
makes a huge difference in the car now
well the bus is the perfect place to do
that exactly now now when when when
eventually of course when the bus is
driving by itself this is a good way to
really enjoy and the design will be such
that there is a space between the front
seats that you can get back in an
autonomous driving situation that's
amazing Wow look at that it's just like
one big plate one big moving living room
beautiful Sun roofs well I think we need
some tunes right where's the radio sir
that's a little more myself you know you
know whenever you're with a with a
Volkswagen bus and you're partying okay
this isn't just this isn't just about
getting there yeah there you go
okay ladies and gentlemen dr. Herbert DS
is sinking it on to partner with you and
thank you thank you all right thank you
thank you congratulations thank you this
is going to be incredible so we're gonna
partner with dr. DS we have teams
assembled and we're gonna we're going to
infuse AI throughout this entire
platform from the way you engage the way
you navigate to the way you drive hey I
will define the future of all of your
cars as I've mentioned the complexity of
autonomous driving the the the
complexity of the software of future
cars is incredible
it starts with of course starting
building a brand new type of processor
we called the drive Xavier an autonomous
machine processor that is able to do
deep learning perception has the ability
to do parallel computing and also
computer vision and high-performance
computing at very very energy-efficient
levels it's got a rich software
development platform because the amount
of software that we have to develop for
this thing is enormous this is not an
embedded controller this is not a
detector this is a computer and it's a
computer that's going to have tons of
software that's never been written
before and so we need a rich software
development platform it has to be
functional safe acyl D I trip I so 26 26
this level of capability requires a
complete redesign and rethinking of
everything from the way you architect
your processors the way you develop your
algorithms the redundancy necessary to
resilience necessary the diversity
necessary um all of that has to be
completely rethought and it includes a
simulation mentality and we've created
an open platform so partners all over
the world can take advantage of the
technology that we've created but also
put on top of it their own
because the on the transportation
industry is so absolutely enormous ten
trillion dollars it's impossible for any
one company to do it all but it's
possible for us to work together on an
open platform to be able to to to engage
it and so you take a look at our entire
stack today dr xavier is the computer
drive AV is the autonomous driving stack
drive aar is the augmented reality
navigation system and drive IX
intelligent experience that turns your
car into an AI thank you very much for
coming to our CES keynote it's a great
way to start the year this year we're
gonna see some amazing technology
created today we announced our Xavier
will be sampled very very shortly you'll
be sampled in q1 its opera it's
performing incredibly well it's the
world's first autonomous processor and
we talked about the three platforms that
I just mentioned we talked about the
Nvidia drive functional safety system
and we announced four incredibly
exciting partners and anchored by one of
the largest car companies in the world a
car company that can take AI and make it
part of our lives for hundreds of
millions of people all over the world
Volkswagen Thank You dr. DS for joining
us today thank you everybody for joining
us happy New Year and have a great CES
Title: GauGAN: Changing Sketches into Photorealistic Masterpieces
Publish_date: 2019-03-18
Length: 119
Views: 1917726
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/p5U4NgVGAwg/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: p5U4NgVGAwg

--- Transcript ---

let's try and talk that up with some
cloud and that's wonderful what if we
were to change all that
to rock let's try waterfall just by
pulling water down from the top that
wouldnt it be great if everybody could
be an artist if we could take our ideas
and turn them into compelling images
this technology allows us to create a
smart paintbrush so that if you wanted
to create a new picture you can just
draw the shapes of the objects that you
want and the neural network can then
fill in all the details if we add a
water feature the network is able to add
reflections not because we told it that
but because it learned it or if we
change the ground to be covered in snow
then it knows that the sky also needs to
be a different color I really think this
technology is going to be great for
architects designers people making
virtual worlds to train robots and
self-driving cars the input to this
model is something we call a
segmentation map it's like a coloring
book picture that describes here's where
a tree is here's where the sky is here's
where the ground is and it doesn't have
any details and then the neural network
is able to fill in all the texture and
the shadows and the colors based on
things that it's learned it from a large
database of real world images we like to
say tree reflecting in that poem the
real advance here is that we're able to
synthesize images with a lot more
diversity and more fidelity than we were
able to in the past I really think this
technology is going to be great for the
dreamers of the world
you
[Music]
Title: History of PC Gaming
Publish_date: 2014-10-01
Length: 791
Views: 30977
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/PEukRq5_c_8/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: PEukRq5_c_8

--- Transcript ---

PC gaming has undergone as many changes
as it seemingly has games what began as
a basic experiment by some curious
researchers has birthed a billion-dollar
industry millions of fans and dedicated
competitions this is the history of PC
gaming PC gaming didn't always look like
this
it started as this originally there was
space war it's called space war
exclamation point and space war was a
pdp-1 game in 1961 space war was
invented and is recognized as one of the
earliest PC games a passion project
created by a team of researchers in the
basement of an MIT lab the team put 200
hours into the first edition of the game
and it pitted two players against each
other in an effort to destroy the
other's ship it's pretty amazing to
think that even in those those older
days the very early days of computers
what did people want to do they didn't
want to just you know calculate stupid
crap they wanted to actually play games
on this stuff but what exactly happened
between then and now that caused such
growth and transformation by the late
70s and early 80s the personal computer
had become more than just an item only
University and researchers could afford
the idea of creating and designing your
own video game became a reality I knew
even at a young age I wanted to make
games and my parents were like you
probably need a PC to do that so I had
my Apple 2 and I started learning basic
and playing around the adventure
Construction Set making my own little
games that was amazing that game was to
me at that time it was unbelievable
technology going through there and
trying to figure out the syntax pick up
object no that doesn't work pick up tool
that doesn't work simultaneously
Sinclair launched the zx80 a low-cost
computer that brought PC gaming into
people's homes for less than $200 a year
later the Commodore 64 is born it is
recognized as the highest selling
computer model of all time later came
the Amiga a computer that exceeded the
64 is 8-bit graphics and at several
coprocessors meanwhile a game company is
on the rise one that would set the tone
for over a decade
me personally you know the name Sierra
is it's powerful uh so many great games
of my early teenage years we wore Sierra
games
I remember watching people play King's
Quest in like not daycare but like early
elementary school on the like computers
that they have there right like Oregon
Trail and then SimCity and then King's
Quest and Kings bus was one of the one
of the first kind of adventure games
that you saw IBM completely funded the
development and marketing of King's
Quest to promote the PC Junior PC junior
from IBM the computer that's growing by
leaps and bounds the PC Junior flopped
but that didn't stop Sierra's push to PC
dominance the era also saw rise in the
space shooter Wing Commander oh my gosh
one of the first space games I ever
played where you know fly through space
shoot enemies action-oriented
wow that was college so I used to have
one of those massive NEC 5 D monitors I
just grabbed this PC gamepad put on that
music doo doo doo doo doo doo doo doo it
took me like six years before I actually
had my own computer at that point right
with my first computer and I got home
and I felt completely victorious that I
had accomplished it the adventure genre
was on the rise led in part by LucasArts
the first PC video game i remember
playing is x-wing and then TIE fighter
that was phenomenal that was completely
different than every other game that I'd
played LucasArts with pushing in the 90s
with Monkey Island Indiana Jones and the
same in Mac series throughout the 90s
gaming had largely shifted from an
arcade experience to one you share in
your own home
the Ultima franchise responsible for
creating the first role-playing games in
the 80s evolved into one of the most
unique online experiences of its time
one of the best things about Ultima
Online was it was it was the first true
simulation in an online world and I
think even to this day a lot of a lot of
games haven't really rivaled it yet you
had the MMO with Ultima it was still
something that people wouldn't naturally
gravitate towards and thus they knew the
Ultima series but whenever quest came
out it was more accessible because it
was using 3d graphics you know they were
filled fully Illustrated avatars and was
using hardware acceleration that
oh wow I can do anything we can actually
do anything we knew we wanted to make an
online game with EverQuest and we wanted
to make it very open and have classes
and do you know kind of a very
traditional RPG that people would sort
of instantly understand it really
brought the whole and the most space to
a much broader audience role-playing is
supposed to be about playing a character
you're supposed to get into the world
and play somebody that's not you
these games are becoming more able to do
that instead of just being you know
number crunching loot gathering games
the mid-90s aw PC gaming come in with a
bang as the 3d shooter was born
doom just kind of like made multiplayer
like the thing that was like oh my god
this is it I was only about 10 when that
game came out as a ten-year-old that
game was pretty terrifying that was
completely different than every other
game that I've played we actually felt
like a dude saving the world from an
alien invasion you know we just those
fighting constantly and then Tim Sweeney
came in from epic like checking out the
nose and they always saw our machines
rebooting I was like why are your
machines rebooting yeah never mind just
so you know we're closely banging that
stuff yeah to me nothing is quite
matched that experience unreal that was
the best thing to ever happen at PC
gaming unreal versus quake there was the
whole back-and-forth of kind of between
epic and it going on where they would
just one-up each other each generation
and you know that keeps pushing the
envelope further and that's exciting
that's why we like working on PC games
we're all pushing each other I think to
Sweeney and the the folks at Epic were
really very good about taking it up a
notch on the graphical side and they
also introduced a lot more arcadey
elements that I think still are exist in
a lot of games out there and it still
remains a very very popular series
well we an epic have been building the
Unreal Engine since 1995 you know in the
second year of Dolf and yet we were
approached by developers who wanted to
use it for their own projects and so
voting the engine for a combination of
our needs and our partners needs has
been a key part of our business all
along to me the thing that really
changed the course of game development
for the industry was the invention of
the GPU on August 31st 1999 NVIDIA
invents the world's first GPU PC gaming
graphics brand ever again going to be
the same
the first time I really encountered in
videos roughly around the Unreal
Tournament 2003 era that was when epic
my former employers started partnering
with them for the whole the way it's
meant to be played campaign the GPU
revolution came along and gave us at an
equivalent amount of silk and about a
hundred times more graphics computing
performance and that that person an
entirely different trajectory and made
much of what has happened since possible
Unreal Tournament 2003 actually opened
with a bumper with the Nvidia logo with
one of the characters kicking it out and
in hindsight they were actually defacing
the Nvidia logo but that's not important
right now
I remember my first real 3d gaming
experience where I actually bought a
video card and I was running in 3d like
really in 3d using OpenGL was quake to
that difference in the graphic fidelity
is just amazing back it's just
astounding Nvidia made its you know big
breakout there and very quickly they
became the dominant force in PC gaming
that they remain today I make software
but I need the hardware to make the
software out so and I get to work with
Nvidia and see what they're working on
and learn about each turn what's coming
next
Blizzard launches World of Warcraft in
2005 immersing millions of players and a
unique radiant experience I think a
break from women for in PC gaming for me
was was wow as well and that was the
beginning of rating I played the
original Warcraft and Warcraft 2 when
you look back now you can see the roots
of the success of Blizzard even in those
early early days because the the games
were that polished I think the first
time we took down Ragnaros we've been
trying him for something like three
weeks and then on that final day first
drive first time we get there goes super
smoothly all the way to the end that was
an incredible feeling even the way they
approached characters right all of the
narrative that's happening in that game
is happening in your head it's all about
what you're doing and how you almost
like you know one of the characters
almost died or almost survived that
ended up saving the rest of your crew by
2008 and as over 10 million subscribe
perhaps most importantly it would
inspire spin-offs of team-based
role-playing competitive games well I
think the MOBA genre in general is a
really good example of something that
you wouldn't think would resonate with
tens of millions of people when you were
able to create a game like that and
create a new genre MOBA that's special
that's saying something when you there's
all these games already and you're able
to create another genre of gaming that
sparks all these other interests and so
I appreciate dota for that
the thing about smite as a MOBA is it's
an incredibly deep RTS type of
experience but you're behind the
character in third-person so there's
actually a little bit of action involved
with a lot of the kind of deep strategy
that speaks to me for that genre PC
gaming is no longer just an activity
between one person and a keyboard it's a
sport it's a league a global community
that prides itself on finding innovative
ways to evolve and create new
possibilities for technology to give
gamers the tools to immerse themselves
in unforgettable worlds what we're
starting to see in PC gaming is or the
evolution of the ecosystem around the
games so you've got things like we're
doing it and in valve actually started
this of you know allowing the users to
actually make stuff that goes into our
game the experience is never the same
it's not like the current generation of
MMO for me the biggest innovation I've
seen in PC gaming besides graphics
besides any sort of physics based things
is the rise of digital distribution
right now when you look at the console
space people are still trying to sell
discs if there's a game that comes out I
want to hit a button I want to you know
go back on Reddit and then find out 30
minutes later I've downloaded and I can
play with my friends that's the world we
live in in 2014 and beyond I think in
the next 10 years there's going to be
something else that makes this
experience even more immersive than it
is now the PC is very unpredictable if
somebody asked me a couple years ago
that VR was gonna be happening in a big
way I would probably have said do you
really think so you think in two years
that's gonna be happening but it is and
I believe that's really one of the big
vectors that we're gonna see PC game and
go into now I'm really excited about all
of the virtual reality
in reality stuff because that's a better
experience for everything not just James
it's better for movies it's better for
newscasts and you like get a sense of
place you're there it's all around you
that kind of stuff is enormous ly
intriguing and very very powerful well
for me it's got to be the oculus rift
the idea that instead of sitting in
front of a screen looking at a 3d world
on the other side of a screen you can
actually be inside looking completely
around being completely immersed total
immersion that's gonna change the world
of gaming completely over the next
decade this simulation and immersion
that people are looking for I think it's
a lot of it is because they've played so
many of the other games and this is just
the next step of PC gaming I think it's
incredibly exciting to be able to just
put on a headset and you're in a
different world and the experience is
yeah I mean it's like nothing else
you've experienced before what keeps me
going in the PC industry is the
community when you do something that
they love they will carry you through
the streets you do something bad say
something wrong the old tar and feather
you faster than anything well PC gaming
is the most pure visceral and high-end
form of gaming in the world I think it
will continue to be that forever for me
I want to get back to that point where
we're making a game it's not just a game
it's not just a service it's almost a
religion for the people that play it for
the people that live it and for even
couples that meet online playing the
game would eventually get married that's
where the magic is at and now here we
are in the year 2014 new PC games are
running all over the world and the most
powerful pcs to date people are having
adventures in far-off lands and imagined
worlds never before possible we've come
so far haven't we if I showed a new
gamer what a video games looked like
then they would just laugh it shows you
that there are still other genres of
games that we haven't even figured out
yet you know if someone can just create
a new genre just on the fly like that
it's like the possibilities are endless
Title: NVIDIA Special Address at SIGGRAPH 2022
Publish_date: 2022-08-09
Length: 2829
Views: 8000421
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/Pev84SGO2r0/hq720.jpg?v=62eaf691
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: Pev84SGO2r0

--- Transcript ---

ready to go once upon a time it was all
fun and games
[Music]
nvidia started off by making chips for
video game machines
graphics became serious business when
people started using nvidia for
blockbuster movies
medical imaging devices
and the world's most powerful
supercomputers
and then one day researchers discovered
that our technology was perfect for ai
[Music]
today nvidia is the engine of ai
[Music]
engineering the most advanced chips
and systems
and software that makes them sing
[Music]
so robots can lend us a hand
cars can drive themselves
and even the earth can have a digital
twin
we live to tackle the world's biggest
challenges
[Music]
and don't worry
we still love our fun and games
[Music]
a decade ago
nvidia's gpu became the engine of deep
learning
the foundational technology of modern ai
artificial intelligence
the automation of intelligent skills
uses computers to write software no
human can
researchers are inventing groundbreaking
advances at an incredible pace
across computer vision
speech
natural language processing
conversation
recommenders
robotics and more
ai is revolutionizing the largest
industries
including computer graphics nvidia
researchers applying the ai capabilities
that our gpus enable are now reinventing
3d graphics the incredible advances of
ai and 3d graphics have laid the
foundation for the next computing
platform
the next big evolution of the internet
the metaverse
welcome to siggraph 2022
we have a lot to share with you today
advances by nvidia research
new technologies products and
collaboration across three major areas
of work
computer graphics
the metaverse and robotics
all are interconnected and can trace
their giant leaps to ai
nearly a quarter of a century ago nvidia
introduced the world's first gpu and
real-time programmable shader
a new type of graphics processor
that did not run fixed function
pipelines but rather executed programs
called shaders programmable shading gpus
revolutionized 3d and made possible the
beautiful graphics we see in games today
four years ago 21 years after our
invention of the gpu at siggraph 2018 we
launched nvidia rtx a brand new gpu
architecture that extends the
rasterization
and programmable shading based gtx
architecture with two new processors
accelerated ray tracing on rt cores and
deep learning on tensor cores
rtx reset computer graphics and open new
frontiers for computer scientists the
advances in new algorithms
many first introduced at siggraph
have been nothing short of amazing
from nvidia's gogan ai image creator and
rtxdi global elimination to asc
character animation to audio to face
nvidia's ai research
is impacting every aspect of computer
graphics
rtx
the fusion of programmable shading
ray tracing and ai
has started the next era
neural graphics
neurographics will be integral to how
artists create 3d worlds and how the
world is animated and rendered one of
the most impactful neural graphics
inventions is dlss and ai that has
learned to enhance the resolution of
motion graphics
today you will hear sonja share the
inventions and breakthroughs of nvidia
researchers
she will show you the art of the
possible and a glimpse into the future
with neural graphics
neurographics is one of the essential
pillars of the emerging metaverse
what is the metaverse
simply
it's the next evolution of the internet
commercialized two decades ago the
internet was about web pages hyperlinked
over a network
a decade ago web 2.0 emerged and the
internet was about cloud services
connected to applications that are
oftentimes enjoyed on mobile devices
now web 3.0 is here the metaverse is the
internet in 3d a network of connected
persistent virtual worlds
the metaverse will extend 2d web pages
into 3d spaces and worlds hyperlinking
will evolve into hyper jumping between
3d worlds like games today
3d worlds are experienced through 2d
displays and tvs and occasion with vr
and ar glasses
what are metaverse applications
they're already here fashion designers
furniture and goods makers and retailers
offer virtual 3d products you can try
with augmented reality telcos are
creating digital twins of their radio
networks to optimize and deploy radio
towers
companies are creating digital twins of
warehouses and factories
to optimize their layout and logistics
and nvidia is building a digital twin of
the earth to predict the climate decades
into the future
the metaverse will grow organically as
the internet did continuously and
simultaneously across all industries but
exponentially because of computing's
compounding and network effects
and as with the internet the metaverse
is a computing platform that requires a
new programming model a new computing
architecture and new standards
html is the standard language of the 2d
web
usd universal scene description
an open and extensible language of 3d
worlds invented by pixar is likely the
best language for the metaverse nvidia
omniverse is a usd platform
a toolkit for building metaverse
applications and a compute engine to run
virtual worlds
rev will share a work
and collaboration with the industry to
advance usd
steven will talk about the algorithms
and the compute engine of omniverse one
of the most exciting applications of
omniverse is robotics
systems that perceive their environment
reason and plan there are many forms of
robots in development
self-driving cars manufacturing arms
warehouse inventory movers
agriculture planters and weeders and
logistics picking machines
robotics is the next wave of ai
and omniverse is essential to our work
to design train and operate robots
one of the most widely used robots will
be a digital human or avatar
avatars will populate virtual worlds to
help us create and build things
be the brand ambassador and customer
service agent help you find something on
a website take your order at a drive
through or recommend a retirement or
insurance plan
creating avatars requires
state-of-the-art ai models that can see
hear understand language be
knowledgeable converse and animate
simon will talk about our digital human
research and our work to democratize the
creation and deployment of avatars
we have a lot to show you today
sonia
make it so
thank you jensen
it's great to be here with you at cigra
2022 neurographics intertwines ai and
graphics paving the way for a future
graphics pipeline that is amenable to
learning from data this will enhance
results help automate design choices and
provide new opportunities for artists
and creators that have yet to be
imagined
ultimately neural graphics will redefine
how virtual worlds are created simulated
and experienced by users there are many
important challenges to address in the
world of computer graphics let's talk
about a few of this and the solutions
provided by neural graphics that can
revolutionize the field
content creation is a time consuming
process
the exquisite craft of artists remains
essential but combining air and graphics
can help significantly streamline this
process
one way to save time is to take pictures
of a scene or an object and try to
reconstruct it in 3d
we can think of reconstruction as the
inverse of the rendering process going
from images back to a 3d representation
by parameterizing the 3d scene with
neural networks and optimizing this
representation using classical rendering
techniques in the loop you can achieve
high quality results in a single
unifying framework
another challenge
is character animation which is
essential to bring virtual worlds to
life
physics-based simulation makes the
virtual world closer to the real world
but building controllers for the
simulated character is extremely
difficult
reinforcement learning automates a
controller development process by having
virtual characters learn how to move in
a physically simulated environment by
imitating human motion data and the
results are astonishing
achieving an immersive and seamless
experience of the 3d digital world is
critical for virtual reality users
especially as we move towards a fully
realized metaphors
currently most vr users access the 3d
digital worlds by wearing bulky head
mounted displays
with the advent of powerful ar
algorithms we can co-design the optics
of this hardware the display size and
quality the optimized design can deliver
full color 3d holographic images in less
than half the size of existing thin vr
displays
let's take a look at the amazing work
our researchers have been doing in the
domain of neurographics
[Music]
[Music]
[Music]
[Music]
[Music]
aion graphics work in tandem
and breakthroughs in each lead to
enhance results when combined together
it's a graph
is presenting 16 papers that are
advancing both neurographics and the
mathematical foundations on graphics
representing collaborations with 56
researchers across 20 universities
we are extremely proud that two of these
papers have been honored with the best
paper award at siggraph today nvidia is
releasing new research and tools to
apply the power of neural graphics
towards the creation and animation of
virtual worlds
we're excited to introduce cowl and wisp
a research-oriented library for neural
fields providing a common suite of tools
and a framework that aims to accelerate
new research re-implementing many of the
existing work in neural fields can be
done in just few lines of code with this
work our goal is to enable and inspire
fast-paced progress in neural and
foundational graphics we look forward to
seeing many more exciting breakthroughs
at the next c graph
next let's hear from rev stephen to talk
about the metaverse and virtual worlds
the metaverse is the next era in the
evolution of the internet a 3d spatial
overlay of the web linking the digital
world to our physical world
in this new iteration of the internet
websites will become interconnected 3d
spaces akin to the world we live in and
experience every day
many of these virtual worlds will be
reflections of the real world linked and
synchronized in real time
many of these virtual worlds will be
designed for entertainment socializing
and gaming
matching the real world's laws of
physics in some cases but often choosing
to break them to make the experiences
more fun
xr devices and robots will act as
portals between our physical world and
virtual worlds
humans will portal into a virtual world
with vr and ar devices
while ais will portal out to our world
via physical robots
just like in the infancy of the internet
no one can predict exactly how and how
large the metaverse will grow
but today we know we can lay the
foundations the foundations of the
metaverse require two things
first a standard open and extensible way
to describe all of the things in the
virtual worlds of the metaverse
similar to html's purpose in today's 2d
web
and second a computing platform designed
for the creation and simulation of
virtual worlds the next era of the
internet the 3d internet or metaverse if
you will
needs a standard way of describing all
things within the 3d worlds
we believe universal scene description
invented and open source by pixar is the
standard scene description for the next
era of the internet
usd is far more than a static file
format
it's a 3d composition engine with apis
for composing editing querying rendering
collaborating and simulating virtual
worlds
usd is unique giving 3d artists
designers developers and world builders
the ability to work in non-destructive
layered workflows
the framework is highly extensible
providing the ability to build custom
schemas for specific workloads or
industry applications
we have a rich and long-term vision for
usd
looking at the 2d web we witnessed
html's remarkable progress from html 1.0
in 1993
where it could only describe simplistic
web pages
to html5 enabling rich interactive media
and dynamic applications
usd is quickly evolving along a similar
path from its origins in m e as a static
description of large virtual worlds
towards a system for dynamic procedural
and real-time worlds
we see the long view of usd and are
fully committed to helping accelerate
its development to reach that future
sooner our most recent contributions
include a custom mdl schema that can
represent physically accurate materials
and specify material parameters
we also upgraded usd from python 2 to
python 3 bindings
and
along with apple and pixar we extended
usd to standardized support for rigid
body physics
we envision usd evolving as a complement
to existing 3d standards most notably
gltf
today gltf is an important 3d file
format used across a vast number of 3d
applications and delivery on the web
we're helping take the initial step
towards harmonization of usd and gltf
with our development of an open source
usd file format plugin
with this plugin creators can directly
leverage the powerful layering
composition and non-destructive editing
capabilities of usd with their existing
gltf asset libraries
our next milestones aim to make usd
performant for real-time large-scale
virtual worlds and industrial digital
twins
this includes building support for
international character sets geospatial
coordinates and real-time streaming of
iot data
we're also enhancing the usd software
stack to enable high-speed incremental
updates and real-time proceduralism
we will continuously test and open
source these builds for our ecosystem of
isv partners and customers
let me show you a sneak peek of what
we've been doing
nvidia is enhancing usd to support
extremely large and complex digital
twins
everything from sprawling factories to
global scale climate change
digital twins need to operate in full
design fidelity at real-time speeds and
optimize for different devices
here are the ways that nvidia is
extending usd to scale to the scene
complexity of digital twins
the source usd is compiled into fabric a
gpu accelerated deeply vectorized data
representation for real-time updates
shown here in drive sim
just-in-time optimizations like mesh
merging and material distillation reduce
scene complexity while preserving visual
fidelity and can be applied at load time
without modifying the source data
this is crucial for making a ground
truth representation of a digital twin
available to both a supercomputer like
ovx
as well as a general consumer grid
device
let's take a look at an example that
shows the performance of a digital twin
in usd
this virtual factory data set from the
lotus sd digital factory team contains
millions of prims in usd
with nvidia's just-in-time scene
optimizers the first pixels for this
dataset appear in just a few seconds
these optimizers can be configured to
device requirements on the fly
without the optimizers the data set
takes minutes to load plays back at 7
fps and can exhaust gpu resources on
lower end systems
let's take a closer look at the
optimization process
in traditional usd rendering pipelines
hydra copies the usd data into gpu
buffers as is
this is where optimizers come in we can
load the entire scene headlessly into
system ram
and then merge meshes distill materials
and stream geometry to the renderer
in this case geometry streaming
prioritizes the draw order based on
camera heuristics such a solid angle
with these just in time optimizations we
achieve a 10 time speed up in load time
and a 10 time speed up in playback frame
rate
this is just the beginning for where we
can take just-in-time scene
optimizations
future work could include out-of-core
scene optimization to relax system ram
requirements and much more
usd is the ground truth for digital
twins scalable to any level of available
computing power
we're not only contributing to the
development of usd but are continually
testing and pushing its limits
we built our omniverse platform as a usd
engine and an open toolkit for building
custom usd pipelines
we built drive sim and isaac sim for
autonomous vehicles and general robotic
simulations on top of omniverse
these demanding use cases require
real-time performance
physical accuracy and extremely large
virtual worlds
we're also working closely with partners
in retail
automotive energy telco and more to
evolve usd and better serve their
domains to further accelerate usd
development and adoption we're building
an open source usd compatibility testing
and certification suite developers can
test their builds of usd and certify
that the custom usd components produce
an expected result
we'll provide a testing suite as well as
a usd build system accessible by
developers anywhere
we want everyone to help build and
advance usd
we've built a wealth of resources
available online for free
including pre-compiled binaries for
linux windows and pi pi
we host usd view and usd builds in the
omniverse launcher and we provide usd
ready scenes on-demand tutorials
documentation and instructor-led courses
we're excited to introduce
simulation-ready usd assets
purpose-built for industrial digital
twins and ai training workflows the usd
ecosystem is vast with contributions
from numerous leading technology
companies not only in m e but in aec
manufacturing and even robotics
industries
we're partnering with many of these
companies to evolve usd and can't wait
to see more names on the list
i've covered the first requirement of
the metaverse a standard way of
describing all things in the 3d web
usd
and now steve parker will introduce our
computing platform for virtual worlds
just like the internet has a compute and
networking engine the metaverse requires
a computing platform to support linking
the digital and physical world this
platform for virtual worlds requires
specific enabling technologies we have
been building the core technologies of
omniverse over the past two decades
mdl material definition language to
simulate physically accurate materials
physics and advanced real-time physics
engine and rtx the world's first hybrid
rendering engine and ai system that
enables us to simulate light and matter
with physical accuracy in real time
ai to assist in building or autonomously
generate worlds make predictions and
automate beyond human abilities
and of course usd the powerful scene
description standard for virtual worlds
these core technology pillars are
powered by nvidia high performance
computing
from the edge to the cloud
let me show you the foundational
technology on which omniverse is built
[Music]
so
[Music]
[Music]
[Music]
[Applause]
[Music]
see you in omniverse
we have several exciting developments in
core graphics technologies today we are
celebrating 10 years of mdl the material
standard for industrial workflows with
mdl we unlock material representations
from current silos allowing them to
traverse software ecosystems
we open source the mdl sdk at siggraph
2018.
now we are open sourcing the mdl
distiller and glsl back-end technologies
further broadening the reach of mdl and
enabling developers to bring mdl support
to their preferred renderers
mdl is a flexible language that can be
used to define complex physically
accurate materials
like cloth with complex highlight
patterns it defines the ground truth of
a material with properties such as
energy conservation that enables robust
physical simulation
many renderers can't render such complex
materials and need to simplify or
translate into their own material model
other renderers may wish to remove some
of the material complexity to achieve a
performance target the mdl distiller
automates such simplification of mdl
materials
the distiller relies on the mathematical
robustness of the core mdl definition
and provides mechanisms to manipulate
the material layers like a symbolic
algebra tool would manipulate algebraic
equations
so now material artists can author one
single truth high quality material
without the need to make compromises for
simpler renderers
the new open source glsl backend brings
mdl support to renderer developers
building on opengl or vulkan
closing the gap to established graphics
api standards
with the mdl distiller and glsl backend
we will see many more developers
leveraging the power of mdl
openvdb is an academy award-winning
industry standard for memory efficient
representations of sparse 3d volumetric
data the visual effects industry uses
openvdb to simulate and render water
fire smoke and clouds
last year we announced nano vdb bringing
gpu acceleration to openvdb
today we are announcing neural vdb the
next evolution of openvdb
let's see it up close
whereas openvdb uses a hierarchical tree
structure neural vdb introduces neural
representations of both values and the
underlying tree structure
this dramatically reduces the volume's
memory footprint allowing users to
interact with extremely large and
complex volumetric data sets in real
time as well as transmit and share them
more efficiently
neural vdb also carries over the gpu
acceleration of nano vdb which nvidia
introduced in 2021
while these side-by-side examples look
identical the memory footprint of the
neural vdb representation is up to 100
times smaller to speed up training by up
to 2x neural vdb allows the weights of
the previous frame to be used for the
subsequent frame
neural vdb also enables temporal
coherency or smooth encoding by using
the network result from the previous
frame this reduces the need for
post-production effects like motion blur
today openvdb has grown beyond its
entertainment use cases into healthcare
industrial manufacturing and design
scientific computing and visualization
robotics and machine learning
applications
by dramatically reducing memory
requirements accelerating training and
enabling temporal coherency neural vdb
opens the door to scientific and
industrial use cases
including massive complex volume data
sets for ai enabled medical imaging
large-scale digital twin simulations and
more
now moving on to omniverse
we have several new developments to
share with you
in omniverse kit and kit-based
applications like omniverse create we
have major updates to physics in
omniverse with scalable sdf
soft body simulation
particle cloth simulation and soft
contact models bringing real-time real
real-world physical accuracy to virtual
worlds
we are also introducing omni live
workflows a major development that
delivers non-destructive live workflows
at increased speed and performance to
users connecting and collaborating
between different third-party
applications
omnilive also enables custom versions of
usd to live sync seamlessly making
omniverse connectors much easier to
develop we also have a new customizable
viewport improved user interfaces
enhanced review tools and major releases
to our free 3d asset library omniverse
now has several free usd scenes and
content packs to get world builders
started faster than ever
omniverse audio to face is now available
with full facial animation and emotion
control
omniverse machinima now has new content
from beyond the wire postscriptum and
shadow warrior machinima now has easily
accessible ai enabled animation tools
like audio to face and audio to gesture
nvidia's modulus physics machine
learning framework is now available as
an omniverse extension
delivering near real-time performance
modulus trained physics ml models are a
thousand to a hundred thousand times
faster while providing unprecedented
accuracy closer to high fidelity
simulations
working closely with our research teams
we are introducing new
ai powered artist and creator tools to
omniverse ai toybox
animal modeler a diffusion model based
ai tool lets artists and creators
iterate on an animal's form with point
clouds and then generate a 3d mesh
nvidia gogan is coming to 3d worlds with
gogan 360.
generating 8k
360 degree panoramas you can easily load
into an omniverse scene
lastly omniverse deep search is now
available for enterprise customers
letting teams use ai to help them
intuitively search through massive
untagged asset databases deep search
lets you search on qualitative or vague
inputs bringing up accurate results for
red rusty barrel
deep search works even if the usd data
does not contain any tags or other
metadata
shared virtual worlds will enable the
next wave of ai
and will profoundly impact today's
industries
omniverse is where all of nvidia's
technology comes together to realize
this opportunity
we are building out more developer tools
technologies for constructing custom usd
pipelines and are enabling full design
fidelity visualization of usd scenes
we're continuing our work in bringing
multi-gpu multi-node hyper scalability
to simulate large-scale scenes without
compromising physical accuracy
we are also continually infusing the
latest ai into capturing generating
composing
simulating and optimizing these virtual
worlds
and to realize the potential of the
metaverse for industrial and scientific
use cases we continue to forge links
from the physical world to digital
worlds through sensors and iot devices
omniverse is a network of networks
nvidia and our partners are continually
developing robust live usd portals from
design
simulation and cad software ecosystems
to omniverse we have already developed
several connectors for major design and
content creation ecosystems
today we are announcing several new
connectors including blender
autodesk alias and civil 3d
siemens jt
sim scale
and open geospatial consortium
connecting users across industries and
disciplines
you can also try the new ptc creo
visual components and side effects
houdini connectors they are now
available in beta beyond these the
omniverse usd ecosystem continues to
grow with downloads and users growing
nearly three times in one year and to
112 connections spreading across huge
software ecosystems
our partners continuously release new
updates to the omniverse ready
connections
maxon's redshift hydro renderer is now
available as is otoyo octane letting
artists and designers use their
preferred renders directly in omniverse
epilogue sync twin built on omniverse is
a new suite of tools and services to
enable development of industrial digital
twins
preview 3d a 3d scanning partner now has
usd support enabling workflows in
omniverse and with siemens accelerator
as part of the omniverse network
industrial customers can unlock the
power of the metaverse for a new era of
digital twins
our community of a hundred and fifty
thousand omniverse users are everywhere
portaling into omniverse with their rtx
enabled studio laptops gaming pcs
professional workstations and ovx
servers with the next wave of omniverse
worlds moving to the cloud
omniverse is available for free download
it at nvidia.com and start creating
today now let me hand it off to my
colleague simon to talk about digital
humans
thank you steven
at its core an avatar is a virtual robot
that can perceive plan and act
avatars will be everywhere they'll
become more personal and intuitive
eventually it'll be as natural as
talking to another person
however creating digital humans is
complex
we need breakthroughs in natural
language processing speech and vision
while simultaneously processing complex
facial and body animations sophisticated
materials and rendering all in real time
everything must dynamically update and
react to us in milliseconds just like
human conversations
nvidia's work on digital human spans
across the company from research and
visualization to animation and
simulation from ai models all the way to
deployment
let me show you audio to face our facial
animation ai
created by close collaboration between
nvidia research engineering and creative
teams
audio to face is an ai model that can
create facial animation directly from
voices
our new version has just been released
it has some significant new updates that
fastly improve how people create high
quality facial animation
let me show you what i can do
[Music]
it's so dark
where
where is it i can't
i can't see it i can't see anything
but i know it's there
waiting
waiting
the beige hue on the waters of the lock
impressed all
including the french queen before she
heard that symphony again just as young
arthur wanted okay i have a few jokes
here
what do you call a fish
without any eyes
we have an exciting road map for
omniverse audio to face we just added
more features to analyze and
automatically transfer your emotions to
your avatar
we're expanding connections and
availability of audio to face to other
engines and platforms
let me show you where we're headed
first we're going to expand our
multi-language support
we're looking at improving different
people's voice adaptability so no matter
what type of voice input goes in the
network will create the predictive
facial animation even more accurately
we're also going to provide a training
sdk so users can train their own data
serving more particular needs people
might have to customize their avatar
and looking even further ahead here's
our vision
it starts with simplifying how to create
our own 3d avatar with our own likeness
using just a single photo
here we see a full cloud-based solution
where you can drag a photo into this app
and the 3d model with textures generated
on the fly very easy and quick
and these 3d models are not nerf or
point cloud data they're actually
animation ready meshes with a clean
topology ready to be animated
immediately with audio to face
next
we're developing an anatomically based
high fidelity muscle simulation approach
to generate any type of facial motion on
any character the facial animation you
see on the right is created through
muscle stimulation only there are no
other hand tweaks or animations applied
we can teach the muscles how to behave
and they learn how to move based on a
video input from the left
not only that
this is a generalized solution that
works for many different types of
digital humans we can train and transfer
these muscles to all kinds of digital
humans enabling us to generate motion
without the need to motion capture or do
4d scans
lastly here's one of the most complex
digital assets to create and simulate
similar to image to 3d we can now
generate strand-based hair from a single
photo what we're showing here is an
end-to-end system from generating hair
to simulation
for grooming hair we're combining a full
physics-based system that allows you to
interact and edit your virtual groom
just like in the real world
we want to democratize building
interactive avatars for every platform
today we're announcing omniverse avatar
cloud engine omniverse ace
ace is a collection of cloud-based ai
models and services for developers to
easily build avatars
ita encompasses nvidia's body of avatar
technologies from vision ai
speech ai
natural language processing to audio to
face and audio to motion where you can
animate your avatar's face with spoken
words all running as apis in the cloud
with omniverse ace developers can build
configure and deploy their avatar
application across any engine in any
public or private cloud
ace will democratize the ability to
build and deploy realistic ai driven
intelligent avatars
let me show you what this looks like
in this demo ace is powering a metahuman
and unreal engine in real time that
includes natural language processing a
custom voice model and animation
behaviors let me show you how it works
hello
hello my name is arjun i am a digital
avatar
what does it mean to be a digital avatar
i'd be happy to explain in computing an
avatar is a graphical representation of
a user or the user's character or
persona a digital avatar can also be a
virtual assistant like myself
can you tell me more
avatars are the digital representatives
in the virtual world the characters we
control in video games can be thought of
as avatars and increasingly we will
adopt avatars as we shop socialize learn
and work in the connected online
environments that we are calling the
metaverse
let me show you another avatar in
metaverse this is sam
i've heard a lot about this thing called
the metaverse can you explain it to me
the metaverse is the 3d evolution of the
internet a network of connected
persistent virtual worlds
i'd love to know more
the metaverse will extend 3d web pages
into 3d worlds and connect the digital
world to the physical world
the metaverse has multiple applications
whether for entertainment socializing
and gaming or industrial and scientific
use cases to build digital twins
thank you goodbye
sure
i am here if you have any more questions
with omniverse ace third-party
developers everywhere can create their
own avatars that run in the cloud or on
a device leveraging nvidia's entire body
of digital human work
our ai models can connect to all of the
applications and engines out there and
power the avatars in their environments
all through the cloud
with omniverse ace we're simplifying the
creation of ai accelerated avatars
we have much more in store for you in
the future this is just the beginning
now back to you jensen
ai
the most powerful technology force of
our time
will revolutionize every field of
computer science
including computer graphics
nvidia's rtx is the engine of neural
graphics
rtx and neurographics is like a time
machine
the breakthroughs you saw today would
have otherwise taken another decade the
power of neural graphics to create
worlds and synthesize images
is an essential pillar of the metaverse
the metaverse is the next evolution of
the internet the 3d internet
today
we made three major announcements first
we announced the significant release of
omniverse including new toolkits for
creating usd applications
and new engine technologies for running
virtual worlds
nvidia omniverse
runs on usd
the leading candidate
for the standard language of the
metaverse
second
we also announced collaborations with
industry leaders
and an open source suite of usd
interoperability tests
virtual worlds are essential to
designing training and operating robots
the next wave of ai
avatars will be among the most popular
of the many forms of robots
there will be billions of avatars
our third announcement nvidia ace
avatar cloud engine is a suite of ai
models used to create avatars of all
kinds
ace runs in the cloud or in embedded
systems
the announcements we made today further
advance the metaverse
a new computing platform with new
programming models new architectures and
new standards our industry is set for
the next wave
the combination of ai and computer
graphics will power the metaverse the
next evolution of the internet
have a great sig graph
now approaching your destination
[Music]
hi welcome to nvidia
[Music]
[Applause]
[Music]
[Applause]
[Music]
[Applause]
[Music]
this is
[Music]
you
Title: Accelerating Lighting and LookDev with NVIDIA Quadro RTX and Autodesk Arnold GPU
Publish_date: 2019-03-26
Length: 169
Views: 50607
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/pghEtE3h87Q/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLDs-4eyQgOi57tYGtOf0LhIoOtl9A
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: pghEtE3h87Q

--- Transcript ---

Arnold is a production past Fraser
it's a rendering engine which means that
it is the software is used to create the
final frame for film animation TV shows
almost aims at simplifying workflows
reducing the number of armitage that an
artist has to be tweaked before getting
a nice image and focus on the artistic
process so what we're trying to do with
Arnold GPU is create a GPU production
renderer and senescyt was really keen on
on working with us on that we've been
you know hearing about GPU rendering for
a very long time but I didn't really
understand what was the scope of the
design changes that were made to the
graphics python as we learned more about
the architecture then that's when it
started to be really more exciting for
us after we did some testing in London
that went very well we decided to do
more testing here in Montreal more in a
production environment we did most of
our focusing on lighting I looked up and
really just wanted to see how far we
could push the GPU most recently we're
working on an Indian film a Bollywood
film where the main character gets to
blast off in a rocket ship
we completed 26 shots in the movie with
the CPU render and once we got the GPU
rendering we thought let's see what we
can do with this one of the major things
that were challenged was keeping a
lighting direction and the continuity of
lighting the same through all the shots
now this is done traditionally here by
separate artists and separate Maya
scenes they also get reviewed separately
it's a long process but I think with
this GPU technology potentially could
have all those shots done by one person
in one scene we've loaded up Arnold GPU
beta and basically put every single shot
in our sequence on the timeline of Maya
so we think a card like the r-tx 6000
would really help speed up that process
we can literally go to any shot at any
time rotate a key light
change the color of a light maybe change
your HDRI and basically have all them
chronologically so you can get all your
lighting direction set this is a huge
step for us when we heard that Autodesk
and NVIDIA wanted us to do some testing
on possibly a real-time renderer we
immediately jumped in this is the holy
grail to very trusted companies that we
use widely so it was very easy decision
and opportunity to see what's coming up
on the horizon we're really feeling the
performance and we're really happy about
the performance and we know it's going
to become even better this is just a
beginning
[Music]
Title: NVIDIA GTC May 2020 Keynote Pt 7: NVIDIA EGX A100 Converged Accelerator and Isaac Robotics Platform
Publish_date: 2020-05-14
Length: 763
Views: 76974
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/phP_c3zi82g/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: phP_c3zi82g

--- Transcript ---

one of the most exciting opportunities
in computing is what's called edge AI
this is where IOT and AI comes together
to revolutionize devices trillions of
devices will be all of the world
embedded with sensors connected to the
Internet data centers running algorithms
that infuse them with intelligence this
is going to be the smart everything
revolution just as the phone became the
smartphone and revolutionized the
industry of communications and created
large industries around it the same
thing is gonna happen to devices and
things devices with sensors are going to
be connected to the Internet data
centers are going to have amazing
algorithms that infuse these devices
with apparent intelligence these devices
are going to be everywhere whereas there
were only billions of smart phones there
will be trillions of things and then
will be in all these different
industries agriculture manufacturing
logistics retail stores warehouses
airports train stations streets we will
have sensors all over the world and
there will be intelligence applied to
them the fundamental difference between
a smartphone and these Internet of
Things is the continuation of the sensor
information that would be coming whereas
most users interact with their phones
every now and then an electronic time
the time separation between clicks is
basically infinite these sensors are on
all the time they're monitoring picking
up data reasoning about what they're
sensing and taking necessary action it's
important that the data center is placed
close to the point of action where the
data is being collected otherwise the
cost of streaming all of this enormous
amount of data onto the Internet is
going to be cost prohibitive it's also
important to put the data center close
by so that you could sense and react to
the environment as quickly as possible
we're a few millisecond makes a
difference the speed of light traveling
to data centers far away takes too long
and finally in many industries there are
data privacy and data sovereignty issues
in the future there will be millions of
data centers
over the world close to the point of
action close to where the data is
generated and sensed and processing AI
instantaneously these data centers are
going to be very much like the cloud
data centers today their cloud native
powerful and most importantly secure we
created a computing platform for this
edge AI
application we call it the Nvidia ejects
egx is made possible by two advanced
processor the first is the ampère GPU
which has been designed for high speed
AI processing but there are other
capabilities that ampere provides the
first is secure and authenticated boot
so that you know that this computer is
authorized to be on the network and run
the applications the second is a new
security engine for confidential a I the
AI algorithms are sensing reasoning and
taking action so it's vitally important
that we know it hasn't been tampered
this secure confidential AI uncle' is
going to protect the AI model so that
it's encrypted and if it's tampered in
any way the program would not be allowed
to run the second processor is the
nvidia Mellanox connect x6 DX it's a
dual 100 gigabit per second Ethernet or
InfiniBand it has a crypto engine to
process security protocols TLS and IPSec
at line speed it has a SEP 2 and packet
processing with single route IO
virtualization that allows this computer
to be secure and virtualized and then
lastly it has a brand new capability
called time trigger transmission which
connects the EGS server with the 5g
radio antenna and synchronizes the
transmission between the two of them the
combination of these two processor makes
up the nvidia egx
and by installing the egx into a
standard x86 server you turn it into a
hyper-converged secure cloud native ai
powerhouse
it's basically an entire cloud
datacenter in one box the nvidia egx
card is the starting point what makes it
amazing it's the software stack on top
we call it the
stack the magic of Nvidia egx comes
alive because of the software stack
fully integrated fully optimized it has
four major pillars the first is that
it's cloud native designed for
kubernetes orchestrating containers
managed from afar the ability to update
software without rebooting the computer
second it's the world's first GPU
accelerated 5g baseband radio our first
partner is Ericsson who is already
developing 5g stack on top of Nvidia
third a fully optimized high performance
AI processing pipeline that we are world
famous for and then lastly this entire
stack is optimized for networking
storage and most importantly security
this one box is essentially a
state-of-the-art cloud native data
center that is secure could be managed
from afar and is tamper proof data is
protected in motion and in place it is
authenticated before it can come onto
your network and as a result you could
manage a fleet of data centers and all
of those data centers are sprawled out
all over your geography and all of your
market connected the sensors applying
intelligence to the products and
services that you offer the egx stack
comes with several reference
applications the 5g reference
application is one another reference
application is called metropolis
metropolis is designed for connecting
multiple high-speed cameras or
high-speed sensors to process the
streaming data and do AI processing on
top of that
metropolis also comes with a library of
pre-trained state of the art models
these pre-trimmed models has been
trained with a great deal of data we
also provide transfer learning tools to
adapt these models to your use case this
end-to-end system is what we call Nvidia
egx this is a demonstration of live
cameras feeding multiple streams of
video which then goes into a 5g radio
emulator turns the live video into 5g
packets the 5g
Hackett's comes into the egx stack which
runs the Phi G aerial SDK GPU software
baseband Radio Arriola metropolis is
running on top of the e GX stack which
is running kubernetes CUDA AI processing
network processing security processing
and storage processing all completely
optimized running on one computer and to
end let me show you another application
of Nvidia egx this is factory automation
the reference application we developed
for this is called Isaac
it starts with Isaac's in a virtual
reality environment that obeys the laws
of physics appear photorealistic
in that world a robot thinks it's in the
physical real world we're gonna teach it
skills and refine its skills once the
robotics models develop we would run it
on the nvidia egx computer running the
Isaac robotics stack the Isaac robotics
deck
includes sensing models localization
articulation models and navigation
models
it receives sensor information streamed
over 5g from the fleet of robots it does
the robotics processing sensing
reasoning in action and sends the
actuation commands back over 5g to the
fleet of robots with extremely low
latency which is one of the features of
the 5g protocol Isaac is n2m from the
virtual reality environment we call
Isaac sim the robotic stack we called
the Isaac SDK the computer and video EEG
X and the environment for 5g and the
robotic stack on top controlling a fleet
of robots to automate factories of the
future let me show to you
[Music]
this is have incredible ladies and
gentlemen today I have really exciting
news we've been working together with
one of the world's leaders in building
amazing machines
ladies and gentlemen BMW BMW has chosen
a video to build the factories of the
future
BMW manufactures some of the most
amazing machines in the world and they
do it in volume but what you don't
realize each one of these cars they have
40 different models have a hundred
options every single day 30 million raw
parts comes in from nearly 2,000
suppliers it goes to 30 factories around
the world and those 30 factories
assemble one car every 56 seconds these
30 million parts comes in sent over to
the workspace just in time for the
craftsmen to assemble those parts into
the car the empty crates taken away a
new crate arrives each and every step of
the way a robot will be involved the
splitting the picking the placing the
delivery the picking up of the empties
this is really a logistics miracle and
it's one of the great challenges of
automated Factory today and this is the
future you're gonna have a factory
that's gonna be designed as a robot mass
production customization going
hand-in-hand and what makes it possible
is artificial intelligence and robotics
I can't be more delighted to work with
all of the great people at BMW in this
great challenge to invent the future of
automated factories as I mentioned the
fusion of IOT and artificial
intelligence is going to create this
whole new computing space we call edge
AI and the applications for it will be
in so many industries we've already
announced previously that we're working
with Walmart on automated retail we've
also talked about how we're working with
USPS the world's highest volume highest
speed sorting system is now powered by
Nvidia egx
today I announced our partnership with
BMW to apply egx IOT AI and robotics
technology to reinvent the future of
factories while these industries are
large the number of sensors
the world is going to be enormous
trillions of them collecting and
processing data continuously vast
majority the world data will be created
and they are processing will be done the
nvidia EG x stack has so many partners
from operating systems security
processing network processing and one of
the most important is the 5g stack and
our partnership with ericsson and mavin
ear and in each one of the vertical
industries there are great partners were
working with to develop applications of
specific types of AI and specific skills
from industrial to medical to robotics
to intelligent video analytics this is
one of the most exciting computing
platforms we've built bringing together
in videos great ai processing and
Mellanox is great network storage and
security processing we've created a
computing platform that has the power of
clouds but could be used and deployed
everywhere at the edge nvidia ejects
Title: NVIDIA RTX Elevates Architectural Visualization at SIGGRAPH
Publish_date: 2019-08-05
Length: 84
Views: 14848
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/PlkgbZlkEx8/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: PlkgbZlkEx8

--- Transcript ---

my name is Carlos rooster I work at me
escape and I'm the director of a rad lab
around a branch near scape started in
1995 as an architectural visualization
studio so we've been doing animations
and renderings for a long time right now
we've evolved into a full-service
marketing agency for real estate so we
provide all sorts of services from
graphic design marketing strategy
branding 3d and animation are still part
of our core services and usually we
think that these technology is gonna
help us new our clients better
experience
so with r-tx in real time ray tracing
you know it's a big part of what we do
we're super excited to try out the
quadrille parts as you know photorealism
is highly intensive computational
endeavor it requires time to render with
RTX we are really looking forward to
expedite in that process right because
our clients their margins are getting
thinner
everyone's margins are getting thinner
so efficiency is the key that's what
we're trying to achieve here like with
this technology we believe that we'll be
able to interact and collaborate with
our clients in real time and see the
actual results with the Quadra and our
TX cards it's just another tool another
device that really makes my life easier
Title: NVIDIA Powers VR
Publish_date: 2016-09-23
Length: 164
Views: 12621
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/PNHpA82mWmM/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: PNHpA82mWmM

--- Transcript ---

nvidia has powered the world's highest
performance graphics experiences for
more than 20 years now we're harnessing
all that innovation to deliver the next
generation of virtual reality at Nvidia
we view virtual reality as the next
major computing platform like
smartphones over the last 10 years
virtual reality will change the way that
we do business the way we enjoy games in
entertainment and the way we interact
with friends and family every industry
from automotive to medical to
architecture to art to gaming will be
changed by virtuality from a graphics
perspective there are two major
challenges the first is that virtual
reality requires an incredible amount of
graphics horsepower up to seven times
that of a normal PC gaming experience
the second is that virtual reality
requires very responsive graphics when I
move my head the display needs to update
instantaneously within 20 milliseconds
in order to provide a smooth comfortable
experience nvidia is building a powerful
computing platform for virtual reality
that has three major components first
are NVIDIA GPUs that deliver
industry-leading performance and are
purpose-built for virtual reality our
new Pascal architecture brings features
like simultaneous multi-project
and graphics preemption that improved
performance and reduce latency the
second arm videos graphics drivers which
provide a great out of box experience
for gaming and professional applications
and lastly our VR Works SDK brings a new
level of immersion through ultra
realistic graphics ray-traced audio
precise touch interactions and
physically simulated environments in
addition to our GPU and sdk technology
Nvidia is creating VR tools and
applications that are both fun and
productive our first virtual reality
experience VR funhouse shows off what's
possible with our Pascal GPUs and VR
works SDK in an interactive carnival
environment where everything behaves
according to the laws of physics our
ansel technology allows gamers to
capture 360 stereoscopic images and
share them with their friends either on
Google cardboard or a PC VR headset and
our array VR technology allows
professionals to render their models
with ray-traced
photo realism and then explore them in
virtual reality to get a
sense of space scale and lighting but
finding the right hardware for a great
VR experience can be very challenging to
help aid this and videos created the VR
ready program that helps gamers and
professionals identify the right
notebooks PCs and graphics cards to
deliver a great AR experience we're
excited to be enabling the VR ecosystem
to create amazing virtual reality
experiences and can't wait to see what
comes next
Title: NVIDIA GRID 2.0 'Tower of Power' at VMworld 2015
Publish_date: 2015-09-01
Length: 120
Views: 10298
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/POxyw_oYIHA/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: POxyw_oYIHA

--- Transcript ---

hi I am Amanda Sanders with the nvidia
grid product marketing team and i'm here
in front of the nvidia tower of power
now this is an amazing demo with over
three hundred sixty-six micro tiles if
that were desktop displays are be over
80 for desktop displays covering this
entire tower which is 16 feet tall eight
feet wide and goes all the way around
with incredible content running on
virtualized desktops now these
virtualize desktops are powered by
nvidia grid which delivers accelerated
virtual desktops and applications to any
user and any device no matter where they
are from the cloud nvidia grid is
incredible technology that is
virtualizing workflows so that people
can work better in the way that they
want to work now at the show we are
announcing nvidia grid 2 dot 0 and this
platform will double the density and
performance that we've seen over
previous generations of nvidia grid it
also allows us to sport both blades and
linux operating systems for your virtual
machines so now absolutely any
organization can leverage the power of
accelerated virtual desktop in their
environment the tower of power is
actually running on four HP blade
servers each of these servers has for
Tesla m6 GPUs
in it these PP use are powered by the
latest Maxwell architecture which is
delivering even greater density and
performance than what we've seen in
previous generations this is an amazing
demo that we have at vmworld and we're
very excited to show off the new and
video grid technology and we hope you
come by and check it out and for those
of you at home please tune in to nvidia
com / grid
Title: NVIDIA SHIELD tablet Android 5.0 Lollipop Sneak Peek
Publish_date: 2014-11-04
Length: 66
Views: 78879
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/pv9qkWm16mM/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: pv9qkWm16mM

--- Transcript ---

hey guys I'm will with NVIDIA and today
we're going to show you a sneak peek of
the brand new Android lollipop OS and
other cool new features that are coming
to your shield tablet which is of course
powered by the Tegra k1 processor now
Google's done an amazing job with their
new OS and you should know that the
shield tablet will be among the first
devices on market to receive the OS via
a simple software update in November so
let's get right to it I want to show you
what the experience looks like so here
we have the brand-new you why you'll
notice that when I swipe left and right
Google's material design philosophy is
prevalent throughout
so now let's take a look at the
brand-new and video dabbler app which
comes with a new UI and a ton of new
features so the other new feature you'll
find is the new shield hub I'm just
going to launch it here and check out
the new UI
so there you have it expect to see the
new android OS the new nvidia dabbler
and the new shield hub in november for
more information check out shield nvidia
com
Title: Accelerating Conversational AI
Publish_date: 2019-12-17
Length: 165
Views: 9384
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/PWfdu4k0n2A/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLDDlY7XhxlkPg5g7bqZdhTOrWtnrg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: PWfdu4k0n2A

--- Transcript ---

so the point of conversational AI is to
have a computer that can communicate
with a person when you speak to another
person you say something the person
listens I understand what you said they
think about a response and they say that
back to you
the person doesn't sit there and think
for 10 seconds before responding and
similarly when you speak with a
computational system it needs to be very
quick and very natural how much is the
white bottle white contigo bottle is $21
because this is actually a long sequence
of computational stages everything from
ASR to NLU to speech synthesis these
stages have to be chained together and
consequently they need to be very
efficient if any one of these stages
takes a long time the entire system
end-to-end will have very long latency
and feel very awkward and unnatural so
there's a huge gap between the potential
that you see in research at these top
conferences like nur Epps or ICML versus
what's actually deployed in real
businesses today and that gap speaks to
the complexity and the sophistication
and frankly the difficulty of these
algorithms and making them work in the
real world you have to design your deep
learning models you have to understand
the algorithms and how they chain
together you have to collect training
data you have to annotate that data if
they train the models you have to deploy
the system each one of these steps is a
place where Nvidia can help what we've
done is accelerated entire pipeline so
for the first time you can easily build
and deploy the most accurate models and
run them in real time with the tools
we're developing today a developer can
easily take a state-of-the-art model
train it on their own data either
starting from a base model provided by
Nvidia or training on data from scratch
export that model so that it can be run
efficiently at runtime what we call
inference and then package that up
inside and easy to use API that
expresses the core concepts of speech
recognition language understanding and
speech synthesis so if you work in
finance and you have your own annotated
financial speech data financial earnings
calls or traders placing orders you can
take that data and customize the trained
model and now you have a model that's
going to be much more accurate and
precise for the tasks that you care
about
our vision is that every computer system
in the world from gaming to websites to
in-home devices to your car should have
a natural conversational interface and
so to enable that across all of these
different industries we need to think
holistically about what does it take to
build these systems and provide tools to
make the process of building a
conversational system easier more
efficient and more seamless and open up
the the entire field to a much broader
set of developers
Title: GPU Technology Conference (GTC) Keynote Oct 2020 | Part 9: "Computing for the Age of AI"
Publish_date: 2020-10-05
Length: 409
Views: 249567
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/pzbhU4ttSvM/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: pzbhU4ttSvM

--- Transcript ---

This GTC you can see how AI will revolutionize computing everywhere.
From scientific computing, cloud, enterprise data centers, edge to autonomous machines
A few weeks ago, we announced our intention to acquire Arm.
Arm is the most popular CPU in the world.
Together, we can offer NVIDIA accelerated and AI computing technologies to the Arm ecosystem;
reaching computers everywhere.
Last year, we announced porting CUDA and our scientific computing stack to Arm.
The performance was excellent and the response from the HPC community was fantastic.
Today we are announcing a major initiative to advance the Arm platform.
We are making investments across three dimensions:
First, we're complementing Arm partners with
GPU, networking, storage, and securities technologies to create complete accelerated platforms.
Second, we're working with Arm partners to create platforms for HPC, cloud, edge, and PC.
This requires chips, systems, and system software.
And third, we are porting the NVIDIA AI and NVIDIA RTX engines to Arm.
Today, these capabilities are available only on x86.
With this initiative, Arm platforms will also be leading-edge at accelerated and AI computing.
Let me summarize some of our announcements at this GTC.
First, the age of AI has begun, and NVIDIA is in full throttle to advance this new form of computing.
DGX SuperPOD is a ready-made AI infrastructure for your AI researchers to become instantly productive
NVIDIA Clara Discovery is a suite of accelerated and AI tools for medical researchers.
NVIDIA AI Inference compute is growing 10X every couple of years and this year surpassed
total CPU compute in the cloud.
Hundreds of companies are operating services with NVIDIA AI Inference
including Microsoft Office, American Express, and Tencent.
NVIDIA Jarvis conversational AI is in open beta.
NVIDIA conversational AI is twice as responsive, 1/3 the cost, and more natural sounding
than running on CPUs.
NVIDIA's Merlin Recommender is also in open beta.
And we demonstrated NVIDIA Maxine, an AI video platform for services like AI video calls.
Second, Data Center is the new unit of computing.
We announced the BlueField-2 DPU, a programmable data center infrastructure on-a-chip,
DOCA with CUDA is the programming architecture.
We announced a major partnership with VMware, the industry-leading data center OS platform.
We're porting VMware to BlueField.
Someday 30-40 million enterprise servers around the world will be accelerated by DPUs.
BlueField has an exciting roadmap. Over the next couple of years
we will increase throughput nearly 1,000-fold.
Third, we're taking AI and data analytics to the world's enterprise.
We announced a 2nd partnership with VMware to enable VMware virtualization
for the three computing domains needed for enterprise AI:  virtualized, scale-out, and container microservices.
And we announced that Cloudera is accelerating with NVIDIA AI.
VMware and Cloudera serve nearly every major enterprise in the world.
Fourth, NVIDIA EGX and our network of partners are helping the world's largest industries
create and operationalize AI services.
It is exciting to see successful EGX trials in manufacturing, logistics, retail, and healthcare.
And finally, for autonomous machines, we've created the AI system to write the software
and the robotics computer to run it.
And with NVIDIA Omniverse, we have a physically based simulation engine
to create the worlds to train the robots.
The age of AI is in full throttle.
Thanks for joining us today at GTC.
Title: Metro: Last Light Tech Video
Publish_date: 2013-05-17
Length: 150
Views: 49815
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/qaeXz8FJwm4/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: qaeXz8FJwm4

--- Transcript ---

Metro last light is an awesome new
graphically advanced title developed by
for a games for the PC platform as a
sequel to Metro 2033 it brings all sorts
of cutting-edge PC features to really
push the boundaries of your graphical
capabilities hi my name is Andrew
Coonrod from Nvidia and today we're
going to take a look at some of those
awesome features in Metro last light
view those features include GPU
accelerated physics dx11 tessellation
and 3d vision surround let's go ahead
and take a look
Metro last light uses direct 11
tessellation to create a more smooth
look on the characters and the
environments in the game and this shot
you can see that the polygons are
clearly visible on the character's head
whereas in this shot with DirectX 11
enabled the head of the character is
completely smooth
Metro last light uses Nvidia PhysX
technology which is industry-leading
physics simulation for the PC platform
and this scene we're going to take a
look at physics particles on the right
you can see as the bullets impact the
walls and the pillars it creates all
sorts of persistent particles on the
ground whereas on the left there are
none of these
next we're going to take a look at smoke
stimulation physics also affects the way
that players interact with environmental
effects by simply walking through a
foggy area the two characters break
apart the mist in the shot with the help
of Nvidia PhysX now we're going to take
a look at both effects combined here you
can see a grenade explosion without
physics enabled but when you enable
physics the entire room fills with
debris and the explosion creates a much
more dramatic scene
Metro last light also plays extremely
well with Nvidia 3d vision surround
combine all these features together and
you get an amazing graphical experience
and if you buy a gtx 660 or above you
get a free copy of Metro last light with
your purchase so gear up and game on
you
Title: Revolutionizing Healthcare with AI
Publish_date: 2018-11-02
Length: 171
Views: 11535
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/qFM1dXFAtJ8/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLBJ_EdNjG3wpRuNgkBqLj6I4MKcTw
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: qFM1dXFAtJ8

--- Transcript ---

[Music]
in the healthcare industry we have a lot
of sort of manual handling of documents
and basically these manual processes
right that have been happening for a
very long time so the simplest thing is
the I will help reduce costs in the use
of text technologies and speech
technologies ai will automate much of
what we do today that's manual a I can
learn and help us in making better
decisions and diagnosis and I think what
we are also going to start seeing is
these kind of deep learning AI
applications that operate on huge
amounts of data that previously were not
feasible right now I think we're already
poised in some fields to really
implement AI in the workflow so in
radiology for example we have a lot of
pain points which we feel AI could
really help us bring better care to the
patients those kinds of things
triage helping us with some simple
decisions really those could help us
right now so probably the first thing
we're going to see the human modalities
right so we basically will see these
systems that are able to talk using you
know voice using using text to
communicate with us the way other people
communicate to us there's a tremendous
amount of information that's in the
record in more recent years such as all
their genomic data and what we look in
the future for is when we really can
very all the information about the
patient and all the imaging to know
everything about the patient you only
deliver the care that a person needs as
quickly as possible and as precise as
possible to their need that will happen
people who are scared of AI today won't
be able to live without it I do believe
that we basically stand on the verge of
this new revolution we're basically
using data and and the the enormous
capabilities to analyze it will be able
to change the lives and hopefully help
people control their conditions and
prevent them from having these
conditions ahead of time I think if you
can show to the care provider that these
AI tools can allow them to respond
faster to their patients
allow them to be more accurate in their
predictions or diagnosis thereby
providing a better outcome for the
patient I think that will aid traction
within the clinical environment
organizations such as hospitals really
need to think about this and how they're
going to manage this kind of addition to
their operation but you see the tools
you put together help someone like you
know make their lives better easier more
efficient they're happy with it that
brings me joy
you
Title: NVIDIA GRID vGPU on VMware vSphere
Publish_date: 2014-08-24
Length: 70
Views: 26806
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/qGASTzqa99w/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: qGASTzqa99w

--- Transcript ---

virtual desktops have been limited by
poor graphics resulting in subpar
performance and frustrated users now
with NVIDIA grid professional graphics
delivered from the data center our
reality mighty managers can support
their most demanding users including
architects designers and engineers
VMware and NVIDIA are partnering to
deliver truly flexible high-performance
3d virtual desktops across the
enterprise nvidia grade v GPU on vmware
vsphere lets you share the horsepower of
NVIDIA GPUs giving IT managers the
flexibility to scale up to eight users
on a single and video grid GPU up to 32
users with an Nvidia grid board and up
to 64 users with an Nvidia grid enabled
server Nvidia Grid vgpu on vmware
vsphere delivering professional graphics
from the data center and helping you get
your VDI deployment ready for liftoff
Title: Top 5 Reasons to Attend GTC DC 2019
Publish_date: 2019-09-12
Length: 124
Views: 24328
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/QgetkOkYbjs/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: QgetkOkYbjs

--- Transcript ---

at GT CDC there are five reasons why you
will not want to miss the number one
developer conference in the world
important to the world's development of
artificial intelligent solutions is a
trained workforce at GT CDC we have
hands-on instructor led training where
we have all day workshops where you can
hone your craft on data science and
accelerated computing these trainings
always sell out so reserve your spot now
sure to impress is dr. Ian buck a
longtime nvidia employee and in fact the
father of cuda ian does a marvelous job
sharing the work that the world has done
and then he ends by giving you context
to what you can go do to bring
artificial intelligence to your agency
or to your company at a faster more
effective rate this year there will be
more than 100 sessions from world-class
leaders focused on use cases such as
humanitarian assistance and disaster
relief cybersecurity platforms
sustainment and more we will have dozens
of companies from around the world that
will bring to life what you heard in all
the sessions the exhibit hall will be
the place for you to see everything come
to life and for you to engage with the
exhibitors on how to contemplate
deploying these solutions inside of your
companies and agencies the number one
reason to attend GTC DC is the amazing
networking that will occur over the
three days with the world's greatest
experts in artificial intelligence you
will learn from these experts what they
have done and what you might do to make
progress across your agency or company
with respect to injecting artificial
intelligence into the enterprise of the
future so reserve your space now at the
largest AI developer conference in
Washington DC I look forward to seeing
you there
you
Title: NVIDIA HGX H100 | The most powerful end-to-end AI supercomputing platform
Publish_date: 2023-05-28
Length: 59
Views: 41737
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/QGpniBJiCZw/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: QGpniBJiCZw

--- Transcript ---

thank you
[Music]
thank you
[Music]
Title: NVIDIA RTX Brings Interactive Ray Tracing and Realistic Visualizations to KeyShot 9
Publish_date: 2019-09-03
Length: 110
Views: 35103
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/qiHnrXz3I0Y/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: qiHnrXz3I0Y

--- Transcript ---

love John was founded in 2003 since 2006
we've made in tax of ray tracing we have
a product called
key shot he shot is used to say CAD data
3d data and we make beautiful images of
it whether it's for marketing for design
for any kind of visualization this yet
secret we are showing he shot nine which
will come out this fall and wanted to
take new features in quiche of nine is
we're adding keep you acceleration most
specifically using our CX and we're
seeing some fantastic speed ups with
that we've been told that using Keshawn
is a joy of the day because it's just
simple it's you know drag-and-drop click
do something experiment you get you get
the result right there right now and you
can make creative choices so I mean
speeding up compute just makes it more
fun for our users to work it makes them
more creative which is important it used
to be that you could create one image
then we added animation capabilities now
they do a thousand images through an
animation now with added VR capabilities
so what happens is that as we add more
power to the software the designers the
marketing people they just expect it to
do more the GPU is just very necessary
to make something like a be art solution
work I mean we need 90 frames a second
high resolution to cover the entire
field of view you need the final frame
quality every single frame 90 frames a
second and the only way to do it is to
do a GPU rendering really last year with
the Turing architecture we could see now
as to time it was just amazing and
that's what we got on board full time
seeing these new laptops and the one
with the RT X 5000 and r6 3000 is just
amazing I hope I can kidnap it after
SIGGRAPH and we will have to send it
back because I absolutely want one of
those it is so fast it is so sweet and
it's nice and compact we absolutely love
it and keeps everyone's fantastic on it
Title: A New Era of Healthcare with AI | I AM AI
Publish_date: 2020-10-08
Length: 133
Views: 27817
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/qlNbC88SU7o/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: qlNbC88SU7o

--- Transcript ---

[Music]
i
am a healer
bringing clarity to our most important
questions diabetic retinopathy can
happen if blood sugar stays too high
over a long period of time and comfort
in times
of uncertainty what procedure am i
having today
you are having a bronchoscopy
shining a light on a path forward
and finding answers when every
second counts
i am watching over our frontline heroes
creating a safer place to do their
life-saving work
[Music]
guiding them to faster answers
and delivering care wherever it's needed
[Music]
i am opening new worlds of discovery
and mapping new treatments for every
individual
[Music]
creating a community to collaborate with
thousands and sharing
knowledge that can heal millions
i am a i brought to life
by nvidia and brilliant healers
[Music]
everywhere
[Music]
you
Title: Assassin's Creed® IV Black Flag™ PC: Behind the Scenes
Publish_date: 2013-11-19
Length: 319
Views: 57753
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/QofPrgUXDpU/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: QofPrgUXDpU

--- Transcript ---

hello everyone my name is Ashcroft
Ismail I am the game director of
Assassin's Creed 4 Black Flag and we're
here at Ubisoft Montreal I'm very proud
to be here with you our PC gamers our
g-force fans this is where we make
Assassin's Creed and it's an absolute
pleasure for me to be able to show you
behind the scenes of how we're going to
make this AC the best AC on PC let's
come follow me
so this is where it all happens this is
the ac4 dev team behind me is an army of
the most passionate and talented game
developers out there there are
programmers engineers designers artists
all in the back they're working to make
the best AC game possible if you'll
follow me I'll show you some more stuff
we're here in our sound booth over there
we have a Foley stage you know sound
design is very important in Assassin's
Creed we need to marry these amazing
visuals with great audio so we do
everything in-house and that includes
sound effects voiceovers and music have
one more thing I'd like to show you so
let's go more emotion so what I'm about
to show you is Assassin's Creed 4 Black
Flag on a 4k screen so I've never seen
this before this is my first time seeing
it and it's going to be the time we show
it to you so I'm really excited let's go
[Music]
with the business stunning this is
absolutely stunning putting the texture
so we're here so then with our PC gamers
everyone this is Silvana our Associate
Producer so Sylvia maybe you can walk us
through and tell us what is 4k mean what
does it mean for PC so
4k basically is like four screen of
1080p in a single screen okay so that
means all the textures everything's just
high-res yeah the game is like just
simply awesome look at all the wood
planks and everything so you see like
the textures like I can feel the grain
yeah what you can see all the different
essence of wood that were used to do to
build a ship and even though the shadows
are high-res yeah the shadow is super
high res so another big advantage of 4k
is that they could this look at all the
ropes so even if you are far it's it's
super hard to draw this thing but with
this number of pixels like the GPU we
have it's showing like super nicely this
is this is actually breathtaking I mean
up I've never seen the game this
detailed I've never seen the game that's
beautiful before Wow
[Music]
so with Assassin's Creed 4 Black Flag
you know when we started this game we we
told ourselves we wanted to make the
definitive pirate experience and from
there we this is where we started
telling the tale of Edward Kenway the
grandfather of Connor from ac3 a British
man who's poor who's a sailor who gets
sucked into the life of piracy because
he gets lured by fame and fortune this
is the story we tell when we started
Black Flag and we said we're doing the
Caribbean we needed to deliver this
picturesque vision of what the Caribbean
is needs to be this really beautiful
place and we found that on PC we can
push a lot of the features a lot of
visuals to have it be much more credible
much more authentic and just visually
stunning and then we ship on many many
platforms but we wanted to put extra
love and care into the PC platform we
love the PC it's it's really one of the
most beautiful ways of playing the game
and seeing the game
working with nvidia from the beginning
you know it was a partnership that was
really about how can we push the pc
version of the game and so we've had our
engineers working with Nvidia's
engineers to really push the tech to
really push what the cards can do will
to push what our engine can do really
all in the hopes of having a really
stunning and beautiful pc version of the
game so you know things like TXA
anti-aliasing we have a lot of rigging a
lot of rope on ships and you know we
can't have jaggies everywhere so we
really wanted the most beautiful version
of anti a single can have on top of that
we have tons of things in the
environment you know things like jungles
things like cities with a dynamic
day/night cycle so we needed to have
shadows that were really breathtaking
that were very believable and so we take
advantage of pcs s on the geforce cards
this is where the shadows have a lot
more life to them they have a lot more
fullness to them
of course we we have a beautiful Sun we
have this amazing cloth we have all
these foliage around you so we needed to
have improved godrays we take advantage
of this feature called turbulence this
is where whenever you have a huge puff
of smoke that comes out and disperses
into the environment in a really natural
way so something like a cannon or pistol
and this is unique to the Nvidia cards
and this is something we take advantage
of in AC for Black Flag
a c4 we set out to make the definitive
pyro game and it is the best AC game
we've ever built it's also the most
beautiful game we've ever built I think
people are going to absolutely eat this
up
[Music]
Title: How NVIDIA DRIVE IX AI Algorithms Perform Intuitive In-Cabin Perception - NVIDIA DRIVE Labs Ep. 23
Publish_date: 2021-01-19
Length: 136
Views: 24510
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/QSrNJdAbUjs/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: QSrNJdAbUjs

--- Transcript ---

in this episode of drive labs we're
switching gears from autonomous driving
software to the ai cockpit with nvidia
drive ix
driveix is an open scalable cockpit
software platform
which provides ai functions that enable
a full range of in-cabin experiences
this video covers one of the key aspects
of driveix
driver monitoring which enables the av
system to ensure a driver is alert and
monitoring the road
current driver monitoring solutions have
a hard time accounting for variables
for example if the driver is wearing a
hat glasses
or these days a mask drive-ix uses dnns
for computing
all features needed for robust driver
passenger and cabin monitoring
the gaze net dnn tracks gaze by
detecting the vector of the driver's
eyes
and mapping it to the road to check if
they're able to see obstacles ahead
sleepnet monitors drowsiness classifying
whether eyes are open or closed
running through a state machine to
determine levels of exhaustion
finally activity net tracks driver
activities such as
phone usage hands on or off the wheel
and driver attention to road events
there's even a dnn that can detect
emotions
this dnn can classify a driver's seat as
happy
surprised neutral disgusted it can also
tell if the driver is squinting or
screaming
indicating their level of visibility or
alertness and state of mind
these driveix monitoring dnns together
with gesture dnn and speech capabilities
enable multimodal conversational ai
offerings
such as automatic speech recognition
natural language processing
and speech synthesis how's the weather
in san francisco
it is currently 66 degrees and sunny in
san francisco
this can be used for in-cabin
personalization and virtual assistant
applications
the driveix software platform is a
holistic solution for advanced driver
monitoring
full cabin monitoring and world-class
visualization
for an innovative ai cockpit experience
that builds
passenger trust in autonomous vehicle
technology
Title: NVIDIA GTC For Everyone
Publish_date: 2021-04-22
Length: 30
Views: 19911
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/qTjB_Mopy68/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: qTjB_Mopy68

--- Transcript ---

welcome to gtc 2021 this is a mountain
of technology
enjoy
in total 1600 talks about the most
important technologies of our time
from the leaders in the field that are
shaping our world
[Music]
let's go now we have better
[Music]
collaborations you
Title: BenevolentAI – Accelerating Scientific Discovery With AI
Publish_date: 2016-12-21
Length: 170
Views: 11067
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/qtkZt5u1XE8/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: qtkZt5u1XE8

--- Transcript ---

[Music]
benevolence AI is an exciting company
based here in London
which is using artificial intelligence
to mine corpus of scientific information
and knowledge out there to really
accelerate and change the process of
scientific discovery
we know that artificial intelligence is
everywhere right across industry and
NVIDIA is the platform underpinning that
research and that intelligence and
innovation and today we're delivering a
supercomputer in a box
the Nvidia DG x1 which is the best
platform for artificial intelligence
anywhere in the world I'm like many
other companies that were looking for
ways to enhance their machine learning
algorithms and do a larger deep neural
networks we were using individual
off-the-shelf graphics processors from
Nvidia and realize there's a scalability
issue we want to do very large models so
the dgx 1 gives us somewhere around 10x
the capacity over what we've already
built here in-house and we're hoping it
open or open more doors and get us
better ideas often times there are
projects that we don't even attempt
because they're so large one example we
have a chemical database of over 170
billion potential chemicals that we will
look at 3d models on and our original
estimates would have put that in our
current systems at around 370 days worth
of a project in lifetime the dgx one is
going to let us do that in about 16 days
so projects that we wouldn't have
otherwise attempted we can now and have
reasonable feedback loops on them I work
as a biomedical data scientists here at
benevolent AI we deal with vast amounts
very fragmented and very
diverse data no human could analyze and
make useful insides of the current
amount of data in their lifetime it's
like looking for a needle in a haystack
so we need computers to augment our
ability to consume these data and make
useful inside of it so although there
are some alternatives in the cloud we
didn't see them as cost-effective and to
do the size models we wanted to do we
now have access to a best-in-class GPU
cluster this allows us to build models
more rapidly test hypotheses quicker and
ultimately get the results into the
hands of our scientific researchers
faster using the dgx one is going to
allow us to explore a lot faster than we
would have been exploring before so some
of the plans that we had on the roadmap
for months and months in the future
we're starting on now versus waiting and
ultimately we hope with research in
areas like drug discovery this leads to
drugs sooner rather than later to solve
major problems in patients around the
world
Title: Sneak peek to Android 6.0 Marshmallow on SHIELD Android TV
Publish_date: 2016-01-05
Length: 140
Views: 62830
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/QtPGPBN5PJQ/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: QtPGPBN5PJQ

--- Transcript ---

build a product launch it done she'll
doesn't like that we continue to bring
exciting new features in full OS updates
to the Platinum and because she'll be
spoke on Android the world's largest
open app ecosystem new entertainment
apps and games are arriving all the time
let's take a look at what's in it our
next shield update coming soon will
upgrade the entire operating system to
Android 6.0 marshmallow first time setup
has never been easier with him you can
simply say ok Google set up my device
from your Android mobile device and your
Google account and Wi-Fi password will
be wirelessly transfer to your shield
Android TV no typing required if you
launch them to the shield home screen
you'll experience the same snappy fluid
UI that you've come to expect the shield
and you can now customize the home
screen experience you can choose which
apps you want to appear on the
recommendations row and you can also now
reorder and flick to locations of your
app and game tiles mmm also makes using
expanded storage a lot easier when you
connect an external storage device like
an SD card or USB Drive M allows the
storage to act like increased internal
memory on your device this enables your
apps and games to look feel and act the
same as traditional memory without any
extra management required the Google
Play Store has been redesigned to make
it easier to find your favorite apps and
she'll that's a ton of great
entertainment apps an app that may speak
now Showtime and a lot more Nvidia has
your movie TV and gaming needs covered
and of course the shield you have access
to your favorite apps like Netflix
YouTube flex and cody all up to 4k Ultra
HD resolution so it's also the only way
to experience the full range of gaming
building on invidious long gaming
history shield features an NVIDIA
GeForce now game services so you can
stream PC games from the cloud we've
recently added games like sniper the BT
the library of membership games shield
also features Android games like
Minecraft story mode and debt effective
that you can download from the Google
Play Store and with NVIDIA game
streaming technology you can enjoy your
favorite PC games like Metal Gear Solid
back to things and stream it from the
GeForce GTX PC redshift extreme TV last
but not least
shield has Google cast already built in
so you can cast entertainment apps from
your Android or iOS phone directly to
your TV that's
photos with your phone or tablet Google
photos watched YouTube gaming on the big
screen or even play family cash games
like monopoly or we look fortune and
there you have it for more info about
the Nvidia shield Android TV be sure to
head over to shield but Nvidia calm
Title: NVIDIA at COMPUTEX 2023 Highlights
Publish_date: 2023-06-02
Length: 231
Views: 19292
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/qxXAz_-lQbc/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: qxXAz_-lQbc

--- Transcript ---

foreign
[Music]
[Music]
we're back
[Music]
our first
live event in almost four years
[Music]
hello from Taipei I'm Dion Harris head
of accelerated Computing product
Solutions at Nvidia
computex Asia's most important Trade
Tech show
is back in full swing after a three-year
hiatus
and this week Nvidia took center stage
at computex from Jensen delivering a
memorable opening keynote to nearly
three dozen Partners showcasing
Technologies on the showroom floor
and an AI developer Meetup and more
Jensen kicked off things with a bang
speaking to a packed House of more than
3 500 with tens of thousands watching
live from around the world in a sweeping
90-minute talk he covered a series of
dramatic announcements and demos
together they showed how nvidia's
accelerated Computing platform and
partner ecosystem are powering a new era
of generative Ai and Industrial
digitalization
here are some of the key announcements
first he announced the Nvidia dgx gh200
AI supercomputer the system seamlessly
connects 256 Grace Hopper Superchips
into a massive 1X of flop GPU enabling
developers to build large language
models for generative AI chat Bots among
other important use cases
second he presented nvidia's mgx and
open modular server design for
Accelerated computing
third
he shared how SoftBank is building a
next-gen data center powered by Nvidia
Grace Hopper for generative Ai and 5 and
6G Communications
fourth
he showed how nvidia's Spectrum X
ethernet platform which is specifically
designed for hyperscale degenerative AI
fifth he talked about announcements in
the Omniverse how leading electronic
makers are embracing nvidia's generative
AI services and Omniverse to digitize
their state-of-the-art factories
and Nvidia is teaming up with wpp to
create a digital ad content engine in
Omniverse
and finally Jensen described Ace for
games which leverages generative AI to
breathe life into virtual gaming
characters hey Jen how are you
later on opening day Jensen appeared as
a special guest at mediatek's CEO Rick
size press event they announced a
partnership to develop in-vehicle AI
cabin solutions for next-gen
software-defined vehicles from entry
level to Premium
on day two Greg Estes head of corporate
marketing and developer programs at
Nvidia gave a talk on racing toward the
industrial metaverse he described how a
new era of 3D workflows is being enabled
by Nvidia and its Partners using
Omniverse generative Ai and accelerated
computing at the non-gang Exhibition
Center over 30 Nvidia Partners showcased
our Cutting Edge Technologies separately
we had a dedicated AI developer Meetup
where they got a chance to meet and talk
with Jensen in person
it's been quite an exciting week for us
all we're already looking forward to
next year's edition of computex see you
then
Title: Google Cloud Platform Makes NVIDIA T4 GPUs Available
Publish_date: 2019-01-16
Length: 85
Views: 6152
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/q_eg61owzrk/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: q_eg61owzrk

--- Transcript ---

yeah yesterday Google cloud launched
Nvidia t4 and GCE it's an infrastructure
as a service offering we're offering in
10 regions at the private alpha today
and we expect to go to beta pretty soon
yeah customers really look for a
high-performance low-cost solutions and
with the Nvidia t4 they're gonna be able
to innovate watch new machine learning
applications as well as do interesting
visualization with the right tracing
technology that video launched yeah
let's talk about the applications a
little bit first ones in France right
whether your startup or a global social
media company right you need low latency
in France and thanks to the Nvidia t4
with tensor RT it's low latency in
France with up to six thousand images a
second for some models it's really
unique position and with Google clouds
ten regions distributed globally we
think our customers no matter where the
users are will have a very low latency
service and we're really excited about
that
the other one is visualization whether
it be your professional workstation
gaming or some sort of service that you
really want to do very dynamic real life
booking services Nvidia stew for api's
and Grid software are gonna make this
product extremely valuable and
interesting for our customers so today
I'm very excited to be announcing this
because our customers always pushed for
the best innovation and fastest we're
really happy to partner in the video to
launch the t4 and get this innovation in
the hands of customers as quickly as
possible
Title: 4 Epic Next-Gen Games You Can Stream Soon to NVIDIA SHIELD
Publish_date: 2017-02-08
Length: 197
Views: 14985
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/R1Am92QfypM/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: R1Am92QfypM

--- Transcript ---

susheel TV announced at CES offers a
wide variety of features including the
next generation of g-force now let's
take a look at some of the g-force now
games that Nvidia announced with the new
shield TV thanks to the raw power of
g-force nails new Pascal architecture
you'll soon be able to pay some of
Ubisoft biggest titles
either by buying direct on the GeForce
now store or by linking GeForce now with
your existing Ubisoft accounts so you
can soon expect to be basking in some
soap San Francisco with watchdogs to
welcome to dead zone this open world
action-adventure sees you put rookie
savant hacker Marcos through his paces
of he and his newfound friends try to
expose corporate and governmental
corruption at the very highest level you
can use your smartphone to hack your way
through security systems goes out new
locations with high tech drones and even
3d print your own arsenal of devastating
weaponry just don't forget to stick to
the speed limit but meanwhile over on
the East Coast you'll soon be able to
enjoy a distinctly colder and somewhat
bleacher near future open-world New York
with pom can seize the division all
things wrong with me I think I need a
doctor left in a city decimated by a
smallpox outbreak you control an agent
of the strategic homeland division
tasked with helping to uncover the roots
of the outbreak and tackle the lawless
power vacuum that in its wake in
addition to working away through the
main storyline you can also step into
the Badlands of the dark zone an
abandoned area of Manhattan Midtown rich
in iron weapons and gear but thick with
dangerous NPCs and other human players
waiting for any opportunity to rob you
blind and leave you for dead
unless of course you can get em first
g-force now also opens up fully
connected huge online multiplayer world
for you to enjoy
if MOBAs are your thing put your little
tired of the traditional top-down view
then Paragon is going to be right up
your lane
coming soon g-force now on the new
shield TV this game drops you down into
the action with a third-person
perspective your hero will join with
four comrades on a selectable map to
battle the opposing team take out their
defensive powers and finally destroy
their core minions will trundle along in
support but you can also upgrade your
abilities as you go crucial if you want
to attain the upper hand GeForce now
offers a vast range of titles to place
instantly but last on this list of fresh
faces for 2017 is the supercharged
MMORPG Marvel heroes now under Furious 6
are Hylian launching seen on g-force now
expecting the iconic Marvel Universe
where you can play as some of the
best-known superheroes right from the
start unlocking yet more as you
progressed the wider storyline was
written by comic book legend Brian
Michael Bendis and see you team up with
other players to foil dr. Doom's evil
plan recover the apocalyptically
powerful cosmic cube and defeat some of
Marvel's best baddies along the way
check out my time you can play a whole
host of great games instantly with
people's now on your Nvidia shield and
even pick up your first month for free
just head over to shield and video comm
for more info
Title: GeForce Garage: Cross Desk Series, Video 4  – How to Install LED Lighting
Publish_date: 2014-10-24
Length: 636
Views: 263178
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/R59z5KB-0N4/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: R59z5KB-0N4

--- Transcript ---

hi I'm Andrew with NVIDIA you're
watching GeForce garage last episode we
installed our custom powder coated
copper tubing to our liquid cooling
system and it looks amazing so now what
we're going to do is we're going to
accentuate our build even further with
some custom LED lighting today we have
Bob Stewart in the house from BS mods
comm he is a true expert at LED lighting
in fact he's told me he's got some ninja
ways to shine a light on our awesome
build here
hi I'm Bob Stewart with BS mods and I'm
here today to talk to you about LED
lighting when people think of LEDs they
think of the actual LED bulb like this
technology come a long way with
different LED types sizes and colors
this is the single white strip in the
50-50 those are nice if you just want a
single color and just have something lit
up nice and bright you can pick the
waterproof type that has a silicone
shield on it sometimes you'll see these
they're a little bit of a goldish brown
color so the silicone does diffuse the
light a little bit better than non
waterproof this is the 3528 RGB LED
strip non waterproof and as you can see
the different colors are on different
chips chosen the green color right now
there's not as many LEDs per meter on
the Roll makes it not that bright the
50/50 tri chip meaning that all three
colors the red green and blue all in one
chip so you get a lot wider color
spectrum from you can see that there's a
huge difference between the two a lot
more LEDs per meter and they are three
times brighter than the 3528 this is the
waterproof one so it does diffuse the
light a little bit better gives you a
little bit more of a glow and some
people do like that rather than the pin
dot look of the non waterproof LEDs and
these can be changed to different colors
so 5050 is really the best way to go in
a PC case a lot brighter
you can always dim it down if you need
to with a controller but you're pretty
much stuck with that amount of
brightness if you get the 3528 I'm going
to show you some of the supplies and
tools that you might need during a
typical installation of LED lighting
starting with some sheers it's a good
thing to have sheers that can cut tubing
or wiring things like that and then also
small snips too to get it really tight
if you need to then you'll need some
strippers these are your basic strippers
these also have cutting blades and these
are for crimping too so it's a
multi-purpose tool then you're going to
need some wire and you have your black
and red which is standard for positive
and negative just want to use a fairly
small gauge maybe
18 to connect the wiring you're going to
need some connectors these are a molex 4
pin connector and you can either make
your own with some pins and connector
housings or you can just use one that is
pre-made just picks them up at a store
and also an removal tool very useful if
you are taking a set of connectors that
are already pre-made you can use this to
pop these out and then do your wiring
with it if you're going to want to
protect the raw wire so you're going to
want to use some shrink tubing comes in
various sizes and colors and the heat
gun to shrink that tubing down if you
are installing your light strips and you
want to make sure that it doesn't come
down may want to use a hot glue gun or
you can use double stick tape it comes
in different ratings we recommend this
heavier duty tape or you can get it in
clear or difficult gray then when you
run your wiring you want to make sure
that you drama your holes and there are
several sizes so you're going to want to
get a grommet that's the right size for
the hole that you drill it's very
important to protect your wires from the
sharp edges once everything is all
connected you can choose to either wire
it straight up to your power supply or
you may choose to wire it up to a switch
where you can control it and have on and
off when you decide which one you want
to use you need to connect it somehow so
you need to have a control box and this
has a plug in on one end for the power
and a plug in for the LED strip on the
other end this will allow you to control
the lighting and there are different
types of controllers you can use a
simple little inline controller that
will do basic functions or you can move
up to the infrared controllers and you
can choose a lot of different colors
with that and if you want to go all the
way up to the very best you can get a
larger 44 key controller and there are a
lot of DIY functions on here so that you
can pre-program it things like that for
powering it all up you'll need to get a
connector that plugs into the control
box so this would just control the power
going in and then you would need to wire
up a molex connector if want to run it
straight off of the power supply rather
than do that if you're wanting to just
wire it in to a set of wires you can
purchase one of these little connectors
here plug that in and now you have a
power connection to the control box you
can get control boxes with more than one
lead for the LEDs so they
you can have two different strips and
these just plug in with pins that are on
the ends of the LED strips so when you
plug those in your lights will turn on
and then you can use your controller to
change colors things like that so Andy
that's basically how LED lighting works
that's amazing that's a lot of stuff so
of all those things you just explained
how we going to install some of that in
our system here
well for starters what I'll do is I'll
light up the GPUs and the water blocks
with the single color white LED strips
so we'll have one light strip right
underneath each GPU it'll shine the
light up through and this whole thing
will glow white awesome what we'll do
for the hot part of the desk is use the
16 foot RGB kit with the remote
controller so that we can pick and
choose what color we want if we want it
to fade or flash all kinds of different
functions and then we'll also use that
on the bottom of the desk so that it
shines down on to the floor and gives it
a nice glow to so the RGB strip can do
pretty much any color so we could have
like a rave party in here for you yeah
absolutely
just with the push of a button wow
that's awesome
so I'm going to install the under
cabinet lighting the first thing you
want to do make sure that it works
before you install it I've got power to
my power supply going to plug in the
molex connector that's plugged into the
control box and then I want to make sure
that I plug the control box in with the
correct polarity all right it's like we
have a working unit then we want to go
ahead and test the remote control make
sure that that works correctly
so there's off back on go through the
red green and blue looks like
everything's a go next thing we want to
do is prep the control box for the
location so we've got our sticky tape
and we're going to cut a piece to stick
to the backside of it so one thing you
want to consider too is where the IR is
going to be because you'll need line of
sight between your remote control and
app and I've determined where the best
mounting location is going to be sort of
close to the power supply we're just
going to run the LED strip the length of
the bottom of the desk here okay so
about that much snip that off so these
strips have healing stick 3m adhesive
back so you can peel that off you want
to make sure the surface is clean we're
probably good to work in a small section
at a time you want to press it in really
good so this is a textured surface on
this desk so I do feel that we probably
need to add the tank because I can pull
this down pretty easy so we'll add the
tape where you'd hear it and it'll be
locked in place for good
so I've got to plug the power in to the
power supply and then also plug the LED
strip into the control box wiring there
we go
then we can either put a piece of shrink
tube on your connector to make sure it
doesn't come loose or a pizza of
electrical tape is fine
just want to make sure that doesn't pop
loose and then again make sure that you
have your receiver I still showing okay
test it again works pretty awesome all
right Bob so after seven days and seven
nights everything's installed and you're
ready to say let there be light yep all
right let's do it ready to go see this
data actually try it
there it is you're the man thanks for
coming in and helping us out you're
truly the master so last thing though
Bob yeah
you promised me party mode I did promise
party mode let's see some party mode I
hope I could can deliver
don't forget to check out our next
episode where we complete the red
harbinger cross test with some snazzy
vinyl decals thanks for watching g-force
garage the ultimate resource center for
designing building and customizing your
GTX PC Nvidia buttoned-down
Title: NVIDIA Powers Virtual Reality
Publish_date: 2016-04-04
Length: 99
Views: 23624
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/rAt0abasUAA/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: rAt0abasUAA

--- Transcript ---

nvidia has powered the world's highest
performance graphics experiences for
more than 20 years now we're harnessing
all that innovation to deliver amazing
virtual reality experiences I see
virtual reality as the next major
competing platform like smartphones over
the last 10 years virtual reality is
going to change how we get business done
how we interact with friends and family
and how we enjoy games entertainment
every industry from automotive to
medical to art to gaming it's going to
be changed by virtuality from a graphics
perspective there are two major
challenges the first is that virtual
reality requires an incredible amount of
graphics performance up to seven times
that of a normal PC game the second is
that it requires very responsive
graphics when I move my head that
display has to update immediately within
20 milliseconds in order to provide a
comfortable experience in videos
building a powerful graphics platform
for virtual reality there are three
components the first are NVIDIA GPUs
which provide industry-leading
performance and are architected
specifically for virtual reality use
cases the second is invidious graphics
drivers that provide a great out of box
experience for gaming and professional
applications and lastly our VR Works SDK
provides developers with the lowest
latency best performance and
plug-and-play compatibility for their VR
headsets and applications we're excited
to be enabling the VR ecosystem to
create amazing VR experiences and can't
wait to see what comes next
Title: Reinventing Retail with AI | I AM AI
Publish_date: 2020-10-05
Length: 120
Views: 30755
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/RAzohJygdmc/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: RAzohJygdmc

--- Transcript ---

[Music]
i
am a helper
powering new discoveries everywhere we
go
[Music]
and helping us reimagine our perfect
home
i am delivering on every promise
predicting what comes next
and stepping up to meet the demand
[Music]
ensuring care in everything we deliver
[Music]
and the safety of those who make it
happen
i am anticipating every need
responding to every challenge
saving valuable time
and simplifying our busy lives
[Applause]
delivering a better future
for us all
i am ai brought to life
by nvidia and inspired minds everywhere
[Music]
you
Title: Summit Early Science Video Series: Fusion Research
Publish_date: 2018-11-11
Length: 325
Views: 10600
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/Rc7uUhpEtC0/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLAJfQmCq3TXR5tIkZFtc9n4-h6CoQ
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: Rc7uUhpEtC0

--- Transcript ---

[Music]
I'm doing research to have a clean
energy which can supply electricity to
mankind for millions of years
the fusion energy is what you see from
the Sun we are trying to copy what is
happening in our own Sun plasmas are
ionized particles when ordinary gas
becomes about ten thousand degrees they
tend to ionize into ions and electrons
once they reach millions of degrees
their energy is high enough to collide
and fuse together to generate other
particles and neutrons and alpha
particles are so hot they can provide a
self-sustained worm and neutrons will go
out to heat up the liquid to power the
turbine we are trying to understand the
fusion boundary plasma which is the most
complicated physics area in a fusion
reactor and we need were the biggest
computer to understand the boundary
plasma the way we want seven countries
got together building a fusion reactor
in France and we call that ether that
will be the first test reactor producing
hopefully ten times more energy than it
consumes you do not want to have trial
and error after the ETH built that's
gonna delay the development of fusion
program unless we understand and do the
simulation and predict the plasma
behavior at burning condition it may
take 20 or 30 years to have a lot of
confidence of what we go ahead to
commercial reactors
kar komak is a magnetic confinement
device we shaped magnetic field into
tonal shape
ideally plasma will follow magnetic
field lines without escaping them then
we found out that plasmas are not really
contained following the magnetic field
lines we did not understand instability
and turbulence existing interconnects
the instability in plasma exists
everywhere especially in the edge we
need certain density of a plasma and
millions of degrees of temperature to
get the burning going so the turbulence
when it occurs try to take that energy
out of the burning core and send it to a
place on the material world so we have
to keep the plasma highly confined and
at the same time keep the world safe
from pointing out scientists found out
by accident if they put in even more
heat plasma chilled turbulence in the
edge or myself
so Eddie welcome find the edge plasma is
calm almost turbulence free and the
whole core plasma in the center is now
confined by their barrier at the edge
until now we did not have much
understanding at the fundamental level
how plasma does it and that's one of the
topics we study in our project I'm
measuring turbulence in a small window
about 10 centimeters wide in each
direction Eddy plasma is filled with
many scale physics
and we have to simulate all that
together in a very odd shaped geometry
that makes the computing very expensive
: strength in this layer which into
white line summit has GPUs called Volta
those GPUs are very powerful and we will
be able to do physics which we have not
been able to do before
with tested our codex EC on summit if
you compare between GPUs and pure CPUs
we were able to get over 11 times faster
computing speed on Summit and also
compared to Titan if you compare the
same number of GPUs then we were able to
get three point eight times faster
computing on Summit our code xse
produces so much data it's too big to
output to the filesystem
therefore we have to put AI into the
compute memory to analyze your data as
our excessive code is running the AI
program will try to select what's
important where is the new physics and
then the selected data will be
compressed so that they can be stored on
the filesystem the final goal of our
project is to do what we call whole
device modeling and build up a virtual
tokamak so that we can predict and
design the next fusion reactor
[Music]
you
Title: SHIELD TV Smart Home with Google Assistant and SmartThings Link
Publish_date: 2017-10-23
Length: 54
Views: 37177
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/RdMBgtZhUbQ/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: RdMBgtZhUbQ

--- Transcript ---

with the Google assistant on shield you
can ask for almost anything like ok
Google played the best of Katy Perry
Katy Perry ok google play some jazz in
the kitchen you could also tell Google
to order a large Hawaiian pizza
pineapple on a pizza no thanks how about
show me beach destinations or mountains
gasp see the San Francisco Giants score
New York Giants turn on the a/c heater
come on man I need this voiceover job
how about auto-tuning horseshoeing beard
girly beard grooming are you serious
well I'm going on vacation ok Google
show me some videos about beard grooming
on YouTube now with the Google assistant
on shield and the smartthings link
you're in control whoa
might want to postpone that vacation ok
Google staycation mode
[Music]
Title: Omniverse Create Overview  | NVIDIA Omniverse Tutorials
Publish_date: 2020-12-15
Length: 263
Views: 19067
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/REHXZE0XEdw/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: REHXZE0XEdw

--- Transcript ---

i'd like to give you an overview of
omniverse create
create is an app built on the omniverse
platform
that is a native inspection world
builder and rendering of usd content
the first thing you'll notice in create
is a viewport
the viewport is a hydra based delegate
system
where allows you to have multiple
renderers so here you can see we have a
real-time ray tracing
renderer developed by nvidia and a path
trace renderer developed by nvidia
and both are extremely fast at rendering
usd content
using ray tracing additional hydro based
renderers are also available like ira or
storm
but create is a native usd editor so
that means you start with some usd
content
and you can bring in usd content either
through the content browser here
you can get local usd data off your hard
drive or you can collaborate using a
nucleus
server as well once you have your usd
data
you can inspect it there's a stage view
that allows you to inspect the usd data
that's available within the scene for
example i can get to the
lighting and i can adjust any property
of that lighting directly in the
stage viewer another great feature
of inverse create is a layer system so
this layer system
leverages usd layers to allow you to
collaborate
and structure your data into layers and
by doing this i can form
structures like for example having a
background layer that has different
lighting in it
and layers can have different opinions
of color and objects
or use it as a way to collaborate with
others and lock
layers for working together
but it's a great system for building out
large worlds
and again here i just turned on my
background lighting
and now i can even path trace that
quickly
another great feature of immersed create
is animation
it supports both usd scale and time
sample data animation from usd so here i
can see i can scrub the timeline
i'm actually getting very fast results
even with path tracing
and if i go to real time mode i can also
play back
this model using the animation system
another great feature of create is
allows you to create new objects
so for example here i can create an
object like a cylinder and move it
around and place it how i want
duplicate it inspect it and duplicate
and instance it if i want but
additionally i can create objects
through scripting if i don't want to use
the menus i can create new objects
through python through scripting
so here i can run a script that creates
a new object that shows up here in my
stage that i can grab move around
so allows you to create new objects
either through
python or through the menu system
now the great thing about create is most
of these menus and tools
are all built using the scripting
language
so when you create a script that you
like you can also share it as an
extension
so we have an extension manager within
create that allows you to share these
these extensions with others for example
it comes with a lot of great extensions
if i look at some of the extensions here
you see things like
flow and physics and lots of
great extensions you could add on top of
create
for example here i'm going to run the
image viewer the image viewer adds a new
capability
within my content browser that allows me
to inspect
materials and textures for example i
look at this texture
and double click on it it'll bring up a
new viewer for that texture
so just one of the many ways you can add
new capabilities to
create on top of the omniverse platform
in future videos we'll talk more about
layers and importing
assets and even the live mode you can
have within layers
that allows you to collaborate live
between multiple applications
you
Title: NVIDIA Quadro RTX Powers Virtual Helsinki Experience by ZOAN
Publish_date: 2019-04-29
Length: 56
Views: 23017
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/REWvG65h6p0/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: REWvG65h6p0

--- Transcript ---

I'm Mika Rosenthal founder and CEO of so
on we're demoing a virtual Helsinki here
where you can actually enter in virtual
reality and go to the different
touristic spots we're building
everything for the Unreal Engine and as
we were using the cheapest cars in the
beginning that was a challenge because
our scene is so large so we got
ourselves a Quadro car that has been a
game changer for us as we've been able
to actually have enough memory to run
the experience in real time and even get
those raytrace shadows and reflections
and even the global illumination in
place our bottleneck was the graphics
card but now with the Quattro cars we
can actually expand the city with more
speed and we can actually get way more
locations into our virtual experience
[Music]
Title: Hitting a Design Grand Slam with NVIDIA RTX and Unreal Engine
Publish_date: 2019-10-02
Length: 65
Views: 12389
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/rjAL95ZTNXg/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: rjAL95ZTNXg

--- Transcript ---

[Music]
this is a ballpark for the Texas Rangers
it's in Arlington Texas this is a real
project and it's under construction
right now it'll be open in April 2020
we're using Unreal Engine and one of the
largest real-time models that probably
anybody's ever produced
and we're combining unreal engines
capabilities with NVIDIA 's r-tx cards
to get to photo realism pretty rapidly
and easily Unreal Engine has always been
challenged by glass reflection which is
a huge part of architecture so r-tx
helped solve that and then it also
brings in a lot of more subtle effects
like ray tracing an occlusion ray traced
shadows and all of those little
subtleties combined together to make a
much more realistic image r-tx brings
capabilities photo realism and shortened
timelines that are just unprecedented
[Music]
Title: All the Feels: NVIDIA Shares Expressive Speech Synthesis Research at Interspeech
Publish_date: 2021-08-31
Length: 138
Views: 85722
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/RknIx6XmffA/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: RknIx6XmffA

--- Transcript ---

i
am a visionary
for the gtc conference in 2017
we debuted a new series called imai we
had wanted to use an ai voice to narrate
and the results were promising but they
really didn't quite sound human enough i
am even the narrator and it wasn't until
early 2020
that our internal ai research team had
actually developed a voice called
flotron that sounded quite human
i am a storyteller
giving emotion to words so we've been
trying for quite some time to have
speaks synthesis that sounded natural
finally we did it so it was successful
in that way but it was certainly not
complete i wanted to be able to direct
the ai voice the way i direct a human we
started working on a model called rad
tts that would allow the director to
actually record his or her own voice and
guide the persona's voice that was the
next step with rad tts i could record
myself saying a particular line when i
emphasized a word when i put more energy
into a word when i lowered or raised my
pitch or slowed down it would affect the
ai actor's voice that way
i am even the narrator of this story i
am even the narrator of this story
in working with raphael he taught me
that words are music speech has notes as
written as a speech researcher from the
music background i'm always listening to
the voice as this instrument that i can
manipulate something that i'm very fond
of is etta james i wanted ai to sing
like eddie james
with this model we now have this liberty
to create artistic things you as a human
are using ai to assist you to create a
work of art
i am a i
brought to life by the researchers at
nvidia and inspired minds everywhere
Title: NVIDIA Special Address | MWC Barcelona 2021
Publish_date: 2021-06-30
Length: 2002
Views: 397674
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/ROIWZjAS3UM/hq720.jpg?v=60dd6317
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: ROIWZjAS3UM

--- Transcript ---

[Music]
hello everyone
and thank you for joining us at mobile
world congress barcelona
my name is ronnie vasista and i lead
telco
for nvidia ai is the most
powerful technology force of our time
and we're really only at the beginning
of the age of ai
nvidia embarked upon this ai journey
over 10 years ago and let's take a look
at how ai powered by nvidia
is already impacting our world
[Music]
i am a creator
blending art and technology
to immerse our senses
i am a healer
helping us take the next step
and see what's possible
i am a pioneer
finding life-saving answers
and pushing the edge to the outer limits
i am a guardian
[Music]
defending our oceans
and the magnificent creatures that call
them home
i am a protector
[Music]
helping the earth breathe easier
and watching over it for generations to
come
[Music]
i am a storyteller
giving emotion to words
and bringing them to life
i am even the composer of the music
[Music]
[Applause]
[Music]
i am a.i brought to life by nvidia
deep learning and brilliant minds
everywhere
[Music]
every industry will be transformed and
reshaped
in the next 10 years the immense
forces of artificial intelligence
digital automation and 5g connectivity
are combining in the fourth industrial
revolution
and will create unprecedented
opportunities
and changes in business and society
as conventional computing and moore's
law started to hit limits
accelerated compute for both
applications
and infrastructure came to the rescue
and started to become a mandatory
element
to achieve required functionality and
performance
the new unit of compute has now
transitioned
to be the data center dimensioning the
right balance
of three essential technologies
cpu gpu and the data processing unit
the dpu to enable scale out
and scale up computing ai
computers automating intelligence
we are just entering into the second
wave of ai
the first wave was reinventing computing
for this new way of doing software and
nvidia
together with the early adopters the
internet companies
has been driving that first wave for 10
years
the second wave of ai is happening
in the enterprise and at the industrial
edge
we need to turn ai from computer science
running in the cloud into smart ai
products at the edge and in billions
of connected devices and computers
we talked today of aiot to describe this
transformation
let's drill down into that for a minute
we
have to think differently billions of
things can be located
throughout the network and data centers
will now be connected at all points
in the network a data center where you
need it
and when you need it where does 5g come
into all of this
a ubiquitous 5g network will connect
these data centers
and intelligent things that the data
rate
latency cost and power
required by the application in addition
as this network morphs to adapt to the
needs in the 5g world
not only will ai drive over-the-top
application innovation
but ai will also be required to manage
organize and increase the efficiency of
the network
itself ai is the superpower
that is going to be part of everyday
life
and everybody's life nvidia
started on the ai journey when the path
wasn't clear
over that time nvidia has adapted
learned and deployed multiple versions
of ai hardware
software frameworks and libraries nvidia
is now using this learning
with external partners to create full
stack
frameworks and domain specific libraries
on our gpu cpu and dpu platforms
and together we are transforming
industries
to date we have shipped one billion cuda
gpus
which result in 250 extra flops
in the cloud together with our network
of two and a half million developers
we have created 2 000 gpu apps
and 150 sdks with
24 million cuda downloads innovation
also comes from startups and we have 8
000 ai startups using nvidia products
and platforms
these numbers keep increasing at a
dramatic pace
as we move into the edge ai and aiot
era nvidia has been at the forefront of
this ai revolution
and we are fortunate to have been
recognized as an influential
ai innovation leader but more than that
we have been fortunate to work with some
of the world's
best companies our ai ecosystem
is as widely applicable as it is deep
and extends to partners working with us
to deliver
the ai platforms the ai infrastructure
and partners using the ai and machine
learning powers
to enrich their products and services
to their customers we are honored
to have built and we will keep building
an unparalleled ai and machine learning
ecosystem
as we bring connected intelligence
to every industry let's take a look
at four examples where we can see how
these real world implementations
of ai together with the super fast
ultra reliable 5g connectivity
have already started to shape industries
and our everyday life the first example
is happening in an automated vehicle
assembly factory
where thousands of cameras are
monitoring the automated process
by using nvidia's intelligent video
analytics framework
part of our metropolis platform and by
working with our innovative
isv partners such as v7
in this case the cognitive capabilities
of visual inspection solutions are
brought to the automated factory
our next ai example uses the same
metropolis platform
this time for monitoring and managing a
smart city environment urban planning
and traffic management are set to become
foundational
in a smart city in such an environment
there can be thousands of people and
things
interacting with each other and the
environment there are multiple
stationary and moving elements
cars motorcycles buildings trees
to name just a few the roads are busy
traffic is heavy and variable at
junctions
our visual inspection software observes
all these moving
and non-moving elements in all lighting
and weather conditions ensuring security
safety better space management and easy
flow of people
and traffic and this can all be done
from a central network operations center
as you can see smart efficient
transportation
smart parking efficient airport
operations
air traffic control visual inspection
we are poised to transform all these
industrial segments
with nvidia ai our third example
uses conversational ai and natural
language
processing using our jarvis platform
my name is misty nice to meet you what
can you help me with
i can help you with current and upcoming
weather at any location
and can also provide interesting
information on meteorology
find us a mexican restaurant the nearest
mexican restaurant is luna mexican
kitchen located at 1495 the alameda san
jose
how's the weather in san francisco it is
currently 66 degrees and sunny in san
francisco
that's the power of ai using natural
language processing
running on nvidia gpus we are building
the services for the future
in our last example we delve into the
world of cloud xr
cloud xr is nvidia's streaming solution
for virtual reality and augmented
reality
combined with 5g through aerial cloudxr
provides a powerful edge computing
platform
for extended reality this application
autodesk v-red is widely used in the
automotive industry
for collaborative visualization of
designs
including in vr however in the past
vr and ar required some serious
compromises
users had to be tethered by a cable to a
workstation
limiting freedom of movement and making
it difficult to collaborate with
stakeholders in
other locations now with cloudxr over 5g
it's possible to stream high quality vr
to tetherless headsets such as the
oculus quest
shown here users can be virtually
anywhere with cloudxr augmented reality
also becomes possible here
v-red is streaming the same content to a
5g mobile phone
so a stakeholder can view design changes
in augmented reality on location
while a designer makes changes such as
switching materials
the ar user can view the changes in the
real-world
environment and finally v-red provides
streaming over 5g of high-fidelity
content
to web browsers such as on this tablet
here the same model is being rendered
with nvidia rtx real time ray tracing
and denoised with artificial
intelligence
all running on a single egx platform
cloud xr over 5g with aerial
provides an end-to-end solution for
visualization at the edge
this collaboration capability has a
profound impact
not only on the productivity of design
but also
in this case on the value of a given car
model
now let's stay on the topic of cloud xr
in october 2019
we introduced cloudxr at mobile world
congress
la at that time we coined the
term extreme augmented reality
and we talk through the vision of vr
ar streaming over a 5g network
you may remember that cloudxr uses late
warp technology to hide network latency
and constantly adapts its streaming
parameters
in response to current network
conditions
today we're very pleased to introduce
cloud xr 3.0 one of the great promises
of vr
has always been collaboration the
ability to work together with colleagues
located around the world
immersed in rich models exploring
possibilities and making decisions at
the speed of light
with cloudxr 3.0 we have significantly
enhanced this collaboration experience
by adding bi-directional audio
support to streamed xr now
even from your mobile device you can
discuss
design options with colleagues while
immersed together
in a virtual experience virtual
collaboration
by using nvidia cloudxr 3.0
is now at an unprecedented level and
will change the way we work
learn and play the nvidia ai platform
consists of chips systems and ai
libraries
and is integrated into all the
industry's popular tools and workflows
it's in every cloud and is run on every
ai
system that includes nvidia gpus
this nvidia ai platform has now been
extended to a market segment
that we have not served before
enterprise computing
using the nvidia egx for enterprise
platform
with this nvidia egx for enterprise
platform
all the nvidia optimizations for compute
and data transfer
are plumbed through the egx stack so
that the ai workloads can be distributed
to multiple systems and can achieve bare
metal performance
finally the enterprise it ecosystem
can build an enterprise ai
infrastructure that
seamlessly integrates into the existing
enterprise environment
over 50 server partners from the world's
top
server vendors will be certified for
nvidia egx
enterprise as electronic design recently
stated
nvidia's ai on 5g platform forms the
centerpiece of smart
everything nvidia aerial a100
is a new type of computing platform
designed for the edge combining ai
and 5g into one egx for enterprise card
with the addition of a software defined
5g
rand stack with full inline
layer 1 acceleration running on the gpu
this egx pcie card
now transforms into a complete
software defined o-ram compliant
7.2 split rand compute
base station the nvidia aerial a100
delivers up to a full 20 gigabits per
second throughput
and can process up to nine 100
megahertz massive mimo with 16 downlink
and eight uplink layers for a 64
transmit
and 64 receive radio aerial a100
is fully software defined and the
bluefield 2
dpu can additionally offload and
accelerate
virtual network functions the advanced
nvidia melanox
5t for 5g features on the bluefield
device
simplifies time synchronization and data
transmission across servers
gpus radios and baseband units
in wireless network rollouts thus
making 5g rollouts easier and more
efficient
5t for 5g includes support for features
such as
precise time stamping high clock
accuracy that ensures extremely precise
timing accuracy within 16 nanoseconds
esipri windowing and time-bound packet
steering
the nvidia egx implementation with
aerial a100
is the first 5g base station that
is also a cloud-native secure
ai edge data center capable of
delivering
the complete nvidia ai suite of
capabilities
we have brought the power of the ai
cloud
to the 5g connected enterprise edge
the two super powers of ai and 5g
will create unprecedented economic value
some research puts the value in excess
of 10 trillion dollars by 2035
retails smart cities hospitals factories
roads transportation hubs agriculture
plants and warehouses the list goes on
the future is starting now and nvidia is
at the center of this industrial
revolution
let's bring us back to today and see a
demo
of how this merger of 5g and ai
in an industrial use case will play out
in this demo an operator is monitoring
the robots
in a digital twin of the factory using
nvidia's
isaac omniverse platform
they also get real-time updates from
sensors on the robots
and then there is a robot in distress
the control room gets an alert with 5g
connectivity they can monitor the
cameras in real time
and execute actions to address the
problem
when the problem is fixed they return
the robot
back to autonomous control
on this journey of bringing the nvidia
ai platform to enterprise computing
we are thrilled to have an unparalleled
and enviable set of innovative ecosystem
partners
working to deliver leverage and enhance
the nvidia ai platform these partners
include application domain isvs
management and orchestration partners
and data and analytics
and machine learning companies the
addition
of ai on 5g capability
using the aerial 5g brings new ecosystem
partners
into the already vibrant and large
ecosystem
these aerial 5g ecosystem partners today
include ran isvs such as maveniere
radasys and capgemini plus network
equipment providers and the list of
aerial 5g ecosystem partners
will continue to grow however
as edge use cases grow dramatically and
the addition of 5g
connectivity transforms industries
there is also the potential of increased
security attack surfaces the aerial a100
platform
enables off-loading and acceleration of
inline security and virtual firewalls
so leading security companies are also
now
joining the nvidia enterprise ai
on 5g ecosystem if we take a moment
to focus on the subject of enterprise
security
which we know is top of mind for all
enterprises
large and small today we are working
with our partners to ensure that ai on
5g is built on a robust
secure platform for example
we are currently working with palo alto
networks
to create a next generation firewall
with intelligent offloads
to provide granular zero trust security
the intelligent traffic offload service
is designed to use an open api framework
for extensibility across offload policy
enforcement
and is based on application
classification
since most of the data doesn't need
inspection through the firewall
it is offloaded to the nvidia bluefield
2
which monitors select data flows to free
cpu cores by providing hardware
accelerated functions
bluefield 2 delivers a 5x performance
increase
resulting in an improvement in
efficiency
and a significant cost savings this is a
great example
of how the bluefield dpu present
on nvidia enterprise platforms is
enabling
not only performance but also security
for service provider and telco networks
the tremendous advantage of the nvidia
ai
for enterprise platform called the egx
platform
is its flexibility egx is cloud native
software defined and can be orchestrated
to run mixed workloads
utilizing the hardware assets of the gpu
cpu and dpu either virtually
or as a container using kubernetes on
bare metal
this makes the platform consistent with
the advantages
of a commercial off the shelf enterprise
solution
the combination of a software defined
high performance 5g rand stack network
function acceleration capabilities such
as 5g
upf core ai driven near real-time rick
front hall termination with o-ram
compliant timing
in-line security vnf offload
together with ai libraries and
application sdks
is not only unique in itself but
bringing
all these assets into a cloud native
nvidia certified server now enables
true enterprise and industry
transformation
at scale scaling this ai
on 5g into the enterprise also can
include collaborating with cloud service
providers
we previously announced that we are
working with google cloud
to take the ai on 5g on nthos
to vertical industries such as
manufacturing
retail healthcare and more
in order to facilitate this activity we
are very excited
to announce that google cloud and nvidia
will be creating an open innovation
ai on 5g lab in this
lab industrial companies system
integrators
and network operators will be able to
develop and test
their ai on 5g enterprise applications
on google anthos
using nvidia ai on the
aerial a100 infrastructure
the aerial a100 hardware unit contains
three main
elements the gpu running the high
performance
full inline layer 1 vran
and the gpu is also capable of running
the layer 2 scheduler
and near real-time rick algorithms plus
of course the ai mech workloads
then there is the bluefield data
processing unit or dpu
which combines industry-leading nvidia
mellanox
and powerful armed cpu computing
providing the full infrastructure on
chip programmability
and high performance networking in the
context of 5g
this dpu is also the termination point
for the esipri oran compliant front hall
additionally the programmable dpu fabric
offloads the 5g upf
from the host cpu as well as
accelerate network functions and manage
the software defined networking
then the host cpu runs control plane
functions
and layer 2 plus we are very excited
to introduce to you now the nvidia
ai on 5g hardware roadmap
going forward nvidia is focused on
integrating functions
increasing performance and lowering
power consumption
today's hardware implementation of
nvidia's
ai on 5g is the aerial
a100 pcie card attached
to an x86 host to
further integrate the gpu dpu and cpu
functions
the next release in 2022
will leverage two chips pulled from the
overall
nvidia silicon roadmap the bluefield 3
and the ampere gpu now with the
combination
of a very high performance arm cpu
cluster
next generation nick subsystem and the
highest
compute capability gpu bluefield 3
aerial a100 delivers ai
plus high performance vran on a single
card
in order to advance the open ecosystem
of vran
we are now excited to announce that
nvidia
is working with arm to expand arms
5g infrastructure ecosystem
arm is a leading provider of ip into 5g
mobile handsets
but as the 5g ram host infrastructure is
evolving
there is an increasing need to open up
the environment for 5g
ran host cpu the powerful
arm ip complex containing 16
a78 cores on the bluefield 3
dpu can now be used for layer 2 plus
functionality
the combination of bluefield 3 and
ampere gpu
can now be deployed in a fully self
hosted configuration that means
bluefield 3 aerial a100
delivers unprecedented levels of system
integration
and efficiency but that's not all
the integration roadmap continues when
we deliver
bluefield 4 in 2024
bluefield 4 integrates the cpu
gpu and nick functionality and various
fixed function accelerators in a manner
that delivers
unprecedented levels of performance and
power efficiency
with the capabilities of bluefield 4 we
will have
ai and vran on a
chip the bluefield 4 chip
in addition to having embedded gpu will
also be expandable
with additional off-chip gpus as
workloads dictate
nvidia is committed to the aggressive
silicon road maps
that underpin this extraordinary
ai on 5g roadmap
nvidia is an experienced leader in ai
and ml having deployed generations
of the nvidia ai platform into countless
cloud applications over the last 10
years or so
nvidia has established a broad and
deep ecosystem based on our ai
cloud platforms we are working with the
leaders today
and the leaders of tomorrow to deliver
ai solutions
across industries and the world
increasingly enterprises are now looking
to ai at the edge
or to device-based ai to enhance their
products and services
and to transform their workflows and
customer experiences
through the nvidia egx for enterprise
platform
nvidia is extending our ai learning
and know-how into the enterprise compute
segment
through the aerial a100 implementation
of the egx for enterprise card this year
and then with the cloud native and
software compatible
ai on 5g hardware roadmap which
now includes an arm self-hosted option
nvidia is combining ai capability
and the self-hosted performance low
power 5g rand stack
on one card we are at the dawn
of a new era of industrialization
driven by ai and digital automation
the data center is the new unit of
compute
and a data center will be where you need
it
when you need it at the edge or in a
device
the data center will be connected
through a high performance
5g network a network that is driven by
ai to be energy and spectrally efficient
and self-organizing these technologies
come together on the nvidia ai
on 5g platform this will
drive innovation and will kickstart
the next industrial revolution
thank you
you
Title: SHIELD Gaming: GameStream
Publish_date: 2017-05-24
Length: 76
Views: 109958
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/rptfPvOB-EY/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLAn06wkVT_IaG5mUk2OCr7uoExByg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: rptfPvOB-EY

--- Transcript ---

Nvidia's supremely powerful gamestream
technology allows you to cast top drawer
PC games straight into the living room
with shield TV in razor-sharp 4k HDR at
60fps so you can kick back on the couch
and truly make the most of your rigs
NVIDIA GeForce GTX GPU it's dead easy to
setup just connect your desktop PC to
your home network then hook up your
Nvidia shield to your TV connect to the
router install GeForce experience and
it's welcome to big school game stream
also means you can really make the most
of that new high-end TV in the living
room and enjoy 4k HDR gaming on the big
screen casting with Nvidia gamestream
save you a whole lot of hassle besides
no more tripping over trailing cables
getting ldap for making the house a mess
or ruining your neat gaming rig setup
and you'll no longer have to enjoy a
cramped co-op huddled around a monitor
instead enjoying the broad expanses of
living room gaming just hit up shield
and video.com to learn more about
harnessing the wild stallion of your
desktop pcs GPU and enjoying 4k HDR
gaming from the comfort of your couch
Title: Tom Clancy's The Division Gameplay, Overview and Interview at E3 2014
Publish_date: 2014-06-12
Length: 136
Views: 13845
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/Rs6rZbuZttg/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: Rs6rZbuZttg

--- Transcript ---

last year at e3 Ubisoft surprised the
world with tom clancy's the division was
an amazingly gorgeous game that
showcased an all-new engine that defied
all known capabilities of graphics this
year we had the chance to go behind
closed doors and see the latest version
of the division and then talk to one of
the lead developers people turn away
from it it run from it if they can and
it's hard knowing that you belong here
the division is an online of the game
and it takes place in a New York City
that's ravaged by a virus the whole
society has collapsed and the city is on
the verge of being destroyed of being
lost so you clay an agent of the
division whose job is to take it all
back clear it's an RPG so a progression
is important for you and together you
need to define the roles that you want
to have you know you want to be more
aggressive you want to be more a
supportive role so its a mix between the
choice of weapon tactics skills talents
and then of course it's about upgrading
modding and you know the synergies of
all those different variations that's
that's really the heart of the game
played in a World obviously on the brink
of collapse you don't have phones and
stuff like that working anymore what you
do have is the map so what that lets you
do is basically understand the vast open
world and all the choices that you have
so the map is really one of the most
essential tools for you to find your way
around to set waypoints to keep track of
contagion levels security levels your
own base of operations etc etc
you know we're an old pc studio so of
course we we were delighted you know we
could introduce to the fans last summer
that we're also going to release the
game on PC we were happy to push that
platform and to give it all it can and I
can assure you that when it comes to the
article's volumetric lighting all that
stuff it's going to be awesome
Title: GTC 2016: Deep Learning for Cars (part 10)
Publish_date: 2016-04-06
Length: 578
Views: 15709
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/RVmV9SXJeBg/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: RVmV9SXJeBg

--- Transcript ---

let me talk about one more thing the
last thing is deep learning for cars
well if it was an amazing year for a is
also an amazing year for cars there are
just so many announcements the fact of
the matter is we all know this AI is
coming to cars AI is coming to cars when
we thought about helping to create
self-driving cars in the beginning the
basic approach was sense plan act using
traditional computer architectures using
traditional computing techniques we're
going to continue to do that as well
we're going to continue to do sense plan
act the basic loops of self-driving cars
is know where you are in the world
figure it you know based on a map also
based on your census localize you see
what's around you and then you you act
plan sense a sense plan act the basic
loop of robotics so we decided to build
a computer dedicated for that very
functionality we built the world's first
AI car computer it's called the Nvidia
Drive our basic approach was this we
would build an end-to-end computing
platform all the way from training the
network that would ultimately run in
your car training the network is the
beginning training your network for your
car as you've heard from all of the
leaders training your network is the
first step of having an autonomous
driving car and so we've created a
platform that allows you to train on any
framework you would like to use Cafe C
NT X C NT k that is excuse me tensorflow
Tiano torch you name it
training your network and then taking
that exact same network running on
something that has incredibly high
performance but it's also very high
energy efficiency as I mentioned today
with gie you can take that network and
run it on a car computer we call that
car computer the Nvidia Drive px it's
the world's first deep learning car
computing platform it's scalable
we believe that AI will be used for car
computing not just for autonomous
driving AI will be used to talk to you
you will give it commands it will be
looking around your smart mirrors your
car will be able to see all around you
and tell you there's a bicycle on your
right of course it's gonna avoid it
anyways but it's gonna tell you
you could talk you could talk to it you
could change temperatures looking at
you're looking at your gesture inside
the car it could tell when you're dozing
off it could tell whether you're looking
in the direction of the oncoming traffic
a I will be used inside the car
AI will be used to drive the car AI will
be used to keep you out of harm's way we
believe a I will be used across the
board and car computers we used one
Architecture from infotainment to
cluster to a dass the self-driving all
the way to mapping it's completely open
platform all the api's are available to
everybody now the last time I showed you
we have started working on object
recognition for our drive computer the
thing that's really amazing is only in
just a few months after a great deal of
effort our scientists our researchers
working on our network hasn't now
achieved the number one score on the
kitty benchmark this is a self-driving
benchmark I'm super proud of them for
doing this
we have the number-one accuracy on Kitty
the thing that's really great is that
number two number three number four
number five Number six number seven
number eight all gpu-accelerated working
with a large community of researchers
that are trying to develop self-driving
cars we're open to work with everybody
we want to encourage everybody to enable
this capability so we can make streets
safer so we make driving better and
hopefully it might even change the way
we design our cities well let's take a
look let's take a look at what it can do
okay so not only is it accurate as I
mentioned earlier gie also makes our
network running on our processor super
fast and so we're detecting cars in the
front on the upper right in the back or
a node front in the upper left in behind
you in the upper right but also in your
side mirrors so the thing that's really
cool is that you could detect objects
all around you using exactly the same
network and all of that is running on
this little tiny processor the small
version the smallest version of drive px
this is the smallest version of Drive px
a hundred and eighty frames per second
180 frames per second detecting cars all
around you now we have detect lanes we
can detect detect signs we can take
anything we wanted to train it to do and
that's really the wonderful thing that's
the beautiful thing about a deep neural
network whatever you're trained to do it
will do whatever you train to detect it
will detect okay so drive px
well the next thing we like to do so
that's an update on perception um we'd
like to also make sure that you can map
well if you want to figure out exactly
how to drive and drive safely you want
to have as much information that you
possibly can this is called HD mapping
you could use lidar but we also believe
that you ought to use lidar and camera
and everything that you have and the
cameras will use photogrammetry
basically motion photogrammetry to use
to do basically what is called structure
for motion to create and identify
important point sness in the in the
surrounding and map reconstruct a 3d
world for you okay that mapping is
called high debt
High Definition mapping and we've
created a platform that allows us to do
that let's take a look at that oh can we
yeah well I think I just leaked that out
about the door okay excuse the sequence
so bi do bi do Andrew and his lab as you
guys know are working on a self-driving
car and he was so enthusiastic about it
he asked me to share with this to with
you guys the thing that's really
important is this while you're creating
a self-driving car you've got all kinds
of algorithms so you want to deploy onto
it
you have sensors and cameras all of your
car what kind of computer which you need
well it turns out they need a GPU
accelerated supercomputer and before the
existence of dry px2
they had to build their own they had to
build their own but they have wonderful
computer scientists they have wonderful
computer engineers and they built a
cluster a super computing cluster that
fits in the trunk of a car now this
isn't something that every car company
can do and this is something that every
car company wants to do and so the thing
that we did was we took all of that
super competing horsepower and we shrunk
it into one platform we call Drive px to
drive px to has two Tegra processors on
it and it has two Pascal next generation
GPUs that hasn't been announced on it
and so if you take a picture of that and
you zoom in carefully which I know you
guys will do you do it to me all the
time
let's see hanger thing I took the serial
numbers off okay and so this four chip
solution where as you were looking at a
one chip solution just now this four
chip configuration of the architecture
would allow you to basically have a
super computing cluster inside cars you
connect 12 cameras to it connect it to
your can bus and you can be doing foot
photogrammetry motion photogrammetry as
well as cutting collecting information
from light are fusing it in real-time
computing the map and instead of
streaming up to your cloud 12
high-definition video streams you stream
up to your cloud point clouds
that are highly compressed and that will
be accumulated in the cloud over time
crowdsource buy all these cars that are
driving around registered an account the
cloud to create one big high-definition
map and so this particular platform we
are announcing today will also be a
mapping platform the car computer would
be Drive px to the computer that's in
the cloud will be DG X 1 n 2 n exactly
the same architecture px 2 in the car
DHG X 1 in the cloud let's take a look
at let's take a look at real-time
mapping so basically what's happening
here is that this car our car
this is using four cameras it's able to
detect up to 15,000 important points it
selects 15,000 important points per
frame per second per camera and so
altogether the four cameras can collect
up to 1.8 million points per second 1.8
million points per second the points are
not only in 3d space but it is also in
color and so we would accumulate all
this information and we would load it up
into the sky and that will be then
registered and calibrated and maintained
using DG x1 okay so this is mapping we
created this platform so that every
mapping company in the world can have
the benefit of mapping cars building
these car computers for mapping are
really it's a really expensive endeavor
you're basically creating a
supercomputer that fits into the trunk
of a car but we've now with Drive px 2
what DG x1 have created an end-to-end
computing platform that car computers
can use mapping companies can use and we
have to map the world before all the
cars can safely travel around the world
Title: NVIDIA Omniverse - The Collaboration Platform for 3D Content Creation
Publish_date: 2019-03-20
Length: 76
Views: 36782
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/rVsp6o9t5nY/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: rVsp6o9t5nY

--- Transcript ---

there are over 200 animation studios in
the world today but one of the greatest
things is that we want to do is find a
way to work together forward documents
we have Google Docs well for 3d content
creation we have nothing we've been
working on this for some time
and want to show you at this great new
technology from our company it's called
the omniverse the omniverse basically
connects up all of the designers and
studios it works with every tool in the
case of mind we're doing the design in
the case of unreal we're composing the
environment in the case of substance
we're painting that airplane we
communicate with all the tools through
their plugins using USD and using MDL we
only exchange all the things that are
dynamic and we put it into this portal
that everybody can see from and so as a
result irrespective of what tool or what
part of the workflow you're working on
you will see one version of the final
content that you're creating in its
highest possible fidelity everybody has
one common understanding of what
everybody is working on at that moment
in time
omniverse is one shared world
Title: Visualize Microscopy Images of Living Cells in Real Time with NVIDIA Holoscan
Publish_date: 2022-03-22
Length: 84
Views: 47925
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/rXG27G3bWzY/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: rXG27G3bWzY

--- Transcript ---

Now with Clara Holoscan and NVIDIA IndeX 
we can visualize the entire large volume of living
cells in real time as the data is being recorded
directly from the microscope.
Watching these living cancer cells move about, we can see normal 
healthy biology and maligent processes at the same time.
The fluorescent marker, rendered in blue, marks nuclei, 
which we see splitting to form two cells from one cell.
A hallmark of cancer is cell division occurring more frequently
and with less error checking than normal, healthy cells.
Using Berkeley’s lattice light sheet microscope,
the ultra high resolution allows scientists
to see what is hidden to normal light optics
– not seen using traditional microscopes.
As we zoom in, watch cancer cells display what is
thought to be a rare event 
even for cancer cell lines – see one cell split into three cells.
This phenomenon has only been reported 
anecdotally in a couple of scientific publications.
Scientists do not yet know what we will see
– but this technique, enabled with real-time
processing and visualization, now allows the scientific
community to discover new unseen events like this.
Let’s see what the future has in store.
Title: The Data Center is the New Unit of Computing (NVIDIA GTC 2021 Keynote Part 3)
Publish_date: 2021-04-13
Length: 351
Views: 12912
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/rzdBHBx3eJk/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: rzdBHBx3eJk

--- Transcript ---

[Music]
data center is the new unit of computing
cloud computing and ai are driving
fundamental changes
in the architecture of data centers
traditionally
enterprise data centers ran monolithic
software packages
virtualization started the trend towards
software-defined data centers
allowing applications to move about and
letting it manage from a single pane of
glass
with virtualization the compute
networking
storage and security functions are
emulated in software running on the cpu
though easier to manage the added cpu
load
reduced the data center's capacity to
run applications
which is its primary purpose this
illustration
shows the added cpu load in the gold
color part of the stack
cloud computing re-architect the data
centers again now to provision services
for billions of consumers
monolithic applications were
disaggregated into smaller microservices
that can take advantage of any idle
resource
equally important multiple engineering
teams can work concurrently using ci cd
methods
data center networks became swamped by
east-west traffic
generated by disaggregated microservices
csps tackled this with melanox's
high-speed low-latency networking
then deep learning emerged magical
internet services were rolled out
attracting more customers and better
engagement than ever
deep learning is compute intensive which
drove adoption of gpus
nearly overnight consumer ai services
became the biggest users
of gpu supercomputing technologies
meanwhile the mountain of infrastructure
software continues to grow
now adding zero trust security
initiatives makes infrastructure
software processing
one of the largest workloads in the data
center the applications and services
should be
the answer is a new type of chip for
data center infrastructure processing
like nvidia's bluefield dpu let me
illustrate this with our own cloud
gaming service geforce now as an example
geforce now is nvidia's geforce in the
cloud service
geforce now serves 10 million members
in 70 countries two years ago
geforce now only had just one million
members
incredible growth gamers are playing
concurrently on geforce now servers
located in data centers
hundreds of miles away geforce now is a
seriously
hard consumer service to deliver
everything matters
speed of light visual quality frame rate
response smoothness startup time
server cost and most important of all
security
currently geforce now uses nvidia's
virtual gpu technology
the virtualization networking storage
and security
is all done in software the load is
significant
we're transitioning geforce now to
bluefield with bluefield we can isolate
the infrastructure from the game
instances
and offload and accelerate the
networking storage and security
the geforce now infrastructure is costly
with bluefield
we will improve our quality of service
and concur users at the same time
the roi of bluefield is excellent doka
is our sdk to program bluefield
docus simplifies application offload to
bluefield's accelerators and
programmable engines
every generation of bluefield will
support doka from now on
so today's applications and
infrastructure will get even faster when
the next bluefield arrives
i'm thrilled to announce our first data
center infrastructure sdk
doka 1.0 is available today there's all
kinds of great technology inside
deep packet inspection secure boot tls
crypto offload
regular expression acceleration and a
very exciting capability
a hardware-based real-time clock that
can be used for synchronous
data centers 5g and video broadcast
we have great partners working with us
to optimize leading platforms on
bluefield
infrastructure software providers edge
and cdn providers
cyber security solutions and storage
providers basically
the world's leading companies in data
center infrastructure
we will accelerate it all with bluefield
though we're just getting started with
bluefield 2.
today we're announcing bluefield 3 22
billion transistors
the first 400 gigabit per second
networking chip
16 armed cpus to run the entire
virtualization software stack
for instance running vmware esx
bluefield 3 takes security to a whole
new level
fully offloading accelerating ipsec and
tls cryptography
secret key management and regular
expression processing
whereas bluefield 2 offload an
equivalent of 30 cpu cores
it would take 300 cpu cores to secure
offload and accelerate the network
traffic at 400 gigabits per second
a 10x leap in performance we're on a
pace to introduce a new blue field
generation every 18 months
bluefield 3 will do 400 gigabits per
second
and be 10x the processing capability of
bluefield 2
and bluefield 4 will do 800 gigabits per
second
and add nvidia's ai computing
technologies to get another 10x boost
100x in three years and all of it will
be needed
a simple way to think about this is that
a third of the roughly
30 million data center servers shipped
each year
are consumed running the software
defined data center stack
this workload is increasing much faster
than moore's law
we know this because the amount of data
we're producing and moving around
so unless we offload and accelerate this
workload data centers will have fewer
and fewer cpus to run
applications the time for bluefield has
come
Title: NVIDIA Special Address at Q2B: Defining the Quantum Accelerated Supercomputing Platform
Publish_date: 2022-07-12
Length: 1119
Views: 11388
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/RzkssSPOG5Q/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: RzkssSPOG5Q

--- Transcript ---

well thank you for tuning in to listen
to this special address from nvidia on
quantum computing i'm tim costa the
director of hpc and quantum computing
product at nvidia and today i'm going to
talk about defining the quantum
accelerated supercomputing platform
since this is a quantum computing
address some listeners may not be
familiar with the breadth and the depth
of the nvidia accelerated computing
platform
so i want to start with a brief
introduction our invention of the gpu in
1999 sparked the growth of the pc gaming
market redefined computer graphics and
ignited the era of modern ai
today nvidia is a full stack computing
company delivering cpus gpus gpus and
software that fuel data center scale
computing solutions
we start with amazing chips but for each
field of science industry and
application we create a full stack
as we build into a talk about quantum
accelerated supercomputing it's worth
taking a moment to consider the
workloads of the modern supercomputer
at nvidia we see five main pillars of
modern supercomputing
simulation remains the foundation of hpc
over 20 years ago computer scientists
and researchers started using gpus to
accelerate a range of scientific
applications
with the release of cuda nvidia ushered
in a revolution of developer
accessibility to the disruptive
performance possible with gpu
accelerated supercomputing
today we have over 2700 gpu accelerated
applications and counting
more recently hpc plus ai has been
gaining considerable steam exploding
from fewer than 100 related research
papers on archive five years ago to near
5000 papers last year
a third workload is the scientific edge
bringing ai supercomputing to the
scientific edge will turn data
collection instruments into real-time
interactive research accelerators
fourth digital twins
digital twins use inputs from simulation
ai surrogate models and observe data
from the edge to create real-time
digital twins that revolutionize
industrial and scientific hpc
and finally we have quantum computing
which we'll dig into for the rest of
this address
the modern supercomputer will leverage
all these technologies to solve the
grand challenges of the 21st century
and now let's focus in on quantum
computing for the remainder of this
address
the progress made by the quantum
computing industry over the last decade
has been truly impressive
we've gone from working on one or two
qubit systems mostly within academia to
systems with up to one or two hundred
qubits available to users through the
cloud
however there's a long ways to go
the community generally agrees that the
promise of quantum accelerated computing
that is applications accelerated by the
addition of quantum computing resources
will come in the era of fault-tolerant
quantum computing consisting of millions
of physical qubits error corrected to
thousands of logical qubits
alongside the question of the hardware
there's serious work to do in developing
the software identifying the unknown
algorithms and building the developer
and application base that will
ultimately be necessary to realize value
from quantum accelerated computing as
the hardware matures
to meet that challenge and prepare for
the quantum accelerated future
governments institutes for higher
education and research as well as
industry are investing heavily across
the board in hardware software and
algorithm development
worldwide we can count 22 national
quantum computing initiatives
over 2 100 quantum computing research
papers more than 250 quantum computing
startups
and over 70 percent of the world's major
corporations forming quantum computing
initiatives to make sure that they're
prepared
one of the most important tools at hand
for advancing the state of quantum
computing today is quantum circuit
simulation
through simulation the community
validates current quantum hardware and
designs the next generation
experiments with noise models and error
mitigation techniques and designs new
algorithms searching for the right
techniques to build the quantum
accelerated applications of the future
ultimately simulation is how the quantum
community is pushing towards and
preparing for a future with quantum
advantage
when we started looking at the ecosystem
for places nvidia could provide value
and push the industry forwards it was
clear there was a big gap between the
simulations being performed and the
performance and scale possible in
quantum circuit simulation with gpu
supercomputing
to address this a little more than a
year ago we announced quantum
quantum is an example of a core nvidia
strategy for accelerating the most
important scientific computing workloads
we look for difficult and portland
problems that are good fits for gpu
acceleration and build sdks and
libraries that enable the entire
ecosystem of frameworks and applications
to unlock scale and performance not
otherwise achievable
ultimately enabling the solution to
problems that weren't tractable
a key piece of that strategy is
partnering with industries and providing
a platform to increase the success of
all the players in the ecosystem and
this is a fundamental tenet of our
strategy in quantum computing we aim to
provide a platform to advance the state
of quantum information science while
partnering with the entire quantum
computing industry
out the gate ku quantum was built for
the community of quantum circuit
simulators and frameworks as a force
multiplier it was designed in
collaboration with quantum computing
hardware and software companies to make
sure it addressed the right use cases
and by creating the djx quantum
appliance a container that integrates
leading community quantum circuit
simulation frameworks with cu quantum it
provides a drop-in quantum computing
simulation platform for end users on any
nvidia gpu accelerated system
we're really excited about the results
our partners are seeing by leveraging
quantum to accelerate their work
now with a short talk and a big new
announcement to make here shortly i
can't review all the successes we've
seen but i'd like to point out a few
recent highlights
pascal has been using quantum to
accelerate their work simulating the
quantum algorithm called differential
quantum circuits for physics machine
learning workloads
dqc runs 100 times faster with cu
quantum on a single a100 gpu than on the
64 core cpu server they were using
previously
and for the same latency can simulate a
circuit depth that is 24x larger than
previously possible
quantum machine learning is
simultaneously one of the most exciting
subfields subfields of quantum computing
and one of the most computationally
challenging
quantum machine learning models
notoriously can take a very long time to
train limiting the pace of applications
research we recently partnered with
xanadu to integrate quantum into penny
lane their framework for quantum machine
learning and aws to make that available
to customers through their bracket
service
combining these we saw a speed up of
over 900x on simulating quantum machine
learning workloads
zapata has integrated ku quantum into
their orchestra platform for building
and deploying quantum ready applications
using the ku quantum sdk an 8 gpu system
is projected to deliver more than 100x
speed up over their previous results as
well as to enable the simulation of one
and a half times more qubits
the list of application areas believed
to be candidates for quantum advantage
is large it's so large that before we
give talks we have debates about which
and how many compute domains to put on
the slides
but a critical consideration as we think
forwards to quantum accelerate
applications is that we know these will
not execute exclusively on a quantum
resource
but will be hybrid applications
leveraging classical supercomputing for
huge parts of the application while
critical kernels are accelerated by a
quantum resource
in fact if we take a lesson from the
20-year effort to gpu accelerate
scientific computing workloads what we
expect to see is the very same
applications being used today
incrementally accelerated by quantum
computing as algorithms are developed
and discovered and hardware matures
this observation that today's gpu
accelerated scientific computing
applications are the candidates for
future quantum accelerated applications
in drug discovery chemistry finance
optimization energy and more
is more profound than it may seem at
first glance and it gives rise to some
big questions for those who are looking
to prepare for the quantum accelerated
future
much of the effort in quantum computing
software today is focused on small-scale
algorithm development
to enable this a group of pythonic
frameworks have been developed
these are household names now in the
quantum computing community
they are absolutely critical tools and
without them we're unlikely to discover
the right algorithms for quantum
advantage
as i pointed out earlier these are so
critical that nvidia's first effort in
quantum computing was to build an sdk
specifically to accelerate them
however there's a big gap between
algorithmic experimentation with these
frameworks and the observation on the
previous slide that today's gpu
accelerated scientific computing
applications are the most likely
candidates for future quantum
accelerating accelerated applications
in order to transition from algorithm
development by quantum physicists
to application development by domain
scientists we need a development
platform built for hybrid quantum
classical computing that delivers high
performance interoperates with today's
applications and programming paradigms
and is familiar and approachable to
domain scientists
of course nvidia has some experience
here
back in 2006 before the launch of cuda
there were domain scientists leveraging
gpus to accelerate their work but very
few
that's because they had to program in
graphics and shader apis or gpu assembly
in order to gain access to the
disruptive performance provided by gpu
accelerated computing
from this perspective what nvidia did by
launching cuda is to usher in a
revolution in accessibility of a
disruptive compute technology the gpu to
the typical domain scientist
and on that note i'm very pleased to
announce nvidia quantum optimized device
architecture or coda
coda is the platform for hybrid quantum
classical computing built to address the
challenges facing application developers
and domain scientists looking to
incorporate quantum acceleration into
their applications
whether through emulated or a quantum
processor
coda is open and qpu agnostic as you'll
see shortly we're partnering with
quantum hardware companies across a
broad range of qubit modalities to
ensure it provides a unified platform
that enables all hybrid quantum
classical systems as well as quantum
algorithm companies and research
institutions to ensure it addresses the
needs of developers
it includes a kernel-based programming
model with both single source c plus
plus and python implementations as well
as a compiler tool chain for hybrid
systems and a stand standard library of
quantum algorithmic primitives
coda integrates with today's high
performance applications and is
interoperable with leading parallel
programming techniques and software
it allows a domain scientist to quickly
and easily move between running all or
parts of their applications on the best
classical computing resources the best
simulated quantum computing resources
and the best real quantum computing
resources
with coda nvidia is kicking off another
revolution in developer accessibility to
disruptive compute technologies allowing
domain scientists to seamlessly leverage
quantum acceleration tightly coupled
with the best of gpu supercomputing
over the next few slides we'll dig in a
bit on some of the features of coda
for starters coda is designed to be
familiar and productive for domain
scientists who work on today's leading
scientific computing workloads
coda follows the cuda anatomic annotated
kernel approach but does so with typed
function objects like lambdas to enable
the development of algorithmic libraries
that are generic with regards to input
quantum kernel code
we see this here with the variational
quantum eigensolver where programmers
can define parametrized quantum kernel
expressions and use them as input to
algorithic libraries in this case coda
vqe
coda provides a set of built-in types
pertinent for quantum computing
here we see the ability to allocate
quantum memory types and operate on them
with built-in quantum operations
we also see how easy it is to construct
the built-in spin-op type to define
hamiltonians for variational tasks such
as this
or for other circuit synthesis
approaches
the overall programming efficiency seen
here for scientists to go from a
parametrized onsots and hamiltonian to a
fully running variational quantum
eigensolver on a dgx platform or a
physical qpu is what really stands out
here
this code snippet demonstrates the
underlying philosophy of the coda
programming model to provide core
concepts to describe quantum code
expressions and then promote the utility
of a standard library of generic
functions enabling hybrid quantum
classic classical composability
now that we've seen some code let's take
a look at some performance
coda is designed from the ground up for
high performance hybrid computing
whether on an emulated processor or a
physical quantum processor
here we're looking at preliminary
performance results of the coda
implementation of variational quantum
eigensolver running on an a100 gpu and
then comparing it to a leading framework
also running on an a100
so we have a gpu to gpu comparison here
and can pull out the performance benefit
of the coda software stack
when running in an emulated environment
coda leverages ku quantum as a back end
we see here that at 20 qubits we're
already up to a 287x speedup over the
leading framework also running on an
a100 gpu
interoperability with today's scientific
computing applications and the
programming models and libraries they
leverage
is a core design principle behind coda
coda inc interoperates with standard
language parallelism openmp openacc and
cuda allowing a domain scientist to
incrementally add quantum acceleration
where it makes sense to their existing
applications
here we're looking at the quantum
imaginary time evolution or kite
algorithm
which is an algorithm for computing
energies in fields like material science
chemistry and nuclear physics
this algorithm is intrinsically hybrid
and iterative with each iteration
depending on a linear solve from the
previous iteration
each linear system solve depends on a
tomography step that's done on the qpu
clearly this is an opportunity for gpu
qp interoperability and this code
snippet demonstrates just that
here we use coda to compute expectation
values of a set of polyspinop instances
and use that as input to a coup solver
linear system solve
the results of which feed into the next
iteration of the algorithm
coda also enables exploration of hybrid
quantum configurations that aren't
practical in the near term
for example we can execute a coda
program on emulated hybrid resources
consisting of multiple gpus for
classical computing
and multiple emulated qpus to evaluate
the benefit of multiple qpus for scaling
quantum algorithms
coda provides core abstractions for
reasoning about the underlying quantum
platform and asynchronously executing
tasks on available qpus
in the example on this slide we're
simulating a hybrid hydrogen chain using
a variational quantum eigensolver and we
see near perfect strong scaling up to
four simulated qpu's of 28 qubits each
on a dj xa100 system
this just scratches the surface of the
quantum design space that can be
explored with a coda program running on
an nvidia accelerated supercomputer
as i noted earlier when reviewing the
adoption of quantum by the quantum
computing community
partnering with the ecosystem is a core
tenant of our strategy in quantum
computing and coda is no different from
the very beginning we've been designing
coda with collaboration from leading
quantum computing hardware companies
algorithm companies and research
institutions
i'm thrilled to announce that coda will
support a large and growing list of
quantum processors including those from
iqm pascal quantinium quantum brilliance
and xanadu
additionally leading quantum software
companies qcware and zapata are
collaborating with us to use coda with
end users who are building the hybrid
quantum classical applications of the
future
and supercomputing centers are working
with us to test and deploy coda for
thousands of scientific computing
developers around the world including at
ulish nursk and oakridge national
laboratories
together the dgx quantum appliance and
coda create a powerful platform for
quantum computing research and hybrid
deployment
with the dgx quantum appliance
researchers can develop and test new and
improved quantum algorithms accelerating
the timeline to quantum advantage
with coda they can take today's most
important application codes and
seamlessly run them on the leading
classical hardware the leading simulated
quantum hardware and the leading
physical quantum hardware or any
combination thereof
when integrated with our partner's qpus
this platform will define the world's
first quantum accelerated supercomputers
quantum computing holds incredible
promise for solving some of our most
challenging and important problems
but still has a long way to go to
realize its potential
today we're announcing a platform that
will help us get there
by providing a programming model that is
interoperable with today's most
important scientific computing
applications we are opening up the
programming of quantum computers to a
massive new class of domain scientists
and researchers
by building a software tool chain
optimized for performant execution of
hybrid code we are removing bottlenecks
on the path to quantum advantage
and by partnering across the entire
stack from qpu builders to algorithm
developers to end users we are ensuring
that we are bringing the best tools to
bear to help solve these problems and
enabling the entire quantum community to
contribute to that effort
Title: NVIDIA Quadro K6000 delivers photorealism and fast rendering for BunkSpeed Pro
Publish_date: 2014-02-10
Length: 116
Views: 20607
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/SawcC854CVk/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: SawcC854CVk

--- Transcript ---

my name is Brian Hiler I'm from bunks
beat I'm demoing bunk speed Pro and this
is running 4k on an Nvidia K 6000 super
pac's graphics card running ARB software
at 4k so what's bunk speed bump speed is
a fast easy fun very intuitive rendering
software and as you can see by the
screen behind me it's really really
interactive as well so in our new easy
mode which is only five buttons on the
bottom of the screen you import your
model paint your bottle and then choose
your environment all using really easy
drag-and-drop technology and then set
your camera maybe we want to go to the
front of the vehicle here and as you see
it's really easy to manipulate through
very fast with that case 6000 so let's
say you want to change the color of the
red car to something like a blue let's
say an easy mode all you have to do is
click the paint button on the bottom of
the screen and find this Monaco blue
click on the material ball and just drag
and drop let go and then the bunch of
the magic happens again and the
rendering picks back up right where it
left off also if you wanted to change
the environment which is really cool we
can click on our middle button here
which is our scenes tab go over to our
environment here and drag in let's say a
photo studio which changes the whole
theme completely changes the lighting
the reflection now we're inside instead
of on the California Raceway we're now
inside a photo studio and the same thing
happens you can rotate the camera find
that great shot let go of the mouse and
there's your beautiful photo realistic
image so for more information about bunk
speed pro or any of our other great
products check us out at bunk be calm
you
Title: NVIDIA and Roborace at CES 2018
Publish_date: 2018-01-10
Length: 71
Views: 13983
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/senkCtqf_18/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: senkCtqf_18

--- Transcript ---

i'ma bring back home I'm chief strategy
officer Rover race you can see the
robocar behind which is a fully
autonomous level 5 racing car and we
have a great partnership with Nvidia on
the technology that sits inside the car
traditional motor racing if you look at
circuit based racing some website
pentacle cars following each other
around from or an autonomous point of
view it's really like a level 2
challenge you know it's kind of lame
keeping and adaptive cruise control what
we're looking at is trying to be a level
5 competition so we're talking about
real road environments with these cars
racing through traffic
so trucks buses cars motorbikes on the
track at the same time with these cars
racing head-to-head through the traffic
the environment that we're creating is
really a future vision of the roads and
if the car can do it in that environment
then it's going to drive you safely
around when you're going down to the
shops or to school or to work the big
change for us now is to move to the
Nvidia driver gaseous platform that's
really the central hub where we
integrate one of the cameras radar lidar
sonar machine vision cameras and then
the GPS inertial system that means all
the sensors can now improve because
we've got the bandwidth going into the
central computer it's incredible because
it gives us new opportunities the
Pegasus platform is really the leading
edge in compute Falls and most vehicles
[Music]
Title: NVIDIA Special Address at RSNA 2022
Publish_date: 2022-11-28
Length: 1191
Views: 218444
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/Sku6h4_ciww/hq720.jpg?v=63827f3a
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: Sku6h4_ciww

--- Transcript ---

- Hi, I'm David Niewolny.
And on behalf of the entire
NVIDIA Healthcare and Life Sciences Team,
welcome to RSNA 2022.
The opportunity for accelerated computing
and AI in healthcare
has never been greater.
And the adoption of these technologies
in medical imaging continues to increase.
Today, we're gonna discuss
innovations in medical imaging,
share the value that NVIDIA is bringing
to the entire healthcare AI development
and deployment ecosystem,
discuss challenges to
widespread clinical use,
and, finally, we're gonna
propose some solutions
to help turbocharge the
AI innovation engine
within medical imaging.
Medical imaging is one of
the most essential tools
in all of healthcare.
Two-dimensional imaging for
screening and early detection.
Three-dimensional
imaging actually provides
spatial understanding,
quantitative measurement and segmentation.
The fourth dimension
introduces temporal information
that's essential for diagnosing
and planning treatments.
Now using medical images,
in combination with realtime deep learning
and computer vision,
brings us into a fifth dimension.
This fifth dimension allows us
to perceive our surroundings,
navigate within the human body,
plan actions, track objects,
and perform surgical tasks.
Let's take a look at how NVIDIA
is driving innovation within
medical imaging today.
NVIDIA's working with leaders
in diagnostic imaging equipment
to provide the
computational infrastructure
needed to improve image
quality, lower radiation dose,
and run AI applications.
Medical imaging breakthroughs
have historically come
from innovations in sensor
technology at the device level.
And for every leap in sensor technology,
medical imaging needs a leap
in computational performance.
Photon Counting CT is the
future of medical imaging.
This breakthrough would not be possible
without a step function increase
in computational performance
provided by NVIDIA's full
stack computing architecture.
Let's go through a couple examples.
Siemens' Naeotom Alpha
uses Photon Counting CT
to improve image acquisition
and reconstruction by
reducing electronic noise
as well as lowering radiation dose
and contrast material on patients.
NeuroLogica's OmniTom Elite
with photon counting detectors
generates spectral CT images
at multiple energy levels,
leading to potentially
more accurate visualization
and segmentation of bone,
as well as improve
visualization of blood clots,
plaque, hemorrhage, and
intercranial tumors.
And Advanced Breast-CT's nu:view product
uses spiral CT combined with
photon counting technology
to deliver compressionless breast exam
with absolutely amazing image quality.
NVIDIA's providing the
computational foundation
for innovators developing next generation
enterprise imaging and AI platforms.
These platforms leverage NVIDIA's
accelerated computing stack
to provide advanced visualization and AI
to medical images from any
device for any modality.
NVIDIA customer, Aidoc,
won a 2022 mini-award
for their AI operating system.
This vendor-agnostic operating system
makes it easier for
healthcare institutions
to deploy AI in radiology
by orchestrating a range of AI algorithms,
both from Aidoc
as well as a variety of
third-party developers.
It truly is amazing to see
what accelerated computing
has enabled our customers to build.
The future of medical imaging
is software-defined and AI-enabled.
NVIDIA's democratizing AI for
the global healthcare industry
by enabling developers
to easily adopt artificial intelligence
from the cloud all the way to the edge.
NVIDIA created Clara to harness
the incredible advancements
in accelerated computing,
computer graphics, and
artificial intelligence,
and then apply them
to key healthcare and
life sciences workflows.
The Clara Medical Imaging Platform
is a set of developer tools, SDKs,
and application frameworks
that enable the medical imaging ecosystem
to accelerate their development
and deployment of AI-powered
products and services,
from the imaging device, to
the data center, to the cloud.
At the core of the Clara
Medical Imaging Platform,
we have MONAI,
an open-source
medical-specific AI framework
that's used to develop
and deploy AI models
into AI applications at scale.
And secondly, we have Clara Holoscan,
a medical-grade realtime
edge AI computing platform
developed specifically
for generating insights
at that point of care.
Let's take a little bit of a deeper look
into each of these different technologies.
Clara Holoscan is a realtime
edge AI computing platform,
specifically for medical devices,
consisting of an application framework
that runs a robotics pipeline.
Clara Holoscan ingests and processes
streaming data and images
and performs AI inferencing
and visualization
at ultra low latencies,
as low as 10 milliseconds
from sensor to screen.
This application framework
is optimized to run in
the recently announced
NVIDIA IGX platform.
This is a production-ready medical-grade
edge AI appliance that comes
with an operating system,
built-in security and management,
safety extension package,
and long-term support.
Clara Holoscan gives the
entire medical device industry
a standard computing platform
for realtime edge AI processing.
This saves a huge amount
of hardware and software engineering time,
freeing up those resources
to develop what the industry needs most,
life-saving healthcare applications.
MONAI is a community-led
open-source framework
developed in collaboration
with King's College London,
designed specifically to accelerate
the clinical translation of AI
within the field of medical imaging.
MONAI is purpose built for radiology,
pathology, and surgical data,
and tackles the entire AI life cycle
with pre-trained models,
AI-assisted labeling tools,
state-of-the-art training techniques,
like federated learning and
self-supervised learning.
MONAI standardizes the application
development, packaging,
and deployment to the
healthcare IT infrastructure.
With over 650,000 downloads,
more than 450 GitHub projects,
160 papers published,
and 11 Kaggle competition wins,
the MONAI community
continues to grow and thrive.
Let's hear from a few
of our community members
who are developing clinical
applications today using MONAI.
- In surgery in general,
and neurosurgery specifically,
you can fix what you can see.
Therefore, visualization
is critical in our work.
- If you're able to interact
and visualize that patient's images,
you're better able to plan
the correct intervention
and make sure that you
apply the right intervention
the first time.
- It's a way to see MONAI as a toolkit
which is dedicated to AI in imaging.
- So MONAI is a community-driven effort
to bring data science into the clinic.
And that's everything from the development
of data-driven products
and the curation of data
to then how we get those insights
in front of clinicians
looking after patients.
- Since adopting the MONAI infrastructure,
it's freed us to focus
on application to
specific clinical problems
and catalyzed that application
in multiple directions.
- What we can do through MONAI
is that be able to segment the tumor,
segment all the critical
structures around it,
and in surgery, provide real feedback data
to navigate the surgeon around the tumor
to make sure most of the tumor is removed.
Therefore, we're providing the patient
with the most benefit.
- What MONAI then brings
is the final layer on top,
which is a way to scalably
apply artificial intelligence
across that entire medical record,
across every patient pathway.
The potential scale of that is massive.
I can see MONAI in every clinic
in every hospital across the world.
- How do we ensure that AI makes its way
into every hospital, every
clinic all across the world?
The challenge is bridging the gap
between research and
commercial deployment.
Adoption of AI in radiology
has traditionally been constrained
by the complexity of clinical workflow
and the lack of a de facto standard
for AI development and deployment.
It's been said that 99% of
medical imaging AI applications
never actually make their way
to clinical deployments at scale.
This puts AI in medical imaging
at a critical tipping point
that Geoffrey Moore famously
refers to as the chasm.
To cross the chasm and bring AI
from the innovators in
research and development
to every clinic, every hospital,
and every patient around the globe,
there are really three key requirements
that must be addressed
to bring AI from the bench to the bedside.
First, you need a wide and
diverse ecosystem of developers,
leveraging a common development
deployment environment.
Second, you need a de facto standard
for your AI application packaging.
And, third, enterprise support
for clinical deployment
of these AI applications.
To expand the reach of MONAI,
today, we're announcing the adoption
and integration of MONAI
by all of the major hyperscalers.
Leading cloud service providers like AWS,
Google Cloud Platform, Microsoft Azure,
and Oracle Cloud infrastructure
are all using MONAI
to develop AI-enabled
products and services.
MONAI is available on AWS SageMaker.
And recently at Health,
they announced their new
medical imaging storage service,
Amazon HealthLake Imaging,
that uses MONAI for fast data
processing and AI inference.
Google Cloud Platform recently announced
their medical imaging suite
and featured MONAI as a core component
to both their imaging lab,
for ground truth data annotation,
and imaging AI pipeline for inference.
Microsoft Azure is focused on delivering
enterprise platforms at scale.
Microsoft's Nuance Communication
partnered with NVIDIA to bring
medical imaging AI models
directly into the clinical setting
by implementing MONAI
within their precision imaging network.
Oracle Cloud has big things planned
with their recent acquisition of Cerner,
one of the largest electronic
medical record companies
in the world.
Oracle has adopted MONAI
to develop their healthcare-specific
medical imaging products and services.
It's so amazing to see the reception
that MONAI is getting
throughout the entire ecosystem,
specifically with the
CSPs and hyperscalers.
To ease the development effort
to bring AI applications
from the bench to the bedside,
today, NVIDIA's announcing
the availability
of the MONAI Application Package,
also known as MAP.
The MAP specification standardizes
how developers build and
package their AI models
into deployable applications.
This ensures a consistent
and repeatable deployment pipeline
that will run exactly the same
on any clinical platform supporting MAPS.
By adopting MAPS as a native application,
healthcare organizations
and clinical platforms
can begin leveraging all
of the amazing AI research
that's happening now in MONAI
that was unavailable to them until now.
MAP streamlines the model adaptation time
from years to weeks,
and application updates
go from months to days.
This together accelerates
the clinical impact of AI.
The MAP specification is truly
the bridge the industry needs
to connect researchers and developers
building the groundbreaking AI models
with the clinical teams
that ultimately realize their benefits.
This is a giant step forward
for bringing AI into every hospital,
every clinic all around the world.
Map once, run anywhere.
Commercial AI application developers,
cloud-based AI inference platforms,
and large-scale clinical organizations
are already taking advantage
of the ability to map once
and run anywhere.
qure.ai, a breakthrough radiology
artificial intelligence solution provider,
taps deep learning technology
to provide automated
interpretation of radiology exams.
This enables faster diagnosis
and speed to treatment.
They've adopted the MAP specification
to widen their network
of deployment options
across their entire partner ecosystem
with the least amount
of development effort.
Nuance Communications, a
leader in ambient intelligence,
has added support for MAP
application deployments
within their precision imaging network.
This brings the medical imaging
machine learning development framework
directly into the clinical
translation workflow,
accelerating innovation
and clinical impact
all the way from bench to the bedside.
The London AI Centre for
Value Based Healthcare,
a team of AI data science
research and clinical experts
working collaboratively
across a wide number of
academic institutions,
like the NHS and others,
have adopted MONAI and
the MAP specification
to standardize their end-to-end workflow.
This platform is called AIDE
and has been deployed across 10 NHS trusts
and is serving more than
18 million patients.
Earlier this week,
I had the opportunity to
catch up with Jorge Cardoso,
the CTO of the AI Centre,
to talk about the work
he is doing with MONAI
and how it's impacting
patients all across the UK.
Jorge, it's so great to see you again.
Thank you so much for
taking the time to join me
and tell me a little bit
about the work you're doing
at the AI Centre.
- Thank you for inviting
me, it's really a pleasure.
- So tell us a little bit
about the work you're doing
at the AI Centre,
how you're using MONAI,
and specifically how are you using
the MONAI application packages?
- We are on the journey
to make all of these hospitals AI-enabled.
And first, to do this,
we need to solve the translational issue
of how do we go from research
and development to deployment.
This is where MAP comes in.
It allows us to transition
from research to deployment
faster than what we could before.
- Fantastic.
So can you translate this
into a real-world use case
with actual clinical impact?
- Of course.
Stroke is currently the largest source
of adult disability in the world.
When a patient has a stroke,
a clinician needs to
act incredibly quickly.
So we have been creating algorithms
to be able to support the
decisions of how to intervene
on the patient that just had a stroke.
We have run tests on these algorithms
and gather evidence on
how well they will work
in real patients.
And we're using now that evidence
to further improve the algorithms
so that we can deploy
an improved version of those algorithms.
So this cycle would only exist
in a scenario where we can
actually reach clinics quickly,
which is almost unheard
in a academic environment.
And MAP really is an
enabler of all of that.
- It's so great to see the
actual clinical impact.
Can you share what you
see is next for MONAI?
- At the moment, MONAI
is a collection of tools
that take care of many of the
pain points that you have,
again, from research to deployment.
But what's interesting is
when you start building on that technology
and MONAI goes from a toolkit
to a more platformic infrastructure,
you will slowly have MONAI
going in the direction
of what I like to call
the healthcare OS, the
healthcare operative system.
If we are able to go that far
and provide that core infrastructure
that anyone throughout the world can use,
we will make AI a reality in healthcare.
- It is so great to see the vision
that Jorge has for the future of MONAI
and how he views it as
the operating system
for AI in medical imaging.
One of the biggest drivers for
widespread adoption of MONAI
is its open source nature,
allowing for collaboration
and contributions
from the entire global
medical imaging community.
To optimize AI deployment
for speed of impact
into the mainstream clinic,
the third and final component we need
is an enterprise-grade software suite.
This allows us to truly
harness the power of MONAI.
Today, we're announcing
that MONAI is now part
of NVIDIA AI Enterprise software suite.
This gives you the best of both worlds.
You can begin your journey
using the open-source
community-supported AI development
and deployment platform.
Then as you make the transition
to production-ready enterprise deployment,
NVIDIA AI Enterprise
gives you the suite of AI
and data analytics software
that's optimized, certified, and supported
for every organization
to deploy AI anywhere,
on-prem or in any public cloud.
NVIDIA AI Enterprise speeds development
and accelerates clinical impact
by providing you the support
you need when you need it.
NVIDIA Clara is the
medical imaging platform
that combines MONAI and Holoscan
in an innovative way to accelerate AI
across all of healthcare.
MONAI is building AI workflows
that address the unique needs
of the AI developer.
From AI-assisted labeling,
to privacy-preserving federated learning,
MONAI will tackle every imaging modality
that drives healthcare innovation,
starting in radiology
and expanding into pathology
and surgical video.
Clara Holoscan is our
realtime AI compute platform,
built to tackle the most
critical imaging use cases,
like robotic surgery.
We will continue to evolve this platform
with new features needed
to meet the requirements
of these mission-critical use cases.
The next frontier of AI development
incorporates new capabilities
enabled by things like
digital twin technology,
allowing us to generate synthetic
data, build AI algorithms,
and validate them in virtual worlds.
Our roadmap illustrates our
extreme focus and commitment
to bringing the most advanced
computing approaches,
AI, accelerated computing,
computer graphics
to the entire medical imaging community.
If you haven't started working
with MONAI or Holoscan,
it just got a whole lot
easier to get started.
MONAI and Holoscan are now
available on NVIDIA LaunchPad.
LaunchPad is an
NVIDIA-hosted infrastructure
with an entire catalog of hands-on labs,
making it easy to test
and prototype with the
latest NVIDIA technology.
Begin your journey with
the MONAI Label Lab,
designed to let you see
how quick and easy it is
to create annotated data sets,
build AI annotation models for labeling.
Then try out Clara Holoscan lab
where you can run sample applications
using the Holoscan SDK.
NVIDIA's leading AI-powered
innovation in medical imaging,
our healthcare-specific
accelerated computing platform
enables the entire
medical imaging ecosystem
to easily explore new
AI-powered capabilities,
speed time to market,
and drive immediate clinical impact.
Device developers can use our
Clara Holoscan on IGX platform
to deploy realtime AI skills
directly at the point of care.
The research community
can build upon MONAI's
robust ecosystem of partners
and begin standardizing on
MONAI application packages
to expand their deployment options.
And for hospitals, clinics,
and medical imaging
platforms around the world,
you can now reduce your
time to clinical impact
by building or buying products
that incorporate and leverage MONAI.
Just as the theme of RSNA 2022
is empowering patients
and partners in care,
you can see that NVIDIA
and NVIDIA technology
is playing a huge role
in empowering the entire
radiological ecosystem
by accelerating the adoption
and clinical usage of AI.
I wish you all a great RSNA 2022.
Come to our sessions,
join our demos given by NVIDIA leaders,
customers, and partners
that'll be available throughout
the entire week in Chicago.
And please keep on driving to
bring AI in medical imaging
from the bench to the bedside.
Title: VR and NVIDIA: Play the Future at Gamescom 2015
Publish_date: 2015-08-07
Length: 380
Views: 36332
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/sN-qxmb57uU/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: sN-qxmb57uU

--- Transcript ---

virtual reality finally becomes a well
reality this year as the first wave of
headsets from Sanaa hit which makes this
year's Gamescom particularly special
because will be a gap first slugger the
final hardware as well as the games that
will be playing on them we spoke to some
of the industry's biggest experts to
find out more about the current state of
ER starting with Nvidia's very own Jason
Paul sad Nvidia our goal is to build the
ultimate graphics platform for virtual
reality that involves three things
they'll beam amazingly fast GPUs with
our GeForce GTX processors building
client software that optimizes your
experience for VR with GeForce
experience and our developer SDK helping
developers to build great games and guys
like HTC an oculus the mill train had
such as our game works be our software
development yet in our latest GPUs
should call Maxwell architecture we've
actually designed a technology that
allows us to accelerate rendering for
the art we call it multi-resolution
shading and it allows game developers to
render a scene at different resolutions
or virtual reality thereby accelerating
performance
so this year at Gamestop were really
excited to be showing up the two great
headsets and virtual realities one from
HTC five and one oculus rift we're
showing some great content along with
these headsets on oculus rift were
showing Valkyries from CCP games Ermac
VR and time machine and then on HTC vive
we're showing a set of great content
including aperture science demo the
reaction from the fans is pretty
incredible
I think the lives are of course of four
hours people can't wait to get in there
and test the latest from oculus or HTC
HD sees flame pizza is a relative
newcomer to the scene but it's already
making huge waves thanks to a
partnership with PC gaming legends valve
and some truly incredible demonstrations
of these capabilities here's Geoffrey
gasses from HTC to tell me more HTC 5 is
our VR head mounted display we've
developed it in conjunction with valve
and there's steamvr platform what we
think it is is the most immersive VR
platform in the market it's a pc-based
system you know great precision tracking
these laser base stations to drive very
precision tracking low latency
and a feature standpoint one of the
things we think is most compelling is
something we call room scale since that
ability not just to look around and look
up and down in an environment to
actually move around a room we optimize
around a 15 by 15 foot room so when you
add that extra element of being able to
move around in the environment it really
just adds another dimension to the
presence it really makes you feel as
though you're immersed in an entirely
new space I think 2016 is the year is
finally here obviously we've committed
to have our product in market before the
end of this calendar year so we'll get a
little bit of a head start but I think a
couple things have happened I think you
have technology has gotten to a point
now or we can support high frame rates
low latency and really provide the kind
of quality experience that people need
in VR I think you also have big
companies now investing significant are
indeed where you have you know Sony
oculus HTC and valve and I think that
just give tremendous credibility not
only to developers that it's okay to go
invest in developing your application
but to consumers that this is finally
here some of us have been waiting for
this stuff for over 20 years now but the
fact that you have these big companies
making investments in commitments to put
product in market tells me it's here and
I think it only goes up from there
ccp was one of the first studios to
start work on a full vr game in the form
of Eve Valkyrie which is also playable
here with the recently revealed retail
version of the oculus headset CCP's
Andrew Willens was on hand to talk about
the challenges creating a compelling vr
gaming experience she Beltre is it's the
ultimate space dogfighting simulator
it's a top gun in space we chose to make
a more arcadey style of game purely to
you know support that immediacy of VR
yeah they were just putting the headset
on and you know you've got a joypad you
know where you are and you know you've
got competency quite quickly and it's
it's different to something like
obviously EVE Online where you know
there's a lot more strategy involved
with that and this is just suits the
hardware at this time it's also I mean
that's what we're going for really as AI
competitive multiplayer VR I think it
really delivers that sense of immersion
and that sense of presence and as a game
designer you know you couldn't want four
more
this is the dream we've been chasing for
yours is to really really kind of sell
the fantasy and the illusion of being
somewhere completely alien and again and
finally it would be remiss of me to not
try out the HTC buy for myself see you
on the other side but a manta ray is
just what I say it was just mind blowing
like utterly captivating and totally
transported in if that's even a real
word I just felt like I've been whisked
away since for I see world's one minute
I'm underwater the next day I'm an
amateur lab the other minute I've just
set up my own art studio but every
single thing in there just looks
incredible you cannot believe the scale
of it until you're in there yourself and
and when you've tried it yourself you
don't mind looking ridiculous either
obviously we're against commerce lots of
lucky people here right now King up to
have a go which must be a nightmare be
washing at home but honestly this is
lined up to be launched later this year
so soon you will be able to enjoy the
very same experiences
of your own home that's it for this
video we'll see you next time
Title: The deadmau5 Project: Episode 2
Publish_date: 2016-07-20
Length: 348
Views: 118018
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/somxklOVwfk/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: somxklOVwfk

--- Transcript ---

So phase one of this project, getting amazing 2D designs done
taking the cars, translating them to artwork that everyone was like
"Wow. Yeah, build that."
But going from 2D to 3D is always the real challenge,
'cause then you're getting into the reality of, "I have to actually make this now."
I don't religiously follow the modding community, but from time to time, you know,
in Gawker or whatever, you know, I'll find pics of really cool builds,
of guys that more or less treat it like an art form.
In some cases it's all about, "Oh, look at the way this looks,"
and the other guy is, "Well, forget that. Look at the way this performs."
And then there's just one guys who's like, "Oh, look at the way this looks and performs," you know.
You really think I'd be more on my shit with the mouse heads.
Everything thinks you must have these in glass cases
like all in display, ready to go.
Dude, no, these are like
we just keep making them.
Check that one out.
This is Frankenhead.
Oh, there's an earlier version of this actually that I have.
It was the same concept -- LED everywhere
And there was a contact here, and I used to always -- you'll see in a lot of really old photos of me,
I always wore this like metal necklace.
And I put it on, and it shorted.
One of the leads to the thing just came right back in, and it was in England. So what had happened
a guy who used to work for me plugged it into a 220, and this is regularly 110 coming in on the converter.
So yeah, that was not pleasant on the neck. But it was during a sound check, thank god,
not during a show. But honestly, I had to like
[Laughter]
throw the thing off.
It was pretty funny.
What's this one?
Oh, ha. This is the widow maker.
Look at this connector.
[Observer] Holy crap.
Yeah, it's legit.
This is the one that -- yeah, so this is the one that shorted.
And yeah, it had video goggles in it and all that.
Oh, yeah, it has a little access hatch. I remember this
there you go -- for maintenance.
Every one of these were hand-soldered.
I think there was over 700
LEDs times four connectors per side of every LED.
That's how many solder points are on this thing.
And I would see out using this little,
totally not noticeable, and almost invisible to the naked eye camera
it's the size of my fucking fist, you know. [Laughter]
[Observer] ... It's like you're jacking into the matrix!
Yeah. Dude, I almost did -- trust me. I almost took the fucking blue pill.
We're working with a few different partners to create these five different cases.
Four of them are going to be themed after the Parvum case platform.
Parvum's a great partner for this project because they're capable of taking our designs
and making just about anything out of acrylic.
So Parvum has a lot of experience in using multiple layers of their acrylic
with different colors and different finishes to achieve the final look.
They have to think of what they want it to look like at the end and then work backwards from there.
These are hands-on guys
that get down and dirty. The shop is awesome. They've got every machine you can want.
And then there's the technical side of it. They actually build all this stuff in AutoCAD.
They think about it in 3D. They plan it out meticulously.
And then they've got their own process where they have a little bit of a leap of faith.
They have to de-assemble everything, pack it up as flat sheets,
send it back to us, and then they have to hope we'll put it back together the right way.
The goal of these PCs is not just to look amazing on the outside.
We want the interior -- the guts of them -- to be completely amazing and the best that NVIDIA can do.
So we partnered with our GeForce garage program
and brought in some of the industry's top modders to help us assemble the interior of all these cases.
Yeah, I come from kind of a fortunate time period,
where since I grew up with it, new emerging technology
doesn't scare me so much because I know where it comes from.
And it just blows my mind how
how lost someone could be just jumping into it now.
With electronic music, it's really more reliant on the one guy now.
In the '80s you'd have a rock band who would be
crazy musicians, but then you'd have a producer, a manager, a mix engineer, and a mastering guy, and all that stuff,
and so they all compartmentalize their thing.
With computers and computer-aided music -- or completely computer-generated music
you have to take on the role of all these [unintelligibe]
like being a mix engineer and all that stuff, and having all those little bits of knowledge from different departments
that you can apply to the one thing, [because] you have to be self-sufficient so that, you know, if I know I've got some bad RAM
I'm not like throwing my hands up in the air, thinking like,
"Wow, I just know how to turn it on."
We wanted to ensure that the systems we're designing for Joel have an incredible amount of performance.
Each computer is going to have GeForce GTX 1080 SLI in it,
and his main system is going to have a third 1080 in it that can be used for PhysX and Audio
When you combine that with OptiCore processors,
32 gigs of ram, PCI express SSDs, and custom closed-loop liquid coolers,
he's just going to have some truly bad-ass systems.
So [Lee and Dwight] at PcJunkieMods have done a great job of building out
five computers with designs that are unlike anything they've assembled before.
By the time they're done with these things, we're going to have five beastly computers
that will play anything you throw at them.
Title: A New King of the Jungle: NVIDIA Quadro Enables the Future of Real-Time Virtual Production
Publish_date: 2020-03-04
Length: 86
Views: 36342
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/sPWM2wyEvvU/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLDeN_BDoCmRemwzqeaMoaxrdzHrvw
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: sPWM2wyEvvU

--- Transcript ---

my name is Rob legato I'm a visual
effects supervisor I'm the second unit
director and director of photography
rendering is the killer of creativity
because of the time and effort it takes
just to go through the process to render
it and then see it again achieving
greatness is you keep on pushing the
envelope and that's why you need
something that will operate in real time
and that's what the graphics cards give
you so what we said on the Lion King was
if we get some badass graphics cards and
we put them up into a game engine and we
write a multiplayer networked game and
then we add all these hardware devices
and track them perfectly
we can probably recreate the experience
of putting the filmmakers in Africa and
that's basically what we did forget that
you're doing a virtual movie act like
you're doing a real movie I can build a
set in the computer in a vive headset
walk around put a camera's ition down
actually put a camera up photograph it
figure out where the lights coming from
I like this manner of working I could
take my limited knowledge go to my
basement and practice alone without
anybody seeing my mistakes and then be
prepared for when I show up on the stage
where I do have limitations you're a way
different person under pressure than you
are without pressure
well preparation makes you not have
pressure so for young filmmakers you can
get experience on your own without
spending a fortune and Mozart will
emerge from that
Title: Official Intro | GTC 2017 | I AM AI
Publish_date: 2017-05-10
Length: 166
Views: 224580
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/SUNPrR4o5ZA/hq720.jpg?sqp=-oaymwEXCNUGEOADIAQqCwjVARCqCBh4INgESFo&rs=AOn4CLBhCFhTFJef_oY08kszPYTtbOPDPQ
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: SUNPrR4o5ZA

--- Transcript ---

[Music]
I am a visionary exploring a universe of
data to sharpen our view of the most
distant galaxies studying black holes to
help proof Einstein's theory of
gravitational waves
I am a healer giving doctors power to
mountains of data into life-saving break
identifying diseases like leukemia from
simple drop of blood even in our own
home and find a new way for cures to
market faster I am a professor keeping
our ocean safe from invasive species and
helping our crops flourish while
minimizing pesticide I am a helper
[Music]
assisting customers in stores and the
disabled in their homes
[Music]
I am a navigator mapping our world one
millimeter at a time and making even the
largest self-driving vehicles safer for
the long haul I am a creator learning to
paint from the master and applying their
styles to create original works of art I
am a teacher
analyzing half a million players every
game to identify strengths and
weaknesses and a learner
discovering new strategies from complex
keys
I am even the composer of the music
you're hearing
[Music]
I am a I brought to life by Nvidia deep
learning it's really is mine everywhere
Title: AI Against Lung Cancer - 12 Sigma Technologies | Season 1 Episode 3 | I AM AI Docuseries
Publish_date: 2018-01-15
Length: 374
Views: 20698
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/Swf_W0vDSDo/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: Swf_W0vDSDo

--- Transcript ---

my job takes me around the world
the more I travel to industrialized
cities the more I noticed how common air
pollution is when you factor in the rise
in smoking it's easy to see why lung
cancer is such a growing problem and
that's why medical startup 12 Sigma
decided to tackle this challenge I'm
Arjun dot with Nvidia and this is Imai
[Music]
lung cancer is the most deadly cancer
the five-year survival rate is only 17
percent this is all of the world in
China the u.s. in Japan the reason for
that is when doctor diagnosed people
with lung cancer is already in the mid
or late stage when diagnosing lung
cancer doctors look for small nodules in
the lungs but these are not always easy
to spot so almost every single lung
cancer started as a smaller not too long
as you have a very big variety of looks
and it takes a doctor years to know all
these different looks doctors are very
good at finding large size of knowledge
such as 6 millimeter or 10 millimeter
but when looking at smaller size
knowledge OU's such as four millimeter
sometimes they have difficulty and
that's what we were trying to solve is
to help doctors to detect with Slough
not just early before they turned deadly
saving someone's life is meaningful then
this is exactly you know where we are
excited about that's why 12 Sigma
started looking at deep learning to
develop an artificial intelligence that
could help doctors identify early signs
of cancer we think AI is suited for
detecting cancers because there has been
a vast amount of data being produced and
also because the advances of computing
power
we can use the latest deep learning and
computer vision technology to process
this data for lung cancer we use city
images from many hospitals in the US and
China differently from the 2d images
that people are we see image net or face
recognition the CT scan it's a 3d volume
so one CT scan consists of probably
three to five hundred images we use
convolutional neural network that
instead of manually defining the visual
features who learn these features
automatically what we call supervised
learning process what they're actually
using is a 3d convolutional neural
network or CNN in a typical 2d train CNN
a convolutional layer process as an
input image with a set of learn filters
each filter produces an activation map
that is typically positive where the
filter matches the image and negative
where it doesn't
these maps then pass through an
activation layer to strengthen the
signal and a pooling layer to reduce the
data and maintain its most essential
components the process is repeated many
times and the resulting data is used to
classify objects in the image when
working with volumetric data such as a
CT scan the process is similar but
extended into the third dimension
evaluating three-dimensional structures
is essential to identifying possible
tumors GPUs are very important for
neural network training and media has
the most advanced GPU that has improved
the computation of speed for neural
network more than 50 times over CPU so
this helped our computers to learn
models faster and we can reduce training
time from weeks or months in two days or
even hours having an AI read and
interpret CT scans may sound like
science fiction but 12 Sigma has already
begun testing their system in several
hospitals doctors have this kind of
feeling
ai is 20 years away they didn't expect
this it's actually here so close already
in China we are putting our products in
about 35 to 40 hospitals already most of
the doctors they are using their naked
eye to look at the images first so that
they are meeting all the regulation
requirements then they're using our
system as a second reader to look at the
image again in many examples they have
found more knowledge that they haven't
found you may need you to go to the
kernel in your salvia Charlotte AZ or
indeed an innkeeper
Duncan central solution for
boom oh sure sure sure there is a
similar Sweden ooh a male candidate
statistical energy William the AI is
precision is impressive but how do its
diagnostic capabilities stack up to an
experienced radiologist we actually do
have some false positives or false
negatives and the doctor will remove
these false positives and adds the false
negatives but I learn again with a fine
tune again using all this new league
labeled data by working with the doctor
and software together we can achieve
very high sensitivity and very low false
positive
it took a doctor 8 to 10 minutes to look
through a CT scan this is without
determining if is 9 or malignant now our
system it took us 2 minutes to complete
the detection and also the
classification so actually cut deduction
reading cut by four and a half hours on
average day one of the main things for
our software is safe doctors time and
have the doctors to focus on more
important things like research and
communicating with patients
[Music]
you
[Music]
you
Title: Transforming Transportation with AI
Publish_date: 2021-09-07
Length: 761
Views: 393240
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/s_Ze2o8ixNM/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: s_Ze2o8ixNM

--- Transcript ---

hello i'm ali canny vice president and
general manager of nvidia's automotive
business it's a pleasure to be here
today at e r mobility 2021.
ai is the most powerful technology force
of our time and it's transforming
virtually every industry from healthcare
finance to energy and manufacturing the
breakthroughs in ai are nothing short of
science fiction and nowhere is this
transformation more significant than in
the automotive industry
autonomous driving is perhaps the most
intense ai challenge but it's also the
one with the greatest impact to society
it will save lives make roads less
congested and change the way billions of
people move around the world
transportation is a 10 trillion dollar
industry it includes not just passenger
vehicles but also commercial trucks robo
taxis delivery vans and even specialized
vehicles used in agriculture
construction and mining
in the past passenger vehicles were
defined by their driving experience a
key capability of a vehicle was its
horsepower
but now advancements in deep learning
and robotics technologies are redefining
the auto industry based on a vehicle's
computing power
eventually everything that moves will be
automated but it will take time and a
significant investment to get there the
mission of the nvidia drive team is to
develop the self-driving platform and
software stack to enable our partners to
bring it to market
these future cars and trucks are going
to be completely programmable computers
business models for the transportation
industry will be software driven this
tech mindset will fundamentally see a
car not as just hardware sold but as an
installed base where the entire fleet
gets new services over the life of a car
i want to start by explaining how
challenging it is to develop
self-driving cars
first you need a high performance
computer architecture and sensor suite
in every car that's capable of
delivering a safe self-driving
experience
a self-driving fleet will generate many
petabytes of data each day all of which
needs to be encoded secured and then
sent to the cloud
this data then needs to be labeled to
identify every object lane marking and
traffic sign and after all of this data
is processed we need to train the deep
neural networks on a data set
these ai models are then tested on a
full range of scenarios in a digital
twin of the real car using simulation to
allow for the testing of rare and
difficult driving situations in a
scalable and efficient manner finally we
need to test autonomous driving software
in the production test fleet to ensure
our autonomous vehicles are safe
this entire process is never ending
autonomous driving software will need to
be continuously developed and improved
nvidia is building our av platform
end-to-end with the highest functional
safety and cyber security standards from
the av chips and computers
sensor architecture
data processing
av perception mapping and cockpit
software to the infrastructure where we
train and create a simulated digital
twin
onto the fleet command and operation
center where you can remotely operate
your fleet and finally road testing
and we're creating this platform in
modules so that our partners can use
exactly what they need
since av is such a grand challenge and
will take years to develop we believe
what differentiates av excellence more
than anything else is the speed of this
development flow the companies with the
best development flow will be the most
successful av companies
ai powered vehicles will get smarter and
smarter over time as the software is
trained with more data for new tasks
tested and validated and then finally
updated over the air into each vehicle
developing an autonomous vehicle
requires an entirely new hardware
platform architecture and software
development process both the hardware
and software must be comprehensively
tested and validated to ensure they can
handle the harsh conditions of daily
driving with the stringent safety and
security needs of an automated vehicle
now that's why nvidia has built and made
open the drive hyperion platform that
specifies a computer and sensor
architecture that meets the safety
requirements of a fully autonomous
vehicle
hyperion is designed with redundant
drive orient computers and
state-of-the-art surround cameras radar
and lidar it's architected to be
functionally safe so that if one
computer or sensor fails there's a
backup available to ensure that the
autonomous vehicle can drive its
passenger to a safe place
we're making hyperion to help our
partners who want to build self-driving
software themselves
we have spent many years developing a
data collection system with high quality
calibration and time synchronization in
3d so our partners can leverage nvidia's
base platform investment so they can
focus their resources on their ai
application development
and since av is such a grand challenge
we have also defined hyperion with a
standard computer form factor so that
customers can design their autonomous
vehicle to scale across generations of
their vehicles
with this standard form factor a
customer can go to production with a
drive or a computer today and easily
plug in a hardware and software
compatible drive atlant computer in the
future
infrastructure services and tools are
the keys for developing an efficient
self-driving platform autonomous driving
software development process requires
several infinite data loops to work
together these loops are designed to
train your ai models simulate your av
software and then to drive
av software also needs to remember your
driven routes otherwise known as drive
mapping and then learn again so there's
a ton of data and they're in loops
ai model training loops simulation loops
mapping loops and each of these loops
are all running continuously and they're
all interlinked
this is not easy to bootstrap and it's
not easy to operate however this is what
it's going to take to run a self-driving
system that's safe and while it might
sound a bit complicated it's very
sensible in a sense this pipeline is
what ai software development is all
about it's a data model
processing driven approach to computer
science
this way of developing software is
profoundly different from the way
software was developed in the past it
requires super computers in the data
center and supercomputers in the car
this end-to-end data factory is the
essence of nvidia self-driving car
initiative
at nvidia we're building a world-class
av stack that can scale to the most
complex scenarios
perception is one of the most important
and compute intensive parts of any av
stack we've built nearly 20 perception
models to help you see cars bicyclists
and pedestrians we've designed networks
to classify lanes
signs lights parking spots and road
boundaries our perception software is
designed to be functionally safe as
we've designed multiple neural networks
to see a given object and we perceive
objects on different sensors
we design our system with this
resiliency such that no single sensor or
ai model becomes a single point of
failure
when you have great perception software
you can perceive lanes signs poles and
lights which can help you build a high
quality driving map
this generated high definition map is
also kept fresh because an av is always
learning drive av dynamically updates
maps after any vehicle in an oem's fleet
drives the same route again
as part of our commitment to expanding
our mapping capabilities we recently
announced our acquisition of deep map as
a leader in scalable hd mapping deep map
will improve and accelerate the
international scale of nvidia's drive av
platform
nvidia is building a best-in-class
autopilot product we want to provide a
great address to address experience for
our customers with continuous expansion
and coverage of scenarios and odds
driven by data and enabled seamlessly
with over-the-air updates
such a product needs to be able to
handle dangerous situations and complex
maneuvers which we are already actively
developing and in many cases already
testing this includes handling
controlled and uncontrolled
intersections busy crosswalks
roundabouts along with addressing
vulnerable road users like bicyclists
a validation strategy is critical to
make sure av is safe
autonomous vehicles must be able to
respond properly to incredibly diverse
situations on the road emergency
vehicles unpredictable pedestrians
congested traffic and even poor weather
conditions there are an infinite number
of scenarios that are very rare to come
across in the real world while we need
to test an av on the road it's really
hard to experience all these corner case
situations that happen in the real world
this is why it's critical to also test
av in simulation and why nvidia has
built our own simulation platform based
on omniverse it's called drive sim and
we use it for hd map reconstruction
synthetic data generation new scenario
simulation and replay validation only
with a combination of on-road testing
and simulated testing can you really
make sure an av vehicle is ready to
safely drive
now let me show you what drive sim
software lets you do in this video
[Music]
over the last year we've made several
significant announcements with key auto
and truck makers
this video shows you the early results
of our collaboration with mercedes-benz
which starts with a development flow
that lets you first develop av software
in a physically accurate simulator
that's essentially identical to a real
car
in this video we have modeled the
mercedes-benz eqs to be true to life
this includes accurate vehicle shape and
dimensions along with real dynamics
based on a vehicle's tires and
transmission so that the acceleration
braking and cornering are accurate
we have completely modeled the sensor
suite we are running av on an identical
computer architecture in the cloud after
building the virtual car we're able to
run av software running in a digital
twin of the real vehicle with drive sim
we can run different traffic and weather
models to test the robustness of our av
software as this car computer doesn't
know it's running on a virtual world in
the data center
we're extremely proud of this
accomplishment as we're able to get av
software running on a new sensor suite
and car within just a few months
by having a physically accurate drive
simulator you can accelerate development
efforts by letting developers first test
in simulation and then test in the real
world
i'm really excited and proud of the size
of nvidia's drive ecosystem which
includes hundreds of companies
developing on our drive platform they
encompass automakers
truck makers tier one suppliers software
startups sensor makers mapping companies
and more from around the globe all are
looking to bring safer smarter
transportation to our roads
some are building software that can get
deployed on oem vehicles while others
are oems building level 2 cars level 4
commercial trucks and even level 5 robo
taxis we've built our platform to be
modular so customers can build the best
product that fits their needs and
requirements
and we're the only company today that
has an open platform for the industry to
build on thank you for joining me today
during er mobility i hope i've been able
to provide you with a deeper
understanding of how nvidia is
developing the core technologies
fundamental to the development and
deployment of self-driving cars to
enable a safer travel and extend the
freedom of mobility to all
Title: NVIDIA DRIVE at CES 2019
Publish_date: 2019-01-11
Length: 175
Views: 12341
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/S_zxgikxS3w/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: S_zxgikxS3w

--- Transcript ---

[Music]
we truly are at an inflection point
within mobility it is the biggest
opportunity to change the way that we
transport ourselves since 100 years ago
when the automobile was introduced it's
saying a new Industrial Revolution
it's absolutely fantastic I've got four
children safety is super super important
because there's 37,000 fatalities each
year in the US and in general 95 percent
of all crashes happen from driver error
so driver assistance systems and active
safety systems are the perfect way to
eliminate 95% of those accidents we
believe that the immediate technology
helps us work inside the car and outside
the car from a safety perspective we're
obviously wanting to go the next level
where technology and road safety a hand
in hand with this race the autonomous
driving following level to the current
level of most of the solutions that we
find in the market moving to level 3
level 4 and above it is very difficult
with the current technology NVIDIA is
helping us very much so this year at the
CES we have decided to announce an l2
plus system in order to bring the
maximum level of innovation we try to
partner with the best players in their
field to bring the best innovation to
our customers the media is probably the
most important component in this system
which provides tremendous amount of
computational powers so that we can
eventually achieve the intense level of
level for autonomous trading for me it's
super exciting that cars are really for
real becoming software products here
technologies have been collaborating
with Nvidia and drive localization which
is a fundamental component of
self-driving vehicles localization is a
way for the vehicle to understand its
position precisely on the roadway which
is incredibly important for a
self-driving vehicle to know
Vidia solution is for us the best choice
because it allows us also to bring a lot
of our security functionality and
connect our sensor systems into the core
computer the embedded drive platform is
a perfect platform with those scalable
affordable solutions with high power and
being able to leverage AI you know it's
a perfect marriage I would say between
the automotive experience that we bring
and then the power of AI that Nvidia can
bring with the advanced processing power
the chipsets that Nvidia has the
experience in bringing those two things
together to form really scalable safe
architecture solutions for all what's
really exciting is the levels of
automation and the systems have really
needed and required a level of education
and one thing that has happened is a
coalition of brands have come together
Nvidia howdy included and it founded
paint which is really there to bring
industry leaders manufacturers and
suppliers together and as a little sold
but we're gonna do it so you can there's
that sense of everybody you meet is
working towards the same goal it's not
about future it's about a present and
it's here
Title: NVIDIA Quadro Powers the 6K Workflow of Hollywood’s Gone Girl
Publish_date: 2015-01-16
Length: 181
Views: 41085
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/t0oW3bhb6B8/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: t0oW3bhb6B8

--- Transcript ---

Nick Gunn you're probably the most hated
man in America right now did you kill
your wife Nick I mean it's always that
story it's constantly about story and
being told in the best way possible I
think technology has simplified the
process we do a lot in house it just
brings speed and efficiency by bringing
so much of the post here or we can do so
many kind of effect shots we want
everything to be done as soon as
possible straight away so you're
reacting and working with finish things
with stabilized shots with effects
already done when we set out to do gun
girl we knew that we're going to be
shooting at 6k we have these amazingly
powerful machines we have these
amazingly powerful Nvidia k5 thousands
inside them and they gave us this
enormous power that we haven't really
seen before all contained within these
two systems the same systems that were
editing on attach to this direct storage
we shot gone girl on the red dragon and
we shot at 6k the essential area on our
movie was 5k so we have that 1k area
around all sides to be able to move and
manipulate the image we have these
amazing graphics cards that are used for
display outs and also to transcode the
footage for our final online the entire
idea behind this production workflow was
to bring the effects closer to editorial
the overall capability of the HP GA 20
with the nvidia quadro k 6000 really
meant that we could look at a 6k frame
make sure that we were just seeing just
the portion about we wanted to see and
display that pixel for pixel onto a 4k
monitor basically each of these six key
frames wasn't scary anymore
we were able to handle everything we
needed to handle with that this is the
first project that I got where I started
to see the kind of processing power that
I really want CUDA inside of Adobe
Premiere as well as the overall power of
the quadric a6000 and known integration
with Adobe Premiere really meant that
we could consistently deliver a level
performance in our earlier benchmarking
that was application-specific nvidia
particularly excelled if Kurt was in the
other room working in Premiere and asked
me to open up that FFX project he could
continue editing and I can be working in
my room working in After Effects and as
soon as I say that it would be reflected
in his timeline
so basically sped up our collaboration
visual effects artists editors and the
director can communicate in a manner
that's so fast that the iterations
hundreds are happening in a single day
these are all things that are benefiting
from because there has been a
streamlined process that makes my job
fast and efficient
Title: The Next Evolution of Industrial Automation with Siemens Xcelerator and NVIDIA Omniverse
Publish_date: 2022-06-29
Length: 173
Views: 46419
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/t6ppwWZUSEc/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: t6ppwWZUSEc

--- Transcript ---

Siemens and NVIDIA are partnering to advance 
industrial digital twins in the metaverse,  
opening a new era of automation for manufacturing.
In this demonstration, we see 
how the expanded partnership  
will help manufacturers respond to 
customer demands, reduce downtime,  
and adapt to supply chain uncertainty while 
achieving sustainability and production targets.
By connecting NVIDIA Omniverse and the 
Siemens Accelerator ecosystem end-to-end,  
we will expand the use of digital 
twin technology to bring a new level  
of speed and efficiency to solve design 
production and operational challenges.
Siemens Accelerator offers the industry's 
most comprehensive digital twin and includes  
best-in-class software for digital manufacturing, 
collaboration, design, and industrial operations.
Omniverse, based on the open standard of 
USD (Universal Scene Description) connects  
a wide range of software tools and users with AI,  
physically accurate visualization, realistic 
physics, and ray-traced RTX rendering.
NVIDIA Omniverse is a large-scale 
full-fidelity virtual world engine  
and ideal for industrial metaverse.
The connected platforms, running on 
GPU-accelerated systems from edge to cloud,  
unlock amazing superpowers from factory 
planning to autonomous robots and beyond.
Uniting the worlds of information technology and 
operational technology, the Siemens Accelerator  
platform uses edge-enabled devices to collect 
real-time IoT data connected to the digital twin.
When problems occur, engineering, 
manufacturing, and logistics teams can  
immerse into the live digital twin to root 
cause the problem and develop a solution.
New solutions can be tested and 
validated using the digital twin  
across thousands of scenarios and edge cases, 
eliminating the need for physical prototypes,  
reducing critical downtime, and 
increasing manufacturing agility.
Siemens Accelerator and Omniverse train robot 
perception AI models on physically accurate  
synthetic data generated from the digital twin, 
accelerating both initial training and retraining
in the event of production changes.
In live digital twins, customers can train  
robots to perform tasks before deploying 
the AI models into the physical robot.
Omniverse can even be used to train 
fleets of robots working in harmony.
Industrial automation is 
being supercharged with AI.
Inside a digital twin customers 
can design environments that allow  
humans and robots to collaborate 
safely and efficiently. 
Together, NVIDIA Omniverse and Siemens Accelerator
 are bringing open collaboration to industrial  
automation enabling full-fidelity live 
digital twins to drive a new level of  
speed and agility in the era of software 
and AI-driven digital transformation.
Title: How Multi-View LidarNet Presents Rich Perspective for Self-Driving Cars - NVIDIA DRIVE Labs Ep. 18
Publish_date: 2020-03-11
Length: 148
Views: 24635
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/T7w-ZCVVUgM/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: T7w-ZCVVUgM

--- Transcript ---

today in Drive Labs we’re talking about our LidarNet deep neural network
an end to end DNN that uses only lidar data
to semantically understand the entire scene around the car
and compute 3D bounding boxes around objects on the scene
lidar can help a self-driving car construct
a detailed 3D picture of what’s around it
in this clip, we see the first stage of
our multi-view LidarNet DNN
the top panel shows the input lidar scan data
while the middle panel shows this lidar data
segmented into both dynamic object classes
such as cars, pedestrians, and bicyclists
as well as static elements
such as drivable space, sidewalks, buildings, trees, and poles
the segmentation output is then projected
into a top-down or "birds eye view" as shown in the bottom panel
this view contains both the semantics and
height information for each processed lidar data point
with each point colorized based on its semantic class
the bird’s eye view representation is then
fed into the second stage of multi-view LidarNet
the second stage operates in top down or bird’s eye view
it's trained to predict 2D boundling boxes
for dynamic objects identified by the DNN’s first stage
here we see the vehicle bounding box proposals visualized in white
these raw detections will be post-processed to compute the final bounding box prediction
as well as to compute different object instances
as shown in this video
where we see the output of multi-view LidarNet’s second stage
post-processed by our lidar object tracker
which tracks different object instances across
data frames and uses the 2D bounding boxes and lidar point
geometry to compute 3D bounding boxes for each object instance
different object classes are denoted by different
bounding box shapes rectangles for cars and cylinders for pedestrians
while different object instances are shown in different colors
the ego car is shown in yellow at the center
drivable space is shown by the concentric green lines
cyan denotes all points that do not belong to drivable space
to complement the DNN output
our lidar processing software also computes low-level geometric fences
around unusually-shaped physical boundaries where the car cannot drive, as shown by the megenta lines
LidarNet along with the rest of our lidar
processing software stack is designed for level 4 and level 5 autonomous
driving and the 3D information it provides can be
combined with camera and radar perception to design an even more robust autonomous system
Title: DeXtreme: Transferring Dexterous Manipulation from Simulations to Reality
Publish_date: 2022-09-22
Length: 90
Views: 4917
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/TAUiaYAVkfI/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: TAUiaYAVkfI

--- Transcript ---

Robotics researchers worldwide are exploring 
manipulation with fine motor control skills. 
Breakthroughs in this area will enable 
a new wave of robotics applications.
Robots will move from factories and 
warehouses into our homes and offices.
In this example, the robot’s goal is to 
move the cube to  the target position.
Every time it succeeds, the target changes.
This kind of agile manipulation is a really hard problem. 
How hard is it?
If we look to the animal kingdom, 
only a few species have mastered it.
We can use virtual worlds to train robots to do many tasks,  
and recent advances in NVIDIA GPUs and our 
Omniverse and Isaac technologies have made it  
possible to simulate thousands of these worlds 
in parallel, much faster than in real-time.
To train an AI for this task,  
42 years of a single robot's experience 
was simulated in a little over a day.
To be able to transplant the robot's brain from 
simulation to reality, we make the problem more  
challenging in the virtual world, using 
Omniverse Replicator to do things like  
randomizing different backgrounds and cameras, 
or making the cube heavier or more slippery.
Once we have done this sim2real work, we can 
evaluate how well our AI does in the real world, 
and then close the loop from reality 
back to simulation again to make our  
next virtual world even more accurate, 
so our robots can do even better.
Title: NVIDIA CES 2018 News Recap
Publish_date: 2018-01-08
Length: 161
Views: 17440
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/tcBRcYbjioY/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: tcBRcYbjioY

--- Transcript ---

[Music]
NVIDIA sped into CES extending our lead
in the race to autonomous driving Jenson
kicked off the tech industry's biggest
show with a wide range of announcements
for partners and products in front of an
audience of a thousand press analysts
and customers first he showed off Nvidia
Drive Xavier the world's highest
performing and most complex SOC ever
built it has more than nine billion
transistors and it's the heart of our
self-driving platform for autonomous
vehicles next he announced that we're
using our Nvidia drive platform to bring
the software-defined car into reality in
addition to our drive AV autonomous
vehicle software stack we introduced the
drive IX are intelligent experience
software platform as well as drive AR
our augmented reality platform bringing
safety and convenience features to the
driver and passengers next we announced
several exciting new partnerships on
three continents that helped bring our
total number partners on NVIDIA Drive to
over 320 worldwide uber has selected
Nvidia technology to power their fleets
of self-driving cars and trucks they've
logged over 2 million autonomous miles
in more than 50,000 rides so far when
you think about uber providing 10
million rides a day the potential is
huge
Volkswagen CEO of Herbert DS joined
Jensen on stage to announce they're
bringing our drive IX intelligent
experience tech into the newest VW
vehicle lineup most notably the ID bus a
modern version of the iconic micro bus
will be launching in the next few years
dr. Dee's and Jensen got to experience
the id buzz in the Nvidia holodeck a
collaborative workspace that allows
designers to work together regardless of
where they are in the world we also
announced a new partnership with Aurora
the hot new autonomous vehicle startup
founded by the former head of Google's
self-driving team along with executives
from Tesla and uber they're working with
the nvidia dr xavier platform to develop
production autonomous vehicles during
the course of the keynote we showcased
amazing demonstrations of Nvidia
technology we're able to use 3d
simulation to generate virtual
environments and use that as input to
test our Nvidia Drive platform
understanding its effectiveness and then
validate its ability to
hazards objects lanes and the
environment and drive safely
finally to serve the world's largest
automotive market we're teaming up with
Baidu and ZF to develop a complete
production-ready autonomous driving
platform addressing the specific needs
of the Chinese market the world's
largest automotive segment
it's an exciting start to CS 2018 and
we're just getting going for more
information check out Nvidia com
[Music]
Title: CES 2019: NVIDIA on the Show Floor
Publish_date: 2019-01-09
Length: 130
Views: 8350
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/tDzj5tjZfns/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: tDzj5tjZfns

--- Transcript ---

nvidia has its biggest booth ever at CES
this week and it's filled with our
self-driving technology since the show's
opening on Tuesday throngs have come
through to see how we're applying AI to
make autonomous vehicles safer and more
responsive among the stars are just
announced Nvidia drive Auto pilot the
world's first commercially available
level 2 plus automated driving system
drive autopilot combines our Xavier
system-on-a-chip
with the latest Nvidia drive software
together they process multiple deep
neural networks for perception as well
as complete surround camera sensor data
from both outside the vehicle and inside
the cabin drive autopilot provides for
full 360-degree perceptions with highly
accurate localization and path planning
capabilities
it goes beyond adaptive cruise control
lane keeping and automatic emergency
braking to handle situations where lane
split merge and safely perform lane
changes our booth features a Volvo s60
and NVIDIA zone self-driving test
vehicle bb-8 each showcasing level 2
capabilities that allow editors
customers and attendees to try out their
advanced perception capabilities other
demos showed how our technology
addresses different types of
environments and weather conditions as
well as the convenience of controlling
the car with our gesture recognition
technology two of the world's the
largest automotive suppliers
Zef and Continental use CES to unveil
level 2 plus solutions based on Nvidia
Drive going into production in 2020
they'll deliver safer vehicles today and
fully autonomous driving tomorrow
speaking at a packed event at the
Mercedes booth jensen and mercedes
Sajjad Khan announced plans for the
German carmaker used Nvidia drive to
realize its vision for next-gen vehicles
in video was also highlighted as a
founding member of pave a partnership
for automated vehicle education this
coalition of automakers technology
companies and the National Highway
Traffic Safety Administration is charged
with advocating safety mobility and
sustainability of self-driving vehicles
things are revving up here check out all
of Nvidia CS news on the video calm
[Music]
Title: GTC 2022 Keynote Highlights
Publish_date: 2022-03-25
Length: 184
Views: 22293
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/tEjH3g3-QOs/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: tEjH3g3-QOs

--- Transcript ---

[Music]
nvidia is pioneering accelerated
computing
an approach that demands full stack
expertise
nvidia accelerated computing full stack
engineering at data center scale has
sped up computing by a million x
a million x opens the opportunities to
tackle grand challenges like drug
discovery and climate science
and with the invention of self-learning
transformers
ai jump to warp speed
today
we are announcing the next generation
the engine of the world's ai computing
infrastructure
makes a giant leap introducing nvidia
h100 the massive 80 billion transistor
chip we designed the h-100 for scale up
and scale out infrastructures
our largest generational leap
ever
nvidia h100 cnx connects the network
directly into h100 through the most
advanced networking chip nvidia cx-7
grace is an amazing superchip
two cpus connected over a 900 gigabytes
per second mv link
chip to chip interconnect to make a 144
core cpu with one terabytes per second
of memory bandwidth
mv link opens a new world of
opportunities for customers to build
semi-custom chips and systems that
leverage nvidia's platforms and
ecosystems
nvidia ai platform used by over 25 000
companies worldwide got major updates
omniverse will be integral for action
oriented ai
ovx will be the infrastructure of
digital twins
ovx runs omniverse digital twins for
large-scale simulations with multiple
autonomous systems
operating in the same spacetime
we announced nvidia spectrum 4 51.2
terabits per second switch
and with cx7 and bluefield 3
will be the first end to end 400
gigabits per second networking platform
omniverse is central to our robotics
platforms
and like nasa and amazon we and our
customers in robotics and industrial
automation realize the importance of
digital twins and omniverse
we updated 60 sdks of this gtc
for our 3 million developers scientists
and ai researchers and tens of thousands
of startups and enterprises the same
nvidia systems you run
just got faster
accelerating across the full stack and a
data center scale
we will strive for yet another million x
in the next decade
[Music]
engage
Title: Taking the Ice with AI - ICEBERG Hockey Analytics | Season 1 Episode 7 | I AM AI Docuseries
Publish_date: 2018-06-07
Length: 410
Views: 20951
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/TkqyIbzQZCg/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLCDu9BfKJh8lzuEhXE4bW_riAGKng
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: TkqyIbzQZCg

--- Transcript ---

if you've ever watched hockey you might
think it's a chaotic and unpredictable
game but even the best coaches aren't
always sure if they made a well informed
decision one company is using the power
of AI to help coaches turn seemingly
disorganized gameplay into powerful
analytics with a competitive edge I'm
Arjun duck with NVIDIA and this is I am
AI I was a sports stats geek and I loved
mathematics and I loved the sports so I
thought that there is like a natural
blend between the two and this is what
my life should be all about hockey is
the most complex sport every year it's
getting tougher and tougher to get that
edge so to get that competitive
advantage over other teams especially in
such competitive leagues I realized
there is no automatic data collection
technology in hockey so every team every
league is collecting data manually
probably take it 10 man hours at least
to try and get a big chunk of those
stats
it's just recorded now you've got to
sort them and now you've got to filter
them and everything and just not enough
time before your next game to prepare
you might get everything done and then
the next game started you're missing the
time to actually implement that stuff so
how can a I accelerate this process to
ensure that coaches get the information
they need in time to make a difference
iceberg sports analytics is a platform
that collects information from hockey
games and presents it on a user-friendly
portal that allows you to analyze your
team or player performance
we've developed a hardware system that
has three cameras and each camera is
tracking each zone so we have a
panoramic view of the ice rink the
cameras capture the whole 200-foot
surface of the ice rink attracts each
player 10 times per second what we have
in a raw data set is about a million
data points of player movements referees
movements and also the puck coordinates
after the game you upload it to our
cloud then using artificial intelligence
the game is broken down for you so the
coaches don't have to spend time
breaking down the video themselves
hockey game can be explained by four
basic events its skating pass a shot
attempt and reception reception is just
any pocket touch and we are applying
some machine learning algorithms to
identify those four events when you log
into our system you're gonna see your
basic analytic stats your shots on goal
your total shots your face offs your
possession percentage which is who
possess the more in that game there
are other metrics that are more unique
to iceberg like x qi x ji corresponds to
expected goals and this concept tells
you what's a probability of a specific
shot or any event in the game to be
resulted in a goal
the key lies in their use of a special
class of recurrent neural network that
uses long short term memory or LS TM
units an LS TM unit uses memory from
previous dates as well as current data
to predict the likelihood of a future
event when provided video from a game
the early stages of iceburg system
perform player identification team
composition and calculation of player
and puck trajectories these initial
levels of information allow icebergs LST
m network to generate more insightful
data about the flow of events in a game
iceberg uses this data at deeper levels
of the system to generate an advanced
metrics such as the expected goal
timeline the iceberg system recognizes
complex events such as a zone entry a
power-play or an intercepted past and
interprets how these events affect the
likelihood of a goal being scored
we felt that not all shots are equal so
we've attached the probability metric to
each shot to improve that analysis for
the teams and this way you can identify
which players are actually bringing you
the value from the offensive side of the
game you're only gonna have one leading
scorer and a team that's you know
everyone else you got to figure out what
their strengths is what they're bringing
to the table back in the day you only
had goals and an assist so you didn't
know who got the puck from A to B which
set up that goal but now with the
analytics the guy that doesn't stick out
you you may notice a diamond in the
rough
we rely on coaches feedback a lot we are
trying to build a solution for hockey
coaches by hockey coaches but hockey is
often so complex that coaches can often
disagree on what happened in any given
play how do they feel about another set
of eyes objectively evaluating every
move of each player as a coach I think
good coaches you're you're constantly
learning and and if you have a
technology is powerful this and it
doesn't change the way you look at
things I think you're missing more than
half the battle it's nice to have the
data and the video and the information
to get the guys ready to go the next day
whether you know whatever backs you up
great and what you thought right away
but either way you want to know as a
coach if you were right or if you're
wrong and how to correct it
I wondered how is iceberg able to
deliver analytics in a time frame that
teams can actually use we had been using
CPUs in production but it took us more
than 15 hours to produce one game
and we realized that we had to switch to
GPUs to decrease the production time
because our clients needed the data
faster part of the reason why we
switched to GPUs is we had to cut the
production costs we've cut our costs
more than twice and also the production
time right now is only five hours which
is a significant improvement
I see iceberg evolving into other sports
the first thought is soccer another
sport that I think it'll delve into
really quickly is lacrosse right now
that I don't believe there's any
analytic stats and lacrosse our
technology and the product is easily
scalable because it's fully automated so
for us it doesn't matter if it's 50
clients or 10,000 clients we just need
to rent more computing power to produce
the games and that's the beauty of
computer vision and automation
[Applause]
Title: Running Wild for Nature Conservation - Wildbook | Season 1 Episode 6 | I AM AI Docuseries
Publish_date: 2018-03-28
Length: 424
Views: 17992
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/TLw_I1ghvLM/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: TLw_I1ghvLM

--- Transcript ---

Kenya is home to some of the most
diverse and beautiful wildlife in the
world but unfortunately also some of the
most endangered Kenna technology like
artificial intelligence really helped
save these majestic creatures
one team scientists think so I'm Arjun
dot and this is Imai
[Music]
I'm in Kenya for the great Grevy's rally
we're using people driving around all
over the country to count
Grevy's zebras 90% of them are in Kenya
they're highly critically endangered
we've got conservation managers working
with County government officials
pastoral livestock owners and the middle
class from Nairobi all converging being
given their cameras given the maps of
the areas that are patrolled and then
for two days they go out and they snap
as many pictures as they can and as the
number of herds of cows sheep and goats
have increased the people have dominated
the landscape and have pushed the
Grevy's zebra into more marginal grazing
habitat but they've also kept them away
from water during the daytime which
means it gets more predictable when they
drink at night for the Lions to eat them
and it also means they can't get water
during the day to cool and evaporate to
keep their body temperatures normal and
so as a consequence the Grevy's zebra
was starting to be out competed by the
livestock in order to understand the
dynamics of populations you actually
have to follow the fate of individuals
so in order to study individuals you
have to recognize every individual and
that's where wild book comes in wild
book is a software platform which allows
us to use computer vision and artificial
intelligence to identify down to
individual animals not only zebra
giraffe turtle a whale but zippy the
zebra and Terry the turtle and Willy the
whale to most people one zebra looks
like another so how does wild book help
the scientists tell them apart each
zebra is uniquely marked by its natural
barcode and the algorithms that the
computer scientists have developed is
they can look at the pixels and the
local neighborhood of pixel change the
rate of change between the black and the
gray and the white identifies hotspots
on the animal and once we get a map of
those hotspots on each individual we can
then compare any new image against the
images in the archive and that allows us
to determine whether is an animal we've
seen before who's been named or whether
it's a new animal that gets a new name
if it's an animal we've seen before then
we have a historical record where was it
who was it with we can do social network
analysis and what was it doing we have
the metadata from the picture or from
the ecological measurements you got one
hundred thousand images of zebras how do
you even start to ask is this zebra in
any one of those pictures in our archive
from the 2016 grade grave is rally about
200 people were taking pictures and we
got 40,000 images of zebras this time we
have more than 700 people driving around
taking pictures we expect over 200,000
pictures of zebras that's an amazing
amount of photos I wonder how do they
sort through them all and then identify
individual animals each image is
processed through a series of
convolutional neural networks and
matching algorithms the first network
determines whether there are any animals
of interest in this case zebras the next
set of networks localize each animal in
its own sub region and classify the
species at this point background
segmentation is performed on the sub
region to generate a rough mask of the
zebra once this is done key points are
extracted from the zebra and feature
descriptors are built these descriptors
serve as a sort of fingerprint that can
be compared for similarity against
previously identified individual animals
in a large database the scores from this
comparison allow the system to decide
which animals are the most likely
matches when the match has high enough
confidence it's accepted immediately
matches that have lower confidence are
shown to a human expert who then makes
the final decision the problem of
finding animals in hundreds of thousands
of images and classifying their species
that is a deep learning problem the
identification of individual animal is
in some way orthogonal to this training
any deep learning model takes a long
time with GPU we can reduce it from
hours to minutes deploying the model
without GPUs it can take seconds per
image which is too long when you're
talking about hundreds of thousands of
images with GPUs we're down to
milliseconds once all this data is
collected and processed how is it used
to help the Grevy's zebras were there
any other personal cameras once we have
our census estimate the first thing a
policymaker is going to want to
well how accurate is that number if it
is consistent then they'll invest
resources to maybe protect the corridor
so the animals can get from food to
water they may be able to work with
pastoralists to leave areas fallow so
the Grevy's zebras can fatten and the
babies can survive given that these
species are mostly found in
conservancies I think this would be very
helpful for us as an organization to get
to know the statistics and then that
will help us engage more with the
government to show them whether these
species are declining or they are
growing and help us to come up with a
strategy on how we can conserve these
issues while book is clearly a great
tool for helping the Grevy's zebras with
their distinct individual markings but
can it be used for other endangered
animals our current technology can be
used to identify any striped spotted
wrinkled or not species there are
animals that we would like to be able to
identify that we would like to count and
track non-invasively one of them is the
savannah elephant if we do not do
anything the poaching rate is so high
that within just two decades we might
not have any more elephants in Africa we
hope that we can push our current
technology to identify elephants
you
Title: GTC China: NVIDIA CEO Jensen Huang Keynote Address
Publish_date: 2018-11-21
Length: 7573
Views: 36915
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/Tom07EPbzrE/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: Tom07EPbzrE

--- Transcript ---

[Music]
I am a protector
exploring the expanses of space to keep
our planet safe
in all the species that call it home
[Music]
I am a healer
giving doctors the vision to create
amazing new cures
in using the hearts own magnetic field
to diagnose itself
I am a guardian
[Music]
searching a mountain of data to keep us
safe when you travel
and a sea of people to find a lost child
[Music]
I am a helper
powering autonomous machines that
simplify our lives
[Music]
caPSURE our greatest adventures
and make us whole again
I am a visionary
enabling taxis that don't need steering
wheels
and those that don't need roads
[Music]
I am even the composer of the music for
hearing
[Music]
I brought to life by Nvidia deep
learning and brilliant minds everywhere
ladies and gentlemen please welcome
nvidia founder and CEO jensen huang
[Music]
GTC China Hawaii and welcome to Suzhou
for a record GTC China this room is
packed is so great to see all of you I
have so much to tell you today so let's
get started my talk today will be in
three chapters the first chapter is how
we are going to reinvent computer
graphics the second is how we're going
to reinvent the future of computing and
third is hell
AI will automate the world with
autonomous machines these three chapters
I have much to share with you and
there's so much going on here in China
so let's get started 15 years ago we
invented the programmable shader the
programmable shading GPU revolutionized
computer graphics almost everything that
you see today in video games in game
consoles and cell phones the computer
graphics was invented as was made
possible as a result of the programmable
shaders that we invented 15 years ago
for ten years for the last ten years
we've been working on reinventing
computer graphics and Turing our newest
GPU architecture is the result of that
incredible work Turing is a complete
reinvention of how real-time computer
graphics will be done whereas Pascal our
last generation GPU had one basic
processor called the programmable shader
the Turing GPU includes three processors
one processor for classic programmable
shading that we invented 15 years ago a
second processor dedicated to ray
tracing to simulate the properties of
light as it bounces all over
illuminating an object changing tone
maybe and ultimately reaching your eyes
billions of race per second are bouncing
all over the room so that the image
could be beautiful and realistic we call
that the arty core real-time ray tracing
made possible for the very first time
and then the third processor is an
invention of ours called a tensor core
the tensor core makes possible deep
learning neural network artificial
intelligence to be operated at
incredible speeds the tensor core the RT
core and the programmable shader makes
up the Turing processor if you look at
the computation if you look at the
computation of these processors the
difference is dramatic the programmable
shaders of Pascal this is the Titan XP
the highest highest end version of
Pascal is almost thirteen teraflops of
computation however turing has an
independent floating point as well as
integer pipeline fourteen teraflops and
fourteen tops
each and then in addition to that the
tensor core processor for deep learning
and artificial intelligence of a hundred
and fourteen teraflops
we're basically increasing the
computation of Turing over that of
Pascal by nearly a factor of ten it's an
incredible speed up the tension core
will be used for all kinds of amazing
things that I'll show you first of all
of course it will be able to generate
images in a way that is impossible today
it can enhance images it can even make
characters animate in artificial worlds
very realistic way and so
Turing a brand new processor with an
enormous amount of computational
capability in three
processors the shader the ray-tracing
engine and the tensor core processor as
a result were able to create special
effects and render images that were
simply not possible for doing
reflections and beautiful shadows and
then a new technology a new technology
we call D LSS D LSS is a neural network
model that runs on the tensor core we
teach we train this neural network model
we train the neural network model to
take an image and make it more beautiful
you give it an input you give it an
input as we do here maybe a lower
resolution input because this neural
network model has been trained with an
enormous number of examples of beautiful
images it knows how to enhance that
image to make it even more beautiful
than when you first started using that
technique using that technique we could
render a smaller image conserving our
computation capability and use that 114
teraflops tensor core to enhance that
image increasing its resolution doing
super resolution and as a result
improving quality while improving
performance this new method of computer
graphics has enormous potential I want
to show it to you what I'm going to show
you here it's the Justice game this is
one of the this is the leading MMORPG
here in China this game is beautiful and
we're going to make it even more
beautiful
I think Kelvin okay and it's the Kelvin
hi Kelvin how are you doing I'm fine
first of all this is our last GTC of the
year sure I would like to thank all of
you for your hard work thank you ladies
and gentlemen this is the engineering
tech band of Nvidia thank you thank you
everybody
we have been all over the world together
we have been all over the world together
we're like a band you know we're like a
band all right Calvin take it away
let's show us what you're gonna show us
[Music]
[Music]
[Music]
[Music]
okay okay Wow okay this is a live demo
the demo is using the Eliza's to render
at 4k resolution okay so we're looking
at a 4k resolution this is with ray
tracing on one of the things one of the
things that you notice is the beautiful
reflections the way the way that video
games do reflections today is that is it
if you will a technique a simplification
called screen space reflection that we
invented several years ago if the image
is on the screen we can map it into the
reflective surface and as a result it
could be shown as a reflection however
if the if the image is not on the screen
it cannot be shown as a reflection the
beauty of ray tracing is that because
it's bouncing light all over the
environment it doesn't matter if the
image is on the screen or not and so
Kelvin is going to show you here right
here notice this image right here this
part of the cart is not seen on top yet
the reflection is here and Kelvin if you
could turn that off notice without ray
tracing and then show me with ray
tracing and notice here show me without
ray tracing and show me with ray tracing
and so before okay before ray tracing
off our TX off notice here and then on
can you see that beautiful right and
it's so natural even the ripples on the
water even the ripples on the water is
behaving naturally I'll show you more
okay Kelly let's go
this is all being rendered in 4k
we'll stop here oh look at this
reflection it's so beautiful now
everything here looks very natural
everything here it looks very natural
but let me show you what today's video
game looks like without RT X without ray
tracing this is what it looks like this
is the technology we invented 15 years
ago and I'm still very proud of it but
clearly it is wrong clearly there are
many things that's missing and clearly
it could be much more beautiful and so
now ray tracing on look at the looking
under the bridge it's caustics
light light is shining reflecting off of
the water up against the bridge and it's
causing that ripple that it's called
caustics of the water turn it off turn
it on look look at the reflection of
this light against the boat
turn it off turn it on
turn it off one more time oh it's so
much joy right so much technology and
look what happens we can create reality
okay show you one more
we have the best job in the world we can
apply all this great technology to bring
joy to people and if we do a good job
then people could tell wonderful stories
look at the reflections on the ground on
everything it's just all natural all
happens by itself look at this notice
the reflection of the metal this is
rough and so the reflection is rough
this is shiny so the reflection is
shinier and look at the front ok and
notice this even notice this this little
tiny piece of metal is reflecting the
environment okay go ahead
RTX off RTX on RTX off RTX on
[Applause]
this is uh you know that Tom Cruise
movie this is with me without me without
me
with me without me not bad right
okay Calvin that's maybe we could sit
here and play all day keep going
[Music]
and then beautiful it's all dynamic it's
all moving and it's completely easy to
do because it's behaving according to
the laws of physics everything just
works all by itself the software
programmers job is much easier in fact
okay
ladies and gentlemen thank you RTX
gently Kelvin thank you thank you Jason
good job now one of the things that you
noticed one of the things that you
notice was we were rendering at 4k we're
anywhere folk' there are two
technologies remember there's the RT
core ray tracing and then there's the
tensor core the deep learning tensor
processing a hundred and fourteen
teraflops and so we're using we're
rendering in this case this is rendered
in 4k this is rendered 4k taa
anti-aliasing temporal anti-aliasing so
this is 4k but this was only rendered
this was only rendered to 1440p half the
resolution and then to artificial
intelligence models were used the first
model is to enhance the image at 1440p
and the second is to expand the image to
super res the image to create 4k and to
do so in such a way as to retain and
maybe even enhance the visual quality as
a result DL SS the tensor core is using
114 teraflops to render the image when
this is only 14 teraflops almost 10
times more computation can be applied to
generate that image as a result the
quality is high and the image quality
and the performance is high that's
called DL SS in combination this is what
it looks like this is what Pascal does
the Pascal 1080 TI is the highest end
GPU at six hundred and ninety nine
dollars $699 this is the RT X 2070 at
four hundred and ninety nine dollars two
hundred two hundred dollars cheaper and
yet it's almost four times faster with
next-generation content it's really
quite amazing this is the power of using
three processors programmable shader RT
core and tenser core combine to generate
the next generation image this is the
first title that will have DL SS here in
China it's called the JX 3 and let me
give you some examples this is this is
the new GeForce r-tx ok
this is the new chief force r-tx and let
me show you what it can do
g-force r-tx even without ray-tracing
even without ray-tracing we could use
deep learning and the 114 teraflops
tensor core to enhance our performance
just to enhance our performance
this is dart top to high end Pascal GPUs
$699 $6.99 and $4.99 ok $6.99 $4.99 this
is what our TX looks like without D LSS
without dl s s so the 27 t is comparable
to a 1080 the 2080 is faster than a 1080
TI and a 20 80 TI of course is much
faster this is without D LSS with TLS S
on this title it looks like this now
look at that amazing performance
improvement that basically says a 2070
without ray-tracing without ray-tracing
can deliver performance at 499 dollars
that's higher than a 1080 TI
as 699 dollars pretty spectacular
performance improvement generational
performance improvement and so that's
made possible as a result of our TX's
tensor core processor now let me show
you what this new GPU can do with a
piece of a demo that we did together
with epic this is really really
beautiful and this was done to celebrate
a 70 year anniversary let's show it to
him
the speed of light a universal constant
never diminishing never ending
70 years of unrelenting progress
the Porsche 911 speedster concept the
speed of light isn't that incredibly
beautiful guys that is not a movie what
you just saw is not a movie this is
completely in real-time what do you
think the reflections the lighting the
shadows it all just works it's all rate
raced ten Giga race per second 10
billion race per second bouncing all
around this car creating this beautiful
image and look at look at this this
light this light is above the ceiling it
is not in this room and yet the
reflection is perfect and the reflection
off of the car is perfect nothing has to
be done it just has to be turned on so
this is the future of computer graphics
and this is imagine this is the future
of video games after 10 years the Turing
architecture is going to reinvent
computer graphics thanks guys
[Applause]
Moore's law has come to an end
Moore's law has come to an end for my
entire career for my entire career every
single year the microprocessor got
faster and faster and faster every 10
years every 10 years Moore's law
predicted and the industry relied on it
to improve performance by a factor of
100 every 10 years every 10 years
microprocessor performance improved by a
hundred times for 35 years for my entire
my entire career that was true for 35
years Moore's law advanced at a hundred
X every ten years the last several years
it has come to a halt now to imagine
this for something as vital to humanity
the single most important instrument of
society today the computer for the
advancement to come to a stop instead of
100 times every 10 years let's say that
it is only 2 times every 10 years
because it's only improving by a few
percent per year in 10 years time the
industry will have a deficit a shortage
of computing power by 50 times or the
cost of computers will increase by 50
times or the power necessary to run our
data centers will increase by 50 times
or we simply cannot have any more
performance improvement Moore's Law
coming to an end coming to an end is
exactly the reason why the capex of data
centers and internet companies are
growing
so fast because it is no longer possible
to wait one and a half years and the
performance will double with no
additional cost and then the performance
double no additional cost that trend has
come to an end that is the meaning of
Moore's Law the meaning of Moore's law
is that in the future unless we do
something different unless we change the
computing approach the cost of computing
will skyrocket the power of computing
will skyrocket the implication to our
industry is severe I think that this
observation that we made almost 10 years
ago when we started pioneering our
approach called accelerated computing is
now common sense everybody now
acknowledged Moore's law has ended we
must do something we've been working on
this approach for 10 years and we've
made enormous progress in fact our
approach is not the easy road
it's called accelerated computing
because general-purpose computing using
C compilers writing general-purpose
programs was very flexible that was its
power
it was very flexible to go forward we
did not want to lose the flexibility of
general-purpose computing and so we
added and we did not remove the CPU we
added a special chip we added a special
chip whose job is to perform the very
heavy workload the parallel work were
the workload the inside part of the
kernel but that is not enough in order
to accelerate the application we have to
completely redesign the software stack
the architecture of the chip the system
software in the system the algorithm and
the applications all have to be
optimized
in order to take advantage of
accelerated computing accelerated
computing is a full-stack problem you
cannot just put the GPU or an ASIC or
anything underneath the software and
expect it to improve the performance you
have to have the expertise to redesign
the entire stack from bottom to top well
the only way to design a architecture
from bottom to top
is to understand it from top to bottom
and that's why NVIDIA is a software and
a algorithm company that's why we have
so many computational mathematicians we
have to understand the software and the
application and the algorithms from the
top and we have to understand how to
accelerate it from the bottom over the
years we started from scientific
computing where the demand for
accelerated computing and the feeling
and the impact of the end of Moore's Law
was the greatest we started with
supercomputers we started with
scientific computing where the
simulation models were gigantic it would
simulate for many many days and the
algorithms is simulating from first
principle laws of physics here I showed
you the Maxwell's equations but over
time industry realized that the number
of first principle methods to understand
many of the world's phenomenons are
simply impossible there are too many
things too complicated for example the
human body it's too complicated for
there to be a Maxwell's equation weather
simulation too complicated because
there's multiphysics social changes too
complicated your preference for movies
your preference for where to eat those
problems are too complicated for first
principle analysis and there are no laws
and there are no equations to simulate
that and that's the advent of deep
learning machine learning and what is
known today as data
science so now we have scientific
computing and data Sciences driving our
architecture and then lastly with any
architecture architectural discipline is
vital one of the greatest decisions one
of the greatest decisions that today's
leading computing companies made long
time ago is to keep instruction set
architecture compatibility to not break
to not have the discipline to chase
everything but to have the discipline to
methodically advance the computing
platform while retaining backwards
compatibility and dedicating yourself
generation after generation after
generation to advancing the platform on
the one hand but taking along your
entire install base on the other hand we
call that CUDA CUDA is our architecture
it is backwards compatible but forward
extensible we can continue to add more
and more capabilities from CUDA one on
Tesla CUDA 3 on Fermi CUDA 5 on Kepler
does anybody in the audience know any of
these generations any any Fermi users in
the audience
any english-speaking audience I feel
what John Cena John John what does it
tingle ma hey ka la voice well it's Owen
Jonathan's ha mates alight Shadia maid
Celeste Radian yuri-chan
tone tone the keynote genuine jumps on
window okay cuz always shine faster
Johnson Yuri Tian what Jungshin sure sir
eggwin $19 someone shot silly eating so
under eating saw the hollow
[Applause]
this audience has no clue what I'm
talking about
my band does not speak Chinese okay so
we have a CUDA 5 could a 6 is Maxwell
CUDA 8 Pascal does anybody in the
audience know Pascal ok as you could see
notice this is where all of you joined
this is in the beginning we were growing
growing the base investing in the
architecture by the time Pascal came the
deep learning revolution started and you
could see the data science and all the
data scientists and all the ADA AI
researchers and all the developers
kicked our architecture into turbocharge
and it reached now 14 million downloads
of CUDA in just last year in just last
year one year almost six million
downloads unbelievable now we're on
Volta and then we're on Turing could
attend each one of our generation we
keep adding to it we keep adding to the
capabilities each generation
backwards-compatible so that now the
applications that are written so many
applications will run on the entire
install base so the benefit of install
base so that developers can can have the
reach of many users on the other hand if
the install base is large and the
platform is capable and it's easy to use
then developers will be vibrant many
developers many users a positive
feedback system and a positive feedback
system looks a little bit like that
every single year it seems to grow even
faster the benefit of accelerated
computing we've been working on this and
we've been investing in this now for
almost 12 years as you could see could a
10 already
almost 12 years and now the install base
is quite large you could see the result
of our work this is just this last week
at supercomputing
at supercomputing Nvidia's presence and
our achievement was very well recognized
and surely felt the highest performance
computer in the world called the summit
27,000 and Vidya GPUs a hundred and
forty-four petaflop s-- this is this is
um this is a this is equivalent to two
probably probably something something
like a million laptops okay
this computer could do something that a
million laptops cannot do 3.3 exa flops
of AI but we were also the number one in
the world the number one in Japan the
number one in Europe were 22 of the
world's 25 most energy-efficient
computers and one of the great
achievements five of the six scientific
breakthroughs on high-performance
computers what we're done on NVIDIA GPUs
and I so I'm very very pleased with that
and this year the number of systems in
the world top 500 is starting to grow
even faster this year almost 50 percent
over last year
127 systems in total high performance
computing high performance computing has
confronted Moore's law and the path
forward is clearly accelerated computing
and after over a decade of investment we
find ourselves in a really great place
to be able to help the industry forward
the second dynamic that's happening the
first dynamic is the end of Moore's Law
and that's going to affect computers and
computing industry all over the world
the second dynamic is artificial
intelligence the idea that we now have
the ability for machines to learn for
machines to write software
it stands to reason that if a machine a
computer were to write software that we
cannot write that this computer will
have artificial intelligence and to be
able to write to learn and to write
software to create software the type of
software that is created is only
possible on computers like this it
stands to reason that a computer will
have to be very powerful in order to
write the type of software that no
humans can write
however as challenging as it is the
impact of artificial intelligence is so
extraordinary it is so extraordinary and
the reason for that is because we have
now discovered a method of computer
science that can solve problems that we
had no ability to solve before now we
can now solve problems that are nearly
superhuman when you think about it let
me give you some examples this is
Alabama this is a singles day 11:00
11:30 1 billion dollars 30 1 billion
dollars if you had if you had if you had
a store that does 10 million dollars in
one day that's a very big store it would
need 3,000 stores to replace that one
day 3,000 stores that's an enormous
number of department stores all done
online however there are so many
products on Ellie Bubba's store millions
and millions of products how do we find
a product that it should recommend for
us to to buy so the recommendation
system has to understand you it has to
be personalized shopping at a very
extreme level it has to learn from you
so that it could make recommendations to
you it is believed that up to 30% up to
a third of the product that products
that we buy on e-tail are recommended to
us by AI not based on what we are
searching
but based on what it recommends to us
and so incredible power of AI the
benefit to society of course is that
it's easy to shop and of course the
number of stores and the acres of
buildings are no longer necessary but it
has to be replaced by personalized
shopping which is only possible with AI
8 billion people in China visit doctors
each year eight billion visitors 8
billion visitors it is now possible it
is now possible to create an AI that is
essentially your AI doctor your AI
doctor your AI doctor can help remove
reduced a number of visits that you go
to go to the doctor go to the hospital
and if you were to go to the hospital
because the number of radiologists in
the world is so limited we can now use
AI to augment to accelerate to reduce
the burden on the number of radiologists
in the world transportation to trillion
miles driven in China alone 2 trillion
miles that's an enormous number of miles
here in China didi is able to connect
spare seats extra seats unutilized seats
which is an asset that a sitting in
society that is not being utilized
unutilized seats connected to someone
who would like to go somewhere basically
a ride that has almost no cost the car
already exists the seat already exists
but using artificial intelligence they
could recommend the right car to the
right the right driver to the right
writer and then make the recommendation
or predict or regress regress what is
the fastest time of arrival and what is
the best path to the destination that's
an artificial intelligence problem
support the ever-increasing volume of
transportation and mobility
manufacturing five trillion dollars a
manufacturer year in China each year if
the equipment was down just 1% of the
time
or even let's take even a smaller number
if the equipment is down only 1/10 of 1%
of the time that's a five billion dollar
loss five billion dollar loss five
billion dollars just the size of a large
company lost simply for no reason wise
engine an AI company could do predictive
maintenance so they know they could
learn their machine can learn their
machine learning systems can learn when
a machine is about to fail and before it
fails you can go and take it offline in
a methodical way to affect manufacturing
as little as possible the benefits in
manufacturing is quite extraordinary
well it turns out the entertainment
industry here in China is really big the
number of movies made every year is
growing and most of the movies are
rendered in using GPUs and software now
using artificial intelligence we could
accelerate them rendering in the making
of those movies and take pressure off of
all of the studios so they could make
more and more beautiful movies
artificial intelligence has impacts
across the world's largest industries
and the impact is always measured in
tens hundreds of billions of dollars and
the reason for that is because
artificial intelligence applies to big
data it you need to have a lot of data
to be able to teach an AI and each one
of these industries from healthcare to
transportation logistics manufacturing
to e-tail these are all data-driven
companies they can now take their data
teach an artificial intelligence network
to predict a better outcome so that they
can optimize their business artificial
intelligence can learn but artificial
intelligence can only learn with a lot
of data ai is going to change the future
of compute
ai is gonna change the future of
computing not just in how it's used but
how it's designed let me now talk to you
about that AI is going to transform
computing altogether there are two types
of computers today and they're very
different they seem the same but they're
radically different on this side what
some people call super computers or
high-performance computers our computers
that are designed for very few jobs but
very large jobs the jobs will run the
simulations will run for many days
potentially many weeks and some for many
months these are gigantic simulations
are gigantic simulations a fusion
reactor or a gigantic simulation of a
molecular dynamics a particular virus it
could be a gigantic simulation of
whether to try to predict climate change
gigantic simulations few jobs few jobs
very gigantic ones and this is hyper
scale this is the birth came out of the
birth of the internet and this is about
serving hundreds of millions of users
with very small jobs hundreds of small
jobs hundreds of millions of small jobs
these markets are very are going to be
these architectures are going to be
fundamentally changed by AI and so this
is scientific computing where it started
few users big jobs
tens of thousands tens of thousands tens
of thousands in the case of the summit
27,000 tens of thousands and this is
hyper scale it was created for search
small search queries unlogged astre
buted databases the formation of Hadoop
and MapReduce
made this possible this particular data
center is called hyper scale millions
millions of CPU nodes weak nodes small
nodes running in a data center in a
distributed system way most of them are
not working together this serves
hundreds of millions of customers
this serves tens of customers tens of
users these two machines are radically
different in architecture strong nodes
tens of thousands weak nodes millions in
the future in the future and today is
future in the future these machines will
also be called upon to do machine
learning data science and AI and the
reason for that is this scientists have
discovered that AI is fantastic is
fantastic to fuse with simulation
techniques so that they could they could
use and analyze much much bigger models
simulation and prediction combined is a
new method of computing data science and
scientific computing coming together
deep learning and scientific computing
coming together machine learning data
suntanned deep learning coming or in
simulation coming together and so that's
the first trend is that scientific
computing will include machine learning
in the future
the second trend is that developers are
going to need these high-performance
computing systems in the future and the
reason for that is because we need it to
write software software developers are
going to need supercomputers in the
future every company every company that
develops software will need accelerated
supercomputers in the future we have our
software engineers now have thousands of
these systems so that they could develop
their software and the the Michael
Jackson dancing I was doing what's train
on a system like this on a
high-performance computer and so in the
future the developers are going to also
include data scientists and AI
researchers and as a result
supercomputers will go from scientific
computing into this from the research
labs out into companies and out into
Internet companies on the other hand on
the other hand in hyper scale what
started out as query
is now internet services that are only
possible with machine learning and
machine learning as you know requires
large computation a machine that is very
very strong and so the method the method
is to take many of these hyper scale
nodes and combine them to work on one
job instead of many jobs running on one
node in the future it will also have one
job running on many nodes and so hyper
scale will also become a
high-performance computing system the
second dynamic is that it used to be
used primarily for the internet services
but in the future this and this this
infrastructure will also be open to the
public for public cloud and the public
cloud users will be AI researchers will
be data scientists will be scientists
and they will need this hyper scale data
center to also be an HPC so the point is
this supercomputers are going to become
AI computers hyper scale data centers
with millions of weak nodes will also
have to become high-performance
computers both of these systems are
changed because of AI and so we created
two architectures two system
architectures to be able to enable both
of these systems to be able to enter the
world of AI to enter the world of
machine learning and to enter the world
of data analytics the first is what we
call the HDX 2d 8 hg x2 is designed to
be a very very powerful computer it is
designed to be able to process both
scientific codes as well as AI codes it
is able to process large amounts of data
on the other hand
we have a little tiny computer it's
powerful but it consumes very little
power
we called the Nvidia t4 I'm gonna tell
you about the Nvidia t4 in just a second
but let me first tell you about the HDX
- oh wow
mine sonde I think this is maybe three
four hundred pounds just a limit of my
my lifting there are eight processors
like this okay so eight processors like
this they're all connected together
using MV link 300 gigabytes per second
connecting every single GPU to every
other GPU so everybody can talk to
everybody at the same time each one of
our GPUs has 32 gigabytes so together
this motherboard the system board has
256 gigabytes 256 gigabytes for the
engineers in the room with a bandwidth
of 8 terabytes per second of bandwidth
so this is one petaflop s--
8 terabytes per second of bandwidth and
256 gigabytes of memory all working
together ok this is like 400 servers 400
servers will require this whole stage
replaced by this computer
[Applause]
Pusa hem PA ADA but wait wait you know
that your data scientists and AI
researchers are the most sought after
engineers and the most valuable
engineers you have right now this is
their tool this will improve their
productivity from hours from days to
hours from days titan-1 from days to
hours from the would you like to try so
my whole song ah yeah oh did you workout
today
you don't need to now push y'all oh okay
so you're AI researchers are so valuable
they're so rare so we want to give them
the best possible instruments so that
their productivity so that research
could be the most impactful okay so this
is the hg x 2 and it is an incredible
computer two of these two of these fit
in here too so that we now have 512
gigabytes half a terabyte of memory to
peda flops of computing and 16 terabytes
per second of bandwidth this is just an
amazing amazing computer and this is
designed for high performance computing
to maximize the productivity of a our
researchers data scientists when they're
doing very large model training
second may I take a break getting old
okay so so hgx - I'm so happy today I'm
so happy today that the computer leaders
of China has adopted the HDX to this
computer this computer will advance the
work of AI researchers everywhere and
from Baidu to Tencent Huawei inspir
Lenovo QCT su Gong Super Micro I want to
welcome all of our partners I want to
thank you for your support they all all
of you have worked so hard to bring h
TTX - to market thank you very much
[Applause]
so that's h GX - designed to accelerate
high performance computing designed for
big data analytics machine learning deep
learning as well as scientific computing
designed for engineering departments
engineering departments AI research
departments that needs to have the best
possible tools and the best possible
instrument so that they can get their
jobs done
once you develop the software once you
develop the software the software is run
in a data center that's called inference
training training is the development of
the software ai is writing software
that's called training we're teaching
the AI how to write software the second
step when the program is finished it has
to run in a large data center and that's
called inference to make inference to
infer to predict to classify to regress
all of those statistics words basically
meaning to make a prediction we created
our first generation inference chip
inference GPU for hyperscale data center
it was introduced last year called the
p4 and we were so successful launching
the p4 so many users they're using it
for large-scale speech recognition
large-scale speech synthesis image
recognition fraud detection anomaly
detection so so many different
applications and the applications just
keep on growing this year we have
something really really special and it's
called the Nvidia t4 the Nvidia t4 is
really small it looks very big over here
because I think she'll de McKenna dama
hints shell de it's like a candy bar 70
watts only 70 watts less than the power
of a CPU less than the power of a CPU
but about 300 times the performance for
deep learning of a CPU less than the
power of a CPU
[Applause]
less than the power of a CPU but several
hundred times faster compared to p4 look
at P this is already much faster than
the CPU and look at a t4 compared to a
p4 in just one year's time in just one
year's time the t4 gave a huge boost the
t4 is 4 times the training performance
the p4 this was not very good for
training this was not very strong for
training however the t4 is universal you
can use it for training you could use it
for inference you could use it for
images sound video you could use it for
almost anything because it is now so
powerful it's four times the training
performance and five times the inference
performance of the last generation which
was already state-of-the-art in just one
year's time t4 is also not just the
highest throughput it is also the lowest
latency the latency is so important
because in the future the internet will
change today you say say the gumball
cheating so he'll sit up it will come
back with a list of restaurants it's a
search result in the future in the
future it will in fact tell me in the
future it will understand my speech it
will do natural language understanding
to understand what it is my meaning and
then it will understand me because it
knows that I am su Jo that I enjoy
spicier versions of the food maybe where
I am and it will recommend the best
restaurant in that area recommendation
systems and then it has to synthesize
the speech for neural network models for
neural network models just to answer the
question
say the gold ball cheating so else it
has to be very very fast so in the
future the latency of neural networks
has to be very fast which is one of the
reasons why we achieved the highest
performance the lowest latency ever
recorded the lowest latency ever
recorded for deep learning incredible
performance
now lastly this is just a performance
this is the processor the performance
for training for inference but the most
important part that people always forget
is the software stack of today's machine
learning the software to stack of AI is
so complicated it is so complicated and
it includes so many things first of
course is on this GPU CUDA ku DNN tensor
RT for optimizing compiler we're now
tensor RT 5.0 we can compile optimize
cnn's rnns MLPs so many different types
of models there are now thousands of
species of deep learning models we can
now compile so many of them and then we
have to run it in the data center the
data center is complicated as you know
essentially kubernetes which is a a data
center hyper scale workload
orchestration system customers users are
bombarding a data center with a bunch of
requests kubernetes has to figure out
which one of the servers are available
and where to put the job and to manage
the entire data center of millions and
millions of nodes of CPUs and memories
and GPUs and storage and Nick's to
manage all of that
gracefully optimizing throughput
optimizing utilization and by optimizing
utilize utilization you can improve the
throughput and reduced
cost of the data center kubernetes is a
very large-scale piece of software on
top the applications are all in
containers and then on top inside the
containers are what is called pods and
inside these pods there are many
different types of containers and these
containers has many different types of
AI models we want to be able to run many
AI models at the same time on top of
kubernetes inside a data center for our
GPU many models anywhere in the data
center maximizing the utilization this
entire stack is possible with what we
call the TRT inference server it's a
server software that sits on top of
kubernetes inside a container today we
are open sourcing this incredible piece
of software and so tensor RT inference
server you can go to the NGC cloud
registry for Nvidia and you can pull it
down it is open sourced well before we
get there Ryan and I are going to give
you a demonstration of the new T 4 but
this is amazing this only started
production 30 days ago 30 days ago
touring T 4 and video T 4 went into
production what we're demonstrating to
you today is running in the Baidu cloud
ok this is in the Baidu cloud Ryan why
don't you start trick well we're
starting with CPU so this is we took a
simple we simplified a service in the
data center do any of you recognize
recognize this demo from last year ok
well show you this year's demo ok
unbelievable this is last year this is
of course CPU running at 5 frames per
second and ryan and i ryan and i every
single year we promised that we will
learn a new flower every single GTC what
what is that we learn b-bomb right
where's b-bomb
well it might get lucky to show up but
we just saw Tigerlily and by the way I
saw it first
[Laughter]
okay so last last GTC we learn Tigerlily
b-bomb right there oh we're people it
was in the corner listed okay alright so
so we now know two flowers out of the
several hundred thousand a few more
gtcys and we'll be ready to go okay so
this is CPU running in the Baidu cloud
right okay go ahead let's uh now run t4
[Applause]
so you're running 4t force is that right
yeah 40 force so 40 force if we if we if
we said we round it up to six images per
second six images per second so it's a
thousand times faster a thousand times
we have four GPUs for G so 250 times
faster okay so this little this
beautiful little piece of amazing
technology is 250 times faster than a
CPU and it's currently in the cloud but
here's the beautiful thing if you have
kubernetes did I go too far
no can I can I uh how many more can I
get oh you can get lots more
so if Baidu has more they have lots more
let's add some Wow Baidu has a lot of
computers ladies and gentlemen the Baidu
cloud is like completely elastic if I
need more just buy more just buy more
can we buy more oh that that's all we
got this time but we can't we can always
add more we can add more in the future
yeah I thought you had 20 something oh
there's 28 there's some waiting can I
get more oh yeah let's scale this up a
little bit so when our demand increases
we simply add more GPUs
[Applause]
how quite help you yeah that's the only
Chinese I know Tyco way good okay
that's all help it means very
inexpensive very affordable it's so good
okay so so it's so easy our GPU our GPU
this t4 look at this this this ladies
and gentlemen is a height this is a this
is a high-performance computer okay so
this dgx is a high-performance computer
this the designed for maximum
performance very big job few users this
is a hyper scale computer this is what a
hyper scale looks like this goes into
hyper scale data centers all over the
world and it's so great if you have any
problems you just pull it out okay so
this is two NVIDIA GPUs like that only
150 watts 70 watts is it's like a laptop
this is like a laptop and then you take
this like a laptop you put it in here
and here is the CPU okay and all you
have to do is put it in here just like
that
step1 step2
you go to the Nvidia cloud registry
where all the software is all
containerized and kubernetes has already
developed you pull the container step
three you open an instance on the Baidu
cloud step two step three
either one step 4 enjoy
okay only four steps so easy just like
that now this is the hard part of course
this computer if it can only run one job
one type of job it's not a very flexible
computer we would like this computer to
run any machine learning model any deep
learning model because we want to
maximize the utilization if we maximize
the utilization the cost will go down
does that make sense okay so we want to
increase the utilization the only way to
increase the utilization is to be able
to run multiple models of different
kinds at the same time so this is what
Ryan's gonna show you next it's called
the TRT inference server absolutely so
let's switch over to that one here we go
so this is before the 10:30 inference
server you can see the same flowers demo
and that's the blue line on the graph on
the right and that is being run on those
two I'll use the mouse here and you can
see these GPUs are being used to run the
flowers model right here so this load
for the flowers model is being used on
those two GPUs and then we have three
other models running in the background
you can see the green the yellow and the
orange curves those are three other
models are running each on their own
dedicated GPUs in the data center so you
guys see that
so one model this server can only run
this model blue this server can only run
green models so maybe for example speech
recognition this can only run maybe
product recommendation okay and this
maybe only can run speech synthesis so
each one of the server is a dedicated
appliance not a general-purpose computer
a dedicated appliance now the problem is
as Ryan was saying the workloads can
change the data center is so big there
are millions and millions hundreds of
millions of people using data hyperscale
data set
all over China the workloads can change
and sometimes depending on the time of
day sometimes because of a special event
sometimes it's you know a very special
actor came to town or something okay
so something happened it caused the
utilization to change and so here's yep
let's spike the utilization again so
flowers got very popular and our network
just took a little bit of a so we do
these live let's hope it comes back it's
okay if it doesn't if it doesn't break
we don't we know it's not live here we
go that's back now so here we increase
the demand to 5,000 and so you can see
that we're only able to deliver with
these two GPUs about 4300 and so you can
see those two GPUs spike up to 80 90
percent utilization but we but we really
want to serve more you see see this so
this workload is a hundred almost 100%
and these servers are not being very
utilized 15% 16% longface yen right
this is oversubscribed that is
undersubscribed
and so we need to create a system we
need to number one we have to have a
processor that can run any model number
two we have to have a compiler that can
optimize any model number three we need
to have a container system kubernetes
container system and a server system to
run every model any model at the same
time on top of one GPU very complicated
however along comes a 10 charity
inference server so we designed this
server to be able to host multiple
different models all simultaneously on
the same process and so here when we
switched over you can see now we're
we've asked for 5,000 we're getting
5,000 everything is running everywhere
now blue orange green yellow look at
that everything is running everywhere
everybody is utilization is nominal go
ahead right keep going yeah and of
course it gets better so when load spike
just like we saw before we get it
automatically just balances out over all
the different GPUs that we have serving
it so you can see we spiked up the load
and that low gets distributed out nice
and evenly across the board Wow 84% oh
it takes a little bit of time for the
big one to go so long feet chin chin
sighs a joint yet Oh our networks kind
of slowed down a little bit oh now is
the quality of service issue all right
come on Network look at look at look
there we go it's getting better
adaptive adaptive and dynamic work load
balancing and so in order to balance the
workload across your data center first
of all every single GPU every single
computer has to be flexible it has to be
flexible it has to be programmable okay
Ryan good job
so that's the that's the nvidia t for
our first hyperscale
GPU training inference any model fits
easily into the hyper scale system
tensor RT inference server on kubernetes
all of the software integrated it's open
source and we can work together to
integrate these GPUs into data centers
the t4 the t4 went into production just
30 days ago and it is already the
fastest adopted GPU server GPU in the
history of our company I am so pleased
to see the world's leading system makers
Internet services adopting t4 so quickly
from Baidu cloud Tencent cloud JD cloud
I fly tech to system makers HD HDC
Huawei inspir inspir Power Systems
Lenovo QCT and soo Gong I want to thank
all of you for your support so system
architecture has changed as a result of
AI we are fundamentally re-architecting
high performance computing adding new
types of instructions so that tensor
core for AI multi precision processing
tensor core and the ability to connect
these GPUs together into one large
system ai processing large memories and
on the other extreme adding Nvidia t4
into hyper scale to boost up the
performance of the hyper scale data
center there's a new piece of software
that we recently announced call Rapids
AI is not just deep learning so far
everything I've shown you everything
I've shown you is deep learning but AI
is not only deep learning AI includes
machine learning in fact the vast
majority of the world's industries use
machine learning machine learning
relatively easy to use most people have
the data from their operations or from
collecting from their environment they
could use data processing ETL to prepare
the data into tabular formats into
columns and rows they could take that
columns and rows data format and
discover the necessary features called
feature engineering once they're done
doing the feature engineering they could
run that into a machine learning
algorithm call extra boost or k-means or
random forests or doing regressions and
classifications that traditional method
classical method of machine learning is
popular all over the world in ad tech
internet companies ecommerce financial
services healthcare machine learning is
the most popular method of AI
unfortunately this method of AI has not
been possible to accelerate and the
reason for that is because it was
created in the time when Moore's law was
still progressing very nicely the size
of data was pretty small and Hadoop and
MapReduce were sufficient Along Came
larger and larger and larger data and
people realized that the terabytes of
data the terabytes of data not gigabytes
of data the terabytes of data the
terabytes of data or the hundreds of
gigabytes of data that they're now
required to train the model with are
simply too gigantic and has motivated
them to look for acceleration method we
worked with the world's leading open
source community that created today's
machine learning platform to engineer a
completely from the ground up stack that
enables us to do acceleration at the
limits it starts with the creation of
the Apache arrow if you haven't had a
chance to look at that it is a vital
piece of computer
work it is column our data it is
vectorized and it could communicate
between different applications using IPC
no memory copies just inter-process
communications just passing a pointer
and all of a sudden this entire memory
can be passed to another application
Apache arrow is foundational to the
future of data science this is a very
big deal the second piece is desc the
ability to turn large clusters of
hyperscale
computers into one gigantic computer
scheduling it distributing the work
orchestrating the work aggregating the
work reducing to work being resilient
that capability of turning an entire
data center into one gigantic essential
computer essentially computer is a
really powerful thing that is called
desk on top of these two foundational
innovations that we worked with the
open-source community on we then built
the rest of machine learning built on
top of CUDA we have CUDA data frames
which is pandas like pandas on Python is
the single most successful most popular
data science library in the world pandas
is essentially the programming tool
library of data scientists they used by
data scientists in all different fields
from biology to chemistry to genomics to
add text you name it millions and
millions and millions of people used
pandas created by Wescott mckinney who
wrote the pandas library and built it on
top of the numpy library which is
created by travis Oliphant these two
giants of the computer science industry
created basically pandas it is now
accelerated on top of qdf on top of CUDA
on top of Apache Aero
coup ml
a machine learning library of algorithms
it is like psychic learned the most
popular machine learning library in the
world
psychic learned its algorithms and
models are replicated in KU ml one of
the most important ones is X G boost
boost a gradient trees incredible
performance I'll show you in just a
second and then lastly if you're using
graph models we have cout graph the
wonderful thing about this is because
everything is built on top of Apache
Aero and Apache arrow has connectors
into other applications with that can
support Apache arrow connectors for
example pi torch you could connect the
output of this work directly into PI
torch and enter into deep learning and
so we have cout DNN and deep learning
frameworks that will be connected into
this platform this now becomes from the
ingesting of data from taking data from
storage from CSV data comma separated
values we can load the data into Apache
arrow you could do ETL on that data
hundreds of gigabytes of data that you
can process in just seconds and then you
could do machine learning on it in
Apache arrow by simply point sending an
IPC a pointer and or you can go directly
into deep learning okay so this is the
Nvidia engineered Rapids machine
learning platform data science platform
we open sourced this entire platform and
the success of that open source the
enthusiasm behind it is just fantastic
we're working with data science
platforms all over the world we've open
sourced into the community people are
plugging it in and connecting it with
connecting rapids into their own work
start-ups are integrating rapids into
their platforms large enterprise data
science systems
it's IBM's Watson studio or Oracle's or
s AP or SAS some of the world's most
successful data science platforms can
now plug in this accelerated data
science SDK we called Rapids and in the
future with all of the cloud machine
learning platforms basically supporting
Python and sidekick learned we can now
accelerate all of their machine learning
services and these are examples of the
ones that we're working very very
closely with now to accelerate the
delivery of Rapids into their services
you could see Rapids and machine
learning accelerated machine learning
accelerated data science is literally
going to be everywhere this is probably
one of the most important pieces of work
we've done in the last three years there
are so many in the world who cannot
benefit from deep learning yet they
don't have enough data or they don't
have enough expertise or they just
simply don't need it yet they can take
advantage of accelerated machine
learning this is an example I won't run
this example for you because it takes so
many hours this is in fact the perfect
example of why data Sciences life is so
painful they spend their whole life
waiting this is an example this is an
example of the Fannie Mae Fannie Mae
loan data over the last 16 years that
data has every single month of whether
you pay back the loan or you did not pay
back the loan or you're delinquent on
the loan it has it for millions and
millions of people over 16 years 280
gigabytes of data 280 gigabytes of data
imagine your spreadsheet your favorite
and largest spreadsheet is maybe a few
hundred kilobytes large maybe is one or
two megabytes large this is gigabytes
large 280 gigabytes large first you have
to read the data into
the computer second you have to run ETL
joins and group buys you're doing
feature engineering on a very large
spreadsheet on twenty CPU nodes on
twenty CPU nodes it takes two hours two
hours on twenty CPU notes on dgx to that
one machine right there it only takes a
few minutes it only takes a few minutes
on a dgx - it takes two hours on 20 CPU
nodes this is running spark which is a
distributed computing system and with 50
nodes it's still almost an hour and 100
nodes it gets faster however this is
machine learning this is actually
extremists because the communications
overhead of machine learning is so high
even as you increase the number of CPUs
the scale factor does not improve
because you're now spending too much
time communicating between the CPUs on a
DG x2 again it takes a few minutes in
combination from 3 hours to just a few
minutes from 3 hours to just a few
minutes think about the acceleration it
is utterly crazy now
of course this is only possible because
of all the debt all the open-source
engineers work that worked with us all
of the data scientists around the world
that worked on these frameworks and now
we have Rapids and accelerated data
science platform
but you know what this is what we really
care about this is this is how many
servers it would take to do data science
in a few minutes
ingest ETL machine learning if you want
to do it in just a few minutes it would
cost three hundred servers three million
dollars and one hundred and eighty
kilowatts a hundred and eighty thousand
watts a hundred and eighty thousand
watts basically two thousand lightbulbs
two thousand light bulbs of power well
if you had this or you could do it this
way
[Applause]
before after
[Applause]
without me
with me okay you kind of never paying
either don't see as you could see the
more you buy the more you save the more
you buy the more you save
taiyang ela incredible I love this
before after we have been so successful
with Rapids already in China it was it
came to China literally just a few weeks
ago it was released and open source just
a few weeks ago it's been downloaded and
it's being used by some of the largest
machine learning practitioners in the
world ping n is the example that I was
talking about earlier of their AI doctor
eight billion people go to the hospital
but with ping hands a eye doctor it's
possible to train in your network so
that it could diagnose some of your
simpler simpler symptoms so that you
don't have to make a trip to the doctor
b:g is using it for cancer immunotherapy
discovery amino therapy is basically
directing your own immune system to
attack the cancer virus you have to
first figure out the pathway of that
cancer and then you have to figure out
which one of all of your natural systems
in combination with a vaccine could open
up that pathway to the cancer so that
your natural viruses can attack just
that cancer well the classification the
classification of all these different
would is called peptides amino acids
that are used in this vaccine there are
millions and millions and millions of
them how do you figure out which one of
them will open the pathway allowing your
immune system to attack that cancer
without the cancer protecting itself
again
well using XG boost they're able to take
something the classification process
from hours to minutes and so they could
discover new vaccines much much more
quickly ping n is able to use XG boost
now to make predictions of epidemic
disease so that it is able to figure out
that there's an epidemic and alert the
public as soon as possible from hours to
minutes from hours to minutes and so now
machine learning can go from hours to
minutes all of this all of this is I've
mentioned before is software we now have
a full software stack for science
there's 600 different applications in
the world that are now accelerated we
have science data analytics as I
mentioned qdf deep learning all the
different frameworks from tensorflow
to tensor RT to PI torch to cafe to MX
net machine learning XG boost and all of
the other machine learning algorithms
that people generally use from random
forests that k-means that k-nearest will
becoming hyper scale inference our
inference server all of our TRT runtimes
the containers and then us of course
visualization all put into the cloud all
of these applications are now
containerized optimized containerized
and put into this cloud and you can pull
it it's open you can pull it register
pull it and run it in any cloud or any
system vendor system maker that is
certified we certify work with system
makers all over the world is certified
their computers to run optimally as well
as with good reliability all of these
software stacks today we're announcing
that in China we have Huawei inspir
Lenovo and sue gong as the first NGC
ready partners so thank you very much
for your hard work
NGC ready systems
well the HPC market HPC has been
redefined what used to be scientific
computing what used to be scientific
computing because of AI is going to now
include hyper scale data centers as they
use AI under hyper scale clutters
clusters and enterprise as they use
machine learning to become a data-driven
company each one of these industries did
not used to be a high-performance
computing company in the future as more
and more companies utilize machine
learning enterprises hyper scale as well
as supercomputing centers will become
high-performance computer industries we
will create computer architectures so
that machine learning AI data analytics
can pervade and can be useful in each
one of them today I showed you several
things I showed you that with our hgx 2
system we can address scientific
computing we can address scientific
computing bringing a eye to the
traditional high performance computing
market I showed you that with T with
tensor core GPUs we've been able to
bring AI to the internet companies our
tensor core GPUs have been so successful
and then now with the NVIDIA T 4 we can
apply inference and we could put GPUs
into large scale hyper scale data
centers and bring H turn those hyper
scale computers into HPC and then lastly
because of Rapids we can now bring
machine learning accelerate machine
learning for all of the companies in
enterprises and industries like
healthcare like retail like financial
services this is the new HP see what
used to be a much much much more niche
supercomputer industry because of AI has
now gone into every industry
it makes sense on first principles and
the reason for that is because AI is
about computers that write software and
it stands to reason that it takes a
high-performance computer to write
software that no humans can write to
learn from so much data and write
software that is incredible this is the
new HPC it is scientific computing in
its data science in its AI I've got some
fun demos for you this is the last
chapter AI computing AI automating the
world everything that moves in the
future will be AI in fact it's just some
amazing statistics I'll share with you
in just a second in order to enable this
future of AI coming out of the clouds
into the world and for these machines to
be working among us cooperating with us
helping us do things moving about by
itself we need a special type of new
processor that's why we created Xavier
Xavier is a first of its kind and this
Xavier processor this Xavier processor
Dane shots
what's this you night not well my son
wait like what could go wrong this is
what happens when we have no rehearsals
this is an exam process er this computer
is complete right here this is an entire
computer amazing computer this is
something along the lines of a fuel at a
few hundred laptops of AI processing
capabilities several thousand engineers
worked on it together at Nvidia this is
now in full production the world's first
AI computer designed for autonomous
machines this is the Xavier it's
actually amazing seven different types
of processors those seven different
types of processors is necessary to
compute the pipeline of a robot this is
the pipeline of a robot first it has
sensors of all different types
it could have cameras and lighters and
radars and sonars all kinds of different
types of sensors we have to process the
sensors depending on the sensor type the
processing is very difficult using a
different type of processor maybe it's a
stereo processor maybe he's an image
image sensor maybe it's a video stream
maybe it's images that is coming in that
has to be D noise door warped tensor
core CUDA is used for perception
recognizing the environment then you
have to do path planning how do I
achieve my mission if my goal is to pick
up this how do I achieve it and then
lastly you have to perform the action to
actuate the action okay so the
processing is very complicated it's very
unique and it's the first time in
history we've been able to put it on one
chip and now Xavier is in full
production xavier plus our GPU is
created to to enable a scale of family
of computers from one single chip to all
kinds of different configurations for
more IO more sensors and more processing
depending on the type of application
maybe it's a delivery robot maybe it's a
medical imaging system maybe it's a
driverless taxi all of these different
types of applications have different
types of requirements for computing and
so we created a scalable family and now
we can address all of these applications
however software is the key to success
these computers is just the starting
point because they're so complicated the
software on top of it is equally
complicated and so we created for
computing stacks the for computing
stacks are one for medical imaging
applying classical medical imaging image
reconstruction and then applying AI to
it we could do detection of anomalies in
real-time we could use it for streaming
analytics whether it's camera or
pressure sensors or vibration sensors or
sound we could use it to stream data
from all over the world's IOT and to be
able to infer and make predictive
analytics on that indoor or slow-moving
unstructured environment robots we call
that Isaac and then there's high speed
live critical but structured driving we
call that that form of Robotics the
self-driving car each one of them comes
with a three layer stack our platform is
open you can have layer 1 which is the
core OS with all of the API is
implemented this is absolutely
world-class at our company because of
our long history of professional and
high-performance computing applications
the second our acceleration libraries
these algorithms are
really well tuned takes advantage of all
the processors inside this chip and all
of our processors to deliver the highest
performance for images and signals deep
learning video in here multi-gpu or
virtual GPU for ray-tracing
for accelerated linear algebra and then
on top of that we have two platform
software we call layer three each one of
them each one of the layer threes are
different some of the layer 2's are
different and most of the layer ones are
the same
this is Clara this is metropolis you see
again three layers the basic layer of OS
the acceleration libraries and in this
case a streaming pipeline framework
neural networks and plug-in algorithms
are piped from one application to
another this is the Isaac robot and here
you see we have very similar core OS
very similar image processing library
however for the application layer layer
3 from visual odometry
to perception 2 localization and mapping
and planning it's completely different
what you saw just now you saw two
versions of Isaac one of them is a
virtual reality simulator that Isaac
learn how to be Isaac we created a
virtual reality simulator to teach Isaac
so that Isaac doesn't have to run into
my leg while it's learning how to be
Isaac
so once Isaac is learned how to be a
good robot then we can send them outside
and Isaac did exactly the same thing I
am so happy to say that there are so
much exciting development in these
delivery robots last mile robots all
kinds of autonomous machines of all
of interesting applications there are 60
is at 50 60 billion parcels 50 60
billion parcels delivered in China each
year 50 to 60 billion parcels you order
online and delivers it to your home
that's up from 30 just two years ago 30
to 60 billion
doubling in two years the demand of home
delivery is growing at Moore's law now
it's a joke
the demand is growing very fast and so
JD comm created a other there the
robotics lab we've been working with us
for a long time and that we work
together and they're using our platform
to create a last mount delivery vehicle
this is this is mate once this is um
shalt die may 28th day may 20 2013 2
nobody goes out anymore 26 million meals
in home delivery in one day alone
they employ half a million people to
deliver all of the meals to your home
now they have may 20 I'll diet hopefully
one day this cute little robot will
bring your bring your your your lunch to
you sign yell did I say that right Tonia
Tonia Tonia also developing a home
delivery system I think this is going to
revolutionize home delivery it has to
because the number of people who are
buying things online is growing
exponentially but the number of truck
drivers and home delivery people is not
enough and so we must be able to
supplement and take that pressure off
using artificial intelligence and
robotics two trillion miles driven each
year and we have exact we have a stack
again drive away
this is very unique because this
requires functional safety this is ISO
26262 certified this is failsafe it is
very resilient and very complicated
software at the OS level we also have
libraries acceleration libraries to take
advantage of all of the processors in
here so that the processing can be fast
and low-power and we have the necessary
driving stack on top we created an open
platform this open platform can be
enjoyed at level 1 layer 1 if you would
like to write everything yourself you
can enjoy level 2 and then write
everything above or you can enjoy level
3 or parts of level 3 you could decide
how you would like to mix and match as a
result the number of people who could
develop on this platform has expanded
tremendously we've been making a lot of
progress in this area I want to show you
every year when I come back I want to
show you some of our progress this next
demonstration next video is surround
video its surround and surround
perception so we're recognizing of
course what's in front of us perceiving
what's in front of us the lanes where
we're gonna drive but we also recognize
the surrounding vehicles and we track
them and we visualize it in a bird's-eye
view by doing so you have great
confidence that your car can see
everything around you that confidence
that confidence is really important for
you to trust the autopilot system so the
first demonstration is surround
perception
[Music]
[Music]
[Music]
okay surround perception now this is
thank you that's just that is the
building block of what I'm going to show
you next
California California where Silicon
Valley were we our home is we're
surrounded we're surrounded by several
highways one of them is called 280 the
other one is 101 and then there's 92 and
85 and the highway is very complicated
and so we challenge ourselves to drive
80 kilometres 50 miles 50 miles from
highway to highway to highway to highway
without touching the steering wheel
basically from point to point across for
highways this video I'm going to show
you is basically raw engineering video
raw engineering video you're gonna see a
lot of different networks a lot of
different neural networks and it's very
noisy but that's the point in order to
achieve very high quality you have to
have many different types of neural
network models not just one there are so
many different models and they're
working together in a fused way in an
ensemble way basically we have multiple
points of diversity and redundancy and
they work together to achieve a good
result just like a good company has
diversity and redundancy just like a
good team has diversity and redundancy
in order to have a good system a robust
system you fundamentally have to have
redundancy and diversity ok so what I'm
showing you here is a whole bunch of
different neural networks running at the
same time and most importantly enjoy
enjoy Silicon Valley
[Music]
what's the weather in San Francisco now
the weather is 67 degrees and cloudy in
San Francisco
Oh God tomorrow there will be partly
sunny skies with a high of 69 degrees in
the lower degrees in San Francisco
tomorrow
[Music]
[Music]
[Music]
gosh this is fun you know for all the
engineers in the room this is just
incredibly fun AI autonomous machines so
many great challenges the type of things
that we can finally do never dream that
it was possible 10 years ago and now
because of AI because of AI computing
all of this is now at our fingertips
what you saw just now is a live
demonstration we just did recently
if you come to Silicon Valley we'll give
you a right this is Nvidia Drive develop
developers kit all of this is in
production today you could get access to
the internal a drive IX drive IX
basically turns your car into an AI you
talk to the car the car talks back to
you it's all of the neural networks are
in the car it's looking at you it's
looking outside and so this car your car
in the future will be able to interact
with you help you find places instead of
typing in anything you simply tell tell
it where you want to go instead of
asking a question about about telling
its punching in the the restaurant
location you simply ask it where's the
best restaurant and then it takes you
there and so not only does it respond to
you and interact with you and talk with
you it also makes sure that you are
paying attention that you are alert in
the future level two cars will function
incredibly well just like a level four
car except the human is in the loop the
human being in the loop is vital to the
experience and vital to the functional
safety of the car and so it has to pay
attention to you to make sure that you
are paying attention and that you're not
following asleep that you're not reading
your email that you're continuing to pay
attention and drive it will also look
out for you on the outside if you're
about to turn and run into another car
into somebody else's way it will
tell you that you shouldn't or you might
avoid keep you from doing so it pays
attention to everything around you so
this car will interact with you it will
drive for you and whenever you're
driving by yourself it will keep you out
of harm's way in the future this this
system also is exactly the same system
as our autopilot system that I just
showed you if you would like to develop
on Drive this kit is available today I'm
super excited to announce that the level
2 system that I just demonstrated the
capabilities that I talked about will
appear in a production car in the near
future in the next couple years this is
a car that is famous for their safety
one of the most technically sound car
companies in the world they fame
themselves on safety and they have
chosen the Nvidia Drive AGX the computer
as their next generation level 2 system
here in China the N V's are racing to
bring electric vehicles to market so
that a large percentage of the future
cars in China could be electric we are
partnering with some of the best expain
single Auto SF motors I want to thank
you all for your support
imagine these cars with the capabilities
that are just described we're also
working with many companies in China on
mobility services from we ride momenta
to simple Auto ex wrote star AI to
trillion miles
if those miles if those miles were more
affordable miles imagine the adoption of
mobility as a service if every single
mile was delivered and it was average
pick a number of $1 for simple math
there's two trillion dollar opportunity
for mobility as a service here in China
extraordinary the trucking industry is
also being revolutionized by AI FTA
fault trucking Alliance F aw the
trucking company and FTA many of you
know is the uber of trucks it turns out
there are millions sir they have a
million businesses that they work with
five million trucks connecting
businesses and trucks is a tremendous
challenge in trucking here in China in
fact most of the time 40% of the time
the trucks are empty there's so much
waste that can be benefited from AI and
so FTA has created essentially the uber
of trucking to connect truck drivers to
the cargos fa w plus AI and ourselves
are working together to create an
autonomous truck so that in the
beginning it could ease the challenge
for truck drivers and then long term it
might be able to drive a short distance
autonomously and then long term more of
it autonomously our ecosystem of
autonomous vehicles is growing and the
number of companies that were working
with here in China is really quite
fantastic JIT Volvo is a Gigli company
Chery here in China
thank you very much ex paying single
Auto we have momento
Auto x+ AI we ride faw.2 simple one of
our Tier one partners working with us to
take the technology to car companies all
over China is Desailly v we're super
excited to be working with Desai as our
top tier 1 this is V thank you very much
everybody and one of our longest
partners in the autonomous driving world
is by do both for mapping as well as
utilizing the computer for their self
driving car stack now a car a car with
the drive computer could be autonomous
in the West as well as autonomous in
here in China so really fantastic one
computer solving multiple software
stacks connected into the ecosystem of
different countries well that's it I
talked about three chapters today
there's so much to share with you I
talked about number one touring
reinventing computer graphics with ray
tracing and artificial intelligence I
talked about reinventing the future of
high-performance computing the new FPC
HPC the new HPC includes scientific
computing data analytics and AI and
there's two types of computers from
hyper scale to high-performance
computers that we have now brought AI
capabilities into them and then lastly
Nvidia agx family
Xavier processor a computer designed for
autonomous machines for robotics is now
in full production and so car companies
who are able who are driving and racing
towards autonomous capabilities and
electric capabilities in 2020 2021 now
have a processor they can ship into
those cars the industry is changing so
fast and AI is fundamentally changing
all of it and as you can see we've
brought AI into every aspect of our
company
from computer graphics to
high-performance computing to autonomous
machines this is some of the
achievements from last year I want to
show you one more achievement from last
year and this this particular
demonstration is not a video I want you
to remember every single second while
you're watching this it is not a video
you got to keep saying to yourself this
is not a video everything about it the
lights and the reflections and the
lasers and the shadows are all being
computed in real time
everything about the materials why metal
looks like metal white plastic looks
like plastic what glass looks like glass
everything is being computed then a
physically based physically based photo
real way and so what I'm gonna show you
what I'm going to show you is a turing
demonstration but it is not a video roll
it please
[Music]
[Music]
[Music]
[Music]
[Music]
[Music]
[Music]
[Music]
[Music]
[Music]
Nvidia Turing ten years in the making
reinventing computer graphics available
today I have one more little surprise
for you and I want to thank all of you
for before I go I want to thank all of
you for coming to GTC this is a record
GTC GTC was created for researchers
scientists developers who either used
computers to do their life's work or
build computers as their life's work
this is where GTC has become the
conference where science artificial
intelligence robotics and computer
graphics is displayed at its fullest I
have one more little treat for you and
with that I want to thank all of you for
coming and have a great GTC thank you
finish it
Title: NVIDIA GTC May 2020 Keynote Pt3: GPU Accelerating HPC and Scientific Computing
Publish_date: 2020-05-14
Length: 608
Views: 95814
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/tpeGZ7nm0J0/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: tpeGZ7nm0J0

--- Transcript ---

let's talk about high-performance
computing it is clear now on
acceleration is gonna be the path
forward for scientific and for high
performance computing and as I mentioned
before accelerated computing has four
pillars the first of course is the
accelerator the advanced GPUs the second
is the stack the acceleration stack for
each one of the computational domains
the third is systems and last is
developers ultimately the applications
that we accelerate this year we did
really great work we've accelerated now
over 700 applications and each and every
single year at every conference I show
you our golden suite the suite that we
track on a regular basis to make sure
that we continue to engineer advances
into libraries into the stack so that
applications continue to improve in
performance even if we don't introduce
new GPUs and as you see over the course
of the last four years we've increased
application performance by 4x and the
green bar is something that I'm going to
talk to you about we're gonna offer you
a new platform and it's gonna give high
performance computing a huge boost we
also brought CUDA to arm computing
systems arm server CPUs are seen
adoption all over the world in hyper
scale there's Amazon Fujitsu
supercomputing caveum part of Marvel now
a new company exciting company called
ampere computing or in China Huawei all
of the suite of Nvidia tools and
libraries are now available for arm we
also introduced a brand-new SDK for iOS
in this year we call magnum IO Magnum IO
includes all kinds of great things from
our DMA of course to the ability to
communicate across multiple nodes and
move data directly from storage to our
GPUs the suite of libraries is going to
continue to advance magum IO is going to
be one of our most important library's
data processing and networking and
storage is going to become more and more
important to data center scale computing
over time we introduced two new stacks
this year and vídeo para breaks for
genomics processing the ability to do
variant calling at very high
performances and a large body of work
that we've been working on for several
years called Nvidia rapid
data analytics machine learning has
become one of HP C's Grand Challenges
the advances of machine learning and the
popularity of this approach has caused
companies institutes and data centers to
collect a vast amount of data the
machine learning pipeline consists of
three things
ETL which creates the data frame does
all the feature engineering necessary
for the machine learning algorithms to
Train on which creates the model which
is then put into operations we call
inference these three stages of the
pipeline have unique and different
computational challenges the first stage
of the machine learning pipeline data
processing is becoming more complex than
ever in fact most data scientists will
tell you they spend the vast majority of
their time doing feature engineering and
data processing and the front stage of
the machine learning pipeline what used
to be processing hundreds of megabytes
to gigabytes to terabytes of data
companies are now routinely processing
tens if not hundreds of terabytes of
data and moving to petabytes of data it
is the reason why SPARC is so popular
SPARC is an incredible computational
platform it turns an entire data center
into a compute engine it partitions a
very large data set to be processed
across a bunch of servers in a data
center it was the brainchild of monte
sahaja at the berkeley amp lab and spun
out and became apache spark it now has
over a thousand companies contributing
to it nearly a million lines of code
16,000 plus companies around the world
uses it for data processing today well
the amount of data that they're
processing is growing exponentially
it is now reaching the limits of what
SPARC can do here's the reason why
the CPUs that is being distributed
across has a fundamental working set in
the order of megabytes a CPU natural
delights to work in its cache and its
caches typically on the tens of
megabytes when the data set is now in
the hundreds of terabytes and into
petabytes the overhead of coordinating
the CPU servers is becoming the greatest
bottleneck and we're starting to see the
limits now what if instead of working on
processors that has tens of megabytes of
working set
let's move towards
processor that has tens of gigabytes of
working set and if we could use multiple
GPUs to create large memories then it is
now possible for us to imagine scaling
beyond that we started working on GPU
acceleration of the data processing
stacks several years ago and it's a
giant body of work ladies and gentlemen
today we're announcing that spark 3.0
the next generation SPARC will be Nvidia
accelerated this is a collaboration
between ourselves and a large community
of researchers and developers in open
source all around the world and the
results are really fantastic it's
possible because of several
groundbreaking achievements the first is
the work that we did with Mellanox
Nvidia called GPU direct storage and the
acceleration of GPU direct storage and
ucx this framework that makes possible
the management of i/o and storage and
multi node computing lightning-fast
second is the scheduler of SPARC
scheduler of SPARC now is aware of GPU
and the GPU memory so that it could
partition work to the GPUs and schedule
it in a distributed way and manage the
computation of this giant network of
computers third a library we call rapid
that has the ability to ingest data
create data frames do feature
engineering do sequel queries and
intercept the calls of SPARC to be
accelerated by our GPU and then lastly
spark has a spark sequel accelerator
they call catalyst and that has been
optimized for NVIDIA GPUs these elements
make possible SPARC 3.0 let me show you
the potential acceleration that data
scientist will be between joy what
you're looking at here is the benchmark
of Rapids the foundation of SPARC 3.0
this particular benchmark is TPC xbb
big data benchmark this particular
dataset is a scale factor of 10,000
which basically has 10 terabytes the
state of the art is a Dell server cost
about a million dollars and has the
ability to deliver 17 gigabytes
second of data movement through this
benchmark this particular benchmark is
hard to beat and the reason for that is
because not only does it have to be fast
it also has to be cost effective and the
reason for that is because price
performance matters the fastest in the
world today is the Dell server at a
million dollars and 17 gigabytes per
second with SPARC 3.0 sitting on top
Rapids Rapids benchmark done
TPC X baby delivers a hundred and
sixty-three gigabytes per second for two
million dollars ten times the
performance and only twice the cost if
you were to look at this in another way
suppose you were to create a data center
that is able to achieve the same
performance as two million dollars of DG
X's accelerating Rapids of this
benchmark TPC xbb
it would cost you 10 million dollars and
150 kilowatts now of course data centers
routinely process a lot more than
terabytes you're going to need data
centers way larger than this in the
future as the data continues to grow
exponentially and so the ability to
accelerate SPARC 3.0 with a library we
call rapid is utterly groundbreaking the
result is really spectacular one-fifth
the cost one third power
one-fifth the cost and one third to
power the more you buy the more you save
in fact data breaks which offers
industrial-strength SPARC at a large
scale as a service is doing fantastic
every single day a million virtual
machines are spun up to do data
processing on SPARC and they're so
delighted by the work and acceleration
that they're gonna go accelerate data
bricks with NVIDIA GPUs they're a
fantastic partner I'm so happy and with
all the work that we've done together
leading cloud service providers are
offering SPARC accelerated in their
cloud or they're accelerating their
proprietary machine learning pipeline
and data processing pipeline with a
video rapid Amazon sage maker Azure
machine learning data bricks google
cloudy
Google cloud data proc are now going to
be accelerated with NVIDIA GPUs for data
processing and data analytics spark
acceleration is a great achievement I'm
so proud of the team it's such a large
body of work and has taken us years and
it requires the collaboration with
hundreds of collaborators and open
source built on several layers of
foundational and fundamental new
technology and now the part that is
growing exponentially difficult the
first stage of machine learning is now
accelerated data scientists all over the
world that can be thrilled entire
end-to-end from data processing to
inference we have three libraries Rapids
for data processing ku DNN our core
library for deep learning AI and then
third tensor RT our optimizing compiler
for these computational graphs that are
created by the training frameworks the
end-to-end acceleration is now complete
and we will continue to advance it over
time but this represents the foundation
of Nvidia AI I can't be more proud the
team have done a great job thank you
Title: Win a GeForce GTX 1080 Ti - and more! #GameReady at E3
Publish_date: 2017-06-09
Length: 64
Views: 18502
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/TQA4Gi18x8w/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: TQA4Gi18x8w

--- Transcript ---

everyone Evan here that's right it's
time for e3 and when you see me standing
next to a huge stack of sweet gear you
know it's also time for a really big
giveaway this year we're covering III
like never before whether you follow G
for us on Twitter Facebook Instagram or
YouTube we've not only got tons of
updates on the latest PC gaming
announcements from the show floor we're
also giving you the chance to win from
our massive prize pool
we have over $100,000 worth of giveaways
and our second annual III is game ready
contest including 50 that's right 50
geforce gtx 1080p is how do you get a
chance to win we've got all the details
over on the official GeForce YouTube
channel if you haven't subscribed you
need to get on it you've been missing
out this year on all sorts of important
gaming announcements and coverage
including exclusive gameplay videos and
trailers you won't find anywhere else
and you don't want to miss out on this
giveaway hit that subscribe button and
find out how to join in on the fun and
excitement of e3 all with NVIDIA GeForce
you
Title: NVIDIA GTC May 2020 Keynote Pt 9: Announcement Highlights
Publish_date: 2020-05-14
Length: 622
Views: 72121
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/tRtRA7gA46M/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: tRtRA7gA46M

--- Transcript ---

and so there you go direct from my
kitchen GTC 20/20 we talked about a lot
of stuff let me quickly summarize the
first thing we talked about was how
accelerated computing is accelerating
and momentum and that we're taking it to
the next level to data center scale
computing where accelerated computing
and data processing and networking are
both vitally important I'm delighted
that Mellanox and NVIDIA are now
officially one company the second thing
we talked about was how real-time
ray-tracing after all of these years has
enabled next generation of computer
graphics and omniverse could be created
with portals for designers with
different tools in different places
doing different parts of the design at
the same time they're able to do that
because the world is interactively lit
it is physically based it'll baised the
laws of physics and it could be created
in real time we call that omniverse and
we're shipping it on our TX server the
third thing we talked about was Nvidia
AI machine learning is the greatest
challenge of HPC today machine learning
has three basic stages the data
preparation the training of the model
and the inference the deployment the
production in the model I spoke about
the data preparation of the machine
learning pipeline and how the amount of
data that is being processed is growing
from tens of terabytes to hundreds of
terabytes to very very soon petabytes of
data the leading compute engine is
called spark spark takes an entire data
center and it turns it into a compute
engine partitioning the large amounts of
data into small chunks that are
processed in clusters of computers after
several years of endeavor today we
announced the acceleration of SPARC 3.0
Nvidia now has the entire pipeline of
machine learning from data processing to
empress through training all completely
accelerated i also spoke about two of
the most
machine learning pipelines in the world
today the recommender system the machine
learning system that predicts user
preferences for billions of people and
trillions of items collecting an
enormous amount of data about objects
about users about usage patterns and by
doing so create a predictive model of
your preferences we created an
application framework called Nvidia
Merlin that simplifies this enormous lis
complicated distributed computing
machine learning pipeline called
recommender systems and video Merlin
we also spoke about conversational AI
recent breakthroughs in speech
recognition natural language
understanding and speech synthesis has
made it possible for us for the very
first time to imagine creating an AI
model that allows you to have natural
conversations we created a
conversational AI framework that
codifies the entire pipeline with
state-of-the-art AI models that are
pre-trained and optimized and tuned for
performance and fast response the entire
pipeline can respond in just a couple to
300 milliseconds as a result you can
have a reasonable conversation
interactive conversation with an AI
agent Jarvis is used by enterprises
around the world to adapt it to their
domain answer healthcare related
questions insurance questions financial
services questions it is now possible to
retrain Jarvis for your domain
conversational AI is now democratized
and right here from my kitchen we
announced the shipment of our ampere GPU
and the data center GPU a 100 ampere is
a miracle ampere is the largest the most
complex processor the world has ever
made TSMC 7 nanometer fifty four billion
transistors connected to 1.5 terabytes
of HBM to memory on a 3d package called
COAS this processor goes into the DG x1
as a third-generation tensor court has a
peak throughput
20 times greater than Volta the most
advanced processor in the world today a
100 is 20 times the peak for training 20
times the peak for inference of V 100 it
has a brand new architecture called MIG
multi-instance GPU it could be
configured as one or seven or something
in between may allows ampere to be used
as scale-up for data analytics or
training as well as scale out for public
cloud instances or inference
this is our first GPU that has such
incredible throughputs and has the
ability to configure itself into a large
or small GPU it's connected also by our
next-generation mblink 600 gigabytes per
second ten times the bandwidth of PCI
Express Gen 4 djx a 100 is the most
advanced AI instrument in the world it
is designed for the entire pipeline of
machine learning from data processing to
training to inference it is the first
computer we've ever built that is
unified for all of those workloads
whether scale up or scale out the Nvidia
dgx a 100 is in production today has the
equivalent performance of a hundred and
fifty high-end servers well over a
million dollars dgx a 100 is available
today for a hundred and ninety-nine
thousand dollars the more you buy the
more you save we also talked about the
coming together of IOT the Internet of
Things and artificial intelligence
creating this brand-new opportunity
called edge AI this is the beginning of
the smart everything revolution there
will be trillions of things connected to
the internet with intelligent AI
services you could just imagine the
scale of this opportunity you have
already heard us announce in previous
keynotes how we're working with Walmart
who's using Nvidia egx for smart retail
or USPS using Nvidia egx
for the highest logistics sorting
operations in the world today I
announced that BMW has selected Nvidia
and the eg
and the Isaac robotics platform to
create their next-generation factories
that was a busy GTC it is great to have
all of you I want to thank all of you
for partnering with us and I want to
particularly recognize all the
researchers scientists artisan designers
that take advantage of our platform to
invent the future one more treat for you
this is something that I'm supremely
proud of Nvidia is one of the companies
in the world that has assembled a great
team of designers and architects and
software programmers and scientists and
AI researchers computational
mathematicians as well as incredible
artists the fusion of art engineering
and science all under one roof is one of
the things that really inspires me I
want to share with you the behind the
scenes of the creation of the video Imai
i think you're gonna love it please
enjoy and see you next time
[Music]
[Music]
[Music]
[Music]
[Music]
[Applause]
[Music]
you
Title: CEO Fireside Chat with Jensen Huang and Raghu Raghuram
Publish_date: 2022-08-30
Length: 1320
Views: 1069017
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/TuggBO97yYg/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLASyyPmZi0HTIq04v1YaRj1UCf70A
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: TuggBO97yYg

--- Transcript ---

we announced a strategic partnership
with nvidia two years ago and
we made remarkable progress
now
i'm super excited to welcome
jensen wong
president and ceo of in media welcome
jensen thank you raghu i'm delighted to
be here you know we've been working
together for so long
and uh this particular project
particularly that we're gonna announce
today
is a is a project that's very close to
my heart
and
we've been working on very very hard so
i'm delighted to be here to talk about
it yeah you and i have talked about the
future of the data center and the
central place uh
this thing we're going to talk about
it's going to come about but before we
get to the dpus right
let's talk about your other favorite
topic ai and ai
in the enterprise this again was a
collaboration we announced some time ago
and bringing your ai stack on top of
our platform
and today we've got customers that are
deploying nvidia ai
in the enterprise in the data center
and together we are changing the world
of enterprise computing two years ago
um
at vmworld we announced
that we were bringing ai to the
enterprise
and
today customers are deploying nvidia ai
on vmware's cloud platform
running their ai workloads in the data
center
and together we are changing
enterprise computing
and helping bringing ai to every
enterprise
and we also announced project monterey
it seemed like yesterday about two years
ago
re-imagining the data center
architecture
and meeting the agility the performance
the security requirements for running
these next generation applications
and our engineering teams are working
together very very closely
on fulfilling this mission
and so today i'm excited to announce the
availability of vsphere running on
nvidia gpus
formerly known as project monterey yeah
it's really quite amazing what we've
accomplished in the last two years
you know the mission and the vision
is to recognize
that this new modern workload called
artificial intelligence
and the way that
software wants to be
developed and deployed in the world's
enterprising clouds
are fundamentally different than the
past and then in order for us to
accommodate this new and bring bring
this new way of doing computing to the
world we have to reinvent the data
center all together this was this was
really a partnership that was really
made in heaven and the reason for that
is because vmware is the operating
system of the modern enterprise
and we wanted to bring a new type of
computing ai computing to the world's
enterprise where the ai engine if you
will
of modern ai workloads
and when you think about when you think
about this particular vision
you have to take a step back and
understand you know what is ai about
it's about
of course
a new way of developing software
and
as a result of that the
computational method
uh the deployment method is so
profoundly different that we our two
companies had to re-architect the data
center from the ground up and today's
today's announcement is really a
celebration of
two really intense years of engineering
activity between our two companies so i
can't tell you how proud i am of what
we're about to show you what we're about
to tell you
that's great speaking about ai
you have said it right it's the most
powerful force of our time
and it's becoming mission critical to
every enterprise you and i have talked
about it five years from now or five
years ago just like nobody talks about a
database enabled application nobody's
going to talk about five years from now
an ai enabled application will just be
assumed that machine learning is part of
every application
and
you have brought about a full fresh ai
solution
with
from workflow to development to
deployment
and it includes your software that's now
certified on the vmware platform with
chanzu
from a full kubernetes and containers
and vms so it gives customers exactly
what they want
the best in ai managed on the platforms
that they trust
and
better still you can run ai not in some
dedicated silo but you can run it
alongside your mainstream application
workloads
so you get better efficiency you get
better efficacy cost management overall
management all in the data center
together we are democratizing ai for the
enterprise
just a simple example right carillon
clinic
which is one of the largest healthcare
organizations in virginia
and they are using your software on our
platform to future proof hospitals
they care for over a million patients
and they're using ai
to accelerate their image processing
using your software
well ai is surely transformative in fact
ai will revolutionize every single
business
there are no businesses in the future
that will not be
using machine learning will not be
transforming the data of their company
and their industry
and harvesting intelligence from it and
deploying it at scale that those
companies simply wouldn't exist so every
single company in the future will be an
ai business
our work together is really about
transforming how every company
could apply
and develop and apply ai now we're
working on all kinds of different ai
applications from computer vision to
speech ai to conversational ai natural
language processing recommender systems
companies are harvesting tons and tons
of data and refining it discovering
predictive models from it
and essentially becoming ai factories
the the ai factor every company is going
to have ai factories in the future and
the data centers that are these ai
factories the way they operate are
fundamentally different than the way
they used to operate in the past if you
think about think about an ai
application unlike the enterprise
applications of the past
where you have many users
running on one particular computer in a
virtualized manner in the future ai
applications have to do two things
large amounts of data has to be
processed
and as a result an enterprise data
center would run distributed high
performance computing applications on
one hand
on the other hand
the applications that are then deployed
with tanzu kubernetes
is a disaggregated application that
distributes across and scales across the
entire enterprise
and so on the one hand it
a modern enterprise is going to be a
distributed ai supercomputer on the
other hand it's going to be a
disaggregated data center running
applications that are essentially
hyperscale
and these this behavior the ability for
a data center to adapt
in this way on the one hand uh do ai
processing to learn the models on the
other hand
deploy and scale these ai applications
across the entire data center
this particular characteristic is very
unique to ai and this was the grand
challenge for our computer scientists
and this is what
monterey has really enabled the ability
to simultaneously on the one hand be a
high performance computer on the other
hand be a hyper scale cloud computer
this capability is really quite unique
and it's really groundbreaking so i'm
super excited about that
ntt com is another example of a great
early adopting customer they
they're one of the world's largest
telecoms as you know and and uh they
have multi-tenant um
uh data scientists that are across the
company that would like to be able to
use their infrastructure and our
collaboration makes it possible for them
to easily do that yeah absolutely and
like you pointed out
the idea of having
the virtualization of the vsphere
software
running in a dpu
is essentially what makes that
distributed
computer and the dispute distributed
hyperscale computing possible
inside of the enterprise and that's why
the dpu and project monterey is such a
fundamental
advancement
for vmware and it is the centerpiece of
our vsphere 8
software release which we are announcing
today
so
if you think about uh the dpus
it's not just in the data center right
in fact a lot of machine learning is
going to happen at the edge
with autonomous driving and
and cashier-less stores and
modern manufacturing and so on and so
forth a lot of that is going to be in
the edge and like he pointed out in the
data center as well so dpus play a
pretty significant role in this new
infrastructure architecture to
accelerate performance
to free up the cpu cycles
and
certainly most importantly to provide
better security which
we should talk about some more and we
have re-architected vsphere to fit on
the
the nvidia gpu
so that we can offload processing of
software-defined infrastructure tasks
like network processing like storage
processing to the dpus right
and now you get accelerated i o
and you can have agility for developers
because all of that storage and network
processing is now running in the dpu now
you have the entire cluster of cpus to
run this distributed workload that you
talked about whether it's containerized
or in a vm that's so that's the power of
the dpu that's exactly right in fact
i would say that vsphere 8
and i've been a customer and a partner
of vmware
for decades and i would say vsphere 8 is
the single
biggest launch
of all the vsphere since the very
earliest ones and the reason for that is
because it transforms the way that
computing is done in an enterprise
you know if you look at if you look at
the work that we're doing
uh this is high performance computing in
a virtualized environment
with all of the benefits of
virtualization the management of it the
security of it the ability to
move and orchestrate workloads with
agility
the utilization of your data center
because of vsphere because of the work
that vmware does has increased
tremendously for all of the world's
enterprise
here comes this brand new application
that is a high performance computing
application that sits on top of the
software defined data center that
vmware revolutionized
however
high performance computing and
virtualized environments as you know are
historically
like oil and water yup
and this is the big breakthrough of
vsphere 8. you've given us the ability
to do virtualization of data center
scale
for distributed high performance
computing applications like ai
model development
as well as containerized deployment
disaggregated computing
to scale out an application from cloud
to on-prem to the edge
this ability to do so with vsphere 8 is
really really revolutionary
and what made it possible
was the recognition that there's a new
type of processor that we're going to
introduce into the data center for the
very first time
nvidia bluefield is the dpu that ragu
you were mentioning just a second ago it
is essentially is essentially an
accelerated computing platform that is
designed for the infrastructure software
of data centers exactly today's data
centers are software defined networking
virtualization storage and cyber
security all of that is sitting in
software in the data center for all
wonderful reasons
however that software stack that
software layer
takes away from the computing resources
that we need for high performance
computing applications like ai
and so with bluefield we've been able to
offload
accelerate and isolate
essentially the operating system of the
data center off on a new type of
processor called a dpu and as a result
freed up all of the cpu resources and
all the gpu resources available for
computation
the benefits
and and and i think that this is this is
what all the enterprise all the
companies
discover the return on investment
the benefits that
dp blue vsphere 8 with nvidia bluefield
will be so fast because it frees up so
much resources for computing
that the payback is going to be
instantaneous it's going to be really a
fantastic return
exactly you you put it very succinctly
the operating system of the enterprise
is sitting on
the bloomfield dpu now
freeing up all of this capacity
of the core cpus to run
traditional workloads modern workloads
containerized workloads etc
the other beauty of this architecture
is
you can
reconfigure your data center at will you
take a regular server all of a sudden
through the magic of connecting it to
the
dpu
now you get all these advanced storage
and networking capabilities on demand as
you need it right so this ability to
make the data center very fungible is
also uniquely enabled by the dpu thank
you i'm
actually supremely proud of bluefield
and and i'm i'm so excited about the
revolutionary benefits that it's going
to bring to modern enterprise
and as part of
a computing infrastructure that spans
edge to prem to cloud
i
absolutely believe that in the future
every single data center and every
single server will be powered by a dpu
and it's going to enable the operating
system of the data center to run an
isolated and accelerated way
exactly speaking of isolation
one of the critical ideas that vmware
puts forward to our customers is the
idea of intrinsic security having
security baked into the platform
and
with the dpu
now we've not only got the vsphere code
base running there but also the nsx code
base running there
so now for the first time
you can handle east-west traffic the
traffic going between applications on
the cpu cores
at line speed and impose security
without breaking up anything that's
right so that again is something that's
possible because of the dpu architecture
the security aspect of it so important
and the reason for that is
as we modernize the applications in our
data centers and it runs in containers
across an entire data center so that we
could scale it out
it's now disaggregated it's running
distributed in a distributed way
and it's all being orchestrated by tanzu
and vsphere 8.
the amount of east-west traffic as you
were mentioning earlier is going to grow
exponentially exactly and east-west
traffic
from microservice to microservice
container to container is going to open
up the attack surface of companies
fairly significantly as a result and
this is where nsx running on bluefield
is so vital and the reason for that is
because of course
perimeter firewalls
was a great invention but in today's
multi-tenant
cloud-native
disaggregated application scaling out
the amount of east-west traffic is so
significant the attack surface is so
many that we essentially need to put nsx
no longer at a data center scale or a
cluster scale but we need to put nsx
literally within every single computer
exactly and this is where bluefield and
vsphere 8 with nxx
running on bluefield is such an
incredible revolution we're essentially
going to have a firewall in every single
computer exactly and protecting every
single container
and in the future serverless and you
name your architecture as the unit of
compute becomes smaller and smaller
the amount of communication between
those units of compute is like you
pointed out exponentially gonna increase
so you literally got to have
it's
in my humble opinion
it's not going to be possible to do
security without something like a dpu
that's right sitting on every uh
computer
and
the last but not the least
in addition to by the way the security
is not just going to be about the
firewalling it's going to be about the
encryption as well that's right right
we're going to encrypt everything on the
line at line rates yep exactly so that's
that's the power of this
and you've got to be able to manage all
of these things together
and that's where the rest of the vsphere
architecture comes in
the beauty of what the vsphere engineers
have done is they've not changed the
management model now you can manage the
dpu just like you manage the server
with the same vcenter with the same drs
and vmotion all of those constructs and
so it can fit seamlessly into the data
center architecture
of today while enabling the future to
come about
yeah
you know i tell you
vsphere 8
bluefield
the work that we did together
with
nvidia ai enterprise
we now have
a stack for
ai computing yup
we have a a brand new way
of deploying applications whether it's
the classical enterprise applications
monolithic virtualized
to distributed ai computing
to disaggregated ai deployment
with
security at every single node and every
single transaction
uh to be able to scale this out of the
data center
you know roger we've reinvented
enterprise computing and a lot in a big
big way yep absolutely i am so proud of
vsphere 8. i'm so proud of the work that
we've done together
and
and we can try it this is just the
amazing thing you could try it now we
have this thing called nvidia launchpad
they're basically cloud
servers that you can come and enjoy
it is powered by bluefield
vsphere 8 nvidia ai
a whole bunch of workloads that are on
there you can come and try it
and reach out to nvidia
and we'll get you set up so that you can
try this at nvidia.com you could sign up
for this vsphere platform
running on nvidia bluefield gpus
and so you don't have to procure it you
just in order to try it you don't have
to set it up just come to nvidia launch
pad and we'll help you enjoy it so try
vsphere platform with nsx on nvidia dpu
bluefield tpus today
i can't wait to try it myself
this relationship has been a phenomenal
relationship like you said it's bought
out of deep engineering collaboration
between the two companies
over
foundational technologies the ai stack
that you bring to the enterprise are
vsphere 8 technology and of course the
dpu
collaboration that is coming to fruition
today in the form of vsphere 8. and this
is just the start of the two companies
i'm sure we've got a lot more to talk to
our customers about down the road so
thank you so much for the collaboration
thank you to your engineering team and
thank you for being here thank you raghu
and
you know i wouldn't have missed it to
celebrate one of the most important
new breakthroughs in the computing
industry it took two companies our two
companies with with uh such deep
computer science capability to really
reinvent enterprise computing for this
modern era
where
ai is going to be central to every
single company in the world
where cloud and multi-tenant cloud
hybrid cloud
development and deployment is going to
be vital to every single company where
the way that we develop and deploy
software has been profoundly changed
because of the way that we would like to
scale out our enterprises from cloud to
on-prem to edge
all of these all of these and because of
course we like to be a as you know have
zero trust architectures and and be
secure enterprises as we
allow
our employees and our customers to enjoy
our companies in a multi-tenant cloud
environment and so so this uh this
reinvention this redesign of the data
center
is going to be really really profound
and it's been a great privilege
privilege to work with you and great
pleasure to work with you and our our
engineers have worked together so
closely over the last couple years and
so intensely i want to thank everybody
for um trying it and i think you're
going to really love it and this is a
very big moment for enterprise computing
thank you
thank you roger
[Music]
you
Title: SHIELD TV WATCH AND WIN Episode 1: No Spoilers
Publish_date: 2017-12-05
Length: 61
Views: 12940
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/TxV44ZUDw30/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLA6HX5IJYBK3hyx5bxAMG4Fvj4grg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: TxV44ZUDw30

--- Transcript ---

[Music]
okay guys it's almost time for the
premiere of the next movie and I of
course of tickets now I have been on
high spoiler alert for months and now
that we're so close I'm gonna take extra
precautions so I'm gonna use my shield
and smartthings link to put this place
on lockdown okay google enter the no
spoiler zone as you can see the lights
are going off I've got the speaker's
playing white noise all communication
with the outside world is jam even the
blast shields are coming down as you can
see it is virtually impossible for
anyone to ruin this movie for me future
me yeah dude I just saw the movie you
are gonna love it
[Music]
you
Title: NVIDIA Make Your Mark — Jing X Hu
Publish_date: 2014-05-08
Length: 197
Views: 14300
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/tZ1XjSYKJmg/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: tZ1XjSYKJmg

--- Transcript ---

I feel like realities of the darker than
imagination and I feel reality is
definitely darker than the surface how
it appears to be my name is Tina who I
come from Singapore I'm a painter
illustrator comic artist and teacher
that's an artist I really feel like
sometimes I feel like a spaceman when
you're painting on when you're drawing
it's a very antisocial kind of process
every day before I go to sleep and when
I'm actually lying on bad I'll actually
probably spend about 10 to 20 minutes
like just closing my eyes and sinking of
the narrative of story and I usually
think of myself as the main character
the storyline is usually inspired by my
own actually my only biographical
experiences because I feel like I can
only draw things that I experienced
firsthand and I understood it things
that happen to me as an artist I think I
should respond to it because those kind
of self-reflection
I feel give me a kind of heightened
Sensibility I having selected as one of
the 10 artists to take part in this make
your mark project we're asked to do
illustrations inspired by a robot and
using direct status technology by making
these illustrations we're hoping to
inspire other people to make their mark
to experiment more with this very useful
tool so I've been chosen to illustrate
chapter 5 about robot that could
actually read people's mind I think I'm
robots as a story is really intriguing
I think the writer Isaac Asimov is
really good in depicting like very
complicated psychology like how robots
are really similar to human in many
senses and by observing this robots were
actually really reflecting about this
problems existing in the human society
I feel the good part about this tablet
is that yes amazing speed so it's like
whatever you draw there's no lag well so
this project hopefully my art will
actually inspire people to try out the
linear qualities the line works in a
precise nur's I'm offered by the screen
and the touchscreen kind of experience
and hopefully I really hope they can
inspire young young artists to make
their mark
Title: NVIDIA GameWorks - E3 Trailer
Publish_date: 2015-06-16
Length: 78
Views: 63202
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/TzGtdqss3_g/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: TzGtdqss3_g

--- Transcript ---

[Music]
[Music]
[Music]
[Music]
[Music]
Title: Secure Modern Apps on VMware vSphere 8 with NVIDIA DPUs
Publish_date: 2022-08-31
Length: 114
Views: 261455
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/U1z8VcLhrBw/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: U1z8VcLhrBw

--- Transcript ---

as the demand for data center resources
grow due to Modern workloads such as AI
machine learning 5G and Big Data
applications and infrastructure compete
for valuable CPU Cycles at the same time
applications are spread across many
locations which means security needs to
be distributed broadly yet enforced
locally
can your data center infrastructure meet
these requirements data processing units
or dpus are designed to offload
accelerate and isolate infrastructure
workloads from the CPU including
networking Storage security and host
management and as part of VMware vsphere
distributed Services engine a
re-architect to VMware Cloud Foundation
Nvidia Bluefield dpus maximize Data
Center Performance security and
efficiency and form the backbone of your
infrastructure Management Services
interested to see how it works vsphere
distributed Services engine powered by
the Bluefield dpu is available as a free
trial through Nvidia Launchpad the trial
accelerates your development time with
live demos and an interactive hosted
environment that gives your it Team Deep
practical experience before any
on-premises deployment vsphere
distributed Services Engine with
Bluefield makes clusters more Dynamic
more secure and optimizes them for
modern applications
by delivering this across all the
locations that VMware Cloud Foundation
runs today data center edge and Cloud
total cost of ownership is reduced for
both VMware esxi and bare metal
operating systems experience benefits of
the Bluefield dpu first hand by
prototyping and testing the same
software stack deployed for production
Nvidia Launchpad lets you confidently
make software and infrastructure
decisions
learn how easy it is to modernize your
Cloud infrastructure apply today
Title: Overwatch: Everything You Need to Know to Win
Publish_date: 2016-05-24
Length: 337
Views: 11122
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/U3kbiRqX_iU/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: U3kbiRqX_iU

--- Transcript ---

Blizzard's lightning pace 6v6 team based
shooter goes on general release on the
twenty-fourth of May and although it's
immediately accessible to even the
greenest of FPS noobs there is huge
depth to explore and much to master but
before you get started we're going to
run through nine things you need to know
about overwatch giving you a beginner's
overview and some more advanced gameplay
tips to help give you the edge firstly
there are a whole host of heroes to
choose from and players right from the
start each of whom fall into one of four
distinct roles offensive characters are
fast moving fast firing and great if you
want to get stuck into the dignity
action defensive heroes have a mix of
weaponry types including snipers and
turret builders tags are great hulking
beasts always short-range clout and most
with force field generators while the
final class of support characters all
about healing and buffing your teammates
and deploying strategic devices each
hero has their own unique set of weapons
and abilities all have a basic attack
activated by clicking a left mouse
button and a melee attack bound to the
VP some also have a secondary fire mode
every hero also has three further
abilities two of which are bound to the
left shift and a keys which acted a
unique weapons create protective
barriers and force fields he'll
teammates teleport around the map or
cast one of many other unique effects
with short moon bounce between each use
and then there's the ultimate ability
charged by time ticking away and
accelerated by causing damage and
helping teammates the ultimate ability
can be deployed by fitting Q and is a
devastating and often wide-ranging
attack mode that if used wisely can tip
the balance of power during around
although overwatch is super easy to pick
up and play you'd be wise to think a bit
more tactically if you want to hit a
winning streak when starting a new game
pay attention to the on-screen tips make
sure your player choice complements the
team dynamic and don't be afraid to
change your class if there's a short
ball in a particular area it also pays
to hotswap during the match itself if
you think your team is missing a trick
but unless absolutely necessary try to
do this only after you've already
Unleashed merry hell with your last
characters ultimate ability as swapping
over will reset the counter to zero
imagination is the essence of discovery
there are currently four main game modes
to tackle an overwatch spread across
wide variety of environments firstly
assault game see you back in your
opponents for control of the map with
the team on offense try to capture
specific objective point whilst the
defenders try to maintain control until
the timer runs down escort games see the
attackers try to move halo to the
delivery point with the defending team
again trying to foil their attempts
until the time is up hybrid assault
escort games see attackers needing to
capture a payload before escorting it to
the delivery point while in control
matches both teams must fight it out to
maintain superiority over a single
objective with the winning team being
first to win two rounds aside from
control matches when starting a new game
you will be assigned to a defensive or
attacking team for the duration allowing
you to tailor your tactics and character
choices affordable defending teams also
have additional time before the round
starts to dig in and set up but watch
out for Wiley attackers who can also
take advantage of set-up time
oops now that the basics are covered off
we can look at a few more advanced tips
to make sure you keep bagging the play
of the game award it's really important
to use your ears just as much as your
eyes when playing on your teammates will
automatically give you voice Q's when
certain events take place thank you like
I on the area brilliantly this is even
triggered when one of your pals spots
someone creeping up behind John giving
you the extra second to react and a
fighting chance to avoid an otherwise
ignominious end also listen out for the
distinctive sound of an enemy activating
their ultimate
as this can give you just enough time to
escape annihilation he might initially
be a bit overwhelmed by all the shiny
new toys you explain exciting weapons
firing modes abilities and game-changing
Ultimates but make sure you don't
neglect the Hondo melee you'll often
find yourself in close quarters with an
empty clip and only a few hit points
left to wipe from your enemies health
bar to make sure you have a finger
poised over the V key to finish them off
yeah another easy to forget basic is
your health in out support character
buffs most heroes will not regenerate
their health automatically so start
learning the locations of health packs
on each map these should be go to zones
when you just escaped fire phones it
might seem like a bit of a drag but
you'll be back in the action much more
quickly by hotfooting it to one of these
rather than dying early in the next
engagement and having to trudge all the
way back afraid that Goodyear if you're
attacking on an escort lap one little
trick the Cahills is using the payload
itself as a mobile buff wagon if you're
playing as a character with a deployable
buffle shield generator then instead of
dropping this on the floor and slowly
trundle away from its benefits you can
actually deploy it on top of the payload
so it moves with you and your team can
all continue to enjoy its warm cozy
life-giving embrace finally if you're
not overly keen on engaging in a bit of
real-life chitchat with your teammate
where it's gonna move on then the thrill
teen communication wheel is just the
ticket accessed by holding down the sea
but you can select from a variety of
messages which can really help your game
communicating the status of your
ultimate suggesting to the team that you
concentrate your efforts or even
requesting some health support can you
help keep your teams fermenting just
don't forget to say thank you
manners cost nothing and they'll make
your mother very proud
Title: NVIDIA SHIELD Accessories @ E3 2013
Publish_date: 2013-06-11
Length: 113
Views: 9762
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/u9WIcmXgjEg/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: u9WIcmXgjEg

--- Transcript ---

hey guys i'm will and we're at e3 2013.
we're at the nvidia booth and i want to
show you a couple new things about
shield you know we talked a little bit
about shield the final design the new
name and the redesign controls making it
more ergonomic and more responsive today
i want to show you what else you can get
with shield accessories so behind me
you'll see we have a custom shield
carrying case right here as well as
custom tags that you can get i'm just
going to open this bad boy up real quick
show you guys what this case looks like
of course cool little branding it's got
some zippers and you open it up and you
can see that this is a neoprene case
that's form fitted for shield you get a
little carrying pouch in here so you can
put your cables and your headphones and
whatnot and you'll notice there's a
little port right here and what that
port does is it allows you to charge
your shield while it's in the case
without taking it out of the case pretty
cool stuff it's a real nice case you got
to feel it to believe it or you can get
a custom tag for your shield to change
the look every once in a while here we
have a custom tag here in the box i'm
just going to pop it out this one
happens to be the glossy black version
and all you got to do
take your custom tag
take off the existing tag on your shield
pop it right back on and you've got a
brand new look for your shield you can
pimp it out with either the glossy black
the carbon fiber version or you can keep
the brushed aluminum look for a stock
feel so that's it that's a quick look at
the accessories you can buy with shield
at shield.nvidia.com and with our retail
partners gamestop micro center newegg
and canada computer
Title: GTC 2016: Photorealistic Virtual Reality with Iray VR (part 3)
Publish_date: 2016-04-05
Length: 507
Views: 72965
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/uAVJ3QsJ0fY/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: uAVJ3QsJ0fY

--- Transcript ---

okay so so virtual reality in these eggs
in these examples we're trying to make
virtual reality as realistic as possible
however in many different types of
applications for example design product
design architectural design realistic is
not enough it's got to be real
it's got to be photo real and in that
particular case you need a whole new
type of rendering technology so the
rendering technology that we were sawing
they were running on a GeForce GTX Titan
that performance is really fantastic
however that level of performance is
simply not enough if you want to create
something that is really photo real in
order to create something photo real you
have to follow photons as it bounces
around the room as it bounces through as
it goes through glass as it refracts
as it picks up different as the
wavelength of the light changes when it
interacts with different surfaces all of
that has to be physically simulated or
your eyes simply will not believe that
what you're looking at something is
photo real now in the case of design
it's utterly important so ladies and
gentlemen today we're announcing array
VR
I rave ER nobody would think that it's
possible this is actually a stunt that I
didn't think was possible the idea that
a supercomputer would be used to render
a scene taking several hours I mean this
is no different than Pixar rendering
their film and an entire supercomputer
is used to compute each and every frame
meticulously and it takes days and days
and days to eventually recreate that
movie for final composition this is no
different here we're creating a scene or
creating a room but we have to create it
in real time well it turns out that ray
tracing simply doesn't have the ability
to do that and so the team created a
brand-new technology they basically
rendered light probes throughout the
area of the room that you would like to
interact with these light probes are
basically entire light fields from that
spot how light would emanate from that
spot each one of these probes is
essentially a 4k render and each one of
these 4k renders takes approximately one
hour on one box of eight GPU server one
hour we render a hundred of these probes
which means a hundred hours were
dedicated to rendering the probes that
go into this particular photorealistic
room next on a workstation with
quadrille M 6000 it needs to have a
really really large frame buffer so that
all of these light probes could be
resident in the frame buffer
we then rasterize the scene from the
point of the eye to figure out the depth
and wear which one of the light probes
is the most appropriate to pick off the
sample from which is then mixed filtered
processed so that it looks appropriate
from that eye so that I is picking up
the information that was rendered from
the combination of those hundred light
probes
okay and so quadrille and 6000 does the
rasterization and does the composition
and then lastly with very very very low
latency and all the work that we did
with VR works and the integration into
the head mount displays an integration
into the engine we can now allow you to
enjoy VR in Irey now I Rea in VR now the
thing that's really really really
amazing it's just utterly beautiful and
there's something that you have to go
enjoy but I want you to be able to enjoy
here now let's take a look at this so
this is a head mount display what you're
looking at what you're looking at is
invidious new headquarter and in fact
until this was done until they did array
VR I had never been inside my building
before now this building is designed to
be energy efficient and so all the
windows that you see in the ceiling is
intended to let light in but just enough
light and the light and the angles of
those triangles are designed so that
during different times of the year the
light doesn't come into the building and
heat up the interiors of the building
making up making it uncomfortable and so
we're trying to use as much natural
lighting as possible and so the the
rendering of this building and making
sure that it's comfortable well lit and
uses as little energy as possible is
really important and look at this the
polished concrete I had never been in
our building before all of this is in
real time and it's in 3d and the
lighting is so beautiful I think I'm
gonna go ahead and finish this building
500,000 square feet it's going to house
somewhere between two to three thousand
and video employees this is their face
one and that is a building inside a
building it's the heart of the building
you come in through the center of that
heart you park underneath this building
we put all the cars underground because
you know here here in Silicon Valley are
our weather is really fantastic and so
no sense no sense reserving all that
wonderful climate just for just for cars
so we put all the cars on the ground and
eventually one of these days all the
cars will park by themselves and so
you'll just get out and your card
meanders down this ramp and finds a
parking space and you know the future
building ok I ravy are you guys this is
really incredible
one hundred light probes now rendered on
a workstation well not too many people
have supercomputers though not too many
people have super computers with array
on it you hit a button it goes off and
renders a hundred a hundred light probes
it goes off and renders light field from
a hundred different positions and then
you put that on a workstation you put
that head mount display on and it's
rasterizing in real time and then re
compositing from those hundred like
light probes to create this virtual
reality environment well this is going
to be unbelievable
for people who are doing architectural
walkthroughs it's going to be
unbelievable for people who are
designing cars people who are doing
serious serious projects however virtual
reality has taken off everywhere as you
know cardboard Google cardboard is being
handed out to everybody it was
distributed by New York Times it's
available on Samsung gear VR it's gonna
be available on mobile devices all over
the world we want people to be able to
enjoy VR irrespective of the computing
device they have and so one of the
things that we're going to do and this
is really exciting we made array the art
light this little brother of array
basically the way it works is this it's
not able to render the full 3d as
beautifully as array VR
however with just a press of a button so
long as
you design in 3ds max or Maya or
whatever design tool that has array
integrated with just the press of a
button it creates a photo sphere creates
a photo sphere and that photo sphere is
completely ray traced that photo sphere
is completely ray traced you download
you know your plugin I already plug-in
or its integrated and then you download
an android viewer and you grab yourself
a cardboard box the Google cardboard
which is fantastically clever or you get
a viewmaster with a phone stuck in
inside it you download this app and you
now enjoy VR we have this experience at
the show floor give it a try it's really
really fantastic array VR
Title: The Rise of Transformer AI and Digital Twins in Healthcare
Publish_date: 2022-11-07
Length: 1828
Views: 25358
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/uDE3tlI_7fs/hq720.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AH-CYAC0AWKAgwIABABGFMgXihlMA8=&rs=AOn4CLC9QuhXaNjfA-a0aIhNQLWr9lZ_Fw
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: uDE3tlI_7fs

--- Transcript ---

welcome to GTC
AI is the most important technology
force of our time
we have now entered the next wave of
Transformer AI now more commonly
referred to as large language models
and digital twins have quickly evolved
Beyond an art and into the possible
this week at GTC you will see Healthcare
is keeping Pace using these Technologies
to deliver new breakthroughs in
healthcare and Drug discovery
this GTC we are hosting so many
outstanding speakers
Dr Aaron Cohen a world-renowned
neurosurgeon recently awarded the Villa
Magnus award known as the Nobel Prize
for neurosurgery has a talk on the New
Frontiers in brain surgery
Johnson Johnson Chief technology officer
Rowena Yao and jansen's vice president
and head of Technology Hal Stern talk
about large-scale hybrid cloud computing
to drive patient outcomes
Johnson Johnson's Peter Clark head of
computational Sciences and Engineering
talks on Quantum Computing simulation
and pharmaceutical research
medtronic's George Murgatroyd vice
president and general manager of digital
surgery and surgical Robotics and I talk
about the present and future
applications of AI in the operating room
Tamo peria from Salk Institute has a
fascinating talk on computer vision
techniques that capture biological
movement of animals to help Humanity
and don't miss the Nvidia Clara product
team sessions that take you on a deeper
dive of all the new features and
platforms I'm going to talk to you about
today I'm honored to host you all let's
get into it we started building Nvidia
Clara over five years ago to harvest the
incredible advancements in accelerated
Computing computer graphics and
artificial intelligence and apply it to
the domain of healthcare and Drug
discovery
Healthcare is the world's largest data
industry and will continue to grow
faster than every other industry in the
world
at the heart of this data growth is new
sensors and medical instruments that
when combined with artificial
intelligence create surgical assistance
drug designers and early detection
systems
we can build domain-specific tools
libraries and platforms to accelerate
the industry's ability to deliver
advancements in healthcare and Drug
discovery
let's start with AI and Healthcare
Medical Imaging makes up 90 of
healthcare data Imaging is used from
beginning to the end of the patient
journey and the field is very
specialized
more than 10 Radiology modalities x-ray
CT Mr ultrasound that Scan 10 organ
systems with more than 10 organs in each
that might have more than 10 different
types of diseases
we're talking tens of thousands of
applications that could assist in the
understanding of what medical imaging is
capturing
this is why we joined forces with King's
College London to establish monai a
domain-specific AI framework for medical
imaging
monai is purpose built for radiology
pathology and surgical data and tackles
the entire AI lifecycle from pre-trained
Models AI assisted labeling tools
state-of-the-art training techniques
like Federated learning and
self-supervised learning and then
standardize these applications as AI
applications into easily deployed
Healthcare I.T infrastructure
monai is a community-led open source
framework with over 600 000 downloads
and half of those downloads happening in
just the last six months showing
incredible momentum
over 450 GitHub projects are building on
one eye
and when I as help researchers publish
over 150 papers
medical research centers medical device
makers and cloud services are adopting
monai
the growing one eye ecosystem and
momentum is a proof point that
domain-specific tools enable a vast and
diverse developer Community from data
scientists to doctors
we are excited to celebrate the release
of monai 1.0 packed with amazing new
features
a brand new model zoo with over 15
pre-trained models and a standard
package to work across all the monai
modules
Active Learning and intelligent AI
labeling tool
Auto 3D an intelligent model selection
and training framework
and when I Federated learning that
integrates Nvidia flare for
collaborative privacy preserving model
development
and Native support for streaming
modalities like endoscopy ultrasound and
surgical video
one of the most powerful tools when I
has is Moni label an AI assisted
labeling tool that has a new feature
called Active Learning
Active Learning serves to build better
data sets in a fraction of the time it
would take humans
when I label Active Learning
automatically reviews and labels large
data sets
it Flags image data that requires human
input and then with a single click uses
the human labeled data to retrain the
model
this Loop reduces the labeling
requirements by up to 75 percent
a win saving the radiologist time and a
win for the data scientists improved
model performance
to make it easy for you to use monai
Active Learning it's been integrated
into six of the industry's most popular
viewers that support multi-modalities
3D slicer ohif pair for radiology qpath
and DSA for pathology and cvat for video
monai is an incredibly unique
domain-specific platform that brings
data scientists and doctors together to
build important AI applications across
Healthcare
here are a few inspiring stories
in surgery in general and neurosurgery
specifically
you can fix what you can see therefore
visualization is critical in our work if
you're able to interact and visualize
that patient's images you are better
able to plan the correct intervention
and make sure that you apply the right
intervention the first time there's a
way to see monai as a toolkit which is
dedicated to AI in imaging
so monai is a Community Driven effort to
bring data science into the clinic and
and that's everything from the the
development of of data driven products
and the curation of data to then how we
get those insights in front of
clinicians looking after patients since
adopting the monai infrastructure it's
freed us to focus on application to
specific clinical problems and catalyzed
that application in multiple directions
well we can do throw monai is they'll be
able to segment the tumor segment all
the critical structures around it and in
surgery provide real feedback data to
navigate this surgeon around the tumor
to make sure most of the tumor is
removed therefore we're providing the
patient with the most benefit what Moni
then brings is the final layer on top
which is a way to scalably
apply artificial intelligence across
that entire medical record across every
patient pathway the potential scale of
that is is massive I can see when I in
every clinic in every hospital across
the world
[Music]
to experience more knife for yourself
on Nvidia Launchpad a monai label lab
lets you see how quick and easy it is to
create annotated data sets and build an
AI annotation model for labeling
Nvidia Launchpad is NVIDIA hosted
infrastructure with a catalog of
Hands-On Labs go try it out
Medical Imaging is the essential tool of
healthcare
Dr Cohen said it beautifully you can fix
what you can see
Medical Imaging gives us the ability to
see inside the body
2D for screening and early detection
but the human body is volumetric and it
is constantly in motion
so 3D imaging provides spatial
understanding
quantitative measurement and
segmentation
while 4D introduces temporal information
that elucidates Anatomy function
essential to diagnosis and planning
treatments
using Imaging combined with real-time
deep learning and computer vision we
enter into The Fifth Dimension
to perceive surroundings and navigate
inside the human body to plan actions
and track objects
to understand the Dynamics and perform
surgical tasks
this is imaging's next Frontier
and has contributed to Innovations in
minimally invasive and robotic assisted
surgery
each evolution of Imaging applications
was enabled by combining breakthroughs
in sensor technology and Powerful sensor
processing technology
we have been working with the medical
sensor ecosystem for over 15 years
and now to enable this new world of
real-time AI sensing we are announcing
the Nvidia igx Edge AI platform
an all-in-one platform that combines
Nvidia or in robotics processor
ampere tensor core GPU connect X7
streaming i o processor and a functional
safety Island and safety microcontroller
unit
Nvidia igx is the ideal Computing
platform for Imaging robotics pipeline
streaming sensor processing
reconstruction detection and
segmentation and real-time visualization
now all future medical devices can take
advantage of the breakthroughs that
enabled self-driving cars
to be powered by Ai and become software
defined
the igx system is a Micro ATX form
factor
perfect for embedded medical devices and
streaming Edge AI servers
a rich ecosystem of medical embedded
odms and oems are designing igx medical
grade systems
the igx platform comes with commercial
grade operating systems
built-in security and management
safety extension package with long-term
support
running on igx is NVIDIA Clara holoskin
delivering a real-time AI Computing
platform for medical devices
Clara holoscan is an application
framework that runs the robotics
pipeline
streaming data processing image
processing Ai inferencing and
visualization at super low latency
from 10 milliseconds from sensor to
screen
Hollow skin on igx is production ready
with medical grade 62304 documentation
covering the entire stack of commercial
OS drivers Nvidia Ai and reference
Pipelines
the Clara holoskin platform gives the
medical device industry a unified and
common platform for real-time Edge
processing saving huge amounts of
hardware and platform and software
engineering making more resources
available to develop life-saving
Healthcare applications
we give developers a huge Head Start by
partnering with the sensor connectivity
ecosystem so holoscan can easily be
connected to all types of light video
and ultrasound
in the latest holoscan SDK release there
is a super low latency video pipeline
that takes 4K 240Hz camera input and
performs AI inference and visualization
with the results in less than 10
milliseconds
and scale is built in
Hollow scan on igx easily supports over
15 simultaneous AI video streams and can
deliver over 30 simultaneous AI
inferences all under the industry
standard of 50 milliseconds or less
achieving real-time human perception
over 70 leading medical device companies
and startups and medical centers are
developing on Clara holascan
holoskin is being adopted by the leaders
in the industry
for applications like Siemens health and
ears nuclear Imaging system that auto
adopts the CT acquisition with smart
zooming reducing the radiation dose and
improving image quality
intuitive's ion is a robotic assisted
lung biopsy platform that uses fiber
optic real-time shape sensors measuring
hundreds of times per second providing
precise location shape and orientation
information to reach otherwise
impossible locations in the lung
and Olympus the world's largest
manufacturer of endoscopes commanding
over 70 percent of the global market is
using Clairol Hollow scan to develop
their next Generation AI endoscopy
platform
today we are delighted to announce Next
Generation surgery systems have chosen
Claire hallskin on igx
active surgical's active Edge platform
is powered by Clara holoskin on igx
to process their unique hyperspectral
sensor information and deliver real-time
perfusion imaging for blood flow and
physical structure visualization to
surgeons during the surgery
Moon surgical is adopting Clara to
deliver its next Generation robotic
assisted Maestro system
a single platform surgical robot that
provides surgeons with enhanced sensory
assisted scope control and operating
room perception
proximi is an operating room
telepresence
they've adopted Clara hallskin deliver
real-time remote surgeon collaboration
that is both secure and safe to use in a
clinical environment
robotic systems require not only a
Computing platform to run the robotics
pipeline in the physical world
that is Claire Hollow skin and igx but
also a Computing platform to deliver the
robotics application in the digital
world
Omniverse on ovx
we are working with Dr Cohen and the
atlas team to realize the incredible
potential of digital twins for surgery
in this demo I'm about to show you we
focus on the power of digital twins as a
tool for medical device makers to
validate their AI applications with
Clara hollowskin and igx Hardware in the
loop
digital twins created an Nvidia
Omniverse can model physically accurate
Hospital environments operating rooms
medical instruments and even a patient's
individual Anatomy medical device
developers can use these digital twins
to create AI enabled Solutions one use
case is Neurosurgery
during a procedure a surgeon needs to
manually reposition a robotic microscope
to maintain focus on the region of
interest and align it with their
instruments this prolongs the surgery
and increases a patient's time under
anesthesia let's see how this can be
improved with an ai-based solution using
a digital twin we start with an Nvidia
igx computer running the Clara
hollowscan SDK which includes a
pre-trained tool segmentation model
real-time inferencing performance is
validated with virtual scenes from
Omniverse
in deployment the igx platform has been
optimized for AI processing and
visualization of high bandwidth data
streams for example 4K 60fps video is
processed in less than 15 milliseconds
without AI tracking the microscope will
not maintain alignment with the
surgeon's tools with tracking the igx
platform automatically tracks the
position of the surgical instruments and
controls the real-time movements of the
microscope this can reduce the patient's
time in the OR and improve surgical
efficiency
the igx platform is optimized for AI
processing and visualization even on
high bandwidth data streams with igx and
Clara hola scan you can build test and
deploy all on a single platform
foreign
is a continuous cycle of generate build
deploy and simulate
and what we call the AI Factory
where AI development and application
testing can first be done virtually
before deployed in the real world
digital twins will be an essential
pillar of the medical device company's
future AI factories
now let's jump into the world of AI drug
discovery
genomics has ignited the digital biology
Revolution with the cost of sequencing
outpacing Moore's Law
the production of sequencing costs has
kicked off genomics programs across the
world
from sequencing newborns to population
programs like the United States all of
us program including one million
Americans
but sequencing is just the first step
sequence analysis requires computers
data science and AI to read and
understand the genome
with the end of Moore's Law new
Computing approaches need to be
developed to lower the cost of analysis
and unlock the full potential of the
genome
sequence analysis is complicated and
computationally intensive Nvidia Clara
parabricks is a suite of software tools
and composable pipelines delivering GPU
acceleration and deep learning at every
step
deep learning is now becoming the
standard for base calling due to the
speed and accuracy improvements
neural networks interpret image and
Signal data generated from the
instruments and infer the three billion
nucleotide sequences of the human genome
in our latest generation Hopper
architecture we're able to optimize the
inference by five times from current
ampere architectures
the next computationally challenging
step is alignment to find the best match
among a pair of sequence reads
a step that has repeated hundreds of
millions of times
on Hopper we have a dynamic programming
API that dramatically accelerates
alignment algorithms such as Smith
Waterman
here again we provide a 7x speed up over
ampere
a critical stage of sequence analysis is
identifying single nucleotide
differences
small insertions deletions or complex
rearrangements that differ in a patient
Sample versus a reference genome
new variant callers use deep learning to
increase both speed and accuracy and
help doctors determine what genetic
disease a critically ill patient might
have or they can look across the entire
population to discover new drug targets
Nvidia Clara parabricks offers the
highest cost performance on variant
calling pipelines reducing CPU
turnaround times from days to hours and
reducing the cost of the analysis by
over half
Clara parabricks is helping to enable
the next wave of genomics
powering both short and long read
sequencing platforms with accelerated
and AI based calling and variant calling
Oxford nanopore Technologies is the
world leader in Long read sequencing
uniquely enabling real-time sequencing
just last week Seattle Children's
Hospital used prometheon to rule out a
genetic disorder in the first three
hours of a newborn's life
ultimate genomics is targeting whole
genome sequences
at as little as a hundred dollars
singular genomics is the fastest bench
top sequencer an MGI g400 has very high
accuracy with adaptive sequencing
and to make Clara parabricks easy to
access it's also available in public
cloud services around the world from
Alibaba AWS Baidu Google and Oracle
to genomics Cloud platform providers
like Agilent DNA Nexus UK biobank g42
and lifebit
and today we are announcing that we are
partnering with the broad Institute to
make Nvidia Clara available in the Terra
Cloud platform
the broad Institute is one of the top
medical research institutes in the world
and the largest producer of human
genomic information
Nvidia clear repair Bricks now gives
Tara's over 25 000 researchers
accelerated whole genome sequencing in
24 hours down to an hour cutting their
compute costs in half
the broad institute's genome analysis
toolkit gatk used by over a hundred
thousand researchers
has also got a new deep learning AI
model to improve accuracy coming in the
October release
partnering with the broad we are
co-developing AI models to give
researchers new AI tools that understand
the language of biology and to Aid in
the discovery of new medicines
let's hear a bit more from Anthony and
Claire at the broad
[Music]
we live in a moment in time when the
most important problems in biology and
Medicine require a convergence between
life scientists and data scientists the
broad Nvidia collaboration is an
Exemplar of how this can work
with Nvidia we'll be exploring
applications
one of the really important
information from Clinical records
large language models allow us to
analyze these notes and understand the
trajectory of disease with unprecedented
resolution
similarly in biology there's another set
of languages that we care about deeply
the language of DNA RNA and protein just
as we train large language models to
analyze human text we can train these
same models to analyze the language of
life
tarot is a platform for storing managing
sharing and analyzing biomedical data
Our Roots are really with genomics data
but over time we've added a lot of
different data types to the platform and
by bringing together different kinds of
data we can enable much more interesting
scientific outcomes this partnership
with Nvidia will create greater access
to those types of multimodal data
analysis and bring that to a wider group
of people who wouldn't necessarily have
access to those sophisticated techniques
previously and something as complex as
the human body cannot be acid with a
single modality we don't just use
Chester X reads or EKGs or DNA
sequencing we need to have a variety of
different tools to interrogate
physiology and health and disease
one of the great opportunities of modern
machine learning is to develop
multimodal representations of the human
body
the partnership between Broad and Nvidia
represents an exciting step forward
where our two organizations will partner
together deeply to integrate diverse
genomic and clinical data sets into
representations of how disease occurs
and how we can better treat it
okay
large language models are helping us
understand the language of biology
Alpha full demonstrated the ability to
predict 3D structure proteins with
experimental accuracy
replacing expensive and complicated
experimentation
esm1 and protrans models demonstrated
that llms trained on unsupervised
learning objectives can learn protein
structure from sequence alone
a stream of papers and model show we are
just scratching the surface on the size
and predictive power of protein language
models and research is demonstrating
that bigger is better
Nvidia developed a supercharged
framework called Nemo Megatron
that uses model and data parallelism to
scale llms to hundreds of billions and
trillion parameter sizes with the new
Hopper architecture
we can now reduce training time from
months to days
with biomolecular data sets exploding
from sequencing and lab automation many
petabytes of unlabeled data sets are
being generated
luckily digital biology data comes with
its own language amenable to
unsupervised learning approaches and
large language models
for DNA it's nucleic acid sequence for
proteins amino acid sequence and for
chemicals it's smile strings
today we are announcing a
domain-specific framework bio Nemo for
researchers and developers to develop
new pre-trained large language models at
any scale and with any type of
biological sequence
we're including pre-trained models esm1
Pro T5 and mega mobile
bionemo is a domain-specific
supercomputing scale framework for
biology leading Pharma protein
engineering and research labs are early
adopters of Nvidia bionemo sign up for
Early Access in October the chemical
space has 10 to the 60 possible
drug-like molecules and the protein
space is even larger at 10 to the 130.
drug Discovery researchers are searching
chemical space for molecules with
specific properties
synthetic biologists are looking for
proteins with specific functionality
developers of biologics are looking for
antibodies with specific binding
characteristics
today's numerical methods limit the
number of interactions we can compute in
the future these llms can be run in an
end-to-end AI drug Discovery pipeline
generative models like mole flow and
mega mulbart generate thousands of
optimized molecules in a second and
infer thousands of representations of
molecules in seconds
then using the likes of open fold and
Alpha fold predict the 3D structure
creating the two necessary ingredients
to provide an AI model like equibind
that predicts how drugs interact with
proteins
with a nearly infinite drug Discovery
Space given the possible combination of
proteins and chemicals
and the fact that we only approve about
50 drugs each year at a cost averaging a
billion dollars
large language models give us a new tool
to explore the infinite World of
biomolecules and chemistry
but llms take weeks to months to train
and setting up environments is really
hard
so today we are announcing bio Nemo llm
service providing researchers access to
pre-trained chemistry and biology
language models first up will be esm1 to
infer representations of amino acid
sequence and open fold to predict
protein structure from sequence
Mega malbar and pro T5 coming shortly
thereafter
the output of bionemo can be used for
Downstream tasks such as generating new
proteins and chemicals or predicting
structure function or reaction
properties
Early Access for bionemo service is in
October increasingly the world of AI
applications will be a hybrid of
large-scale AI Frameworks and optimize
inference services
simile AI workloads need a hybrid
infrastructure to optimize for scale and
cost
Johnson Johnson is the largest
Healthcare company in the world spanning
Pharmaceuticals to medical devices
thousands of data scientists and
researchers spread across 35 countries
need access to the world's most advanced
Computing platforms to drive Discovery
and innovation
Johnson Nvidia have a strategic
collaboration building a hybrid
multi-cloud that offers gpus as a
service so that data scientists can
focus on developing and training AI
models not where they should be run
Johnson and Johnson's hybrid cloud is
built on Nvidia AI serving the most
advanced AI workloads seamlessly from
on-prem gpus to gpus across multiple
public clouds AI is the most important
technology breakthrough of our time
the time to make decisions in healthcare
delivery can be shortened if we apply AI
to help us see what we capture in
medical imaging
monai 1.0 is an important milestone for
the industry
Nvidia Clara hollowskin running on igx
gives the industry a real-time Edge AI
platform to become software defined
Next Generation surgery platforms active
surgical Moon Surgical and proximi have
chosen Hollow skin on igx to deliver AI
to the operating room where every second
counts
we have over 10 000 diseases with only
500 cures
genomics has ignited the digital biology
Revolution
Nvidia Clara parabricks is kicking off
the next wave of genomics with new
instruments and Cloud analysis platforms
large language models are the next wave
of AI Nvidia bionemo framework and the
service will accelerate the development
of AI that understands chemistry and
biology
we are excited to be partnering with the
broad Institute to make Clara available
to the 25 000 users of the Terra Cloud
platform and work alongside the
world-renowned researchers to develop
artificial intelligence that understands
the language of DNA
thank you and have a great GTC
Title: GTC November 2021 Keynote Highlights with NVIDIA CEO Jensen Huang
Publish_date: 2021-11-10
Length: 162
Views: 28873
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/udmMNu2U8vY/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: udmMNu2U8vY

--- Transcript ---

Welcome to GTC!
We covered a lot today, so let's get started.
NVIDIA accelerated computing is a Full-Stack,
Data Center Scale, and Open Platform.
Today we are announcing the NVIDIA Quantum 2.
The first networking platform to offer
the performance of a supercomputer and
the share-ability of cloud computing.
Cybersecurity is a top threat of companies and
nations. We are announcing BlueField DOCA 1.2.
The leading cybersecurity companies
are working with us to provision their
next generation firewall services on BlueField.
The next wave of AI is
enterprise and industrial edge.
We highlighted three important
technologies needed to enable edge AI.
Unified Computing Framework is
built for robotics applications.
Clara Holoscan is a new software-defined medical
instruments platform and runs in the data center or
Orin, our new superfast robotics processor.
The new Maxine connects
computer vision, RIVA speech AI,
and avatar animation and graphics
into a real-time conversational robot.
Our Metropolis engineers used Maxine
to create Tokkio, a talking kiosk.
Our Drive engineers used
Maxine to create Concierge.
We've been building speech synthesis with
my own voice. A conversational AI, Toy Me.
Another demo showed Maxine as
a video conferencing avatar,
doing simultaneous multi-language conferencing.
And Omniverse, our virtual world simulation engine,
was a common thread throughout our entire keynote.
Drive Sim Replicator is a synthetic data generator
for autonomous vehicles and is built on Omniverse.
Robots, autonomous vehicle fleets,
warehouses, factories, industrial plants,
and whole cities will be created, trained,
and operated in Omniverse digital twins.
Better optimization is vital to the $10 trillion
logistics industry at the heart of the world's supply chain.
Accelerated computing launched
modern AI and the waves it started
are coming to science and the world's industries.
I have one more announcement.
Earth Two. The digital twin of Earth,
running Modulus-created AI physics,
at Million-X speeds, in Omniverse.
All the technologies we've invented up to this
moment are needed to make Earth Two possible.
I can't imagine a greater and more important use.
See you next time.
Title: SHIELD: Explore the amazing world of Android gaming on NVIDIA SHIELD
Publish_date: 2015-08-10
Length: 122
Views: 54592
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/UF1LPT136ds/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: UF1LPT136ds

--- Transcript ---

shield is the most powerful Android
gaming device you can find and we've
built in some special features to make
your gaming experience amazing you could
download hundreds of Android games from
Google Play but to find the best games
go to shield hub download games here
you'll find some of our favorites
including shield optimized and shield
exclusive titles we've organized the
games into a few categories so you can
find something for everyone to see a
list navigate to the left we're gonna
take a look at games that support
multiple controllers let's take a look
at one of our favorites juju from here
you can download the game directly from
the Play Store or open it right away if
you already own it for two-player action
you'll need to connect another
controller to the system just go to the
home screen scroll down the settings and
over the shield accessories in this menu
we'll see a list of all the connected
accessories and their current status now
we can connect a new accessory touch and
hold on the Nvidia button on the new
controller until it starts to blink in
just a few seconds you'll see the green
checkmark to get back into the game just
hit the home button all of your
installed games appear in a row right
above the apps and because shield
supports Google Play game services you
can stop your game at any time and
resume it again on your phone or tablet
without losing your place or your
achievements
that's a quick look at Android gaming
which is just one of the many ways you
can enjoy great games on shield have fun
Title: Lords of the Fallen and NVIDIA GameWorks Technology
Publish_date: 2014-10-27
Length: 121
Views: 111829
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/UFlQ9CtLi-0/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: UFlQ9CtLi-0

--- Transcript ---

[Music]
Lords of the Fallen is a brand-new
action RPG by CI games the game follows
harken a convicted criminal who's been
given a chance for redemption by
stopping a war with an army of a
defeated God the game plunges the player
into fast-paced action using powerful
magic alongside a complex and satisfying
combat system Lords of the Fallen was
created on the PC platform and most of
the visual effects suite for the custom
game engine was built using Nvidia game
works technology game works enables
developers to quickly push the limits of
gaming by providing easy to integrate
libraries physics particles enable
artists to easily provide a much more
immersive experience by using physical
particles that can interact with their
environment this allows effects like
interactive fog which moves if
characters walk through it additional
particle interaction can also occur
through wind or explosive fields
likewise combat sparks from weapon
blocks bounce off the environment and
persist leaving a lasting result of the
parry the characters clothing throughout
the game appears very natural thanks to
physics cloth it is also used on the
many flags and banners encountered
throughout the game magic also plays a
significant role in Lords of the Fallen
and is brought to life with physics
particles
the game also features Nvidia turbulence
for the magical effects in this boss
fight to make it look more natural
Lords of the Fallen is a great example
of how game works enables developers to
create more compelling in Richard Conte
you
Title: Solar-Powered Hydrofoil Boat Designed with Quadro GPU
Publish_date: 2015-07-08
Length: 100
Views: 13789
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/Uigdqj94pZ8/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLCW3drl9o0nUlqc84tsB9-vyWmeYg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: Uigdqj94pZ8

--- Transcript ---

the theory of salivating is a student
team which consists of students of
nearly all faculties of Delft University
of Technology when combining the
knowledge of all these diverse students
the team is able to build hydrofoil
boats surly pirates on solar energy
creating the soda boat every individual
parts goes through three phases one is
design the second is the prediction and
the last one is the test or validation
where you see if your original design
meets your requirements a students from
the TU delft we're always looking at the
most innovative products that the market
has to offer and we can incorporate all
those innovations into one package so we
have the best part about
the goal of the syllable team is to
always improve and to have the most
innovative and fastest early bird out
there because the competition is hard
and we can only make that happen with
the help of our sponsors
Title: NVIDIA GameStream
Publish_date: 2014-04-01
Length: 83
Views: 49012
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/ukxcv5Z_6Zc/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: ukxcv5Z_6Zc

--- Transcript ---

hey this is andrew from nvidia today i
want to talk about game stream an
awesome technology that allows you to
enjoy gtx quality gaming on your Nvidia
shield game stream will allow you to
play your favorite game stream directly
from your GeForce GTX powered PC first
you can game stream within your home say
you've been playing on a GeForce GTX
powered PC you can seamlessly stream
your gameplay action to your shield
portable anywhere in the house that
means all the same high performance GTX
class gaming in the palm of your hands
and then if you want to play with your
shield on the big screen you can in
console mode game stream your shield
content directly to your big-screen TV
using a Bluetooth controller all from
the comfort of your living room with
Nvidia shield you can take your PC games
on the go so say you've been enjoying
your GeForce GTX PC gaming experience at
home with the Wi-Fi or another internet
connection like a hotspot tether or MiFi
you can stream your favorite PC titles
to your shield so as you can see Nvidia
shield with game stream gives you a ton
of unique ways to enjoy smooth stunning
low latency GeForce GTX game
[Music]
Title: Building AI-Powered Virtual Collaboration and Content Creation Solutions with NVIDIA Maxine
Publish_date: 2021-04-13
Length: 125
Views: 22168
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/UOEpRAD57Vs/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: UOEpRAD57Vs

--- Transcript ---

[Music]
hi everyone i'm alex and i'm the product
manager for nvidia maxing
these days we're spending so much time
doing video conferencing
don't you want a much better video
communication experience
today i'm so thrilled to share with you
invading
a switch of ai technologies that can
improve the video conferencing
experiencing for everyone
when combined with invaded jarvis maxine
offers the most accurate speech to text
see it's now transcribing everything i'm
saying all in real time
and in addition with the help of jarvis
maxine can also translate
what i'm saying into multiple languages
that makes international meetings so
much easier
another great feature i'd love to share
with you is maxine's eye contact feature
oftentimes when i'm presenting i'm not
looking at a camera
when i turn on maxing's eye contact
feature it corrects the position of my
eyes so that i'm looking back into the
camera again
now i can make eye contact with everyone
else the meeting experience just gets so
much more engaging
last but definitely not least magazine
can even improve the video quality
when bandwidth isn't sufficient now
let's take a look at my video quality
when bandwidth drops to as low as 50
kilobits per second
definitely not great maxine's ai face
codec
can improve the quality of my video even
when bandwidth is as low as 50 kilobits
per second
the most powerful part of maxing is that
all of these amazing ai features can run
simultaneously together
and all in real time or you can just
pick and choose a film
it's just that flexible now before i go
let me turn off our maxing features so
you can see what i looked like
without maxing's help not ideal now
let's turn back on
all the maxing features see that's so
much better
thanks to nvidia maxine
Title: NVIDIA CEO Jensen Huang keynote address at GTC China 2019
Publish_date: 2019-12-19
Length: 8348
Views: 99900
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/uPOI4T2SwOo/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: uPOI4T2SwOo

--- Transcript ---

I am AI I am a creator
freeing our imaginations
and breathing life into our wildest
dreams
I am a visionary
anticipating the needs of others
simplifying our busy lives
and bringing us closer
[Music]
I am a protector
the sink creatures out of harm's way
watching over
as we explore the world
and helping our heroes make it home
safely I am a human
revealing more together
picture
precision
an innovative
finding smarter answers to complex tasks
and working in harmony
[Music]
I am even the composer of the music you
are hearing
[Music]
a
brought to life by Nvidia deep learning
everywhere
[Music]
ladies and gentlemen please welcome
nvidia founder and CEO jensen huang
[Applause]
- aha point eyes are like Nvidia GT see
that's all the Chinese I'm going to
speak one day I will be able to speak to
you with a universal translator I will
speak in English you can hear in any
language you choose but today I'm gonna
have to speak in English I have so much
to tell you because we've been very busy
this year so let me get started and
Vinnie as 25 years old we've dedicated
our company to build computers to solve
problems that normal computers cannot
solve we built computers for the
Einsteins the Michelangelo's the
Leonardo's of our generation we built
our computers for you and this is your
conference GTC this year sixty-one
hundred registered attendees growing
250% in just three years time no
conference in the world covers this much
about the future of technology and
future society the work that is being
shared here is incredible look at all
the topics artificial intelligence
inference cloud tools data science edge
computing automotive autonomous machines
gaming 5g rendering design finance
high-performance computing healthcare
life sciences graphics virtualization
artificial intelligence frameworks and
industrial applications when you say
that all out loud in one sentence it is
really quite mind-boggling the impact
the computing technology we have created
and that we have done together I want to
thank you all for that
particularly I want to thank all of our
partners and sponsors for making GTC
possible so I want to please join me in
thanking them for helping us and
supporting us for GTC accelerated
computing is now recognized as the path
forward as Moore's law comes to an end
it is very clear we need another
computing approach accelerated computing
that we have been pioneering for over 20
years is a very sensible and logical
approach use the right tools for the
right job a program has sequential and
parallel components run the sequential
part on the CPU run the parallel
components on a processor designed for
parallel processing that was the great
insight of our company to use the right
tools for the right job not to replace
the CPU but to accelerate the portions
of the application that another
processor could do much better
however accelerated computing starts
with an amazing chip it starts with a
revolutionary processor we call the GPU
however that's just the beginning
accelerated computing is a full-stack
engineering challenge in order to gain
the benefits of accelerated computing
you have to re-engineer and re optimize
the stack from the processor to the
design to the algorithms the system
software the tools and even the
applications by working together as an
ecosystem we've refactored the most
important applications in the world and
have achieved speed ups that are simply
unimaginable 10 times speed up 25 times
speed up 500 times speed up sometimes
incredible speed ups the journey of
refactoring the software allows us to
scale the performance beyond the
processor itself so that now the system
can contribute
- accelerating the application as a
result nvidia has been a system
architecture company thinking about
accelerated computing at the very large
scale even the data center scale but the
single most important thing about
accelerated computing is architecture
and the architecture part is one
architecture it is so important for
developers to have one coherent
consistent reliable architecture that
they can look forward to and develop
software to engage the install base
since the very beginning of CUDA we have
dedicated ourselves to be consistent and
have one CUDA and CUDA everywhere every
single GPU that Nvidia produces whether
it's for gaming self-driving cars cloud
supercomputer your laptop your desktop
even your embedded systems has one
architecture one single architecture all
compatible as a result of that a
developer can dedicate themselves to
improving the software stack knowing
that 200 million people can benefit from
it there are now over 1.5 million CUDA
developers around the world the number
of CUDA downloads each year continues to
grow and it's really really quite
exciting it is very clear we have
reached the tipping point and this
architecture is here to stay
each one of the developers when they
optimize the performance benefits the
entire install base this positive
feedback system is the reason why we are
now seeing such incredible embrace for
invidious architecture but this is just
the concept the reality is the reality
is when you have one architecture you
are future proof when you have one
architecture the software continues to
improve when you have one architecture
your computer will continue to improve
over time even though you purchased it
long ago
our company has dedicated ourselves as I
mentioned to a full stack this full
stack is complicated if you look at the
software that's on this full stack it's
some of the most incredible software
that is available in computing today
CUDA of course we are in version 10.2 -
think that the market texture is
available since 10.2 and every single
application that's developed for CUDA
will run in computers that we have not
built yet and every software that is
written to the future will run in CUDA
today that capability is really quite
remarkable on top of it however is some
of the richest computational algorithms
and libraries in the world today this is
our in fact my greatest source of pride
the fact that this computing stack has
benefited so many people in so many
fields of science in so many industries
is really quite phenomenal this year
alone we reduce we released enhanced new
and enhanced 500 different libraries 500
different libraries this year alone and
in each case in each case the computer
got faster for example without changing
the computer training performance
improved by a factor of four in two
years without changing the computer just
the continuous refinement of the
software in the libraries between us and
our developers enabled the install base
to quadruple its performance in just two
years in the case of inference something
that we've been working on for the last
couple of years doubling in just one
year the computer didn't change the
silicon didn't change the GPU didn't
change the software and across this
entire stack allowed performance to
improve by a factor of two what took
many weeks what took many weeks with GPU
acceleration became days what took a
couple of day can now be done in just a
few hours this continuous refinement of
software this continuous dedication to
one architecture is what has put us here
today and so I want to thank all of you
and all of the developers around the
world for working with us to
continuously improve the stack thank you
very much
[Applause]
nvidia innovates at the intersection of
three main technologies we innovate at
the intersection of computer graphics
high-performance computing or scientific
simulations and artificial intelligence
nvidia is a simulation company we
simulate the world we simulate physics
we simulate human intelligence we
simulate the world we are a simulation
company at the core and we do it so fast
new applications are possible today my
talk is going to be in three major
chapters first chapter is computer
graphics then I'm going to talk about
high performance computing and then I'll
talk about one of the topics I'm sure
many of you are interested in artificial
intelligence and some of our work that
we're doing there computer graphics last
year we reinvented redefine the future
of computer graphics by realizing a long
term dream of making real-time ray
tracing possible a dream of one of our
researchers 35 years ago last year we
introduced the technology that redefines
how computer graphics will be done we
call it Nvidia r-tx with r-tx we can
simulate light in a much more natural
way as a result shadows reflections and
all of the subtle beautiful things that
we see in the world are made possible
and made possible
in a very natural way without all kinds
of of course software optimization that
is necessary like light probes or
reflection probes and pre-baking with
global illumination all of those
techniques that make modern computer
graphics attractive but it also makes it
inflexible I'm about to show you a
demonstration of a very simple game on
surface this is minecraft
it is the single most popular game in
the world hundreds of million
subscribers 300 million here in China
alone a hundred million active players
each month 100 million active players
each month the reason why minecraft is
so incredible is because it allows you
to create any world you like minecraft
is really a world simulator and that's
one of the reasons why we love it so
much but minecraft is created by you you
create the world which means that it's
impossible to Bri bake with offline
computer graphics to improve the visual
quality everything is done in real time
and as a result we have to simulate the
light in real time we simulate the light
we simulate the way it bounces in the
environment we simulate the way and
interact with the materials the way in
reflects to refracts as a result
minecraft can be beautiful but it is
computationally intense what I'm about
to show you is something we've been
dreaming for a long time and it's all
completely in real time now this is a
project we're working on with Microsoft
and hopefully over the course of the
next several months we'll be able to
allow more people to enjoy it ladies and
gentleman let's take a look at a video
of the work that we've been doing in
Minecraft
[Music]
[Music]
[Music]
[Music]
what do you guys think minecraft the
amazing creativity of gamers and NVIDIA
r-tx
coming together for that incredible
experience there are so many amazing
game developers here in China and
they're doing beautiful beautiful work
and they're telling wonderful stories
there's just a few to show you but it is
very clear now that RT X has taken off
the ability to calculate in real-time
reflections and shadows make everything
more realistic without artificial light
probes and environment probes and
pre-baking of light using global for
global illumination we can make it so
easy for these video game worlds to
appear like it is in the real world the
reflections are wonderful the shadows
are wonderful these are just incredible
games I can't wait to see them it is now
a foregone conclusion that ray-tracing
is the future and RT x has been a home
run
we have redefined computer graphics this
next one I'm going to show you is really
quite amazing this video game was
created by one developer I don't mean
one developer studio I mean one person
it is not possible to do this unless you
have an army of artists to help you do
light probes reflection probes pre-baked
lighting for global illumination and
ambient occlusion this is simply not
possible with just one person but it is
now with r-tx ladies and gentlemen let's
take a look at this
[Music]
[Music]
ladies and gentlemen bright memory this
is the work this is the work of one
extraordinary developer but of course it
is one developer I want to show you
something else so last couple of years
been working on this project called max
Q max Q we've always imagined wouldn't
it be amazing if we had our gaming
system with us all the time
unfortunately gaming requires powerful
pcs and powerful pcs tend to be large
when we started working on notebooks
gaming notebooks they look like this
they were quite large
they were quite large so that the GPUs
which are very powerful requires a lot
of thermal and electricity and therefore
the system's tend to be quite large but
today we love computers that are thin
and sleek and beautiful and so we set
ourselves up dedicate ourselves to
create a whole new category of products
we call max Q from the architecture to
the design to the system software and
the system engineering that goes in with
all of our partners we made it possible
to put the highest end GPUs into sleek
beautiful gaming notebooks the success
has been incredible the success has been
incredible just this last year we sold 5
million gaming laptops here in China
alone this is now unquestionably the
fastest-growing new gaming platform let
me show it to you this is one example
right here thanks ball guys you guys saw
that this is a bright memory look at
this can you guys see this ray-tracing
in my hand
what was previously just 2 years ago
completely impossible to do now we're
doing ray tracing in my hand
and because I've been working out I
could do this for a long time Paul thank
you okay max q we don't just want to put
incredible gaming on a notebook we also
want to help everybody who has a weak PC
to enjoy amazing games we estimate that
the Nvidia installed base of active
gamers about 200 million there are 200
million gamers who are enjoying
beautiful games on their PC some of them
most of them on desktops some of them
increasingly will be on notebooks
however we estimate some 800 million
gamers whether they have low-end pcs
their laptops are not powerful enough or
don't have the necessary graphics maybe
they have a MacBook maybe they have a
Chromebook and the PC games they want to
play don't play on those computers and
so we've been working on cloud gaming
for some time I'm super excited to
announce today that the largest game
publisher in the world one of the great
internet companies in the world
Tencent is going to launch a new cloud
gaming service they call start and it's
going to be powered by Nvidia
we're going to extend the wonderful
experience of PC gaming to all of the
computers that are underpowered today
the opportunity is quite extraordinary
they're starting to do beta trials all
over China and we can extend PC gaming
to the other 800 million gamers in the
world that aren't possible today so let
me welcome Tencent and let's thank them
for their support look forward to their
service rate racing and video games of
course is going to make it more
beautiful more fun the special effects
are going to be more incredible one of
the applications where people would have
thought that Nvidia would always have
been part of is film quality rendering
film quality rendering photorealistic
rendering has historically been only
done on CPUs and the reason for that is
because the program is too complicated
it is too large the algorithms
unfortunately did not map well to
fixed-function graphics and even
programmable shaders had its limits
until we created the Nvidia r-tx with
Nvidia r-tx we can now do ray tracing in
a general way and so with it path
tracing ray tracing rasterization and
all of various hybrid techniques of
computer graphics are possible we've
been working with all the world's
leading developers to bring the most
important rendering software packages to
Nvidia r-tx and today I'm delighted to
announce that the three top rendering
software packages in the world are now
r-tx accelerated Autodesk chaos group V
rip and blender the open source blender
ladies and gentlemen let's welcome them
to the RT X these rendering packages
will run out of the box on the brand-new
systems that we've created we even
created special line of computers for
creators
they're underserved for a very long time
and yet the creative platform is so
computationally intensive everything
from the amount of day that they have to
operate they have to work on the
rendering software they run it is a
computationally intensive application
and so we created a whole line we called
Nvidia studio the Nvidia studio ranges
all the way from laptops beautiful
laptops sleek and thin to desktop
computers to powerful workstations with
four r-tx 8000 GPUs each with 48
gigabytes of memory connected into one
system or a server with 8 r-tx 8000 GPUs
this entire range completely
architectural II compatible every
application runs on every computer and
every application is now incredibly sped
up and so we would like to make it
possible for all the developers in the
world to be able to enjoy their
creativity and their art without the
burden of the heavy computation that
they have to experience we announced
omniverse earlier this year
omniverse solves a really really great
problem it turns out that high quality
3d animation is one of the most complex
workflows and most heavyweight
computational pipeline of applications
we know when you think about the
workflow of 3d animation from the
concept art to the geometry to the
rigging of the characters to the
animation to the texturing and the
lighting each one of these thefts and
stages requires different tools and the
number of tools that they use are just
really quite incredible and because a 3d
animation requires so much labor so much
art so much experts so many experts in
so many different fields of tools
they're in different studios around the
world it is not possible for one studio
to be able to create one large movie by
themselves and so that work is spread
all over the world among many many
studios so you could imagine the amount
of data that they create is gigantic
terabytes and terabytes and terabytes of
data the tools that they use is
complicated and different and the
expertise is different and diverse and
various and the studios are spread all
over the world how is it possible that
they can get their work done well they
don't have the benefit of a Google Docs
or cloud Docs where they could share
content until now we created we created
a universe we call omniverse a place
where all of the content creators could
create portals from their application
into this world by creating a portal
into this world they could share their
data they could share their content they
could share their design with different
tools and different pipelines across the
entire workflow they could do that from
around the world because omniverse is in
the cloud it's in the data center it is
remote and it gives you also one
Universal view so that you could see
everybody's work with one ground truth
and we call that omniverse it looks
basically like this omniverse sits in
the middle it's a it's a database that
updates with creation with portals from
USD Universal seeing description length
descriptor language that was created by
Pixar incredibly popular every single
application that has created a portal
through USD into omniverse can attach to
this developers from different parts of
the world in different parts of the
pipeline can see one common view of the
content we created a viewport that is
able to render physically accurate
physically based photorealistic ray
tracing
and path tracing with physics integrated
and with the state-of-the-art material
language that we created called MDL all
integrated into this world we announced
that in the industry has been so
enthusiastic so excited to work with us
and today we're announcing a brand new
part today we're announcing that we're
bringing omniverse to architecture
engineering and construction this
industry is of course booming it's
booming for two reasons
maybe more the two reasons that are
exciting to me is that the future
buildings are essentially gigantic
products they're gigantic machines there
are such spectacular miracles with so
much complexity it is impossible to
achieve this spectacle without
completely designing it in digital
simulating it every step of the way with
software that are interconnected from
design to simulation it has to be
created just like a machine these future
miracles are going to make it possible
for us to welcome another billion people
into the world in the next 15 years and
so this area of engineering is going
through a complete revolution the second
part of course the second reason is that
this industry is growing so fast
because more and more people are moving
to the cities and as a result buildings
have become more sophisticated cities
are being reinvented and these buildings
are works of amazing technical marvel
and technical and beautiful works of art
let's take a look at one of the examples
and I'll come back and explain to you
how this all works so this is a
rendering from the omniverse viewport
that I just described and the lead of
omniverse the chief architect and the
engineering lead revela marion is going
to talk to us
rev why don't you show us omniverse
alright so what we have here is a a tool
used by many architects and industrial
designers Rhino by Robert MacNeil and
associates inside this this tool we can
see a building in Chen said it's the
China resources tower designed by KP f
KP F is a leading architectural firm
they've designed for out of the 10
tallest buildings in the world Rhino is
an excellent tool used by these
designers to create the most complex
buildings and architecture and design
them so inside this world we can we can
take a look at this building in all of
its glory it's a special building
because unlike normal buildings it has
an exoskeleton with 58 vertical columns
on the outside to maximize the the
amount of space you can have on each
floor of the building these fifty eight
columns converge to 28 columns at the
botton at the top there's a lot of
complexity here this isn't just about
looking at a beautiful renderer this is
the actual design of the building so
with our new tooling we created a portal
to go from rhino into omniverse plugin
so let's take a look at what it looks
like when we go into into omniverse a
typical workflow for architects earlier
on in the design is to create a foam
model that's usually on a tabletop where
they can take a look at what the
building will look like without being
distracted by all the materials and and
those details architects like to see it
in context of what it'll look like in
the city and this is Shenzhen Bay over
here with the building place accurately
around the other buildings but one thing
we can do inside this virtual world that
you can't do in a real world is simulate
what the Sun will do in terms of shadows
and lighting we can do that here inside
our tool and we can do it in real time
change the time of day
and match exactly where the sun's
placement will be at this time of year
relative to to the buildings at that
latitude and longitude now first of all
so first of all rep will just go quickly
until honor first came came came about
this tool
rhino and this tool weren't able to
communicate and so now they're able to
communicate through this portal that
both of them has created into omniverse
the second is the ray tracing global
illumination is done completely in
real-time and so you've probably noticed
you probably notice the beautiful
shadows that are being casted the
shadows and the indirect shadows the
indirect lighting that's that's
happening it's all completely done in
real-time otherwise this would have
taken hours to render each frame and
that's the reason why people made clay
models or phone models alright so let's
go to some some Beauty shots of what the
building looks like after we apply the
materials and go to the next stage of
design we so you might have thought that
that last scene was actually a
photograph or a pre rendered image in
fact just now you saw Revd make the move
this is completely done in real-time
let's show you some one of the shots
let's take a few more everything I hear
is done in real time every single scene
all Rev is doing is changing the camera
angle inside that world and we can also
change the light source to change the
side the Sun in real time and the last
one you probably notice the flowers look
like flowers because the flowers have
subsurface scattering look at the
beautiful surfaces REME is that amazing
every everything was done in real time
it was rendered on eight GPUs on just
eight GPUs rendering this entire scene
in real time and Rev was able to change
the camera angles anywhere inside that
scene and it instantaneously recreates
that photo real image now designers and
architects could enjoy their building
before they build it and know exactly
what it looks like exactly how it feels
now
omniverse makes all this possible I'm
diverse makes all hey thanks a lot guys
that was great so what you were seeing
what you were seeing were the different
applications
it was portal portal into on Bert
omniverse this other universe that is
shared by all of the users different
designers of different tools and in
different locations that they like and
they see one viewport that viewport
could be streamed to any device it could
be streamed remotely to a Chromebook or
a MacBook or a PC or a laptop our phone
doesn't matter where it is it could
stream it to you as a result everybody
can enjoy the ground truth what it looks
like in exactly the same way and from
anywhere okay I'm nervous
ladies and gentlemen this is omniverse
for AC early access is available now the
enthusiasm all over the industry is just
incredible thank you very much guys we
want to put rendering everywhere we of
course would like to make the computers
faster-- themselves but one of the best
things ever is cloud computing anybody
with just a small budget can have an
opportunity to enjoy what a
supercomputer can do in the cloud here
in China here in China the single
largest cloud rendering platform is
called ray vision ray vision renders for
85 percent of the studios and designers
here in China the top three movies were
made with ray vision this is a gigantic
rendering cloud ladies and gentlemen
today we're announcing
that ray vision has adopted Nvidia r-tx
so that they could accelerate clout
rendering in the cloud for all the
designers here in China now let me show
you the amazing results in the end in
the end what it what is what a developer
what a creator wants is to be able to
create their art as cost-effectively as
cost-effectively as possible this is a
comparison between running the rendering
on the CPU or accelerating the rendering
on NVIDIA RTX GPUs look at that
485 hours for a sequence of shots
costing 310 dollars to 39 hours and just
$40 unbelievable from three weeks to
just a couple of days one-seventh the
cost you guys have heard me say this
before the more you buy the more you
save that's right that's right the more
you buy the more you save and that is
the perfect example of that
okay so rendering in the cloud that was
the first chapter RTX games rendering
creatives omniverse let me change pace
now and talk to you about the second
chapter high-performance computing I
want to talk to you about
high-performance computing from the
context of a few new applications that
weren't possible before something that
you've probably not experienced before
and so the first thing is this of course
simulation is possible nasa and vidya
have been working together on simulating
their march lander by the end of 2030
nasa will send astronauts to mars six
astronauts will be inside a lander and
this lander is about the size of a
condominium
so imagine we're gonna put six ash
in a condominium fire them into Mars
into space and have them land on Mars
when they come to Mars they will be
traveling at 12,000 kilometers per
second 12,000 kilometers while per hour
12,000 kilometers per hour they're going
to be moving incredibly fast as they
entered that low atmosphere which is
just a fraction of earth the amount of
propulsion necessary to stop it in time
to stop it within just six minutes or
that it could land safely on the surface
of Mars is incredible it has to fire the
retrorockets at exactly the right time
exactly the right angle exactly the
right intensity and so we've been
working with them on simulating hundreds
of thousands of these simulators
simulations fluid dynamics simulations
so that they could experiment and
imagine what it's like and designed
their propulsion system design their
lander designed their landing algorithm
so that they could safely land these
astronauts however the simulation
results generates tons of data about a
hundred and fifty terabytes of data a
hundred and fifty terabytes of data now
the question is what are you going to do
with that data and how do they analyze
it and so we created a platform based on
our dgx a brand new software stack we
call Magnum IO for being able to stream
very high data rate from storage we call
GPU direct storage connected to a whole
bunch of Mellanox Knicks and rendering
on top of a software stack called Nvidia
index distributed volumetric rendering
software so all of that technology
the dgx index Magnum I owe the Mellanox
Knicks directly connected to DD and
storage is going to make what you're
about to see possible it looks like a
movie because it is a movie but it is
completely rendered in real time ladies
and gentlemen take a look at landing on
Mars
[Music]
[Music]
[Music]
every everything you saw was simulated
everything you saw was simulated there
was no art all of that is fluid dynamic
simulation 150 terabytes of data now you
can fly through it with a supercomputer
the first application I want to tell you
is basically that HPC will be used for
analytics in the future whether it's for
scientific simulation because you create
so much data or is for data science you
have so much data analyzed you need a
supercomputer to analyze that the first
is high-performance computing for data
analytics second we've been so
interested in this field for so long
whole genome sequencing the ability to
analyze completely the human genome the
benefit of that is you could predict you
could predict as a result alterations in
your DNA and to find early on potential
inherited disorders or cancer mutations
that are causing its progression or
discovering a widespread disease the
ability to sequence the human genome in
totality is incredibly powerful this can
be used of course to enhance our
understanding of life improve our health
but also in agriculture in livestock so
that we could protect livestock and
enjoy better better vegetables and farms
that produce more effectively there are
so many applications for whole genome
sequencing WGS the breakthrough that
makes it possible our recent advances in
NGS where the goths next-generation
sequencing machines and one of which is
the bee GI live science supercomputer it
is able to sequence 60 whole genome
sequences per day what used to just
recently took 15 years to do the first
human genome and still millions of
dollars a decade ago it is now possible
with the
I live sign supercomputer to sequence 60
genomes per day 16 whole genomes per day
in order to sequence and understand the
human DNA the first step is to do that
and it generates a whole bunch of small
fragments of your DNA these little small
fragments is called short reads these
short reads have to be reassembled into
your genome the way they do that is they
compare it to a reference they compared
these little tiny short segments to a
reference and they figure out which one
of the segments go in front of to which
ones which ones connect so they
reassemble and reconstruct your genome
from all these little tiny fragments
comparing it to a reference and then
most important thing the goal is to for
them to do what is called call variant
to identify variations the
identification of variations allows us
to detect as I as I mentioned earlier
earlier mutation potentially cancer
developing cells or that you might have
a disorder that was inherited from from
your parents and so that reconstruction
process genomics analytics is done with
a toolkit called gatk genomics analytics
tool cave that's the industry standard
it was created at Broad Institute it is
used all over the world
it is incredibly computationally
intensive as you can imagine because the
human genome has three billion base
pairs it's like a sentence that with
three billion characters inside okay so
this is a very large book each one of us
has a very large book encode that
encodes us and codes are our DNA and
that human genome 3 billion base pairs
comes in a bunch of little tiny
fragments that we now have to figure out
exactly how they're ordered and whether
there are potential variances gatk is
the toolkit that is used to do that is
an incredibly CPU intensive in fact it
takes about 30 hours it runs on one node
of computer is not very scalable gatk
runs on one node of computer it takes 38
30 hours a day and a half to go and
compute one of the 60 genomes that are
being
generated and and sequenced by the BGI
LiveScience supercomputer once that is
done petabytes of data are going to be
used to analyze what's going on this is
a classic machine learning problem in
fact bioinformatics was one of the first
industries just started to use machine
learning
before there was Python there data
science is prepared at Ascension
languages called R and they used machine
learning to go find clusters and
irregularities deviations and so on
terabytes and terabytes petabytes and
petabytes of data are being analyzed to
find anomalies this is the three stages
of what is called bioinformatics from
gene sequencing using a next-generation
sequencing machine that goes into a
genomics analysis toolkit so that you
could sequence the whole human genome to
create data that you could compare
against all of the other previously
sequence genomes represents the
bioinformatics pipeline this is one of
the most important workflows in the
world as you can imagine why and so it
has we have dedicated ourselves to find
a way to make this more productive now
one of the things that you already know
is we're working on machine learning and
so all of the work that we're doing with
rapid that I talked to you guys before
rapid high-speed accelerated data
Sciences of large data this is going to
go directly into helping them here
however one of the areas we have to
dedicate ourselves is the genomics
analytics pipeline gatk we've been
working with a small company called para
brakes para brakes are amazing amazing
scientist that focuses in this space and
they created a toolkit they call para
brakes and it accelerates gatk this
industry standard produces precisely the
same answers their work their work and
our work with them was so exciting was
so exciting that we decided
to join forces and have para BRICS join
us and for us to really double down and
put this technology in the hands of
every single jananam genomicists and
genomics scientist in the world so
ladies and gentlemen today we're
announcing that Nvidia para bricks our
genomics analytics toolkit is available
to everybody pair breaks the software
the pipeline is available on NGC it's
accelerated with acceleration you could
imagine to speed up 30 to 50 times so
instead of instead of instead of a
server and rendering it for 30 hours
while the sequencing machine is
generating it at 60 per day now we could
keep up with the data rate of the BGI
LiveScience supercomputer now this is
such an important first step because in
the future we're gonna sequence more and
more people and the reason for that is
this we've discovered that every human
is not exactly the same and based on our
ethnicity based on where we grew up
different regions there are different
standards and different references using
one common reference for all human is
not accurate enough and not sufficient
so we're going to create many more
references we're going to make it
possible so that we could of course
compare against all those references and
find varying call and duke variant
calling against all the references so
the amount of computation for genomics
is going to go up and of course we would
like to do it more frequently okay and
so this is an area that is really really
important I'm super excited about that
this year this year if I didn't include
if I didn't include the first
application for HPC that I mentioned
earlier which is ray tracing we added
two new applications to the Nvidia CUDA
stack and two applications that have
extraordinary importance to the future
of society the first of course is BGI
BGI using our pair Brix stack for
genomics analysis but the other I
announced several months ago
Nvidia's CUDA is now able
to process very large-scale 5g and the
first user of our platform we call Arial
is Ericsson and so it is very sensible
to all of the computer scientists in the
room that Nvidia CUDA should be good at
the radio the virtual radio virtualizing
the radio and running it completely in a
software stack now we could put that
virtual radio inside a data center
instead of the edge and have it be be be
much more flexible in adapting to the
different traffic patterns that changes
over time and also to be able to layer
on and add AI functionality that was
impossible in the past could you imagine
if we could use AI to optimize for
traffic in the patterns and adjust the
beams in real time so that the amount of
energy that's used for 5g could be
reduced the bandwidth can go up the
quality service could go up all of those
benefits are possible to brand new
applications that brings the Nvidia
accelerated computing into to new
industries one telecommunication and
then the other genomics thank you very
much
armed the most pervasive CPU in the
world a hundred and fifty billion
shipped almost everybody in the world
has an ARM processor
1700 licensees 95% of the world's
customers OCS are based on arm it is the
most configurable CPU core and CPU eisah
we have in the world
it is therefore sensible to see so many
industries in so many countries there's
so many companies building arm servers
two reasons for that the first reason of
course is that high performance
computing requires very low very good
energy efficiency very good power and so
we could scale it up into a gigantic
machine energy efficiency is vital to
high performance computing and second
because the world is moving to cloud
hyper scale is completely open sourced
hyper scale is a brand new stack anyways
and hyper scale does not care about the
CPU eisah and that's the reason why you
see hyper scalers and supercomputers
adopting and creating custom arm CPUs
these custom arm CPUs allows them to
change the ratio of course to memory
bandwidth to i/o bandwidth and as a
result solve a whole lot of problems
that a single CPU cannot you see all
kinds of amazing CPUs ampere is building
a CPU they call emag amazon recently
announced graviton two really exciting
looking CPU marvels thunder x2 Fujitsu
the most energy-efficient supercomputer
in the world number one is Fujitsu were
number two but they're number one
Fujitsu with their arm a 64 FX and
Huawei with their component 20 that's
dedicated to edge computing as well as
hyper scale and AI all of these
processors are super exciting and the
industry has been crying out to us and
asking us is it possible for us
to accelerate all of these appliqué all
of these processors because as you know
acceleration is vital to the future of
computing and so this year we announced
just a few months ago that we're
bringing CUDA to arm and to fill that
hole so that we could enable arm to be a
fantastic platform for high-performance
computing as well as AI and so this is
our arm reference system our arm
reference system has two external PCI
Express connectors and it goes every
every two CPUs connected to four GPUs
the reference system is based on marvels
thunder x2 and in just a few short
months look at the ecosystem all the
tools and all the amazing applications
that are now running on arm we have
molecular dynamics we have quantum
chemistry fluid dynamics finite element
analysis we have computer graphics we
have ray tracing let me show you one of
them this application is the simulation
result of Nandi Nandi is a molecular
dynamic simulation after you're done
understanding the molecular machine so
that you can understand at the
simulation level understand this
biomechanical machine you want to
visualize it essentially this this
application is called VMD visualization
of molecular dynamics VMD is essentially
a computational microscope and you could
look under this confident computational
microscope and understand how molecular
biological machines function and so you
simulate and then you put that
simulation result in essentially another
supercomputer and you use VMD to
visually understand it on top of VMD is
made possible with two applications two
libraries from nvidia one of them it's
called optics and the other is streaming
our ability to capture and code and
remote that computer graphics makes what
you're about to see possible
so VMD VMD is a vitally important tool
and let me show to you now so we are
looking at a life rate tracing of the
Rift Valley fever virus running in vmd
on an ARM based computer this virus is a
biosafety level 3 pathogen and it's
fatal to approximately 20% of the
infected people the problem is that we
don't have a vaccine yet and to design
vaccines it is important to understand
the structure of the viral proteins
professor Li from Tsinghua University
managed to solve the structure of the
virus so this can potentially help a lot
of people in the future and the data set
to solve the structure we are seeing
here was about 50 terabytes large and it
took one week to process with nvidia
tesla 300 GPUs and they probably
couldn't have done it without it this is
really fantastic let's thank professor
Lee in the Xinhua University tip for
helping us demonstrate this incredible
capability and also thanking them for
the important work that they're doing
guys thank you ma'am d VMD on CUDA
unarmed the single most important
application in the world HPC application
in the world uncorrelated sir flow
tensor flow is being used of course for
machine learning and artificial
intelligence but is used for scientific
computing now it's used for industries
is used in business this particular
application and application framework
tools framework is pervasive scaling up
tensorflow
is a high-performance computing
challenge it is just not easy to do so
and so we've dedicated ourselves over
the last about five six years working
with Google to enhance
tensorflow so that it could be
accelerated to a limit
this is tensorflow 2.0 it just came out
just a couple of months ago and ladies
and gentlemen you can now know that it
runs on CUDA on arm and the performance
at scale is very close to the
state-of-the-art this says something
about the power of CUDA and the power of
factory factorizing all of this software
so that it could be accelerated this
runs on a multi GPU configuration in a
multi no
the entire stack is identical to the
stack that we use on x86 and the
performance is fantastic I want to thank
the folks at Google for working with us
on on tensorflow and and all the people
that worked with us on making CUDA
unarmed possible thank you let's talk
about artificial intelligence in 2012
the confluence of several factors kicked
off modern AI the abundance
the overwhelming abundant abundance of
data and some researchers using our GPUs
to run deep learning these new
algorithms that automatically detect
features and because as because of its
deep structure can hierarchically learn
the representation of knowledge so that
it could be generalized the
effectiveness of deep learning has been
extraordinary the confluence of these
three factors Big Data deep learning and
NVIDIA GPUs kick-started modern AI since
then since then amazing things have
happened we've seen in just a few years
time the achievement of superhuman
levels of image recognition superhuman
levels of speech recognition and now
some very important breakthroughs in
natural language understanding when you
look back on 2012 and it was just this
incredible breakthrough we call deep
learning and the winning of imagenet
almost no one realized the potential
impact of that moment on the industries
around it image net or Aleks net Aleks
net in 2012 really kick-started
innovations in so many different
industries as a result of computer
vision finally being solved it's not
completely solved of course but computer
vision has achieved incredible results
as I mentioned earlier superhuman levels
that moment has made it possible for us
to have kick-started the self-driving
car revolution and
the work that's happening is
kick-started amazing photographs to be
taken on phones to the point where you
can now take pictures in the dark you
can now have radiologists be able to use
AI to detect disease the number of
applications in a number of industries
manufacturing robots are now going to be
advanced the number of industries that
will be affected is truly daunting all
started as a result of imagenet last
year something very happy very important
equally important happened an arguably
long term will be even more important
which is the creation of Bert this
natural language model that is pre
trained on a large corpus of information
and somehow understood by looking at all
of this data in Wikipedia all of this
text all these stories all of these
sentences it learned the structure of
language Bert just as imagenet is
decoded has decoded computer vision Bert
is in the way in the process of decoding
the code of human knowledge natural
language language is the way that we
communicate and we transmit knowledge to
each other we encode it to protect it to
remember it we encode it to transmit it
and share it
language is the code of knowledge and
now with Bert we have decoded the code
of knowledge understanding the code of
knowledge as if h.264 or JPEG or
understanding that code unleashes all
kinds of incredible innovations we're
going to find in the next several years
the ability to do some amazing things in
the way that we deal with text and to do
way we deal with language at the core of
what makes that possible of course it's
the engine of learning from large data
in the last several years in the last
five years we have accelerated training
of deep learning by 300 times just think
about that in five years Moore's law in
five years is 10x
we've accelerated training by 300 times
and the ability to Train large models
tremendously we did so by innovating at
every single level as I mentioned
earlier from the GPU architecture v100
in inventing the brand new tensor core
the COAS packaging system where chips
are layered on top of chips and wafers
the 3d packaging of memories the use of
high-speed memory call HBM connecting
all these GPUs together into a super
high-speed link we call env link
creating a system we call dgx all of
that so that deep learning researchers
can continue to explore the boundaries
to explore the limits of what's possible
with AI it is the number one fastest
computer on m/l perf challenge two years
in a row
this engine is really has really helped
researchers all over the world and
happened just in time if you take a look
at what's happening notice what's
happened everything was moving along
just fine and computing computing levels
was moving at traditional rates and all
of a sudden all of a sudden the
emergence of deep learning learning from
all of this data computers that write
software by itself computers that learn
automatically important features and
patterns from a large amount of data
all of a sudden emerged and it caused
computers to skyrocket the computer
workload rising quickly makes sense and
the reason for that is this if you say
to yourself that AI is about machines
computers and software that learns to
write software by itself why wouldn't
you want the world's fastest computers
to write the world's best software and
so the number of number of HPC
installations around the world has
really skyrocketed in just the last
several years and now performance
requirement this is workload
computational workload is doubling every
three and a half months this trend is
likely to continue and the reason for
that is because one of the areas that we
would like to explore is multi mode
learning to be able to take different
molds modes of information combine it
together and learn from it find patterns
across different modes of information
whether it's sight vision and language
video and language images and language
previous text your history and language
all kinds of different areas could be
mixed together for us to learn patterns
and connections from our AI journey
starts our AI journey starts with the
creation of the platform for training
models dgx is our training system this
is the beginning of the journey the most
important journey of course is applying
AI applying AI and so the question is
how do we apply AI and bring this
capability to the world the first step
is sensible it should be in the cloud
the availability of data the type of
services are simply not possible without
AI without machine learning and so our
first journey was cloud the platform we
call for cloud computing is hyper scale
and therefore hgx dgx was our deep
learning GPU accelerator dgx was
designed for training hgx is designed
for cloud egx is the next stop of our
journey as we move a eye closer to where
the action is we call that edge and that
system is called eg X and eventually we
would like to bring AI all the way out
to the world where the AIS are
autonomous and moving among us that
system and that platform is called AG x
AG X for autonomous or robotics eg X for
edge hgx for cloud d GX for training one
architecture but the software stack and
the computing stack completely different
and the reason for that is big very
obvious because each one of these
computing platforms whether its
autonomous at the end or in the cloud
have very different capab
the requirements the way you manage the
way you operate its form factor all
different and so this represents the
taxonomy the language if you will the
mile the mile markers of the rest of my
talk with you I'm going to talk to you
from this side to that side we have now
in the AI chapter the first section
we've already talked about I've got
three things I want to talk to you about
the first the first is the single most
important AI model on the Internet this
is my schematic of the single most
important AI model of the internet
without this model it is impossible for
us to enjoy the Internet and the reason
for that is this the internet gave us
access to the world's information
unfortunately the world has a lot of
information the amount of data that's
out there it's trillions of web indices
hundreds of billions of tik-tok videos
I think it's hundreds of billions right
billions of products on Taobao news in
the millions books into millions movies
into millions the era of search has
ended if I put a trillion things a
billion things a million things and it's
changing all the time how can you
possibly find anything the era of search
is over the era of recommendation is
here everything has to be recommended of
course the Internet companies have known
this for some time
it started with of course different and
much more simple versions of
recommendation systems using matrix
factorization whether it's collaborative
filtering or content filtering most of
those applications were light on
computing but it had limited
capabilities
didn't have the ability to understand
for example information that are
unstructured for example it takes deep
learning to understand information
that's unstructured it has limitations
on how far or how big of a corpus it can
learn from how big of a directory can
learn from how big of a database you
could learn from this is a block diagram
of a if you will canonical
recommendation system the most important
thing is this there are users and items
users and items we have six billion
users in the world there are tons of
items as I mentioned two billion
products in just how about alone there's
all kinds of movies and social videos
and websites and stories and books and
every single one of those items is
something that we're interested in it is
in the trillions there are billions of
people there are trillions of choices
somehow we have to find something we
like somehow we have to find something
that makes sense to us how do you solve
this computer science problem well
recommendation systems is the way that
it's done on this side ultimately are
billions and billions of choices and it
has to go through a candidate generation
system from it's a filtering system to
go from billions to hundreds from that
hundreds this is the most important part
it has to figure out how to rank it to
you the way it ranks it to you depends a
lot on your what is called implied
preferences it has to learn your implied
preferences what do you like and from
that implied preference that it learns
it will rank order this list and present
to you a few choices from billions and
trillions of items billions and
trillions of items and changing over
time to just a few recommendations it
could recommend to your news it can
recommend to your books can recommend to
your food restaurants you should go to
recommend you products
recommend videos you should enjoy that
you will likely enjoy tweets that you
want to read so many things that it
could recommend the recommendation
system is extraordinary as I mentioned
it used to be based on CPU based
collaborative filtering techniques and
and content filtering techniques but now
the industry is moving to deep learning
moving to deep learning has all the
benefits of being able to learn
preferences from unstructured data the
ability to scale to a much much larger
system however the processing time is
also much more intensive there's a
couple of things that has to happen the
input the input are the billions of
dimensions that I mentioned the billions
and billions of dimensions billions of
people billions of things that is
extremely high dimensionality it goes
through this concept called embedding
kind of like word Tyvek embedding which
takes a high dimensionality reduces the
dimensionality from that dimensionality
of embedded embedded items and users we
can learn information learn
relationships and interactions and
connections associated with those
embeddings and it's the reason why it is
the reason why King - man is equal to
clean it is the reason why pair Apple -
Apple is equal to pair those embedding
systems allow these relationships to be
learned the embedding system and the
ranking system is learned together using
deep learning
it is incredibly computationally
intensive as you could imagine this
recommendation system that used to be
based on CPUs are now moving to GPUs and
that is a really really important moment
so bi do bi do they want to go to deep
learning and they want to create this
new thing called AI box and it's based
on wide and deep it takes it takes wide
vectors wide vectors really super sparse
entries that are because of human known
preferences we know that people have
these preferences and so we don't need
deep low
to do it and that sparse sparse table is
billions wide a hundred billion white
dimensionality is very very large the
sparsity is very very high on the other
hand they have the embedding table ten
terabytes large together between that
white and deep they have to Train this
ranking system this ranking model well
it turns out a hundred billion a hundred
billion dimensionality what a dimension
of 100 billion and a ten terabyte
embedded embeddings file and betting's
table is simply impossible to do cost
effectively on CPUs and so we work
together to move it on GPUs and they
were able to reduce this reduce the
training time and reduce the cost by 90
percent just one tenth of the cost just
one tenth of the cost as a result of
course they could reduce their cost but
very importantly they have so many
models they have to train and they would
like to move all of them to deep
learning hundreds of models from
products to news to websites to banners
all kinds of models has to be developed
for deep learning and so this is really
a great achievement they're paid their
poster is outside their researchers
outside and they would love to love to
talk to you about more about it and and
it solves this problem of taking
enormous amounts of data that's
available on the Internet to many
choices to filter it to filter it
through the recommendation system so
that you only see ten how do you take
this massive amount of data hundreds of
billions of large and dimensionality and
in the future
trillions large in dimensionality and
reduce it to just ten items that's
frankly a bit of a miracle and this is
the miracle of deep learning this is the
miracle of AI and Baidu calls it AI box
and the results are really fantastic and
the thing that I would like to say the
thing I would like to say
that because of the incredible reduction
in cost the more you buy the more you
save Chinese people love saving money
right I love saving money makes me so
happy
speaking of saving money alibaba's
recommendation system also powered by a
video you guys know Alibaba singles day
Singles day the single largest
e-commerce event the single largest
shopper shopping event in the world in
the universe in the galaxy it is
impossible what happens here in China on
singles day why are there so many
singles people two billion products 500
million shoppers on one day 1.5 times
the size of United States everybody
shopping who is working everybody
shopping 500 million people shopping on
one day trying to decide which one of
the two billion products to choose from
billions of queries per second now one
of the things that I mentioned earlier
is of course this model this
recommendation system is fairly
universal this is a canonical
architecture it's fairly universal and
this Universal architecture has all
kinds of different algorithms and
different architectures and there are
lots of different ways to refactor it
but basically the concept is the same
and to be able to filter all those
choices down to just a few and to do it
with deep learning the computational
load for deep learning was too high for
CPUs when they moved to deep learning
the effectiveness was higher their
click-through rate was higher
and you know that click-through rate in
e-commerce and Internet directly
contributes to the success of the
Commerce and so click-through rate
absolutely improved however the
computation time also improved using a
CPU alibaba's model could only do three
queries per second three queries per
second I mentioned earlier that there
are billions of queries per second these
500 million shoppers and all the people
that interacted with Alibaba but did not
shop created a lot of queries those
queries has to be responded quickly and
cost-effectively if it can only query
three queries per second with one CPU
and there are billions of queries per
second how many CPUs do you need the
entire world CPU is not enough and so
therefore we work together to accelerate
their deep learning model on our GPUs
that is the power of deep recommender
systems deep recommender systems unlike
traditional collaborative filtering or
content filtering matrix factory matrix
factorization was very good for CPUs but
unfortunately it wasn't very good for
GPUs deep recommenders are more accurate
can handle unstructured data the
features of your implied preferences
could include a lot more things the size
of the data could be much larger and it
can be GPU accelerated whereas the CPU
could do three queries per second a t-4
GPU can do 780 g by bosses versus Sun
very big difference this is the perfect
example of the more you buy the more you
save perfect example this also is very
interesting this is one of our one of
our brilliant data scientists and this
is what Taobao recommended to him this
music player called a Basso and the
world's largest hamburger he must be
very very hungry
incredible alright so number one thing I
want to say is deep learning inference
it's wonderful for deep recommender
systems and the recommendation system
the recommender is the engine of the
Internet everything that we do in the
future well everything that we do now
passes through a recommendation system
and it's going to be based on deep
learning the future one of the most
important tools that we create is called
tensor RT 10 so RT is a computation
graph compiler and optimizer for CUDA
GPUs it takes the output of tensorflow
and it goes through and finds
optimisations nodes and edges that can
be optimized shared fused and generates
optimized CUDA code to run on any of our
GPUs tensor RT makes it possible for us
to not only train with tensor flow but
to take the output of tensor flow to run
in a very fast way on GPUs tensor RT is
very important last year we announced
here in China tensor rt5 tensor RT 5 has
the ability to handle CNN's views
horizontally in the same layer at this
in the same layer of the graph to be
able to fuse horizontally different
nodes combine nodes and edges it could
fuse vertically it has the ability to
automatically detect places where it
could reduce the precision of the
mathematics and use different parts of
our tensor core GPU FP 32
p16 or into eight to accelerate the deep
neural network applicant I'm while
reducing power and not sacrifice
accuracy that entire loop was made
available last year
that's called trt 5 it has 30 different
optimizations and transforms that it
does unfortunately unfortunately as you
know some of the most important
applications are not CN NS but they're
based on our n ends CNN's is a
feed-forward network data propagates
forward in the network RN ends is
complicated RNs is a feedback network
state of the passed previous state
previous memory along with current data
affects the next output it's a state
machine RN ends is a feedback network
and supremely complicated well ladies
and gentlemen I'm really excited to
announce something that's really really
important and it's probably one of our
greatest achievements tensor RT 710 so
RT 7
tensor rt7 has the hint has the ability
to handle CNN's of course but also
transformers rnns auto-encoders which
are which are versions of CNN's and be
able to do that automatically whether
whether you whether it's whether it's
the the different all of the different
configurations of aren't ends that needs
to be generated the first thing it does
is it does code generation kernel
generation automatic kernel generation
so that there are mathematic kernels
that has to be generated for CUDA that
wasn't possible to have pre described
because RNs have so many different
configurations they have so many
different activation functions there's
so many different ways you could
remember the past the state machine is
very complicated we fuse horizontally
wherever we can we fuse vertically
wherever we can we Auto generate code
for all of the custom kernels of RNN we
even fuse over time we look for
opportunities in the processing pipeline
over a period of time to find ways to
share the computation to reduce the
amount of processing load on memory to
reduce the amount of traffic fuse over
time and then lastly of course we still
automatically detect all the different
areas where we can reduce complexity
reduce precision while continue to keep
the accuracy if you look at the number
of models that we support last year last
year we supported basically resonant 50
and SSD and all the other versions of
CNN's those two versions those two
networks were you know of all that these
are the top networks being used in the
world the types of networks that people
use those two were the only ones that
were supported by tensor RT 5.0 and all
of a sudden this year with tensor RT 7.0
automatically it will compile and
optimize for all versions all varieties
of RN ends and LST M's transformers of
all kinds and in fact the most important
neural networks of our time today it has
thousands over a thousand different ways
of optimizing kernels and fusing fusing
operations from 30 to over a thousand
and so tensor RT 7.0 is available now I
think it's going to be put up on NGC in
the near future and then you'll be able
to develop the networks that you would
like it would take the computational
graph and optimize it into a CUDA
runtime and you can get the benefit of
really really fast inference ladies and
gentlemen
tensor RT 7.0
so what can you do with TR t7 what can
you do that you could not do before well
of course of course
of course you could do them slowly of
course you could do it with more cost
but there are some things you simply
cannot do if you don't accelerate the
entire pipeline of neural networks one
of them is one of the most important
developments in AI today conversational
AI several breakthroughs have made
conversational AI possible for the very
first time the ability to do speech
recognition at a superhuman level
natural language understanding models
that correct what it heard wrong so that
the precision is really really high to
have the ability to understand your
intention make recommendations do
various searches and queries for you
then come back summarize what they
learned the AI learned into a
text-to-speech system and then to
synthesize the voice in a very natural
and happy way that loop is now possible
it takes 20 to 30 models to make this
possible however all of the technology
are now in place the challenge of course
is that it is too slow conversational
conversational speed is of course
real-time and if you ask a question and
doesn't respond fast enough if you have
to wait several seconds it seems like
the individual the person is not even
interacting with you low latency
computing is really really vital and
it's we now have achieved with tensor
rt7 the ability to compile and optimize
every one of those networks from end to
end and to do it in 300 milliseconds it
is now possible to achieve very natural
very rich conversational AI in real time
okay so this is what tensor RT 7 can do
thank you
since last year our journey of TRT has
really made progress since last year our
journey to accelerate inference has made
enormous progress we postulated when I
stood here in front of you last year I
said that inference is a super
complicated problem and the reason for
that is very obvious inference starts
with many many people developing very
complicated software and this
complicated software is written by a
computer and that computer is gigantic
that gigantic computer is generating
very complex computational graphs the
largest the world's ever seen
some hell we have to have another
program understand that graph optimize
that graph and target it into a computer
so that it can run at very high speeds
at very high speed so that it could be
deployed largely broadly at very low
cost the targeting of that generic
computational graph which is already
very complicated and make complicated by
our intents and feedbacks to be able to
target that into a machine is really
quite a technology undertaking and
that's the reason why I've always felt
that inference was going to be a great
challenge there's another reason why
inference is a great challenge the rate
of innovation and the number of ideas
that people are coming up with and deep
learning is growing not slowing it's
growing not slowing and so therefore the
number of architecture a number of ideas
like Bert are growing not slowly it is
impossible to know it is impossible to
know whether a computer will be ideal
for inference unless you knew that it
was ideal for training if you can learn
on the computer of course it will run on
that computer someday and so one of the
benefits one of the reasons why I felt
so strongly though this is a an area
where Nvidia should put our energy in is
because
inference needs to be future-proof the
number of data centers all over the
world tens and tens and millions of GPUs
or CPUs we need to know that that body
of computers will be compatible and
optimal for software that's going to be
written three years from now four years
from now it has to be future proof well
one of the great things about CUDA is
that it is future proof we are in CUDA
ten cuda ten is compatible with CUDA
applications written today it's
compatible with applicate with
architectures we ship long ago the
nvidia cuda is future proof applications
can continue to grow on top and we will
continue to optimize software develop
software that would run wonderfully on
the install base years from now and as a
result one by one by one as all of the
internet companies in China moves
towards using deep learning and
recognize the the power of
conversational systems natural language
systems and recommendation systems we've
seen an enormous success here so I want
to thank all of our partners here and
all that worked with us to make this
possible thank you
very quickly we have enjoyed our
industry has enjoyed the AI moment two
things made it possible the smartphone
and the cloud the smartphone in the
cloud has made it possible for the
iPhone moment the smartphone moment to
have made it possible for the industry
to be completely reshaped and society be
completely reshaped well it is time now
that every industry is going to enjoy
the smartphone moment we're gonna see
the smart everything moment everything
will be smart in the future and the
reason for that is because sensor will
soon be connected to everything some
people call I or II and that sensor
information will be streamed and it will
be processed by AI to recognize
automatically patterns changes and to
reason about what actions should be
taken and take it we are going to
automate everything automation is going
to be one of the great forces that are
going to make each and every industry
more productive the automation of
everything smart everything revolution
that AI cannot run in the cloud that AI
has to run at the edge at the point of
the action the reason why it can't run
to the cloud is because sensor data is
being streamed continuously 24/7
these sensors many of them are going to
be high resolution sensors they could be
cameras they could be light hours they
could be radars imaging radars they
could be all kinds of infrared cameras
they're gonna be streaming continuously
from everywhere it is not possible to
stream all of that data to the cloud
second the latency of traveling to the
cloud and processing and back is too
long it is impossible to have a robot
that is working with you responding to
you and the processing done far away the
latency is simply too long and then
lastly data sovereignty and data
ownership cannot be guaranteed in the
cloud sometimes you don't want to put
private data in the cloud and so there's
a lot of reasons why we want to put in
intelligence at the edge we call that
system egx we started working on egx a
few years ago and the momentum has been
fantastic
we now realize there are so many
different applications far beyond our
dreams and every single industry has the
benefit of now applying AI to automate
their industry whether it's in health
care a third of the world's population
are elderly they should be monitoring
and watched all the time to make sure
that if they fall if something were to
happen to them we can send help right
away
550 500 17:4 in the world there's no way
to have people monitor all of that we
put sensors everywhere
two million factories will be automated
13 million stores 27 trillion dollar
industry has the opportunity to be
automated and to make more productive
and more profitable the Universal
Translator call centers that understands
every single language there's so many
different applications smart cities more
convenient airports all of these
different places has an opportunity to
be automated we call that intelligence
at the edge and the computing platform
is called egx we've seen incredible
success egx recently let me tell you
about a couple of them one of them is
Walmart has put into all of their stores
they're deploying into their s'mores
stores smart retailer system smart
checkout systems make it much more
productive if they could save just a few
dollars a few small percentage of a
twenty seven trillion dollar industry
the returns fantastic you still United
States Postal Service a half a billion
mail pieces per day and using computer
vision at a very large scale and very
high rate streaming basically it's
almost like streaming AI we can now help
them do a much better job with sorting
mail all kinds of interesting
applications and and one application
that I'm very excited about it's the
ability to put our GPO into the fabric
of wireless communications so that AI
can also run not just on internet but
also on the Wi-Fi excuse me on the telco
telco networks okay so the work that
we're doing with Ericsson is really
important
let me now talk about robotics robotics
is a special type of computing robotics
autonomous systems has to basically do
three things we all basically do three
things we sense the environment we
reason about we might reconstruct the
environment in our mind we reason about
the environment we reason about the
environment relative to our goals and
then we plan action sense reason plan
that loop is continuous in intelligent
systems we would like to put that loop
that high-performance real-time
computing loop of sensors and reasoning
and planning right at the edge so that
robotics would be possible we called
this basic system Jetson and we also
created several applications and stacks
for different applicant different types
of robotics applications one of them the
most important one in the moment is
drive using robotics technology to make
autonomous vehicles the second is
putting AI into instruments in the
future your medical instruments will be
self-driving self-driving medical
instruments so that while you're taking
and acquiring the sensor image the
system will help you and assist you in
acquiring the proper image and acquiring
the best image the a I would then
improve the image the AI would also help
in detecting disease and so AI
autonomous self-driving medical
instruments from the acquisition to the
processing and the detection pipeline
really really profound we call that
Clara and then lastly for robots that
are maneuvering inside an unstructured
world cars can drive on lanes but robots
cannot they have to operate in
unstructured worlds or manipulators
those robotics algorithms are different
and so we created several different
stacks jetson for general-purpose
embedded drive for autonomous vehicles
Claire
for medical instruments and Isaac in
each one of the cases our offering is n
10 our mission is not to create
self-driving cars our mission is to
create the infrastructure the computer
and the software so that every company
in the world could build self-driving
cars we believe that everything that
moves in the future will have autonomous
capability where there's passenger owned
vehicles or vans or trucks or shuttles
or trucks trucks or or delivery BOTS
everything will have autonomous
capability it could be completely
autonomous or it could have us in the
loop autonomous and the algorithms and
the computing structure is basically the
same the first thing we do for
self-driving cars from the data
collection data labeling to the training
of the models the simulation of the
self-driving car to the in-car computing
platform we create all of that we create
all of it and we operate it ourselves as
if it's our car however we make the
entire computing platform open the
software stack from the operating system
to the middleware the reference
applications and the pre train networks
the networks that we use to to create
this autonomous vehicle are all made
available to our partners from
infrastructure computing stack as well
as pre-trained networks this is our this
is our body of work and when we work
with our partners all of its made
possible available to them today we're
announcing something that's rather new
we're going to make our pre train models
the models that we create and there are
tens of them there are so many different
types of models that it used in order
for self-driving cars to be possible we
have been requested time and time again
because of the quality of our networks
we treat these networks as of their
industrial strength networks that we
will ship and maintain for as long as we
shall live and so these models are done
with extraordinary care to the highest
possible quality
and their capabilities are really quite
incredible we have a lot of expertise in
this area and so our partners have asked
repeatedly whether it was possible for
us to share our models with them and the
answer today that I'm announcing is from
now on it is possible for all of our
partners who are using Drive to come to
us and we would share our models with
them the pre train models the question
is what can they do with that it comes
with it a tool the pre train models the
pre train models of course are designed
and optimized for our car configuration
with the transfer learning tool it is
possible to download from NGC the NVIDIA
GPU cloud registry you could download
these pre train models and you could use
the tensor that transfer learning tool
to adapt it to your own configuration
you would of course collect your own
data we're happy to share with you our
labeling system and show you the
standards by which we label data that
collect the data could then be used to
refine refine adapt our pre train data
into a new network which would then be
optimized and compiled using tender
tensor RT into your platform now all of
a sudden if you're developing something
where the sensors location are slightly
different than us maybe maybe you're
creating a truck while we're creating a
car and maybe the camera location is
slightly higher maybe you would like to
you like to collect data for a
particular use case in region that you
feel that our data collection is under
contributed there are a lot of different
reasons why you can then now take our
day our pre train networks and adapt it
to your car we call it drive transfer
learning this is the first step the
really exciting stuff the really
exciting part is federated learning we
have been asked repeatedly whether it's
possible to partner together
where your data and your data collection
your labeling and our data in our data
collection literally could come together
to train a common network but we don't
have to move data because the amount of
data is so gigantic or it might not be
possible to share data but we could
share a common training of a model
network and we call this federated
learning we've now developed a system
all the infrastructure is put together
and so when you collect your data you
train it with our transfer learning tool
it is now possible for us to now return
the weights the Delta weights of the
network back to the master server for us
to do
federated averaging of all the different
partners combine that into a new to new
network update all of the partner
networks this has the ability to protect
data keep our privacy reduce the amount
of movement of gigantic data that will
happen over time and enable cross
company collaboration across different
countries as well okay so federated
learning is really really powerful today
I'd like to announce our next generation
Xavier Xavier was the world's first
robotics processor it was the world's
first robotics processor because we
designed it for one purpose we designed
it for the computing stack of real-time
sensor mapping localization and planning
that loop is the fundamental robotics
loop and we created a processor called
Xavier for that fundamental purpose it
was the world's first SOC designed with
that only application in mind and has
been a gigantic success and I'm so proud
of it today we're announcing our
next-generation a giant leaf or we call
it Oran this is the AG X horn
it is seven times the performance of
Xavier it is seven times the performance
of Xavier it's cheap a
it's seven times what industry from
generation to generation increased
performance by seven times the reason is
because the robotic stack and
self-driving cars is a very hard problem
we would like to increase the resolution
of the sensors we would like to add more
sensors we would like to increase the
processing speed so that reaction time
of the cars are higher it's very very
important we would like to make it
software-defined so more software can be
put onto the computer and this is the
most important part we would like to
make it safer than ever so the amount of
redundancy the amount of redundancy
inside this computer is much greater
every single CPU can lockstep with other
CPUs every single GPU can lockstep with
other GPUs so we can run these
processors concurrently and we can check
their results because they were running
in lockstep inside oran is a very
special security processor so that all
the data in motion and all the data in
place are encrypted we want to make sure
that this machine is safe to cyber cyber
attacks and that it is safe to tampering
the robotic systems of the future must
be safety first in mind and so Orin was
designed with these characteristics 17
billion transistors 12 CPU cores 12 CPU
cores 200 trillion operations per second
Xavier was 30 this is over 200
incredible amounts of performance this
is what the xavier stack looks like we
create a one architecture that's scaled
from l5 all the way down to l2 l2 all
the way down to l5 one architecture
completely software-defined all the
software that runs on xavier runs on
xavier plus GPU runs on Pegasus which is
multiple xavier zhan multiple GPUs some
customers chose to build l2 s by taking
a fixed function a dashed
chip and adding a CPU so that you could
have link keeping and ACC some customers
decide to use Xavier for l2 so that you
could have the ability for some round
cameras and be able to do lane changes
this basic approach this stack up is
consistent across the world almost
everybody who's doing self self driving
vehicles that are Software Defined are
using Xavier and then there are fixed
function a dash systems sometimes with
CPUs looking forward looking forward we
believe that much more people will
continue to use software-defined and the
reason for that is because operating a
fleet of cars is like operating a fleet
of phones it is not possible it is not
possible to operate this fleet without
the ability to have continuous CI CD you
want to continuously enhance you want to
continuously update and add features to
the whole fleet fix problems as quickly
as possible
CI CD is vital to the future of IOT
systems this smart everything revolution
this stack that worin enables
essentially increases performance by a
factor of seven or performance by a
factor of four while reducing power at
the same time one of the things that
we're doing with Oren is we're creating
for the first time a cost reduced Oran a
cheaper version of Oran so that l2
companies l2 companies with just one
camera with just one camera and maybe
some round radars could have an
entry-level AV car that's also
software-defined one of the things that
we really love about this and our
partners love is that the development of
the system all of the software that you
develop here will carry over torn as you
know everybody in the audience knows
that software cost us the vast majority
of the engineering cost in the future
you ship it you ship a computer once but
you maintain the software forever you
ship the computer once but you maintain
the software in an enriched it with new
a is forever
this is going to happen in autonomous
vehicles the capability is so
complicated we are going to be
developing software for as long as we
can see and we would like to use the
same architecture so that the software
can be updated across the board across
many many years across many many cars in
your fleet one architecture completely
software compatible from l2 to l5 from
generation to generation okay so that's
word Xavier was designed for the 2020
start of production cars Oran is
designed for 2020 to three years from
now started production cars today I'm
now seeing a very special partner we've
been working with him for some time we
worked with them across a ice as they
connect connect customers writers to
writers to drivers we work with them
because they're one of the world's
largest largest AI companies the body of
work that they have to do is quite
extraordinary
you know you know you know that that
there's a company here that that that
connects millions and millions and
millions of drivers and writers every
single day and that company is DD
they're the world's largest ride-hailing
company and we work with them we work
with them across data analytics across
developing of AI to connect and
recommend which driver could connect
with which writer and now we're working
with them to bring autonomous
capabilities to their fleet of cars DD
has selected drive AV for their
autonomous AV systems and they're going
to be testing their fleet in the near
future and Shanghai let's welcome DD
okay the largest industry in the world
ten trillion miles driven per year a
hundred trillion dollar industry we
believe that everything that moves will
be autonomous someday and will have
autonomous capability someday and the
number of things that move from cars to
trucks mobility services their startups
the delivery vehicles the ecosystem in
order to enable this autonomous future
is large this is not the work of one
company this is the work of one industry
we've created an open platform so that
we can all team up together to go
realize this autonomous future and so
our rich ecosystem is really rich you
could see all the companies that are
working on here just incredible
companies that are building all kinds of
really innovative products and mobility
all the way from building cars to on the
other end simulation this rich ecosystem
is really a testament to the openness of
the platform the software-defined nature
of the platform so all of these
companies can innovate on top of it and
the fact that they could rely on this
platform for their entire fleet as well
as generations to come and so I want to
thank all of our ecosystem partners for
helping us and partnering with us to
realize this amazing future of
autonomous transportation thank you
okay I'm going to show you a quick video
this is a video of our latest work what
you're about to see every year when I
come I show you a quick video this one
this one is a dress to a dress from a
car stopped drive across 17 miles three
highways four interchanges with lights
autonomously merging into traffic
changing lanes the car had to create its
own map if a car is a great self-driving
car it's also a great mapping car and so
the first time you drove it maps the
roads it fuses multiple drives together
and so now I didn't remembers I've been
here before and it's a guide to help it
localize after which it does real-time
perception all of the models that I
showed you earlier is in operation and
this is captured this last week our bb-8
driving in california
so we'll shoot sensitive
[Music]
[Music]
[Applause]
[Music]
what do you guys think
open platform BBA driving by itself the
car even makes sure that you're paying
attention sophisticated AI posture
recognition all kinds of interesting AIS
are going to do for a cockpit as well as
the confidence view so that the a I
could show you what's in their brain the
confidence view is very very important
to give you a sense that the AI is doing
the right thing we need intelligence to
give us feedback the confidence view is
what gives us feedback so that's our
that's our latest bb-8 the platform is
open and we love we love all the work
that we do with you guys I want to thank
you for that we've done the same thing
for robotics same three parts the
infrastructure the AI development
infrastructure robotics development
infrastructure the computing platform
the computing platform that's the
computer the software stack the
algorithms and reference applications in
the case of self-driving car in the case
of drive AV there's the reference
application for AV and there's the
reference application for IX intelligent
user experience drive AV drive IX are
the two reference applications in the
case of robotics our to reference
applications are carter and leonardo
carter designed for indoor navigation or
navigation in unstructured worlds
navigation in unstructured worlds the
reason why that's so different and so
hard is because well first of all
self-driving cars very hard but it has
lanes and signs it has lanes and signs
inside a warehouse or inside a building
there are no lanes and signs and so it
has to navigate in a different way how
it understands word it where it is
mapping localization and path planning
are different and so Carter is the
reference robotics application for
indoor navigation we also have a
Leonardo reference application that's
for manipulation manipulation indoor
navigation recognize that these two
applications even though
way you collect data is fundamentally
different and the reason for that is
because a self-driving cars
degree of freedom is basically this
forward left and right and as a result
we can collect the data for lane signs
lights cars trucks people we can collect
all that data and carefully label it
unfortunately for robotics the
perception has to be completely free
formed it is impossible to collect every
angle of information to label it
accordingly so that the robots can learn
perception in 6 degree of freedom that
has to be done in simulation as you know
NVIDIA is very good at computer graphics
and so we created a world where Isaac
the Isaac world where robots can learn
how to be robots it learns how to
recognize and perceive things in six
dimension and so the ability to simulate
to train as well as simulate to navigate
those pipelines are available on our
platform and then you could develop the
software it's designed so that it's very
easy to use and then you could create
these robots yourself if you have
learning if you have special needs for
deep learning we also have pre trained
models and again these models could make
be made available to you you could use
the Nvidia transfer learning tool and
you could adapt it for your needs let me
show you what our engineers did with
this SDK in just a weekend okay so the
next video this is how easy it is to
create robots now
[Music]
[Applause]
isn't that cool with the Isaac SDK you
could create all of these and it's so
it's so simple and of course there's
nothing easy about robotics but the
algorithms the algorithms and the
applications and all the technology has
been encoded and embedded into the Isaac
SDK is open to you you're welcome to use
as much or as little as you like and to
customize it and create your own
networks and hopefully together we could
create some magical amazing robotics
together let me show you one more thing
this next one is really quite a miracle
and it's impossible without simulation
we know that articulation human
articulation is one of the great
challenges the ability to teach a robot
and to generalize it the important thing
is to generalize it so that independent
of the environment and independent of
how you interact with it it can respond
to you and that they could respond
quickly and gracefully it can respond
safely it can understand and perceive
six dimension of freedom six degree of
freedom pose it could work in seven
degrees opposed and of course it can
interact with you meaning that it can
replan whatever it was trying to do it
could replan in just the most clever way
to reach around to interact to back away
as it interacts with you we call this
robot Leonardo and it's a work of some
amazing researchers at the Nvidia
robotics research lab up in Seattle and
so to to show that to us as its lead
architect Nathan Radcliffe he's gonna
bring out I hope Leonardo
hey Nathan hey guys hey we saved the
best for last
Wow okay well first of all Nathan you
want say hi tell everybody what were you
about to see hey this is a fundamental
fundamentally collaborative robot it's
one of the first robots built from the
ground up the system is engineered from
the ground up to be fundamentally
interactive and that's really because in
the future in the very near future a lot
of the applications that we want to do
will be around people now like 90% of
the things that we would like robots to
do can't be done currently because
robots are so dangerous and so we use
perception here real-time perception and
real-time reactivity to make sure that
they're safe and reactive and perceptive
around now Nathan
Leonarda has a RGB camera and as a depth
camera yeah and it has inside-out
perception it also has outside in
perception or just inside out perception
it basically has as a perception coming
from here presumption coming from here
this kind of egocentric perception you
can see my hand so it's always reacting
to what what I'm doing it has perception
from the outside so it just kind of has
a more broad perspective of what's
happening it can recognize each one of
these blocks constantly has feedback
about where it is are you sure you
didn't pre-programmed that come here oh
come on it's okay
it's okay I know I'm not Nathan I'm not
your daddy but I'm nice here I'll give
you it no how about this one really
really Wow
well what else could well I was making
what else could you only know how to do
it's gonna start pushing these around
kind of clearing out the space you gave
it a mission I gave you the mission yeah
yeah it's gonna start picking these
things up and just stacking them as
precisely as it can so here it's this
kind of like making sure that everything
is nicely organized and the reason for
that is because Leonardo wants to stack
it in that spot underneath and so it's
moving some stuff out and so so one of
the things that you realize that
Leonardo right now has to understand
6-degree pose right so so 6-degree pose
not not the car where it's looking at
the world through just one angle it's
looking at the world through a lot of
different angles and and the blocks are
all in different orientations in
different wicky-wicky wonky shapes and
so now it has a mission that it has to
now go and stack them together and so
our first thing it did was it cleared
the space just like we do we cleared a
space it's pretty cute
so it's taking a look at where that
thing is it recognizes what the pose is
then it's gonna go for it it's
considering
it's a little bit frightened by the
audience in the bright light please guy
the clock is ticking
this one here Oh Oh No do it again do it
again
there we go Wow and so it's interesting
so remember it's eyes are very close to
his hands and so once it places
something it has to take a step back and
look at the world again yeah yeah so you
see a do that that's actually really
interesting and so please sir go ahead
go ahead
it's like as grabbing as with his mouth
and so it comes in there I can't really
see anything it picks it up it's not
entirely precise but the place is there
in it then it pinches and lines it in
the future we need some separation
between our hands and our eyes right
yeah yeah and which is which is kind of
the reasons why this separation kind of
makes sense too exactly yeah oh wow all
right fantastic thanks graduate wait
first of all Leonardo had to learn and
you can't learn by just physically doing
it and so we have to create a virtual
reality simulator and you guys know
we're quite good at virtual reality
simulation and it has to be sufficiently
real that Leonardo thinks that it's
really learning and it has to be
physically accurate meaning it has to
obey the laws of physics and so what
you're looking at what you're about to
see here is a virtual reality simulator
of Leonardo learning how to be Leonardo
and it's Fitz photo-real so Leonardo
can't tell whether it's in the real
reality or not and it obeys the laws of
physics what you're seeing also runs the
entire computing stack it is all
completely based on hardware in the loop
so the entire computing stack is working
right in front of you okay so let's take
a look at that Rev this is Leonardo in
virtual reality
and Leonardo is learning and these are
all the different paths that it could
have taken and then showing the path
that it did take
it's just quietly in this lab learning
how to be a good robot
we could put ops objects in this in this
way look at that we put objects in this
way it moves away from it can you see
that and once we stop getting in its way
it could get back to work
it is it is running hardware in the loop
which means the softer computer that is
here running Leonardo and the computer
that's running that virtual reality
simulator same computer same AI
everything is working the perception of
the cameras the perception that comes
from the z-depth camera the laws of
physics the weight of the cubes all
exactly the same simulate everything and
this is how we're going to create robots
in the future
and part of the Issac SDK part of the
Issac SDK comes with the simulator
without simulation it is impossible to
create robots in the future they need
this virtual reality environment to
learn six degrees of pose perception
perception for visual perception for
touch now of course we would like
Leonardo to learn a lot faster right and
so definitely yeah see one of the great
things about Leonardo is it's got a
whole lot of friends and and replicated
itself and so it's every single one of
them are running on a real Leonardo
computer running the entire the SD the
the Isaac stack connected to this
simulator and it's learning and every so
often will checkpoint select the
smartest Leonardo replicate all there
are there AIS and then start again
ladies and gentlemen Leonardo
Nathan congratulations thank you Nathan
Radcliffe amazing AI researcher this is
a great achievement it's a great
milestone for robotics so this is this
is our stack this is the Isaac SDK the
processor Xavier the Xavier robotics
processor I happen to have one in my
hand in my pocket here we go
little tiny computer
the Xavier robotics processor the Isaac
software stack the rep the the reference
applications Leonardo and Carter Carter
for navigation Leonardo for manipulation
the simulation environments one to
simulate navigation we call Isaac sim
the other call Isaac Jim so that the
robot can learn how to be a good robot
all of this connected hardware in the
loop so you're looking at running real
software today we also like to announce
that universities across China are going
to adopt Isaac so that we could teach
robotics in schools and so that
researchers could advance this field and
help us discover the future of
artificial intelligence the next great
adventure
it has been an amazing year it has been
an amazing year accelerated computing
accelerated computing the path forward
has made some really fantastic
achievements because of our
collaboration we discovered three new
applications one application is rate
racing another applications by G and
another application genomics analysis we
put r-tx the rate we invented RTS
redefined a future of computer graphics
and we partnered with 10 cent to put it
in the cloud so that 800 million gamers
who don't have access to sufficiently
powerful computers can now enjoy PC
gaming at the level that we should we
invented a new way of sharing and
collaborating very complicated workflow
we call it omniverse portals from
applications to be connected into
omniverse so that designers from around
the world across different workflows
could collaborate together we created an
application for AEC omniverse AEC is now
available for early access we brought
CUDA aren't all of the body of work and
all of the applications we accelerate
including one of the most important
today tensorflow
now available on arm we made enormous
progress in the last year in inference
we are accelerating the single most
important model in the world on the
internet called the recommendation
system as the world moves towards deep
recommenders we have the ability to
power the internet and of course tensor
RT 7.0 makes it possible for us now to
compile all kinds of neural networks
from CN NS to transformers and very
importantly our intents which makes it
possible for the very first time for us
to have interactive real-time
conversational AI from end to end we
announced our Oram processor the
next-generation
robotics processor seven times more
powerful and includes all kinds of new
technology for function functional
safety and security and then lastly we
announced a brand new SDK we call Isaac
to enable the next
generation of AI we call robotics and
demonstrate to you Carter and Leonardo I
want to thank all of you for your
collaboration to make accelerated
computing amazing and thank you for
coming and enjoy a GTC
[Applause]
Title: Eliminating Collisions with Safety Force Field - NVIDIA DRIVE Labs Ep. 8
Publish_date: 2019-07-24
Length: 117
Views: 17072
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/V1f0iXNkyOo/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: V1f0iXNkyOo

--- Transcript ---

Today we're talking about DRIVE AV Mission
60, the NVIDIA Safety Force Field or SFF.
SFF is collision avoidance software that takes
in perception results obtained from sensor
data, identifies potentially unsafe situations,
and if necessary takes appropriate action
to avoid a potential collision.
Here we see a simulation of four cars heading
towards each other, and their primary planning
and control systems have failed.
With SFF off, they collide.
And here we see the same simulation but with
Safety Force Field on.
In this case, SFF corrects the controls of
each car and the collision is avoided.
In this simulation, the safety force field
is computing the ego car's forward trajectory
in yellow, and as soon as it determines this
trajectory will touch the stationary vehicle
ahead, SFF overrules the primary control system,
applies the breaking safety procedure, and
avoids the collision.
And here we see SFF applying steering in a
safety procedure to steer the car around the
stationary obstacle.
In this simulation, the primary planning and
control system initiates a safe lane change
in congested traffic and the Safety Force
Field does not override this decision, which
shows that SFF does not over-constrain their
primary system in normal everyday driving.
Here the Safety Force Field is running live
in the car in a four-camera surround perception
setup.
The SFF computed relative trajectories for
each car are visualized in different colors.
In the rear camera, we see that the car behind
the ego car is starting to follow too closely,
and we see that because its SFF trajectory
in yellow starts touching the ego car.
If the Safety Force Field were running in
this car, it would have it slow down, which
also matches what the primary motion planning
system, the human driver in that car, does
in this case.
SFF is always running in the background and
independently checking the actions of the
car's primary motion planning system.
In doing that, it's bringing important diversity
to overall planning and control software.
Title: GeForce Garage – Spotlight on Custom Rigs for Hunter & Alexis Pence
Publish_date: 2017-02-17
Length: 303
Views: 11583
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/v3O8Kl2-06w/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLAy1GNZbtUarNd6S1k0CW6Q66d5kQ
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: v3O8Kl2-06w

--- Transcript ---

what's up everybody and welcome back to
G forth garage my name is Dwight and I'm
joined with my good buddy Brian Carter
Boddicker of Bob's mods we've had the
wonderful opportunity to work with San
Cisco Giants outfielder hunter pence and
his wife Alexis on a wonderful charity
build for project open hand there's
another video that recaps that charity
builds and this one we're going to be
checking out these custom mods that we
build for unturned lexing
[Music]
so Brian coming your process with
building both days crazy pcs well the
first thing we did we met with hunter
lucky and got their ideas and
suggestions for what they want to have
in their PCs and for Lexi's case we kind
of went with the colors that she chose
for her youtube page which is orange and
pink and ensure favorite thing is a
pineapple with sunglasses this is part
of her logo so we integrated that onto
the front panel and came out pretty cool
yeah the lighting you kind of emulated
with those same colors and they're
lighter let's get Lexi logo on the
bottom modulating between those colors
for the inside of the case some of the
panels on the inside with painted hot
pink to match I think this has the
world's first hot pink 1080 it ever made
so that's a special thing right there
I'm again I'm going to need one of those
by the way I want to go hot pink on
everything so performance was we knew
that she was going to be doing a lot of
streaming for her YouTube channel and so
we wanted to make a system that she
could do a lot of video editing so it
has 4 terabytes of storage as well as a
HDMI capture card built in now which I
say you did a very cool job of actually
integrating it with the PC like you
don't need to has a little dongle or
anything that's right don't matter they
man degraded into the case so that would
be simple for her to plug in and just
work
[Music]
so Brian you packed a lot into Lexi's PC
is a lot of power in a tiny little
package but what you did for hunter is a
completely different ballgame
well back to the mystery to an Intel i7
32 gigs of Corsair Dominator memory and
the four terabytes of the Western
Digital drive space so what can you tell
me about the modifications with this
thing I noticed you have a pretty
amazing reservoir right there yeah well
we were talking with hunter and we told
them that you know we could make him
anything that he wanted scene wise but
he really wanted to show appreciation
for the Giants and so with that we went
with the baseball theme and black and
orange and as a last minute item I
wanted to add a water feature that
included the NSF logo and so this is a
two-channel water feature that each
letter has its own water path duck is an
inlet and outlet from the radiator to
the CPU what about the leather that's on
here don't think the thick son will take
leather on it
no we covered the entire case with black
leather we kind of wanted to go for a
baseball glove theme so we had some
leather lacing around the edges and a
couple features that you would normally
find on a baseball glove and for the
stand off on the p5 we change the stand
off to bat and so I made it look like
the bats were going through the case and
I personalize it with elements that from
his actual bat that he uses yeah I see
that model hp9 yep and he got a cool
fangirl back here as well yes we he also
wanted to emphasize the world series
that he was part of the 2012 and 2014
World Series champions and so he
reintegrated those logos
to the fan grill so I noticed that the
feet here these don't look standard did
you make it yes the stock be kind of
looked like the Golden Gate Bridge and
so I took that San Francisco Giants team
a little bit further and integrated the
UK speed to make it look like the Golden
Gate Bridge so the last big thing that I
saw in the case was an insane piece of
glass sits in front of it can you tell
me more about that yeah it was the the
main feature on the case hunter really
wanted to personalize it with an image
that he's really fond of and that was
when he won the 2012 World Series so he
wanted that picture on the front of this
case and so we got rid of the quarter
inch acrylic that came stock with the
case and we replaced it with a 3/8 inch
thick piece of glass and had it itched
and painted with that image onto it so
thanks everybody for watching these are
two amazing amazing mods Brian thank you
very much for all your hard work
thanks for Evan yeah anyways if you guys
want to see anything else or Hunter and
Lexi's reactions is going to be a link
down below that you can check out thanks
again for watching two-foot garage
you
Title: NVIDIA RTX Comes to Unity to Power Real-Time Ray Tracing
Publish_date: 2019-03-27
Length: 96
Views: 25082
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/VE1tshplrW4/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLCOGCLxnb_EKce5HdpcYk7thKkDJg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: VE1tshplrW4

--- Transcript ---

[Music]
yesterday the GTC keynote we unveiled
reality versus illusion and Jensen
challenged the audience to tell the
difference between the coordinates that
we took on site of a real car versus the
rendered footage this one's not real
this one is r-tx that one is real is
that amazing by mimicking the physical
properties of light RTX has made it
possible for us to perform real-time ray
tracing for the very first time having
the hardware in there is like a huge
game-changer for actually making this
technology a real and this is a
long-term partnership if we went from
working together to really amping up
work possible and the thing that is big
is moving from real-time and likes
always been big because you get such
beautiful bounces it just is such a high
quality but you have to pay for that by
waiting for renders and we can move into
this real-time space you get that
immediate feedback for disabilities are
able to improve and iterate on your
projects and just get a much better
result
Ivan our background started out wanting
to make art from video games before I
kind of wandered off and got more
involved in technology
I am like super excited like one of
those tools we've have like a visual
stage of creator and now you can create
a shooter and that will be able to work
with real-time retreating without you
having to jump through a bunch of Hoops
that we're not holding people back with
the tech that they're using and that
we're able to just let them create
[Music]
Title: Five Things to Know About USD | NVIDIA Omniverse Tutorials
Publish_date: 2020-12-15
Length: 359
Views: 39506
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/vFxytzQlOEs/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: vFxytzQlOEs

--- Transcript ---

hi everybody my name is dirk van gelder
and i'm here to tell you five things
to know about usd or universal scene
description which is a core technology
that on reverse is built around
now the first thing to know is that usd
or universal scene description
was developed by pixar for its animated
feature film production
at pixar they needed a way to be able to
describe rich complex 3d scenes like you
see in this shot from coco
and they needed to have a format where
they could represent these scenes and
also have hundreds of artists
across the studio all work together
collaboratively
and so usd is the culmination of 30
years of software development at pixar
to produce
a representation for collaboration in a
3d world
now it was written to be a foundational
library that was used
as a basis to build other things on top
of so here you see pixar's presto
animation system
here used to create the fur for the
raccoon and incredibles 2
or here showing the animation interface
that was used in finding dory so it was
it was written to be a thing that you
build other applications upon
and that is what we thought was
particularly interesting and useful for
us
in omniverse so something to know is
that
number two is that usd is an open source
standard with wide industry adoption
in 2016 pixar released usd
as an open source package here we see a
demonstration
from the day it was open sourced and
we're seeing 3d content from the movie
finding dory
but it's being played back within a
software called usd view
that is completely free every all of the
software that you're looking that you're
looking at is open sourced by pixar
and the the code is available on github
which is a website for developers so if
any developer wants it they can just go
ahead and grab this code and use it
now because it's open and free and
because it's an excellent
representational
medium for three-dimensional scenes a
number of other
companies in the industry are adopting
it we're seeing wide industry adoption
so here autodesk this is an example from
their maya package
is integrating usd you have examples
like this from side effects they've
written the solaris
interface within houdini which is
excellent and uses usd
have companies like epic with their
unreal 4 engine has a usd
integration so adoption across a number
of different places here finally we see
apple
this is a video from one of their
developer conferences in 2019
talking about why they choose usd as a
representation for their augmented and
virtual reality scenes
that they have on their macs and on
their iphones
now usd has a lot of features inside of
it that
can't that pixar developed in order to
represent their scene here's a set of
of features that was within usd that
apple thought was
advantageous and why they built their
software upon it
now another thing to note is that
omniverse is the first application to be
built entirely around
usd this is not a format for import and
export
the actual document that you that you're
working on is usd itself so here we have
omniverse with the attic scene
um and all of the things going through
our beautiful real-time ray tracer here
are represented in usd and you see a
list of them on the right
so you can change properties within them
like changing the light
changing the lighting or you can move
this horse and as you see on the bottom
right there
as i move that horse a value in the usd
is changing so the document is usd and
that is something that is a first with
omniverse and we think is pretty cool
now one more thing to note about usd is
that omniverse extends usd to enable
live collaboration with other
applications
because usd is a great way to define
three-dimensional worlds and a standard
way to do that we can use that
standardization
to enable our connector tools to be able
to integrate omniverse with other things
in this
example we see the tools rhino 3d
3d studio max and revit and you want to
be able to have those things communicate
live with something like omniverse so
here omniverse is showing
a real-time ray tracing this is
something frankly we do better than
anybody else
so if you want to have your artist while
they work in their 3d tool
be able to see their results in
real-time ray tracing you can do it with
omniverse as you see here where an
artist is modeling with rhino 3d
and seeing the lighting change in their
model in omniverse here
in 3d studio max you see buildings going
up on the block and you see the lighting
change
within omniverse as you look at it so
this live collaboration
is something that is new uh with
omniverse here you see
revit placing uh tables around this room
and you can see the lighting change in
omniverse live as you work and that is
enabled
by our use of universal scene
description
now finally usd enables complex asset
structures that are shared between tools
just as pixar needed to do to have its
hundreds of artists be able to create
these rich complex three-dimensional
worlds
nvidia does this as well so you see
these assets that were created within
omniverse using universal scene
description
this is our headquarters in santa clara
and we this was developed by artists
distributed all around the globe
using the standardization and
integration of usd
uh here are some other models that you
see again all of this was modeled in usd
by artists distributed around the world
and then run through our real-time ray
tracer to display these images
so i hope we've given you at some
understanding of what universal scene
description is and why it's important to
your use of omniverse
i hope you've signed up for our open
beta program and enjoy using omniverse
thank you very much
you
Title: CES 2017: GeForce NOW: PC Gaming from the Cloud (NVIDIA keynote part 3)
Publish_date: 2017-01-05
Length: 407
Views: 109231
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/vg80Tjmr3Jo/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: vg80Tjmr3Jo

--- Transcript ---

if you think about the number of PC
gamers in the world who would like to
get into PC gaming or their people that
that they have a thin and light notebook
but they would like to enjoy some PC
gaming every now and then and just
haven't haven't dedicated themselves to
build a new GeForce PC the number of
those those PC users is shockingly high
there are 2 billion PC users in the
world we estimate some billion users
with integrated graphics or Macs or
older pcs or thinner like notebooks that
would love to be able to enjoy games but
simply don't have the capability to do
so and in fact there's really no real
way of installing a new GeForce graphics
card into those computers and and maybe
they're they're rather intimidated by
opening up their computer in the first
place and building something new and we
thought that would it be amazing would
it be amazing if we were to put a
GeForce gaming PC a state-of-the-art
GeForce gaming PC powered by Pascal in
the cloud like AWS build a supercomputer
like AWS just for enterprises we would
do this for consumers and as a result
these supercomputers in a cloud could be
shared by millions of gamers around the
world they could try video games for the
very first time if they don't play play
frequently they could launch a game
whenever they want wherever they want
and if they could just enjoy
state-of-the-art video games like this
with just a click of a button
when that'd be utterly amazing so after
many years of endeavor this is just one
of those incredibly hard problems and
it's incredibly hard because the
computational capability necessary for
video games is so high and the
interactivity is so requirements so high
that any little bit of latency would
ruin the experience and so after just so
much refinement so much.we architecture
so much engineering the team has finally
done it ladies and gentlemen we're
announcing today
GeForce now 4 pcs
[Applause]
g-force now it turns any of your PCs
with the download of a little tiny
client and to essentially your most
powerful gaming PC and it's all in the
cloud just one click away when we take a
look at it okay so what you're looking
at here two computers in the back one is
based on PC one is Mac the thing that
you instantly recognize on the PC side
is all of the major stores and hubs are
now available and they work perfectly
steam with 150 million users works
perfectly origin from Electronic Arts
works perfectly you play from Ubisoft
works perfectly multiplayer works
perfectly all of your state storage
states your checkpoints all of your
friends all work perfectly every single
game works exactly as it should all of
the software has been updated and Dave
are you in the back yeah hi Jen hey hey
Dave Happy New Year first of all happy
New Year to you so so why don't we do
this why don't we launch steam steam is
the single most popular video PC gaming
store in the world and here David's
launching steam on GeForce now okay
there you are
and there you are just a few seconds and
you're into steam and all of the games
that you've purchased or all of the
games you brought into steam are all
there it also works on the Mac so when
we go over to the Mac it's just one of
the apps on Mac double-click on that
there it is
steam on Mac and you can buy a game
right there you can buy a game right
there we won't do so right now but we
could buy a game and usually when these
video games are so large they're so
large because they have so much content
and takes hours and hours to download it
but in the case of GeForce now it only
takes about a minute and so why don't we
go ahead and launch a game now that its
installed let's launch it and this game
doesn't run on a Mac and this game runs
very poorly on integrated graphics but
yet we're going to see it working here
and it's full fidelity and one of the
things that's really cool is that it
just works exactly the way you would
expect an app to work on a computer it's
completely seamless to you you pretty
much forget that it's even in the cloud
and video games are complicated in the
sense that the game is always being
patched there's always digital download
content the drivers need to be enhanced
all the time and we can do that all
behind your back update it all the time
keep your computer always fresh and
there it is Tomb Raider running on
GeForce now
over to cloud a click away video games
for the other billion PC users okay
thank you
so g-force now will be available in
March for the early users it's coming
very very soon we're putting the final
touches in there and it'll be available
for $25 for 20 hours of play
it's basically a GeForce gaming PC on
demand GeForce gaming PC on demand there
will be several grades of performance
that are available to you the higher
performance grades you'll have fewer
hours for every $25.00 or credits
okay so GeForce now incredible value for
somebody who doesn't have the ability to
access a gaming PC somebody who hasn't
taken the effort to build a gaming PC or
who's somebody who just placed
infrequently but would love to be able
to enjoy a videogame from from time to
time GeForce now thank you
[Applause]
Title: How NVIDIA GPUs Accelerate Adobe Illustrator CC
Publish_date: 2014-06-19
Length: 156
Views: 69763
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/VHR0-qR927A/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLBvo38MNc_1V3g3YG_UIDBnLERpNQ
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: VHR0-qR927A

--- Transcript ---

[Music]
hi i'm sean from nvidia today i want to
share with you an exciting new
enhancement nvidia has been working on
with adobe to improve adobe illustrator
CC performance as you know adobe
illustrator is the premier vector based
software tool used by designers to
create everything from digital graphics
and illustrations to typography for
print web interactive video and mobile
media now Adobe and NVIDIA are bringing
increased GPU acceleration to Adobe
Illustrator to give creative designers
and artists a serious productivity boost
this speeds up the entire Illustrator
canvas across all features and functions
as a designer you want smooth fluid
interactivity with your creative tools
but artwork complexity and growing
display resolutions make it tough for
even today's fastest CPUs to keep up
result is choppy and a frustrating user
experience that's not only incredibly
disruptive but interrupts the creative
process NVIDIA GPUs can now accelerate
the entire Illustrator canvas to enable
real time interactivity regardless of
the image size feature mix or display
resolution let's have a look it's really
easy to enable this new feature for
documents using RGB mode the GPU
performance feature is already turned on
by default you can select enable for
CMYK documents in the GPU performance
settings sections in the experimental
featured tab of the illustrator
preferences panel so how does it all
work
illustrator is a vector-based graphics
application because it doesn't use
rasterize pixels it enables precise
drawings in typography that's scaled
without compromising image quality but
vector rendering takes a lot of
processing power so Dobies engineers
work closely with Nvidia to implement
GPU acceleration using OpenGL based path
rendering on Nvidia graphics card
the result is a great creative
experience and depending on the GPU it
can be over 10 times faster than with a
CPU alone this performance enhancement
is available to all illustrator users as
an experimental feature for
windows-based pcs and workstations with
NVIDIA GPUs
although GPU acceleration isn't
available for illustrator on OS 10 today
we do realize the importance of the Mac
platform to designers and continue to
investigate how to best enable new Jeep
new features in the future for now Mac
users can still take advantage of this
new feature by running Windows on
bootcamp get started today by getting
the new version of Adobe Illustrator CC
and upgrading to an NVIDIA GPU
accelerated workstation from one of our
industry-leading partners and please
send along your feedback so we can keep
making creative tools like illustrator
even better
[Music]
Title: Harrison.AI Uses NVIDIA AI to Improve IVF Success Rate
Publish_date: 2019-11-12
Length: 147
Views: 3564
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/vHr5o-olYEM/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: vHr5o-olYEM

--- Transcript ---

[Music]
hi my name is Angus I'm the CEO of
Harrison AI so we our company to aim to
apply the powers of deep learning and AI
in solving some of Medicine's most
challenging challenges so one of the our
most successful projects so far is a
project called Ivy which is a deep
learning based project that able to look
at sequence of time left videos of an
IVF embryo developing over five days and
this model is able to look at that video
and decide what is the probability that
that given embryo will create a
successful pregnancy and video solution
has been critical both in terms of
software and hardware with the advances
in GPU especially given boltar
technology and a deject station we're
able to run multiple experiment parallel
and designing the most optimal solution
and architectures so that we get the
best possible performance in the
shortest amount time are we also looking
at creating an artificial intelligence
software that is able to detect
tuberculosis disease through a frontal
chest x-ray and we're hoping to grow
that project out in Vietnam so providing
the national wide screening program to
identify high-risk patient to provide
specialized treatment for and screening
for them at a very low cost so at the
Harrison AI what we aim to do is to
harvest the power of AR and deep
learning and taking advantage of GPU
accelerated computing - essentially
providing healthcare at a cheaper cost
fastest and of a higher quality a tryout
that traditionally never really happened
so my dream for AI is eventually will
get to a point where some of the
difficult decision that we need to make
in healthcare will be largely automated
by AI and that free up the doctors to do
what doctor is good at which is
empathizing with the patient catering
the treatment to specific patients and
their social circumstances and
essentially be able to develop cheap and
affordable healthcare to the mass
especially in limited restricted
country like Vietnam India and China in
the third-world country and that's why
I'm very excited about this and I can
hardly think of anything more
stimulating to work on
[Music]
Title: The Bureau: XCOM Declassified Launches With NVIDIA PhysX Effects
Publish_date: 2013-08-25
Length: 85
Views: 22019
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/vJClNPiyOYQ/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: vJClNPiyOYQ

--- Transcript ---

is Jan from Nvidia you might call the
recent release of the Bureau
XCOM declassified did you know that the
PC version contains lots of exclusive
tech like tessellation and Nvidia PhysX
take a look at the trailer
if you like what you saw the good news
is that the game is available worldwide
now so you can get it from various
retailers and of course on Steam
Title: SHIELD Tablet: Built For Gamers
Publish_date: 2014-07-22
Length: 143
Views: 285696
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/VohrddwVQqg/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: VohrddwVQqg

--- Transcript ---

hi i'm andrew with nvidia many of you
are familiar with shield the ultimate
portable for gamers we achieve this by
designing a building shield with the
most advanced technologies and
components but mostly by focusing on
delivering the best experience possible
for a community that we know extremely
well gamers today I'm really excited to
introduce the next products in the
shield family shield tablet and chill
wireless controller like shield portable
we built it to gamer specifications and
standards there are many tablets out
there but this is the first specifically
designed for gamers with shield tablet
we sent out to redefine mobile gaming
with the Tegra k1 mobile processor Tegra
k1 uses the same Kepler GPU architecture
as our industry-leading GeForce GTX
products it's super fast and comes with
all the graphics goodness that serious
gamers have come to love with the power
to run Unreal Engine 4 on shield tablet
you'll get to play games with
technologies like DirectX 12 and OpenGL
4.4 shield tablet gives you graphics
performance that no other tablet can
provide she can enjoy PC class gaming on
the go and insanely loud stereo speakers
powered by pure audio technology
the gaming tablet needs two things a
great controller and a flexible stand so
we built the shield wireless controller
with super low latency Wi-Fi direct and
the ergonomics of a high performance
precision controller the cover folds up
into three different gaming positions
when you're using your wireless
controller and protects the screen when
not in use we've ensured you get games
specifically crafted for this platform
PC class titles like trying to war
thunder anomaly 2 and hundreds of Tegra
optimised titles that look and play
great on shield tablet this is the only
tablet with Nvidia game stream
technology paired with shield wireless
controller you'll be able to stream your
favorite PC games like titanfall over
Wi-Fi or the optional 4G LTE from your
GeForce GTX Power PC you can also play
hundreds of android games optimized for
controller support and stringer games to
twitch using Nvidia shadowplay
for more information visit shield nvidia
comm and thanks for watching
[Music]
Title: Chaos Group V-Ray GPU Delivers Accelerated Creativity
Publish_date: 2017-11-06
Length: 122
Views: 18831
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/vt5BsPxtvWw/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLBQLFc2RLNbbKFJ1XNOAM-OaAlaEg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: vt5BsPxtvWw

--- Transcript ---

[Music]
originally when we did very GPU we
thought that people will be using it
mostly for interactive rendering but we
quickly realized that people want to use
it also for final frames my name is Lada
I'm the CTO of chaos group and I'm the
original developer of the v-ray renderer
v-ray is a photorealistic render it's
been around for 15 years it's used for
architectural visualization it is for
product design TV commercials TV shows
film visual effects our goal is to have
all the features supported on GPU more
and more people are using the GPU render
especially now we added hybrid rendering
so people can actually use also the CPUs
that we have in that machine and it
really helps a lot for people who just
want to experiment with GPU rendering
but they don't have the hardware yet
it's been amazing really because the GPU
speed is going up so much faster and
then the speed of CPUs we just had the
Pascal release like a few months ago and
now we have volt already so it's kind of
amazing
people have more and more complicated
scene these days like millions of
polygons lots of pictures and we were
worried that memory is going to be a
problem but it looks like it's not that
much of a problem anymore
gee p100 is a very interesting piece of
hardware not only because of envy link
which allows our customers to put
multiple GPUs and then share the memory
between it also the GPE 100 is a lot
faster than we expected it's like up to
40% faster in the previous generation
cars which was really amazing we didn't
expect it to be that much faster
I think GPU rendering your next five
years will be everywhere obviously
people render more and more complicated
stuff especially for VR and GPUs really
helpful
[Music]
Title: Omniverse Create - Rendering Overview | NVIDIA Omniverse Tutorials
Publish_date: 2020-12-28
Length: 159
Views: 23291
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/vTGI7NXe9g0/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: vTGI7NXe9g0

--- Transcript ---

let's talk about rendering in omniverse
create
i'm reverse rendering is based on the
hydra rendering api
developed by pixar it's an open source
standard for multiple scene graphs and
multiple renderers
omniverse create comes with several
hydro based renderers
an rtx real-time mode built from the
ground up to support significant scene
complexity
physically accurate materials lighting
global elimination
and multiple gpu support rtx real-time
is unique
and that its core is ray tracing
unlike traditional real-time renderers
that are built on rasterization
ray tracing can support massive amounts
of polygons making it ideal for complex
virtual worlds
cad data and film quality assets
there's also an rtx path trace mode
the rtx path trace mode is designed
around ground truth rendering using path
tracing
it's interactive and supports multiple
gpus
for the most visually demanding scenes
there's also iray when you need
physically accurate rendering leveraging
mdl
and spectral rendering for true
simulations of reality
additionally storm is also available
this was developed by pixar and is a gl
based raster renderer
great for fast previews on almost any
gpu
you can use both usd preview surface and
mdl
for your materials across these
renderers
lighting is also based on pixar usd lux
lights
enabling accurate lighting area lights
disk lights
sphere lights dome lights directional
lights
which are all interoperable across a
wide range
of content creation tools
on top of that you can use post effects
including
depth of field fog bloom tone mapping
color correction and more
and these effects work across multiple
renderers
stay tuned for more supported renderers
i look forward to what you can do
with real-time ray tracing
you
Title: How AI-Based Perception Helps AVs Better Detect Speed Limits - NVIDIA DRIVE Labs Ep. 25
Publish_date: 2021-09-08
Length: 132
Views: 18110
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/VUywXkqNQ2g/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: VUywXkqNQ2g

--- Transcript ---

today in drive labs we're talking about
creating an advanced speed assist system
or sas that can understand implicit
explicit and electronic speed limit
signs as well as their relevance to
different paths on the road all by using
live perception only
here we see our live perception sas
understanding an implicit speed limit
based on this entrance to motorway sign
in germany specifically the system
detects the sign classifies a sign type
triangulates its 3d position
associates that 3d position with driving
paths to which the sign applies and
interprets the maximum minimum and
advisory speed limits implied by this
sign based on local rules and
regulations
until this implicit information is
superseded by an explicit sign as we see
here in which case our sas sets the
speed limits to the explicit value 60
kilometers per hour
and then increases the maximum and
advisory speeds to 80 kilometers per
hour based on the latest explicit speed
limit sign information
and here our sas is interpreting speed
limit information from an electronic
variable message sign which contains
lane specific speed limits our sas
correctly understands that the speed
limits shown here apply to the left and
center lanes but not the right lane we
also note that this is a particularly
challenging scenario for live perception
sas because of the electronic flicker
caused by the difference in refresh
rates between the vehicles camera and
the science electronics
and here we see our sis interpret a
series of explicit speed limit signs
that apply to just the highway exit
lanes
and then reset to implicit speed limits
when the end of speed limit sign is
encountered
and here our live perception sas is
performing sign to path association on a
u.s highway and it correctly understands
that the 25 miles per hour explicit
speed limit applies to just the exit
lane
site to path relevance information
provided by this live perception system
can also be fused with information from
a map to further enhance robustness and
increase coverage for a wide range of
real world scenarios
Title: AI and NVIDIA GPUs Give Unparalleled Freedom to Creatives
Publish_date: 2018-11-28
Length: 73
Views: 7785
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/VV9Wph3VTSY/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLDDKsq0L5knBfioHxSGiM-CaTU-_g
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: VV9Wph3VTSY

--- Transcript ---

my name is Chris Bob Otis and I'm the
director of immersive at Adobe
so what we're showing today at the
experts bar is a series of products that
work in After Effects and Premiere Pro
and they'll help you along with
cinematic 360 VR Adobe Photoshop and
Adobe After Effects our Bar None to me
the pinnacle of software I'm a content
creator speed equates creativity to me I
find them synonymous the more I can
iterate the more I can experiment the
better the results of the content that
I'm creating
so the GPU of course has a big big part
in it adobe has a whole environment
called sensei sensei makes its way into
a lot of the Adobe products today
including project arrow that we'll be
showing today and many many other
products Photoshop eventually
after-effects Premiere Pro so sensei and
machine learning are huge we've got this
amazing thing called machine learning
that comes into play so I can actually
revisit algorithms that were gonna go
into nowhere tack on machine learning
and expand on that and get the results
that you've never seen before
Title: 7 Crucial Survival Tips for Rainbow Six Siege
Publish_date: 2016-01-15
Length: 175
Views: 16445
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/VxzBwGb4uNo/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: VxzBwGb4uNo

--- Transcript ---

for those who are new to siege and
haven't played it yet here are seven
basic tips Rainbow six siege is the
thinking man's shooter running and
gunning just isn't going to cut the
mustard in the latest installment of you
basalt Tom Clancy series instead success
in the world of siege relies on
understanding the environment around you
and knowing how to use the tools of the
trade so here are our top seven tips for
surviving siege besting your opponents
in siege is all about surprise finding a
new angle of attack or a new line of
sight together drop on them and you can
work the environment to your advantage
to achieve this stud walls made of
plaster board or wood can be destroyed
to give you line-of-sight or even access
where as concrete and metal rules can
bar your progress when defending use
your preparation time wisely you can fit
an unlimited number of wooden barricade
defenses in doorways and windows and a
maximum of two wall or trapdoor steel
reinforcement panels razor wire is also
great at slowing down attackers at key
choke points giving you an extra heart
beat to make a killer shot when on the
attack barricades can quickly be
overcome with gunfire melee hits or
breach charges while walls reinforced by
the defenders can only be destroyed with
special exothermic charges which brings
us onto sieges operator system playing
the game gradually earns you renowned
points which can be redeemed against a
roster of counter-terror stop chips from
around the world adding to your options
for attack and defense each comes with
unique equipment perk like the attack a
sledge estampa a great Thor hammer and
thermite steel slicing breach charge all
the defender smokes toxic gas canisters
and to chancres mounted light machine
gun when attacking use your remote
camera drone wisely in addition to
locating your main mission target try
and find a hidey hole from which you can
tag enemy positions if of course you can
avoid spinning your circuitry over floor
first
don't forget the while defending you can
also tag approaching enemies through the
local security camera system and no
matter where you are always listen out
for telltale footsteps and other sounds
to help you find your target if you come
into Rainbow six siege expecting the
same kind of experience as some other
well-known FPS titles think again
in siege it pays to take your time and
think before you act
if you check your corners stay low and
slow and keep your finger off the sprint
button you might just make it through
the round in one piece finally don't
forget that siege is not a game for solo
survivalists work closely with your
teammates and communicate by doing so
you'll be able to enjoy the very best
the game has to offer reaching with your
mates ready to lob in a stun grenade
before the full assault covering blind
spots and guarding the hostage while
your crew clears out the last of the
attackers these are the things that set
Rainbow six siege apart from the rest of
the field and are worth embracing right
from the start
Title: GPU Technology Conference 2016 - Day 3 Wrap-Up
Publish_date: 2016-04-08
Length: 149
Views: 17470
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/VYupDAw7hLY/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: VYupDAw7hLY

--- Transcript ---

hi I'm Danny Shapiro we are at the
closing day of gtc it's been an amazing
conference attendance rocked it we had
over 5,500 people i was up forty percent
over last year we were over 500
different sessions packed to the gills
topics ranged from artificial
intelligence and deep learning to
virtual reality of course self-driving
cars highlights of the week include
three amazing keynotes first Jensen Wang
co-founder and CEO of nvidia talked
about how artificial intelligence is
becoming democratized available across
all industries the day two keynote
featured Rob hi the CTO of IBM Watson he
talked about how nvidia gpus are at the
heart of watson and enabling the
understanding and cognition of data in
human-like ways and finally our day
three keynote featured Gill Pratt the
CEO of toyota research institute he
talked about how Nvidia and Toyota are
partnering to develop technologies for
self-driving cars in particular using
simulation technology toyota is focused
its number one goal on safety and using
technology to deliver a safer product in
the expo there are over a hundred
different exhibitors showcasing the
innovative ways GPUs are being used but
in particular the VR village highlighted
nvidia gpus at the center powering these
amazing virtual experiences exhibitors
and attendees both lined up to
experience the amazing world of virtual
reality putting on oculus rift and HTC
vive headsets powered by nvidia gpus you
could scale Mount Everest you could
catch bullets on a train and take a walk
through oven videos new headquarters
that's under construction a key topic at
gtc with self-driving cars we showcased
our dr DX system and how its able to
sense plan and execute all phases of
autonomous driving we also announced
that the dr PX product will be in every
robo race car now this is very exciting
future circuit where autonomous race
cars are going to compete
against each other there's so much more
going on at gtc if you weren't here
check it out online all the sessions are
posted we look forward to seeing you
next year
you
Title: The Future of AI-Accelerated Industrial Automation with Siemens and NVIDIA
Publish_date: 2022-06-30
Length: 1784
Views: 3341712
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/vzgutG4ppWA/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: vzgutG4ppWA

--- Transcript ---

for over a decade now
our digital twin technology has been
helping customers across all industries
to boost their productivity
today we offer the industry's most
comprehensive
physics-based digital twin
it not only looks like the real thing it
also behaves like the real thing
it represents
the exact functions and behaviors from
the system in the real world how does a
product behave if we shake it or heat it
up
or
if we run our software on it
the twin will tell you
let me give you an idea
of what could be possible in the
industrial metals
imagine
your factory in china is slowing down
it produces fewer parts every day
nothing bad
but it adds up
your team at the factory has no idea why
this is happening
to fix this you bring together a global
team
so they meet and collaborate
in the metaverse
they immerse themselves in the digital
twin of the plant
which is the perfect mirror image
of the plant and of course it is not a
still photography
it also mirrors exactly what's happening
in the real plant and in real time
down to the physical behavior of the
robots
now the team is looking for the problem
in the virtual field first in the
digital twin to all of them it feels as
if there were at the real factory in
china together
in part that's because of
the photorealistic visualization the
team decides to travel back in time in
the metaverse
to a point when the output
had been strong
so finally they realized that one robot
on the feeder line had missed its latest
software update and is out of sync a
rare miss
so time to fix it the team updates the
software in the digital twin
first
the virtual problem robot immediately
speeds up
now
the team is confident enough to update
the software on the real robot at the
real plant
problem solved
this is the moment
the real world
starts to mirror the digital twin a
closed loop between the real and the
digital worlds
so of course the industrial metaphors
will not just help us fix things when
they break it will be a place where
engineers workers anybody
can experiment and test and improvements
try out big new ideas
also ai bots
will tour our digital twin 24 7 on their
own
and come up with innovative ideas to
make things better
well the industrial metaphors will be a
place where we innovate at the speed of
software
it will offer enormous potential to
transform our economies and industries
now you ask yourself who will build the
metaverse
that's like asking who built the
internet
it's all of us
that's why today
we are announcing a partnership with
nvidia a company well known for its high
power computing
visualization and iii capabilities
and i have invited jansen huang ceo and
founder of nvidia to join us today
if you look at the last
decade of digital transformation of
industries
siemens played a giant role in laying
the foundation of the journey of digital
digitalization
and i started all the companies towards
putting all of their design information
all of their operations information all
of their
planning and and manufacturing
information all into a digital format
siemens accelerator
from cad
to plm to erp all the way to ot
has enabled companies to enter the world
of digital
the next decade
is going to be fairly exciting it's
going to be completely transformative
and it's going to do for
the world's industries what it has done
for the world's
consumer internet companies
and the two enabling technologies that
had to be invented of course one of them
is artificial intelligence
and we've been working on that for the
last decade and we've made enormous
progress
we now have the ability
to
write software that no humans can
and perform perception sensing reasoning
and activation
software that that is really quite
magical
however something else that needs to be
invented in order for us to
deliver to develop the ai
and also to deploy the ai and let me let
me explain why
in the world of developing ai for
internet services everything is software
anyways you're recommending a movie or
recommending a search or recommending
news or recommending music
and
the recommendation using ai has made it
possible for us to all enjoy the
internet in a personal way
however that technology as miraculous as
it is and it is miraculous indeed
makes a recommendation that in the final
analysis
if not exactly what you prefer
does no harm
in the case of the world's industries
two things
are unique one
the recommendation that our systems are
recommending
engage
the physical world means it has to make
a recommendation that understands the
laws of physics and the second thing is
the recommendations it makes has
significant life security and otherwise
implications and so we have to do an
incredibly good job and so the second
piece of enabling technology is this
concept of a live virtual world
otherwise some people call
the metaverse this live virtual world
has the ability to
mimic the physical world
in full fidelity
and it has to mimic the physical world
in real time this is the only piece of
software
that
in the industries will engage from the
moment they start designing their
product plant or process
all the way through operations and it
will live on for as long as of course
that product or that process
is in operation
this is where the partnership between
our two companies is so important no
company in the world has done more to
integrate and to develop a comprehensive
suite
of tools and applications for the
creation of what is called a digital
twin
however between our two companies and
now with our two companies we can
connect
what siemens makes
and what nvidia makes ai and omniverse
and turn that digital twin
into a live digital twin we can now
fuse information fuse data from the
point of design all the way through
product life management all the way to
the the the automation of plants
to the optimization of the plant after
deployment that entire life cycle can
now be in one world just as we're all in
the same world of of the internet we can
now be in the same world of the
metaverse and because it obeys the laws
of physics because it's real time
we can we can invite literally everybody
who is part of the design process from
designers to planners to operators
into this world into this world of the
industrial metaverse and so our our
partnership of connecting what siemens
makes and what nvidia makes to enable
this industrial metaverse is a first of
its kind it's a gigantic
technology breakthrough frankly and i
think that 10 years from now when we
look back we would realize today is
actually a very important day that we
have we have taken digital
transformation and gave it a giant leap
forward
this what you just said this is kind of
a replay
what we discussed nine six months ago in
november when we met and we figured out
that when we bring our competencies our
technology our platforms together we can
do something great we can we can
basically go for the full full-fledged
industrial methods
giving customers the potential the
possibility to have faster decisions
real-time decisions with higher
confidence
and then we figured out that
why don't we send a bunch of people from
your side and our side on a mission and
say what can you do in six months from
now in order to show what what our
technologies can do together yeah as a
first step
and why don't you have a look what they
what they created that's terrific let's
do let's see
siemens and nvidia are partnering to
advance industrial digital twins in the
metaverse opening a new era of
automation for manufacturing in this
demonstration we see how the expanded
partnership will help manufacturers
respond to customer demands reduce
downtime and adapt to supply chain
uncertainty while achieving
sustainability and production targets
by connecting nvidia omniverse and the
siemens accelerator ecosystem end-to-end
we will expand the use of digital twin
technology
to bring a new level of speed and
efficiency to solve design production
and operational challenges
siemens accelerator offers the
industry's most comprehensive digital
twin and includes best in class software
for digital manufacturing collaboration
design and industrial operations
omniverse based on the open standard of
usd universal scene description connects
a wide range of software tools and users
with ai physically accurate
visualization realistic physics and ray
traced rtx rendering
nvidia omniverse is a large scale full
fidelity virtual world engine and ideal
for industrial metaphors
the connected platforms running on gpu
accelerated systems from edge to cloud
unlock amazing superpowers from factory
planning to autonomous robots and beyond
uniting the worlds of information
technology and operational technology
the siemens accelerator platform uses
edge-enabled devices to collect
real-time iot data connected to the
digital twin when problems occur
engineering manufacturing and logistics
teams can immerse into the live digital
twin to root cause the problem and
develop a solution
new solutions can be tested and
validated using the digital twin across
thousands of scenarios and edge cases
eliminating the need for physical
prototypes reducing critical downtime
and increasing manufacturing agility
siemens accelerator and omniverse train
robot perception ai models on physically
accurate synthetic data generated from
the digital twin accelerating both
initial training and retraining in the
event of production changes
in live digital twins customers can
train robots to perform tasks before
deploying the ai models into the
physical robot
omniverse can even be used to train
fleets of robots working in harmony
industrial automation is being
supercharged with ai inside a digital
twin customers can design environments
that allow humans and robots to
collaborate safely and efficiently
together nvidia omniverse and siemens
accelerator are bringing open
collaboration to industrial automation
enabling full-fidelity live digital
twins to drive a new level of speed and
agility in the era of software and
ai-driven digital transformation
[Music]
wow that's almost like it's a real thing
it is and this is the start uh what did
it take uh from your perspective what
what did you engineer yeah there's
several things that you saw the amount
of data that we have to load into
omniverse
is extraordinarily large scale remember
a plant a manufacturing plant could be
millions of square feet
there could be
millions of moving parts hundreds of
millions of parts in total
and all of these all of this data all of
this geometry data has to
be able to mechanical data has to be
able to work in this virtual world
second we have to fuse
electronics information and software
information what you saw earlier is not
animation but it's a simulation
the mechanical information the
electronics information and the software
information from largely different
organizations
have been brought together in this
virtual world in this
industrial metaverse and once you can
bring all of this data together fuse it
together then you can activate it turn
it on just like we turn on the factory
we virtually integrate we virtually
assemble the factory and now we can
virtually operate the factory the amount
of data that that we saw just now is
extraordinary
because of accelerator because of all
the design suites that already captures
comprehensively a digital twin and does
it so well and because omniverse has the
ability to ingest
in its full fidelity all this
information and turn it on activate it
in virtual integration virtual
simulation uh the engineers took several
months right it's not an animation it's
a simulation that's right it's not small
detail but this makes the big big
difference the software of the robot is
operating the software the plant is
operating to
the company to the industrial company
that virtual factory is exactly the same
as the physical factory and that's
exactly the point exactly let me give at
this point a great high five to to our
teams who did it so everybody who was
working on it um well done fantastic and
we need more so um next point is about
how to bring it to the customers how to
scaling um how can we get traction on
the market one of the things about the
indus industrial applications is
although manufacturing is you could you
could imagine manufacturing uh
theoretically being very similar uh
manufacturing of cars of ships of plants
of of processes have some similarities
but they're very very different in many
ways and that's one of the reasons why
the domain expertise of siemens is so
valuable in this partnership we bring
the technology
of artificial intelligence and a virtual
world omniverse however siemens's domain
specific expertise in all of these
different forms of manufacturing is
invaluable to us this partnership
activates that and nothing is more
wonderful when you can
find a customer who is pioneering in
this way right who wants to build a
future who has the technical expertise
to engage us to build this future so
that we could
start this journey together and i think
both of us have a mutual friend who
exactly and this is a perfectly way to
um our next guest here uh milan dalkovic
he is
ceo from
bmw and he's to with us today one of the
most innovative customers in the
automotive space
but let me start with the challenges
what kind of things are ahead of us of
you
yes roll on first of all thank you very
much for the invitation today i have to
say it's uh great to be here with you
and jensen since uh we see is
as a customer but of course we are more
we have partners on that one we are
working together and and i think
especially working on on and the digital
world and the future is something
exciting and can set the pace working in
a collaboration and doing it together
and since you're mentioning the the
challenges and changes somehow
everything is in a change and
all at one time covet lockdowns we had
semiconductor shortages we had to react
very flexible and spontaneous to changes
and always to provide the right product
to the customer in in that time i think
with bmw we have a highly flexible
production system but looking into the
future it will become more and more
important to link the supply chains the
suppliers to get transparency in the
whole process chain and that's the
digital challenge we are facing there
at the same time we have a
transformation of the industry
in different dimensions
one is for sure the electric mobility
it's ramping up customers are buying the
cars we are we are providing the right
cars for the customers by the end of
next year bmw will offer 13 fully
electrified cars they need to be
produced so we are changing our
equipment we are
changing our production environments
processes and equipment and and all that
needs a good planning sustainability is
for us a significant transformation
field
premium means
sustainable
and that's why why we are today already
benchmark in the automotive industry but
we have set ourselves a target still in
this decade till 2030 to reduce 80 co2
emission by car
on top of that
and all that shows there is much to do
and an enabler for all that is again a
digital uh
world milan i've had the benefit if i
could just say say something about bmw's
miracle
i
i've had the benefit of visiting your
factory
in
a digital
metaverse in omniverse
and and this is really quite an
extraordinary thing you build two and a
half million cars a year 99 of it is
customized you're manufacturing this in
31 different factories
and somehow you you roll out a car i
think it's once every
is it minute or an hour every minute and
roll out a custom a fully customized car
every single minute and now you have to
custo you have to replan your factories
to be able to support ice as well as
electric
really quite quite a miracle
yeah
thank you very much for this compliment
looking onto the complexity that's
that's the core element we we have
somehow to manage and find a way how to
plan it in the right way and especially
how to steer it in the right way and
since we are working on this digital
twin approach together
we see that the chance to get a grip
onto it and that's a great perspective i
think it's worth looking in in detail
into it and and setting ourselves a
significant target and these challenges
you have to manage all at the same time
and this is what i understood so it's
it's a real transformation what we see
in the automotive industry
and bmw's answer to that uh is ifactory
is that right the bfwi factory is our
master plan for all these
transformations so it's our vision of
the production of the future and and and
we say the ifactory is lean green and
digital and i think it explains it by
itself so with lean we want to have
efficient processes lean processes but
also flexible processes this is the core
the heart of our production system we
want to be green we are developing
individual footprints for each single
production site worldwide
and we have it to adopt new ways of
energy management in in our sites
and of course digital that's the core
enabler we must
transform from our legacy systems into a
future where data is seamlessly
transmitted where transparency is given
where we are talking of flow systems
which link into our machinery with edge
computing and which goes into
applications and all that must be one
unit as a twin of of the production and
the steering complex of the production
eye factory the first time i heard that
from milan
it it really i realized that they
realize
that the factory is as much of a product
as the product that the factory is
trying to make
and yet all engineers would design
digitally with cad we would simulate we
would stress test we would optimize we
would be connected to our product we
would continue to refine it with ota
over time
but yet very few companies would treat
their plants that way to design it
digitally
to simulate it
diffuse all the data to simulate it
cohesively to
stress test it and iterate it and
optimize it over time so the the fact
that you named the factory a product
says a lot about your attitude about the
factory your manufacturing sites um yes
this the idea is once you have the
digital twin that you go for
optimization in a continuous way i mean
you you will build a brilliant one but
there will be always improvement
potentials you have changes in your cars
you have changes your products and then
you improve it
that's part of the digital is that they
exercise while you digitalize too right
yes yes for us it's important to get
transparency and also to use the
capabilities for digitalization and we
have different pillars we are looking at
one is connectivity we we must get
seamless connectivity easy connectivity
flexible to adopt 5g networks in our
systems to to easily have small
applications being used and then again
change over time
we need artificial intelligence being
used in our equipment that for me
artificial intelligence is the next big
thing for automation in a production
field to use it for camera systems in
quality areas to use it for the
logistics transports
uh to use it in maintenance and and
machine steering so artificial
intelligence is is boosting right now
and gives us huge opportunities and and
last but not least
virtualization and the digital twin
to bring everything as a digital twin
into a virtual world
opens a complete new dimension we
we are getting the opportunity to set up
our systems plan the systems
link them into the real
planning
fields connect people
and at the end of the time even steer
the whole plant with it so so it's a
huge huge field we are we're tasking and
then when when i when i just quote a
little bit of our discussion which we
had then i said oh i mean let's start
with the line and you said no no no this
this ditch has to be comprehensive it
has to be a full digital twin from from
the shop floor to the roof uh from your
robots to you to the cars to the
products it has to be complete right it
has to be complete jensen we we did some
work the last year to to bring our
facilities into a digital twin to to
generate more as a basis for a digital
twin
but the important thing is to link it
into existing systems to really
have the reality and the twin always fit
to each other
and then have the capability with
artificial intelligence inside the
digital twin even to steer the factory
that's that's a huge huge approach and
then when we talk always about having
real-time decisions with confidence
that means at the end of the day if you
simulate something on a little twin and
you figure out you you want to change
something in the real world then you
should that means you really change the
real world and you better get it right
first time this is the big difference
between what you prescribed before
the the internet or the metaverse and
the industrial metabolism we are talking
about impact in the real world that's
right you know if you look at almost
every engineering project today of any
significant complexity we simulate the
product
before we go to production
and because almost all products today
are software defined there's a great
deal of software that sits on top of it
we essentially all companies that build
products have a digital twin bmw has a
digital twin of the car
and they would simulate
new software releases on the car before
you release that ota into the fleet
nasa does that for
the rovers companies do that for phones
we do that for our chips and computers
yet for most plants in most factories
it's nearly impossible to do that today
and the reason for that is because a
plant
building a whole bunch of products is
way more complicated there are millions
and millions of parts many of them are
moving the amount of software that's
inside a plant is incredible and so we
need to create a very large scale
simulation platform which is what
omniverse is about it has to be
physically based just like omniverse and
it has to fuse data of all modalities
whether it's mechanical electronic or
software we need to be able to put that
into omniverse and simulate this plan
and so in the future milan you will
develop new software you will you will
change the layout of your plants uh
maybe you change the procedure of the
workers in the plant and uh
you would simulate that
in your
virtual digital twin
and you will prove that it is uh
optimal and safe before you ota into
your plan that sounds great
but that seems to be really expensive um
but why would i invest in this one i
mean do you think about it about the
investment which you have to do in order
to make that really this really true
twin
the core element is to get started i
mean we we got started and we are going
from one step to the next it's an
adventure
since we're approaching new fields the
digital twin itself
i mean many fields as you say have
digital twins already the digital twin
itself is not the challenge the
challenge is to link into this digital
twin the existing systems
one by one and to to to have any change
in the digital twin being reverted in
the original planning tools
to get the digital twin as a full
digital twin means to use it with
sensors into the manufacturing and use
it as a steering
brain
for the reality
this is something from today's point of
view it's a vision it's
not here yet but work on it step by step
and at the end of the day the benefits
are huge so i think it's worth doing it
you told me a long time ago
that you would like to improve the
operational efficiency of manufacturing
by 30 percent
now for a company the enormous scale of
bmw 30
of manufacturing could drive enormous
efficiencies and cost savings that's one
of the reasons why it makes so much
sense and so compelling for you to make
the investment
the world's industries represents
hundreds of trillions of dollars over
time
and to be able to find several percent
of efficiencies represents trillions of
dollars a year that's one of the reasons
why people
want to invest and now we have the
technology capable for them to do so the
second thing is is uh
the source of truth
the source of the data has to be
has to have high integrity and has to be
full fidelity which is exactly the
reason why we're connecting today this
partnership today is about
exactly the problem statement you just
described which is to connect directly
from the tool that the engineers are
using directly from the data that
they're actually using connecting it
directly into omniverse and creating the
digital twin the live digital twin and
so that's our that's the purpose of our
partnership and uh we're gonna go make
it happen and jensen you're completely
right 30
it's absolutely realistic if you think
of of a steering of
safeguarding the processes of being able
to visualize them and change them before
you have them in hardware and before you
have any mistakes so the potential is
huge and and for me the vision would be
with our new plant the devils in fully
electrified uh plants
100 co2 free it's the vision of the eye
factory so to get the full digital twin
in place with deborah chen that would be
the right approach and i do believe that
the leverage on on the on the levers is
it productivity increase is it
shortening cycle time is so big that
justifies investment and it justifies
really taking the effort
which brings me to um to a hunger and
plan which uh and we talk here here
comes the fun part you talk about a
vision
many very often people think visions are
out maybe 10 or 15 years so i think the
challenge is to make that happen by 2025
20 25 oh yes remember we are 2022 right
now um so that's that's uh that's i mean
you said before three years in in my
business that's that's quite long that's
that's forever 2025.
it's 2025 for the virtual version or the
physical version the physical version oh
really okay so we're going to suggest
because you and i have to have the
virtual version about six months
and so that so that milan could
integrate it operate it optimize it
before he
breaks around so we use it for steering
so we should give it a try that the
digital version is faster than the real
one there's no question that digital
version will be faster in the world
let's make it happen let's try it let's
give it a shot let's do it
Title: GeForce GTX 980 & 970 Product Video
Publish_date: 2014-09-19
Length: 560
Views: 311800
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/W1LQZI7c7Ng/hq720.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AHUBoAC4AOKAgwIABABGFkgXyhlMA8=&rs=AOn4CLBVFTtmovOh7kYpCuidVBWYlqfShg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: W1LQZI7c7Ng

--- Transcript ---

[Music]
hi this is tom peterson with nvidia and
today i want to introduce you to our
next generation gpu architecture maxwell
it comes in two variations gtx 980 and
gtx 970 both of which use the power of
architecture to give you the best perf
per watt possible
these are the world's most advanced gpus
the gtx 980 delivers 2x performance over
the gtx 680 and in fact it's the fastest
gpu we've ever created
the 970 is no slouch in its own right
with best in class performance stacked
with cutting edge features
of course since these gpus are based on
the new maxwell architecture they
deliver performance in an incredible 165
watts which means your pc will be
whisper quiet as it crushes through the
latest games and we're delivering better
gameplay experiences using cool new
technologies
multi-frame anti-aliasing or mfaa is one
example where we can deliver high
quality aaa at much higher performance
it's a simple idea that says by looking
at multiple frames over time we can
actually combine them and generate an
image with the same quality as msaa but
we can do that at a fraction of the
performance cost of traditional
multi-sampling let's start to understand
mfaa better by understanding how aaa
works
this image shows a grid of 25 pixels
that represents a very small area of
your screen
if a game draws a straight line on this
grid the gpu must determine the correct
color to use for each pixel the way the
gpu does that is by sampling and
sampling means that we pick some
location which in this case is the very
center of the grid to use as the test
location to determine if a geometric
shape intersects a pixel and should thus
color it
you can see that we get a very jagged
edge when we use one sample since each
pixel needs to make a binary decision
about coverage
traditional aa improves this a lot by
taking more than one sample per pixel
in this image we're showing two sample
points per pixel now for each pixel the
gpu calculates two coverage points if a
particular geometry covers only one of
the two sample points then the final
pixel color is of course the average of
the two colors
this technique dramatically improves
image quality by reducing jaggies but
the cost can be high as now the gpu
needs to do more calculations on each
frame
mfaa dramatically improves this by
recognizing that the averaging that msaa
does per pixel can actually be done over
time
in the first frame let's assume that the
following grid of sample positions is
used you can see the image here is
similar to what no aaa looks like
now in the next frame we change the
sample positions you can see how the
generated image is slightly different
from the prior frame with just a few
pixels changing from light to dark and
vice versa
we can then run a sophisticated filter
on the gpu to combine the series of
frames and deliver an image that's
nearly identical to 2x msaa the great
part about this tech though is that the
delivered performance is nearly
identical to no aaa
[Music]
dynamic super resolution or dsr is a
great new maxwell technology that allows
you to get a 4k experience on a 19 by 10
monitor the way it works is pretty
simple
gpus use a frame buffer to store a
rendered image before they're scanned
onto a monitor most of the time that
buffer is organized at a resolution that
matches the resolution of your monitor
but many gamers have found that
sometimes it's desirable to lower the
resolution of the frame buffer to get
higher frame rates out of the gpu in
this case the gpu scales up the rendered
image as it's scanned onto the monitor
dsr however goes the other way with
super resolution technology we allow the
game to specify a resolution that's
actually larger than your monitor this
means the gpu will generate a very high
quality image in the local frame buffer
and then use a sophisticated filter to
down scale it and put it onto your
monitor this reduces artifacts caused by
rendering at low resolutions
this image as an example is a fragment
of an image generated at 19 by 10.
that same scene is shown again here but
now it's rendered at 4k using dsr and
then it's down scaled to 19x10 the image
we're showing is exactly as it would
appear on your monitor and comparing the
two you can see the improved texture and
edge quality
this technology works really well with
objects in motion as well let's take a
look at this 19 by 10 image here running
without dsr notice the scintillation in
the grass as textures kind of pop in and
out
in the second sample you can see the
image is much clearer and the objects
look much more solid
let me tell you a little bit about vxgi
or voxel global illumination it's a
technology that allows us to simulate
light inside of your game in real time
that means that shadows look better
colors bounce around and the scene is
much more realistic
you're looking at a ray traced image of
the cornell box which is a classic set
of geometry from the early days back in
1984 when everybody was trying to figure
out how to do real-time rendering the
problem with this box is that it's
actually very difficult to calculate how
light bounces it around it in real time
so in graphics we use simplification
techniques the easiest way to calculate
light is what's called direct that means
we imagine there's a point light source
that illuminates directly down onto the
geometry but only lights the surfaces
that it hits directly that's why we call
it direct light in this case you can see
the top of the balls are very bright
white the right wall is green and the
left wall is red and that direct light
kind of it looks good but it's missing
because clearly you understand that
there's going to be reflections well
reflections and calculating reflections
in real time has been a really difficult
problem for graphics and really until
now it's been practically impossible
this image is showing a voxel view of
that geometry now voxels are volume
pixels and it's a way to represent
geometry sort of coarser so that we can
model real-time light reflectivity
in this view you're looking at what we
call an opacity model and the opacity
model is used to calculate how light is
blocked by objects and you can see that
it's made up of these tiny little boxes
that simplification versus the real
geometry helps us run this in real time
now the next view you're looking at is
called an emissive view in this view
we're taking direct light and we're
lighting the voxels and then we're going
to in the next stage use that lit voxel
to calculate the first bounce of light
in this case you can see the right hand
wall is very green and the left hand
wall is very red in the next stage those
surfaces are going to emit light onto
the rest of the surroundings that's
effectively how we calculate reflection
now as i move the light around you can
see that the voxel geometry is actually
changing in response to the way direct
light is illuminating the scene this is
just another way to say okay we're going
to calculate a bounce so if you look as
that light pans from the green wall to
the red wall the white voxels appear and
disappear
in this next scene the voxels are now
emitting light or reflecting the
original direct light you can see the
ball on the left hand side has this sort
of reddish glow to it that's there
because remember the left wall is
emitting red light and on the right hand
side you can see there's kind of a green
pail to the right hand ball and that
again is coming from the right hand wall
which is emitting green light what's
interesting though is you can also see
both walls are now kind of the opposite
color that they were originally because
of course each wall is gently lighting
the other side
this gives you a real sense of how
reflected light would work in the real
world and now we're able to do that in
real time and now let's take all those
different forms of light and combine
them together to get a final image in
this scene you're looking at the direct
light coming from the top and it's
hitting the walls and we're adding in
that light that we calculated using vxgi
that light gives us the specular
highlights it gives us very beautiful
ambient occlusion the balls look like
they're on the ground what's really
extra cool is that you can see the
reflection of one ball in the other ball
if you look closely at the ball in the
back you can see this interesting silver
shape which is actually the ball in the
front
vxgi is a revolutionary new dynamic
lighting technology being delivered with
maxwell gpus it's going to change the
way games look forever and we're all
going to be the better for it
the advancements extend beyond the new
technologies to new ways to play maxwell
offers the best solution for 4k gaming
with the horsepower to hit the frame
rates you gotta have and 4k surround
using sli and even 4k shadow play you
can now take your pc gaming on the road
with game stream and our shield family
and of course gtx 980 is ready for the
next generation of games with full
support for dx12 and virtual reality
with new technologies like mfaa dsr and
vxgi they are simply the best choice for
serious gamers
[Music]
Title: NVIDIA RTX Comes to Pixar’s RenderMan
Publish_date: 2018-08-30
Length: 71
Views: 151141
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/w2ft7JhAkQQ/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: w2ft7JhAkQQ

--- Transcript ---

lead engineer for random and Pixar
animation studio and I'm here with my
crew to talk about random and and random
and XPO which is our next generation
production Pat racer we started using
optics at Pixar as part of internal
production to was quite a while ago it
was like relatively easy to adopt an
integrator it doesn't force you into a
particular rendering pipeline or
anything like that is a tool key that
used to build a renderer if you look at
one of the movie that Pixar makes or one
of the other visual effects project that
some of our customer made with a random
and complexity is there at every level
in the demo you're seeing right now is
now like Dino is the or or filter or
upscale but we want to show the real
deal the ground truth I like what the
Nvidia r-tx platform brings to this
table like we can develop on top of it
and we can push the boundary and help
Nvidia through our contribution to push
the boundary even further like thinking
about next generation already what can
we do next and better
Title: Graphics Virtualization with NVIDIA GRID
Publish_date: 2015-08-30
Length: 86
Views: 56807
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/W5Zj8u1-CBY/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: W5Zj8u1-CBY

--- Transcript ---

imagine a solution that gave employees
throughout your enterprise the freedom
to run any application at full
performance on any device wherever they
are that solution is now a reality
introducing accelerated virtual desktops
with NVIDIA grid the same NVIDIA GPU
performance that has transformed
everything from video games to movies to
cars
is now in your datacenter accelerating
virtualization across your business
improve mobility for every user on every
device
streamlined collaboration to let
employees work together everywhere boots
security by keeping your critical IP in
the data center and enhance productivity
so users can get more done
faster how are your business like never
before deliver on the full promise of
virtualization with accelerated virtual
desktops only from Nvidia
Title: Editing REDCODE RAW Footage with NVIDIA RTX
Publish_date: 2019-01-24
Length: 138
Views: 13663
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/W6uNCu1vkkM/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLC_RoK1fz24y2WJ94Pk9V0ts2xgCA
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: W6uNCu1vkkM

--- Transcript ---

red is a great camera that a lot of
videographers and filmmakers choose to
use because it lets them capture
ultra-high resolutions such as 6 K or 8
K and it retains all the color
information that they would need to
calibrate their videos however working
with the red file is computationally
intensive
it requires decoding debayering and
coloring also that you can just play
back the video at nature frame rate
currently the decode of a red code roll
file is CPU based and this is the
experience that one would expect if
you're trying to playback an 8k red code
roll file natively you'll notice that
it's got severe stuttering and it isn't
close to what the video output should be
if I'm trying to find the perfect frame
in this video I would like to scrub in
this timeline as you can see it's not
responsive at all and so this would lead
to a lot of frustrations for video
editors what they really do is create
low res proxies and in order to
transcode that it'll take a long time
it'll take hours or days depend on how
much footage one might have we've been
working with red to accelerate the
decode of red code war files and so here
we've got the same 8k Clips playing back
smoothly with the GPU accelerated decode
on this Nvidia tiny r-tx you can see how
much more responsive the scrubbing on
the timeline is and so it's actually
possible for a video editor to use
footage like this playing back at full
8k resolution but our work with red
doesn't stop there and so here on this
20 80 max Q notebook we're sharing a 6k
red curve wall file playing at full
resolution at 30 frames per second
being able to play red code roll footage
while still on location will give
videographers the conference that
they've captured everything exactly how
they envisioned it we're really excited
about our collaboration with red
in order to give all videographers and
video editors the freedom to be much
more creative and to accelerate their
workflow
[Music]
Title: The journey of dropping NVIDIA SHIELD's living room from 10,000 feet.
Publish_date: 2015-11-25
Length: 649
Views: 21960
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/WbX9OxgPUaA/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: WbX9OxgPUaA

--- Transcript ---

I'm out here in beautiful Phoenix
Arizona at this airfield because we are
about to push a living room out of the
back of the plane I originally got a
call from one of our coordinators in
town got this gig and we're gonna build
some living room sets and throw them out
of the back of a c-130 you probably need
to get in contact with someone that is
able to drop a living room from 15,000
feet who's that young for this he's
looking at me I was like you you crazy
of course I'm in like you know the whole
concept was that it was a guy having a
very normal experience in a very
extraordinary and chaotic environment
and they wanted the guy to be cool under
pressure and to just kind of sort of
have the essence of cool and Jeff
immediately came to mind I mean that's
just who he is and what he is that we
were discussing about dropping content
out of the sky related to the fact that
there was no limit if there is no limit
sky's the limit that you really wanna be
in the sky and that's the key to a good
idea to show people an image they've
never seen before
it always grabs their attention this is
not movie magic we are actually throwing
a living room out of the back of a plane
actually we're doing it three times
there's two different styles of rooms on
this project one of them is a parachute
room the two other rooms are set up for
freefall so therefore you know that you
can plan different shots the first one
was the guy chillin you know using the
remote control the second drop was
parachute so there'd be more time where
the living room would be kind of stable
and that would be an opportunity for us
to get him playing with the products a
little bit more and on the third one it
wasn't gonna have a parachute and we're
just gonna do everything to it and we
play around with that the biggest
challenge a was to build the platform in
a way that's instead of spinning in the
air it could actually be quite firm
way that you beat any kind of violent
movement in the air is the inertia of
weight so these rooms are built to be
twenty five hundred pounds plus the
weight of the furniture and the skydiver
we were able to design the surface area
of the room and then add fairings to it
that would direct air and then at the
same time we have vents in the middle of
the room it's as aerodynamic of a
platform that I can think of and then
you build the living room on top of that
it's a set designed in triplicate three
three versions of the same set so
everything needs to be very secure so
that pieces of the set don't become
airborne projectiles and potentially
hindering our talent so we have to plan
for that thing to be able to tip over
and go 200 miles an hour without coming
off
in pretty much everything I film in the
sky is it requires that I'd be able to
go after things aggressively but not hit
them in order to do that I wear a camera
wingsuit that has a wing that goes from
my wrist down just about halfway down to
my knee and so I use that in order to
chase after things and then as soon as I
get there bang I put the brakes on I'm
filming and so because you have two
minutes in the air you have to cover a
lot of things at the same time or you
just set all the cameras up at the same
time that's what's going to hook people
in those tiny details it's such a shame
that we're just like letting them crash
into the dirt because it's built so well
it's so nice it's like it's the nicest
thing I've ever jumped out of a plane
with when I woke up this morning it was
like the first thing in my head it was
like dude you're about to go jump out of
an airplane to write a living room yeah
I'm awake I'm just basically sitting in
the chair with my feet up against the
coffee table so I'm kind of wedged a
little bit between the table and the
couch and then I have a like a seat belt
around my waist it's a passive system so
I don't have to release anything all I
have to do is let go what I have to do
is wait for this room to come toward me
and literally let part of it pass me so
they cut the straps 5 4 3 2 1 right at
that moment I exit the room with it and
try to get as close of a shot as I can
of the unit with the airplane in the
background and it's kind of like
slow-motion a person's hips back and
then you feel the wind like oh you know
and then you're like alright we're in
the air now they rocked quite a bit but
it wasn't violent or anything and you
know but just to be sure if it did do
something really quick and fast and
jerky that it wasn't just gonna send me
out of there I want to leave that thing
what I want to not when it wants to
I am grabbing as much air as I possibly
can and I'm trying to go as slow as I
possibly can to fly with this thing I
was like checking out my altitude for
like the last thousand feet I'm like oh
yeah about a couple more seconds and I
just let go and dobe out the side I just
grab the handle and rolled out it was
like it was easy the first drop went off
better than we could have anticipated
you know it the weather was great the
plane went over we had the clean drop it
landed exactly where we wanted we got
the crash shots we got everything that
we needed and then when we were
preparing for the second jump we ended
up going to landing zone anticipating
that the next one was gonna fly over 15
20 minutes later and we wait and we wait
15 20 minutes later we asked what's the
ETA on this eventually realizing that
they had a mechanical issue with the
plane the plane broke down and this
hurtless flames meets apart that is just
it's available in meza we brought the
part back and they're fixing the plane
but the plane is aren't gonna be ready
on time with in a way that it can fly
with this weather
so that is forcing us to do this shoot
perhaps in a few days Monday morning we
wake up I mean this is really it we've
had to problem-solve in order to get the
resources there the stunt guys the stunt
coordinators the stunt directors the
aerial cinematographers the pilots
everyone's ready to go the plane comes
by we see this thing come out
we had a delay the parachute coming out
I was concerned I thought that doesn't
really look like it's gonna open it was
about a seven or eight second countdown
just watching that thing open and
instead I'm just looking down just like
oh please open you know and then I saw
it open it was like aha
okay here we go it's a GPS guided
parachute so this thing is essentially
going to land within a certain radius of
that spot
it circles back comes through and lands
smack maybe less than 50 yards right in
front of them got great footage from it
and in the GPS parachute if it didn't
have the kind of control
that was on that thing we could have all
been diving out of the way we're in the
Coolidge hangar and we've got one more
drop left this is it we just need this
one platform to get off the bird and fly
we see the bird fly over so we're
looking up and you just see a little
speck come out of the back of the
airplane I knew almost instantly as we
were getting pushed out of the bird I
can just feel it like it kind of liked
it a little bit and I can feel like
almost dipping that back down to the
back right and I knew that we were gonna
deal with like another access here now
we started noticing there's there's
something strange going on there because
it looks like the speck is like flipping
over you know I did a couple flips stop
and flip the other way roll again it was
very like very wild very unpredictable
it's definitely not good up there my
main concern was a tuba looked at I
remember looking at the aquarium I look
solid alik solid TVs solid damn tuba and
I just looked at my exit strategy
so we start noticing that this thing is
falling right above us it become this
kind of a parent that there's a living
room that might land on us everyone
starts to like look around and wonder
like we're where do you go what do you
do we noticed that it's going directly
toward my rental car which is good
because it wasn't coming towards us
anymore so I start thinking about what
I'm gonna tell the insurance company and
drawing them a diagram of of the car
with the living room crashing into it
from an airplane and this thing just
plummets it hit the ground at over a
hundred miles an hour and just exploded
pieces of living room all over the place
in all directions I think I made the
right call I'm getting off but when I
did I think that was like
the moment I had to
I'm more proud of this than any other
shoot that I've done the stuff that
we've captured is it's it's breathtaking
it's so grand and it's on such a large
scale when you see this stuff that it
it's hard to believe that this actually
happened but I guarantee you everything
was done practically we have actually
thrown the living room three times out
of an airplane
you
Title: Mirror’s Edge Catalyst now with NVIDIA Ansel
Publish_date: 2016-07-14
Length: 144
Views: 76039
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/wc85tPN9zyg/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: wc85tPN9zyg

--- Transcript ---

mirrors edge I would say revolves around
this young woman named faith she's a
runner and she belongs to a community of
runners and they are for artists and
martial artists the main themes of the
game revolve around individualism and
freedom versus control and collectivism
creating a better gaming experience is
about fueling passion of the player with
invidious new hardware we at dice
basically saw the opportunity to create
something new to create something better
the invidious ansel brings us one of the
core pillars of Mirror's Edge catalyst
which is jus TC juicer generated content
council supports full camera motion you
can pan tilt and roll so you can
basically frame your shop the way you
want it also brings you the possibility
to create 360 degree images and you can
share those on Facebook by using a
mobile phone you can look around and see
actually how the city looks like and
bringing this technology to virtual
reality you can actually step into the
city of glass and be totally immersed in
ordinary games when you use to take a
screenshot you're basically done if you
want to do any changes in cropped image
we're down still you'll have more
control you can actually control the
lens of the camera which enables you to
set field of view by doing that you can
create much much more quality in your
screenshots with post-processing filters
you can change the image after the fact
that's been taken so you can choose how
you want the picture to look
I'm very proud of what we built it's a
beautiful city that has a lot of variety
to it I think would Ansel the
possibility for them to showcase it to
others I think is amazing when Ansel
this is looking even more stuff
you
Title: The Unboxing Experience: SHIELD Tablet, SHIELD Wireless Controller and SHIELD Tablet cover
Publish_date: 2014-07-29
Length: 130
Views: 24484
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/wdKy3O_7ktY/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: wdKy3O_7ktY

--- Transcript ---

hey guys I'm will and today we're
unboxing the shield tablet the shield
tablet cover and the shield wireless
controller so let's get started
shield tablet offers extreme performance
thanks to this Tegra k1 processor and
its 192 core Kepler GPU it comes in both
a 16 gig Wi-Fi only flavor and a 32 gig
Wi-Fi and LTE variants and because this
is a shield device it will play your
android games stream your PC games and
cast your games directly to twitch now
that we're on wrap you can see we have
our dual front-facing speakers here here
for booming audio your 5 megapixel
camera on the front and a 5 megapixel
camera on the rear here we have direct
stylus to our GPU accelerated stylus on
the side of device you'll find your
micro SD card slot good for up to 128
gigs of extra storage here is your
volume rocker your power button and
along the top edge of the device you'll
find your 3.5 millimeter headphone jack
your mini HDMI port and the micro USB
port right here in the box you'll find
your charging brick your documentation
and your USB cable now let's move on to
the shield tablet cup a shield tablet
cover has a microfiber lining that will
protect your screen and keep it from
getting scuffed up and it's got these
two magnetic connectors here and here
that snap right into your shield tablet
when you close the cover it'll put it to
sleep and when you open it it'll
automatically wake the tablet up and
it's got three different positions to
serve as built-in stand now let's unbox
the shield wireless controller it's the
most advanced game controller out there
and it's compatible with both your
shield tablet and shield portable the
precision buttons on the front
complement your capacitive navigation
buttons the shield button your touchpad
and your volume rocker your headset jack
right here and the built-in microphone
allows you to use voice commands in the
box you'll find your documentation here
and under this flap is your USB cable so
that's been your unboxing experience for
the shield tablet the shield tablet
cover and the shield wireless controller
for more information make sure to go to
shield and video.com
you
Title: GeForce GTX 780 Ti Launch Video
Publish_date: 2013-11-07
Length: 182
Views: 83214
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/wFjNY-Osn_k/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: wFjNY-Osn_k

--- Transcript ---

[Music]
hi i'm andrew with nvidia i'm here to
talk to you about our latest gpu geforce
is all about designing and building the
ultimate gaming gpus and technologies
for gamers with the geforce gtx 700
series we introduced the idea of pure
performance
while fbs delivery is an important
measure there are now many factors that
define the best experience for gamers
pure performance is about finding the
perfect balance
awesome power
silent operation and super smooth frames
for the pc gamer who demands the best
performance conceivable and an elite
gaming experience we are introducing the
geforce gtx 780 ti
gtx 780 ti is simply the best gaming
graphics card on the planet it improves
on perf while retaining the same
combination of smooth frames amazing
acoustics and power efficiency of its
smaller brother the gtx 780 this is pure
performance accelerated giving you the
freedom to play every title at ultra
settings and max resolutions on today's
highest definition displays
it's even ready for nvidia g-sync our
new monitor technology that delivers a
smoother sharper and tear-free gaming
presentation
so what makes the gtx 780 ti the best
graphics card for gaming well first of
all it's super fast
it has 2880 cores which is 25 more than
the gtx 780.
second you have control
with gpu boost 2.0 optimize performance
by dynamically adjusting your clock
speeds and temp targets to max out your
gaming system
so you have the power to play the
hottest new titles in a cool quiet
environment without distracting fan
noise batman arkham origins for example
is stacked with our tech including
nvidia physics particle simulation it
also has txaa anti-aliasing technology
that reduces flickering and crawling in
motion
with physx txaa and other nvidia tech
like directx 11 and tessellation all
turned up to high settings with the gtx
780 ti you get an even more immersive
experience out of this game
[Music]
nvidia technologies are also in other
big releases this fall including
assassin's creed 4 black flag
powered by nvidia gtx 780 ti with
two-way sli you can enjoy these titles
at extreme settings
imagine engaging in intense combat with
multiple 25 by 16 monitors
or enjoy truly next-gen gaming in 4k
resolution for 780 ti with sli offers
the best way to play an incredible
realistic 4k game so if you want the
best gaming graphics card check out the
gtx 780 ti
it's super fast quiet and smooth with
the power to play next-gen gaming
experiences with the latest games
the gtx 780 ti comes out this november
[Music]
Title: HERE, NVIDIA Partner on AI Technology for HD Mapping from Cloud to Car
Publish_date: 2017-01-09
Length: 123
Views: 18986
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/wGzU5IAd8js/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: wGzU5IAd8js

--- Transcript ---

here is a leading global provider of
digital mapping content and we're
creating an open location platform that
enables anybody to create location-based
services on our platform as the leader
of highly automated driving having
really powerful compute platforms to
create essentially the reality of
automated driving is very important so
here is decided to partner with Nvidia
to create what we call the self-healing
map that inquires significant investment
both in cloud technology in vehicle
technology as well as HD map content and
ensures that that content is live
up-to-date and fresh so the maps that we
see today in vehicles the standard
turn-by-turn navigation map that you
would see on a Google Maps or even Waze
is what we call a standard definition
it's got a resolution anywhere from five
to fifty meters an HD map is
fundamentally different it's actually
meant for machine to machine
communication not machine to human
communication the fidelity of the map is
exceedingly high we're aiming for
anywhere from 10 to 20 centimeter
relative accuracy of this map content
and then much more detail about the
roadway everything from barriers to lane
markings to shoulders everything in
between on the roadway we're also
providing a localization layer and what
it does is it helps the vehicle
precisely position itself on the HD map
and then lets it enable autonomous
functionality as the roadway in the
environment changes that map has to be
up-to-date so in the future every
vehicle that uses the HD map is also
going to be a little map maker they're
going to be taking data about what they
perceive in the environment sending it
back to our cloud we're going to be
aggregated information and then updating
the map and sending it back to the
vehicles behind this is a fundamentally
different architecture that sort of
require significantly higher compute
capability than is in the vehicles today
so Nvidia's GPU lineup and their
capabilities in terms of computer vision
and deep learning is going to be
fundamental to solving this problem so
this partnership is really going to
enable both sides of that transaction
[Music]
Title: NVIDIA Omniverse ACE: Cloud-Native AI for Interactive Avatar Development
Publish_date: 2022-08-09
Length: 106
Views: 20737
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/WKzWg87tEWk/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: WKzWg87tEWk

--- Transcript ---

I'd like to show you how easy 
it is to drive your avatars  
through NVIDIA's Audio-to-Face using 
Avatar Cloud Engine or ACE for short.  
In this demo, ACE is powering a meta-human and 
Unreal Engine in real time that includes natural  
language processing, a custom voice model and 
animation behaviors. Let me show you how it works.  
Hello. Hello. My name is Arjun.  I am a digital avatar. 
What does it mean to be a digital avatar?  
I'd be happy to explain.  In computing, an avatar is 
a graphical representation of a user or the user's  
character or persona. A digital avatar can also 
be a virtual assistant like myself.  Can you tell  
me more? Avatars are the digital representatives 
in the virtual world. The characters we control  
in video games can be thought of as avatars and 
increasingly we will adopt avatars as we shop,  
socialize, learn and work in the connected online 
environments that we are calling the metaverse.  
Let me show you another avatar
in metaverse. This is Sam.  
I've heard a lot about this thing called 
the metaverse. Can you explain it to me?
The metaverse is the 3D evolution of the internet -- 
a network of connected persistent virtual worlds.  
I'd love to know more. The metaverse will extend 3D 
web pages into 3D worlds and connect the digital  
world to the physical world. The metaverse has 
multiple applications whether for entertainment,  
socializing, and gaming or industrial and 
scientific use cases to build digital twins.  
Thank you. Goodbye. Sure. I am here 
if you have any more questions.
Title: GPU Technology Conference 2014: Audi Self-Driving Car, CUDA Everywhere, SHIELD Giveaway (part 10)
Publish_date: 2014-03-25
Length: 735
Views: 36589
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/wNSWWOf6-Hw/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: wNSWWOf6-Hw

--- Transcript ---

now all of this technology gets put into
one of its most important one of its
favorite my favorite applications cars
at CES Riki hoodie the head of
electronics of Audi and a great
visionary visionary of the car industry
said that they needed a supercomputer to
power future cars and he wasn't kidding
about the metaphoric use use of the word
he was using literally I need a
supercomputer in future cars because
that car needs to be self piloted that
car needs to be your smartest robot that
car needs to keep you out of harm's way
that car needs to get you from point A
to point B with so much joy and delight
that it's almost unimaginable
Ricky hoody said he needed a
supercomputer at their press conference
the next day Ricky announced that Audi
and NVIDIA are partnering together to
build and develop the next-generation
outtie self piloted car based on the
Tegra k1 let's welcome
Andrei ascribe to the stage so that you
can tell us more about it
andreas welcome it's good to see you
Alex good 3ff you thief
thank you very much before before we go
on I just want to say something to to
the friends right you know Ricky and
Matias wanted to come but they couldn't
make it I wish you guys were here I know
you guys are busy see you guys next year
I just wanted to say hi and so so well
tell us about tell us about you know
what before we talk about it
why don't we show them one of the many
features one of the many capabilities
that is necessary for self piloted cars
okay
imagine a self pilot car and what it has
to do remember some of the things I've
always talked about I've already talked
about and so let's go ahead and run the
video and Andres and I will watch it
together and we'll talk to it okay here
you can see a few of the car from the
thoughts on the camera off in the mural
and there's a car driving the fight
parking cars you introduced structure
for motion and we are using this
technique to build up on theater and
subscription of the environment of the
car in this in this circuit it put
fences you can see it here if this
fences splits a complete space with a
free space the free space is important
for us where the car can't drive without
collision since it's very important for
Paulo the driving of course and so let
me do this let me apply some of the
things that I talked about earlier to
this particular image so basically
what's happening if there's a there's a
camera on the side of the car yeah and
it's looking sideways here and what it's
doing of course is identifying first
interesting features interesting
features it should track and then it
tracks those features and by their
tracking and the multiple images we're
able to project into 3d space and as a
result we can reconstruct the 3d space
and we can tell that that particular car
is closer to us and as a result you
paint the fence around it and as we
drive just as we drove past earlier the
space was sufficiently large and so it's
good enough to park a car and you did
draw a fence and that information is
extracted using 3d interpretation
here you need a lot of computing power
we have four cameras in the car it's at
one megapixel camera through 30 frames
and so we have to calculate 120 million
pixel each second and though we are
using Tegra k1 it brings the poor
computing power into the car with normal
pcs it's not it's possible in real time
so of course a car could drive itself in
the future yeah I mean meaning they can
really drive itself in the future right
yeah I forgot I forgot the cue you guys
let the car drive itself in the future
for example maybe some other random cue
so that the people in the background in
the back okay I bring something with me
you brought you brought this in your
suitcase ladies and gentlemen
outtie self piloted card
now there's really no reason to back
away from it I think we're not in harm's
way and so so I'm so let's take a look
at this I mean what's really amazing is
is I mean it's kind of ghostly actually
there's nobody inside have a look at
that
is there a short person inside is it in
the trunk
maybe it is in the trunk let's take a
look at the trunk okay it's got to be in
the trunk bank everything is in the
trunk you I will open it Wow Andreas
look look at that there's something
really beautiful in the trunk yes and
what is that
Tetris Tetris is essentially if you for
driver system system and for pull out
the driving and a very important point
in sassette first is the Teague or k1
here you can see a trunk of the past and
one two years ago with a lot of
computing powers here we have four hot
computing pcs inside to realize
environment to calculate the path and so
on and with this computing power it was
not possible to a calculate structure
for motion and that was a check okay one
it's possible now so that little tiny
Tegra k1 now fits in this computer
module and it fits right here you can
see right here yeah that the finer form
factor and here if a factor a beam of
time well this is really fantastic it's
really easy to access and and so if we
if we wanted to upgrade our car to a
Tegra k7 we can just pull it out and
give it a new brain right yeah flag and
infotainment we have the fame we can put
out Tigra and put a new one in that well
I've got a lot of friends who would just
like to upgrade it themselves yeah
and so this is gonna be fantastic
andreas thank you very much okay what a
great piece to be here thank you
beautiful car outies self piloted car
well next time we're gonna give it a
much much larger runway I'm gonna have
some fun with it
so here it is the Tegra k1 z fast fits
into the trunk little tiny area in the
trunk and powered the the brain of your
future self piloted cars really really
exciting well let me summarize very
quickly what we talked about we
announced six things today we announced
six things today the first thing was we
announced Pascal our next-generation GPU
architecture enabled by several very
important technologies MV link and 3d
memories which allows us to get a huge
speed-up huge spike up in memory
bandwidth and connectivity bandwidth as
a result we're gonna continue to scale
the computational capability of these
processors for years to come
the second we announced Titan Z the most
powerful CUDA GPU we have ever built
what's amazing about Titan Z is not only
is it powerful it is whisper quiet and
fits naturally and simply into your
average PC one of the hallmarks of Titan
not only is it
beastly in performance but it's elegant
in the way it performs and then there's
cloud we announced machine learning
running on CUDA because of the torrent
of information and the near infinite
amount of computational capability
machine learning with deep neural nets
is gonna see an an incredible turbo
charge in the coming years our
partnership and announcement with VMware
the largest most influential enterprise
virtualization company in the world
we're partnering together to integrate
grid with ESX grid with view and grid
with view horizon horizon view deaths
we announced the I rave ECA
the world's first photorealistic
scalable rendering appliance because of
its clustering capability because of his
distributed computing capability we're
able to take the simple little appliance
connect it to the network and if you
have array in your application it can
turbo charge it array is designed into
3dsmax Koteas CATIA and other packages
and then lastly the Jetson tk1
the world's first mobile supercomputer
and one of its best applications
computer vision for applications like
self piloted cars like robotics these
technologies these announcements today
are all based on one unified
architecture called CUDA we now have
CUDA NPCs we now have CUDA in the cloud
and finally we have CUDA in mobile the
leverage that our developers can get as
a result of this unified architecture
and all of these different platforms is
really really wonderful you can develop
on the PC deploy in the cloud you could
develop in the cloud deploy in a car the
same program works literally everywhere
I have one last announcement
and this one is just for fun
one of the greatest games one of the
greatest games this is the part of the
keynote where all of the timing comes
unraveled one of the greatest games of
all time is valves portal this is a game
that runs on PC the Xbox 360 the
PlayStation 3 this is a fantastic puzzle
game this is a game that every child
should play it's a three-dimensional
teleporting puzzle game you have a gun
that creates a teleporter teleporting
gun you create a tunnel you jump in to
the tunnel
whatever physics was happening before
then gets carried through the teleporter
so it's really cool you could be flying
sideways into a teleporter and because
that teleporter is connected to the
floor of another chamber you come flying
out of it from the bottom-up unique
valve physics really really cool well
we're so excited that Valve has finally
ported their application portal to
shield and I'm so excited about this
application and I think all of you guys
should enjoy it is so much fun to play
it on shield then I'm gonna give every
single GTCC registered attendee a free
shield today
they're ready for you outside you just
have to show your badge and get your
free shield ladies and gentlemen thanks
a lot for being here this is the best
GGC ever have a great time
Title: ITF World 2023 Keynote with NVIDIA CEO Jensen Huang
Publish_date: 2023-05-16
Length: 1112
Views: 54690
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/wrdqsMqmAig/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: wrdqsMqmAig

--- Transcript ---

Luke a steamed scientist at iMac
industry colleagues and partners
and honored guests
I am delighted to address you at ITF
World 2023
today the chip industry is the engine
that powers nearly every other industry
iMac has played an essential role in
advancing this incredible field founded
in 1984 during the dawn of personal
computers iMac has helped transform
Gordon Moore's observation of doubling
chip density every 24 months into an
industrial reality enabling the
transistor to shrink to the scale of DNA
strands
Nvidia owes its very existence to this
work
IMAX open Innovation model allows the
entire semiconductor ecosystem to
collaborate in pre-competitive research
sharing the burden and risk associated
with complex r d
one remarkable example of IMAX
collaboration model is their partnership
with asml in demonstrating the
feasibility of double patterning
immersion lithography and later in the
productization of euv lithography
asml's euv lithography system
fires laser pulses at a drop of 10 50
000 times a second
vaporizing it into a plasma that emits
13 and a half nanometer euv light nearly
x-ray
multi-layer mirrors then guide the light
to the mask reticle which takes
advantage of interference patterns to
produce features down to three nanometer
the wafer is positioned
within a quarter of a nanometer and
adjusted 20
000 times a second to account for any
vibration
asml's EV lithography system is a
scientific and Engineering Marvel
today we can build three nanometer chips
with features a hundred thousand times
finer and processing capabilities
millions of times more powerful than
when my career began
the progress of semiconductors allowed
us to invent the GPU and accelerated
Computing which let us tackle problems
that traditional computers can't handle
like computer graphics
scientific computing
artificial intelligence and self-driving
cars
so what lies ahead
we are experiencing two simultaneous
platform transitions
accelerated Computing and generative AI
for nearly four decades the exponential
performance increase of the CPU was the
governing dynamics of the technology
industry
CPU design has matured and Chip density
scaling at ISO cost and transistor
performance scaling at ISO power have
decelerated
as a result Global demand for cloud
computing is causing data center power
consumption to Skyrocket
striving for Net Zero sustainability
while supporting the invaluable benefits
of computing requires a new approach
the industry has turned to accelerated
Computing and AI
Nvidia pioneered accelerated computing
coupling the parallel processing gpus to
work in tandem with the CPU
by offloading and accelerating compute
intensive algorithms Nvidia routinely
speeds up applications by 10 to 100
times while reducing power and cost by
an order of magnitude
scientists used Nvidia like a time
machine accelerating time to insights
while expanding the scale of their
simulations to explore complex systems
such as climate change and human biology
Gamers use Nvidia to run virtual world
simulations
data centers accelerate compute
intensive workloads to save money and
reclaim power to sustain growth
using Nvidia modulus a physics informed
AI tool Siemens energy emulates its heat
recovery steam generators and wind farms
achieving results
250 times faster than traditional
methods
Lawrence Berkeley and Los Alamos
National Labs use Nvidia accelerated
Computing to process complex numerical
simulations more than 20 times faster
than CPUs
saving 350 million dollars
and nearly 200 gigawatt hours of energy
each year accelerated Computing is
sustainable Computing accelerated
Computing unlike general purpose
Computing is domain specific and
requires full stack engineering from
Chip architecture algorithm acceleration
libraries and application re-engineering
each field of computational science
physics biology
chemistry fluids quantum
mathematics Finance social sciences
graphics and imaging has algorithms that
need to be refactored and accelerated
methods like finite element
navier Stokes and Smith Waterman Gene
sequence alignment have been around for
a while
others are new and groundbreaking like
deep learning
a decade ago in 2012 Alex karshevsky
Ilya suscover and Jeffrey Hinton won the
imagenet visual recognition challenge by
a vast margin
alexnet a deep neural network was
trained on two GeForce GTX 580 gaming
gpus
Nvidia gaming gpus were accessible and
cost-effective supercomputers for
hinton's researchers to train deep
learning models the entirely
computer trained model
stunned the computer vision and AI
Community by beating the painstakingly
human engineered best-known algorithms
alexnet meeting Nvidia Cuda gpus
ignited the Big Bang of modern AI we
recognize that deep learning was a
fantastic computer vision algorithm
and much more deep learning is a whole
new way of writing software
with novel and large enough model
architectures
training data and accelerated computing
we can create Universal function
approximators of any Dimension and learn
predictive models from data inspired by
the possibilities of tackling previously
impossible problems we dedicated our
entire company to pursuing deep learning
and AI
working with researchers across the
industry
Nvidia reinvented the entire Computing
stack
from the new tensor core gpus coup DNN
neural network processing Library
multi-gpu chip to chip NV link
melanox and finiband within Network
processing
the dgx AI supercomputer
and optimizations for countless
Frameworks such as tensorflow Pi torch
mxnet paddle paddle Onyx Jacks Nvidia
tensor RT Triton and much more
we reinvented Computing for deep
learning the first wave of AI that
focused on computer vision and speech
recognition has achieved superhuman
capabilities
and has opened up multi-trillion dollar
opportunities in robotics autonomous
vehicles and Manufacturing Advanced chip
manufacturing requires over a thousand
steps producing features the size of a
biomolecule to make chips with
quadrillions of features each step must
be nearly perfect to yield any output
sophisticated computational Sciences are
performed at every stage
to compute the features to be patterned
and to do defect detection for inline
process control
chip manufacturing is an ideal
application for NVIDIA accelerated
Computing and Ai d2s and IMS Nano
fabrication build mask writers using
e-beam to create photoresist patterns on
a mask
Nvidia gpus do the pattern rendering and
mask process correction
tsmc and KLA use euv and duv
illumination for mask inspection Nvidia
gpus process classical physics modeling
and deep learning to generate synthetic
reference images and detect defects
KLA and Amat use Nvidia for their e-beam
wafer inspection systems and for their
Optical wafer inspection systems which
detect defects smaller than the pixel
amazing
the data rate is extremely high at 40
gigapixels per second to achieve this as
part of process control
recently we announced that tsmc asml
synopsis and Nvidia work together to
accelerate computational lithography
as you know computational lithography
simulates Maxwell's equations of light
passing through Optics and interacting
with photoresists
it is a vital step in ship making and
the largest computational workload in
chip design and Manufacturing consuming
tens of billions of CPU hours annually
massive data centers run 24 7 to create
reticles for new Chips
we have already accelerated the
processing by 50 times
tens of thousands of CPU servers can be
replaced by a few hundred Nvidia dgx
systems
reducing power and cost by an order of
magnitude
the savings will reduce carbon emissions
or enable new algorithms like ilt and AI
necessary to push Beyond two nanometer I
am thrilled to see Nvidia accelerated
Computing and AI
in service of the world's chip making
industry Nvidia relentlessly drove up
the performance and scale of AI
Computing and drove down the cost of
training large models with each
generation AI researchers scale the size
of models and the amount of training
data in the past decade we've increased
the scale of deep learning by a million
x
and then chat gbt arrived the AI heard
around the world
a new wave of AI is here it's called
generative AI
in the first wave AI learned perception
now ai can understand and generate
information chat gbt a generative
pre-trained Transformer is a large
language model with hundreds of billions
of parameters trained on trillions of
words and sentences
it has learned the representation of
human language and can generate text
chat gbt is revolutionary due to its
ease of use and incredible capabilities
it's a glimpse into the future of
computers that understand any
programming language and perform a broad
range of tasks already
over a thousand generative AI startups
are inventing new applications
tab 9 for example is a contextual code
assistant that helps complete lines of
code or generate entire functions from a
prompt description
they can program in multiple popular
languages from JavaScript and python to
rust go and Bash
Runway uses generative AI to create and
edit images and videos
its production quality is so impressive
that several oscar-nominated films have
used it
with runway's generative AI anyone can
tell stories through pictures and video
in silico medicine uses generative AI to
create a candidate drug in one third a
Time
and one-tenth the cost of traditional
methods which usually take over four
years and cost around 500 million
dollars
in silico medicine is used by over 20
pharmaceutical companies the market
impact of perception AI is already
significant
the impact of generative AI will be much
bigger every form of information will be
understood and enhanced by generative AI
from human language
music
pictures video
3D
to genes proteins and chemicals
the content on 200 million websites will
be personalized and generated by AI
billions of customer service calls will
be automated by AI generative AI will
assess 25 million software developers
and hundreds of millions of creators the
yield of the 250 billion dollar annual
drug Discovery r d will be turbocharged
by generative AI
from the Advent of personal computers to
the internet mobile and cloud computing
each wave has expanded the realm of
computing AI is a new wave of computing
that requires no programming skills and
uses plain human language to perform
seemingly intelligent tasks
with AI eight billion people will have
access to the instrument of knowledge
that was previously limited to only a
few million programmers the remarkable
ease of use allow chat GPT to reach over
a hundred million users in just a few
months making it the fastest growing
application in history
the impact of generative AI is profound
and as we advance the capabilities of AI
we must also be mindful to put even more
emphasis on advancing the safety of AI
utilizing Technologies like guardrail to
keep it within the boundaries of its
intended application and using human
feedback to properly align with our
values what is the next wave of AI and
body AI refers to intelligent systems
that can understand reason about and
interact with the physical world
examples include robotics autonomous
vehicles and even chatbots that are
smarter because they understand the
physical world
let's take a look at Vima a multi-modal
embodied AI
Vima can perform tasks from Visual and
text prompts like rearrange objects to
match the scene
it can learn Concepts and act
accordingly such as this is a widget
and that's a thing
and then put this widget in that thing
it can also learn from demonstrations
and stay within specified boundaries the
IMA runs on Nvidia Ai and its digital
twin runs in Nvidia Omniverse
physics informed AI can learn to emulate
physics and make predictions that obey
physical laws such as the conservation
of energy or mass Nvidia is building a
digital twin of our planet called Earth
2 which will first predict the weather
then long-range weather
and eventually climate current numerical
weather prediction models are time
consuming nvidia's Earth 2 team has
created forecast net a physics AI model
that emulates global weather patterns 50
to 100
000 times faster forecast net runs on
Nvidia Ai and the Earth 2 digital twin
runs in Nvidia Omniverse
Fusion reactors essentially form a small
star inside magnetic bottles researchers
at UK's atomic energy Authority and the
University of Manchester are creating a
digital twin of their Fusion reactor
using physics AI to emulate plasma
physics
and Robotics to control the reactions
and sustain the burning plasma
scientists can explore hypotheses by
testing them in the digital twin before
activating the physical reactor
improving energy yield
predictive maintenance and reducing
downtime
the reactor plasma physics AI runs on
Nvidia Ai and its digital twin runs in
Nvidia Omniverse
I look forward to physics AI Robotics
and Omniverse digital twins helping to
also Advance the future of Chip
Manufacturing
thank you all for this opportunity to
speak at ITF World 2023
you are driving advancements in the
world's most important industry these
breakthroughs include gate all round and
carbon nanotube transistors
Beyond copper metal systems backside
power
High numerical aperture euv
computational lithography with ilt and
AI
3D Packaging
CPO integrated silicon photonics and
generative AI Eda Technologies
to explore the massive design space the
next decade in the chip industry is
going to be incredible
we're counting on you
Title: What's Next in Conversational AI
Publish_date: 2019-08-13
Length: 176
Views: 90451
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/Wxi_fbQxCM0/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: Wxi_fbQxCM0

--- Transcript ---

[Music]
humans express ideas through language we
talk we need our computers to be able to
understand us
conversational AI is trying to take that
to the next level where the computer
actually understands what we're
intending to ask for and is able to help
us when you have a conversation with a
person you're building up a history of
shared knowledge that you both refer to
in order to understand each other and
for computers that's actually very hard
because that requires understanding
meaning I speak you speak right back
sometimes you anticipate what I'm going
to say so part of conversational AI is
actually the real-time nature of it
which means it's computationally very
demanding the Bert model is a really
popular model that's being used by a lot
of companies and other organizations
research and conversational AI and
deploying it hagh Renee do you have any
questions
how many CUDA cores is that Jetson nano
is 128 CUDA cores when you talk to a
computer a whole bunch of different
things have to happen first the computer
needs to actually transcribe what you
said then those words need to go into a
language model like Bert that's gonna
figure out what you meant and then
generate a response great I will charge
it to your account and all those things
take time and add latency and latency
makes the interaction slower and not
feel natural so every millisecond of
latency that we can shorten in this
pipeline is going to make the user
experience better one of the things that
we found very helpful is vastly larger
language models that are trained on
vastly larger amounts of text the
problem is that these models are so
large that they're difficult to train
and they're also difficult to deploy but
NVIDIA has really made the training
procedure for these large language
models much more efficient and we can
now train one of these large Bert models
in about an hour which is faster than
anywhere else
researchers are constantly improving
their models in order to make
conversational AI better and so the time
it takes to train and model translates
into the research progress
the quicker we can train a model the
more models we can train the more we
learn about the problem
the better the results get until
recently it would have been crazy to
think about deploying one of these huge
bird models in production and a
latency-sensitive application because it
would be just too slow but we've been
able to take birth run it through tents
or RT and now inference time is only two
and 1/2 milliseconds this means that
these applications can now feel natural
so by making Bert faster we're actually
making a whole class of models faster
and more useful for the whole industry
so we've seen that as language models
get bigger and we train them on more
data they get a lot more useful and
that's why we're excited about a project
we call Megatron which is to train the
biggest baddest transformer out there
and today we're able to Train eight
point three billion parameter models
using Megatron this is about twenty four
times larger than Burt large we're
really excited about the possibilities
of using these models to make
conversational AI better
Title: Remote Work With NVIDIA - RTX Virtual Workstations
Publish_date: 2022-02-22
Length: 241
Views: 20789
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/x2tf7A_kLvQ/hq720.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AH-CYAC0AWKAgwIABABGDAgUSh_MA8=&rs=AOn4CLBCWZRdDbGb0uQxwoE21fM1W00mjQ
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: x2tf7A_kLvQ

--- Transcript ---

vmware and nvidia are working together
to transform the data center with vmware
cloud foundation vmware horizon and
vmware vsphere combined with powerful
nvidia gpus and nvidia rtx virtual
workstation software to bring
professional design and visualization
applications to technical and creative
professionals everywhere
our rich end-to-end solution enables
organizations to maximize utilization
and insight with a single infrastructure
nvidia rtx virtual workstations power
the most demanding modern workflows
including augmented and virtual reality
when paired with nvidia cloud xr or it
can enable a new era of collaboration in
a virtual world when combined with
nvidia omniverse here we launched
autodesk revit powered by nvidia rtx
virtual workstation notice as the user
navigates between different viewports of
the design how quickly and smoothly the
software interacts
the user is moving dining room furniture
within the 2d blueprint and placement
changes are instantly reflected in the
3d viewport
nvidia rtx virtual workstations feature
nvidia rtx one of nvidia's most
important advances in computer graphics
ushering in a new generation of
applications that simulate the physical
world at unprecedented speeds enhanced
with the latest innovations in ai ray
tracing and simulation rtx technology
enables incredible 3d designs
photorealistic simulations and stunning
visual effects faster than ever the
vmware horizon and nvidia rtx virtual
workstation solution seamlessly enables
engineers and creative professionals to
work on design and visualization
applications from anywhere on any device
here we have a virtual machine running
solidworks visualize using nvidia rtx
virtual workstation because solidworks
visualize is an rtx accelerated
application the user can interact with
the model using real-time ray-traced
lighting and achieve the most visually
compelling experience possible see the
responsiveness of the application as we
create change and rotate the model
nvidia gpus and nvidia rtx virtual
workstation technology enables faster
rendering of the image allowing users to
work more efficiently and arrive at
their best work faster than ever before
here we are showing esri software
running on rtx virtual workstation this
software has a wide range of uses such
as government planning as well as
architecture engineering and
construction for planning line of sight
studies disaster recovery and flood
simulation notice the smooth rendering
of the 3d surface on the rtx virtual
workstation on the right whereas on the
left the cpu only virtual machine is
struggling to keep up with staggering
render cycles traditionally rendering
and analysis tools with the software
were executed upon the cpu but nvidia's
gpus when combined with the vgpu
software provides accelerated 3d
analysis workflow acceleration and
graphics performance users can connect
remotely to the rtx virtual workstation
from anywhere on any device with the
same performance of their physical
workstation these were just a few
examples of how nvidia rtx virtual
workstations enable organizations to
rapidly provision virtual workstations
to support creative and technical
professionals working from anywhere
deliver access to the most demanding
applications with stellar performance
eliminate workflow bottlenecks let
dispersed teams collaborate in real time
see the links in the description below
to try nvidia rtx virtual workstation
today with our 90-day free trial
you
Title: Stand up an NVIDIA GRID™ vGPU™ POC in less than an hour.
Publish_date: 2015-06-11
Length: 107
Views: 12324
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/x7R_03Np2Is/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: x7R_03Np2Is

--- Transcript ---

in virtual desktop infrastructure I
think has this myth about it that it's
an incredibly complex thing to to even
create a simple proof of concept so the
challenge was we needed to debunk the
myth that building a proof of concept of
POC in your environment is something
that takes an inordinate length of time
we wanted to do something fun around
this so reached out to VMware we were
coming up on gtc 2015 we would do an
event called 60 and 60 that's 60
desktops in 60 minutes we would
demonstrate that somebody could in fact
put together a POC of 60 Enterprise
desktops with 3d graphics running from
bare metal up and do it in an hour this
event fell on st. Patrick's Day so we
decided to have a little bit of fun with
it we put two teams together vide a
green team of course as well as the
VMware blue team each team took a
slightly different approach both teams
leveraged a similar resource the
deployment guide that we have available
we collaborated with VMware to lay some
ground rules essentially rule number one
had to be that any customer that watches
this or leverages the knowledge that we
pulled out of this event that they would
in fact be able to immediately go back
to their data center and do this on
their own so in the end both teams spun
up 60 enterprise-class desktops in 60
minutes we proved this by having
audience members login demonstrating 3d
graphics running on a lab that an hour
before quite frankly with bare metal to
do this yourself grab the link at the
end of the video get the deployment
guide and get started and deploy your
own POC see if you can beat us and do it
in under an hour
you
Title: A New Era of Digital Twins and Virtual Worlds with NVIDIA Omniverse | Computex 2022
Publish_date: 2022-05-25
Length: 1212
Views: 797142
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/XcHi6vKYb9o/hq720.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AH-CYAC0AWKAgwIABABGEAgWShlMA8=&rs=AOn4CLAAclJ4pxjB0BKb8_fM9wsuV5WzOw
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: XcHi6vKYb9o

--- Transcript ---

Hi, I'm Richard Kerris, Vice President of our Omniverse Developer Platform here at NVIDIA, and today we're
going to talk about virtual worlds.
First, let's get the word metaverse out there, what it means.
We've heard it all over the place recently. 
Well, quite simply, the metaverse is the network.
It’s the network of the next generation of the web, which will be 3-D.
Now, we're all excited about it, as are many, because the idea of being able to go from
one virtual world to another virtual world seamlessly
means that we're going to have a lot more capability in what we do and how we do it.
Now here at NVIDIA, we believe virtual worlds are essential for the next era of AI.
They'll be used for things like training robots and autonomous vehicles; 
digital twins where you can monitor and manage a factory; 
even whole cities will be done as a digital twin as we've seen with some of our customers. 
And the big thing that we're working on is Earth-2, which will be making a digital twin of the entire earth.
3-D workflows require teams to hold an arsenal of extremely specific and wide range of skills.
These artists, designers, engineers, or visualization experts
 are often part of the hybrid workforce working remotely or spread around the world.
Each skill and expertise requires its own system set up from laptop to data center, physical or virtualized,
 and each discipline requires its own software applications.
Today there's over 36 companies selling over 50 CAD tools with countless more being launched annually, and these are
specialized CAD programs for specific industries, regions, or tasks.
Every major design software company has its own independent planetary system of add-ons,
adjacent products, and customizations, and they are often incompatible.
This leads to workflows plagued with tedious import and export often causing model decimation, mistakes, and time lost. 
This has often forced customers to unwillingly adopt integrated products from the same vendors which can limit productivity,
creativity, and the ability of teams.
Heavy 3-D production pipelines are becoming increasingly complex as artists, designers, engineers, and researchers
integrate technologies like global illumination, real time ray tracing, AI, compute, and engineering simulation into
their daily workflow.
In M&E, the growing expectation for high quality content means that firms are spending upwards of 6.5 billion compute
hours per year rendering content.
In AEC, advanced technologies in perfectly physical, accurate visualizations are
quickly becoming crucial to an industry that is expected to breach 12.9 trillion dollars in the next year.
Today, a 3-D artist typically works sequentially across multiple applications
like 3ds Max for modeling, 
then Substance Painter for texturing, 
and finally Unreal Engine to arrange the scene, 
exporting large files many times along the way.
Now with Omniverse, artists connect their apps 
and then compose the combined scene using Omniverse Create. 
And once in Omniverse, an artist can draw on NVIDIA’s superpowers 
like Physics which lets artists use true-to-reality simulations
that obey the laws of physics 
and RTX Renderer to see their scene rendered in real time, 
fully RTX ray traced or path traced.
Omniverse also lets you collaborate with other artists 
and from across the room or across the globe connecting
their favorite apps into a single, shared scene. 
Changes made by one designer are reflected back to the other artists,
like working on a cloud-shared document, but in 3-D. 
This is the future of 3-D content creation and how virtual worlds
will be built.
Now, how are we going to do these things? 
That's what Omniverse is all about. 
We built Omniverse to be the connector of these virtual worlds.
You can think of it as two parts of a journey. 
The first part is the creation where you create content or virtual worlds
 using tools you're familiar with today, connected live to the platform of Omniverse so that
you can collaborate with creators and designers across the globe. 
The second part is the operation of those things that you created in a digital twin scenario, 
so that you can manage them in real time and understand what's taking place in
the synthetic world and how it relates to the physical world. 
Omniverse hasn't been possible until now, and there's four main points that have made that possible. 
The first is RTX technology, the ability to have real, true-to-reality
simulation in real-time on a laptop all the way up to servers and even in the cloud. 
RTX is the technology at the very core for photorealistic, real-time rendering.
The next thing is being able to scale these GPUs together 
so that we can handle very large and extremely large data sets.
So whether you're working on a project that's an environment 
or a project that's an entire city 
or even the grand project that we're working on with Earth-2, 
we have the ability to scale this RTX technology so that the true-to-reality
simulation can scale with it as well.
Now the third key thing is USD, or Universal Scene Description. 
Pixar's Universal Scene Description is the foundation for Omniverse. 
The open source 3-D description and file format is easily extensible. 
USD can be thought of as the HTML of 3-D. 
Originally developed to simplify content creation 
and the interchange of assets between different industry software tools, 
USD began in the visual effects industry to enable collaboration across globally spread teams.
Today, it's becoming widely adopted across all visual industries
including architecture and engineering, manufacturing,
product design, and robotics. 
With open standards from USD and leading-edge acceleration from NVIDIA, 
the Omniverse platform harnesses both broad support for third-party software vendors across industries
and the power unique to NVIDIA technologies 
such as ray tracing, simulation, and material libraries. 
It will enable consistent 3-D experiences of virtual worlds 
no matter what kind of industry they're in or no matter what kind of content is being presented,
so you'll have a seamless ability to teleport from world to world.
And the fourth key point is the AI Revolution. 
AI allows us to do many things with these virtual worlds. 
We can use AI to train robots in the virtual world.
We can use it to simulate true-to-reality things like weather patterns 
and how that might affect things that we're building in the physical world. 
It even allows us to do things like create a time machine
 where you can look at the past that's taken place 
and use that information to project what may take place in the future.
And you can even change the variables of that, so you can have a more complete understanding of all the things
that may or may not take place in the physical world by studying them in the digital world.
We built Omniverse to run on RTX. 
That means it can run on a laptop all the way to a workstation or a data center and beyond.
And most recently we announced OVX, a computing system dedicated to Omniverse that can start with a workstation
and scale all the way up to a SuperPOD, 
handling enormous amounts of data to have true to reality simulation at any scale.
Omniverse is a platform that serves developers, 
that serves creators and designers, and even enterprises. 
Now for developers, it's free to develop on and free to deploy 
anything that you create for Omniverse, whether you're connecting
with an extension or connecting to an existing app 
or modifying the application or even building applications on the platform.
Omniverse for developers is a rich and robust environment with modern tools for you to create right out of the box.
The next thing is Omniverse for creators and designers. 
This is where designers can create and work together in the
tools that they're already using and connect to Omniverse and be able to collaborate with other artists and designers
whether it's across the room or across the globe. 
You have the ability to work in real time and work in the virtual
environments as if you're all together at the same time.
And we have Omniverse Enterprise. 
Omniverse for enterprises is for those companies who are leveraging the platform for
design collaboration and building and managing digital twins. 
Omniverse can be thought of as a network of networks, 
and what we mean by that is you can connect existing applications that you're using today
to take advantage of all the things the platform has to offer, 
you can extend the platform to customize it to meet your needs, 
and you can even build applications on top of the platform. 
And we've seen a number of companies that are already doing this.
We have 82 connectors as of today and more coming all the time.
They're connecting their asset libraries, their applications, and all sorts of things like that. 
Plus we've had over 160,000 downloads of Omniverse for individuals
who are taking the product and using it in a wide variety of ways. 
And we're seeing amazing work being done that's shared on
all the different types of social networks that are out there today. 
The Omniverse stack is built for maximum flexibility and scalability. 
By building and leveraging Omniverse, developers can harness over 20 years
of NVIDIA technology in rendering, simulation, and AI. 
They can integrate it into their existing apps, 
or they can build their own apps and services.
Now, once you're connected to the platform, you get to harness 
all of the power that we've been talking about with Omniverse,
like physics, to understand how things are going to behave in the digital world. 
So using things like water,
smoke, fire, 
and adding them into the workflows that you're currently using. 
For example, let's take a look at how
Siemens used the power of Omniverse to quickly perform 
physics-based super resolution simulations on wind farms that
accurately reflect real-world performance using NVIDIA Modulus Physics ML Models.
Total worldwide wind energy capacity currently exceeds 750 gigawatts 
and is growing faster than ever with an additional
650 gigawatts to be added in the next five years. 
For companies like Siemens Gamesa Renewable Energy, 
optimizing the configuration of each new wind farm is critical 
for getting the most out of their investment
and reducing costs for consumers.
When designing a wind farm, it's critical to place each turbine 
so as to minimize the effects that turbines have on each other 
due to the wake that they create.
Accurately modeling the wake requires high resolution, high fidelity simulation
data that is specific to that wind farm such as the geographic location and the terrain.
The gold standard for generating this data is the large eddy simulation shown here, 
but to run just one iteration for a single turbine can take 40 days on a 100-core CPU. 
And with so many iterations needed to develop an accurate model for a specific site,
using CPUs is impractical. 
Using NVIDIA Modulus and NVIDIA Omniverse, 
Siemens Gamesa has been able to reduce that 40 days to just 15 minutes,
approximately 4,000 times faster. 
This is accomplished by running the model at a lower resolution, 
and then with a Physics ML model, trained using Modulus, 
to enhance or super-resolve the data, 
and the results are functionally equivalent to having run the model at full resolution.
Let's take a look at the same data presented in a way that clearly highlights the coarseness of the low-resolution simulation. 
Here they look like pixelated blobs. 
In the super-resolved flow field, you can see the finer vortex structures.
Now that we have the simulation data, we can build an accurate weight model for the site 
and optimize the placement of each turbine.
 Thousands of iterations accelerated by NVIDIA GPUs are run as part of the optimization to
achieve the maximum power output for each farm while minimizing the cost, 
and the impact is huge. 
When using NVIDIA Modulus and Omniverse for a typical 1000 megawatt offshore wind farm,
optimization can provide power to supply up to
20,000 additional homes and to do it at 10% lower cost.
You got many other types of capabilities that you can do. 
The key thing here is that with Omniverse, you extend and enhance your existing workflows. 
The second part is the operation or the digital twin. 
A digital twin is a physically accurate simulation of the real world in a virtual environment, 
and it brings with it all sorts of capabilities. 
All types of industries will use digital twins to understand how they're manufacturing something, 
how the operation of that device will take place afterwards,
 and all kinds of scenarios that they wouldn't have been able to imagine before. 
There are many steps to building a digital twin, 
and today Omniverse Enterprise customers can start with the building blocks.
First is full fidelity visualization. 
This is true to reality 3-D simulation of what's taking place in the virtual world,
identical to what's happening in the physical world. 
Enterprises can, for the first time, aggregate 3-D data sets
from disparate design and CAD softwares and visualize them in real time. 
And they can leverage the Omniverse's platform
multi-GPU rendering for extremely large data sets.
You can also build custom 3-D tools and pipelines. 
One of the most difficult constraints of 3-D simulations is the
disconnected, disparate, incompatible tools and data sets that teams must connect. 
Omniverse is an easily extensible development platform 
that provides building blocks for enterprises to build their own plug-ins and tools to enhance and
that provides building blocks for enterprises to build their own plug-ins and tools 
to enhance and extend their existing workflows
without the need to rebuild their entire existing pipeline.
Lastly, many of our digital twin customers already have robotic perception systems in place 
or are planning to deploy them.
Robotic systems require large amounts of physically accurate synthetic data to better train and optimize their
performance in the real world. 
With Omniverse, enterprises can leverage the platform to generate synthetic data that is
indistinguishable from reality, 
reducing training time and increasing the AI system's accuracy in the real world. 
We have a large number of customers across all types of industries who are already using Omniverse
to connect their 3-D workflows.
Customers like Amazon Robotics and BMW are using Omniverse
to build digital twins of their warehouses and factories.
Companies like DNEG are using Omniverse to do pre-visualization for the work that they're doing in the media
and entertainment space.
Another company, Ericsson, has used an Omniverse platform 
to build a digital twin of a city.
This helped them to understand how they're going to propagate 5G antennas across that environment. 
Foster and Partners and KPF are using
Omniverse to build full-fidelity, physically accurate simulations of the buildings and environments that their
architectural work is being done in. 
These are just a few of the companies using Omniverse across many types of use cases,
whether it's Factories of the Future, Media and Entertainment, 
or visualization of landscapes we have yet to imagine.
Now in this final clip, we're going to show you how Amazon Robotics 
used Omniverse to build a digital twin of
their warehouse and be able to work with that in real time. 
Take a look.
Every day, hundreds of Amazon facilities 
handle tens of millions of packages  
with more than two-thirds of these customer orders 
handled by robots. 
To support this highly complex operation
we deployed hundreds of thousands of 
mobile drive robots and associated storage pods  
which allow us to store far more inventory 
in our buildings than traditional shelving  
and which help us move inventory in a safer, 
more efficient way. 
Key to the scaling has been our ability to simulate these buildings
and understand their performance before we build them.  
Let's take a look at how NVIDIA Omniverse is 
helping us optimize and simplify these processes.  
At Amazon Robotics we're able to create 
full-scale digital twins 
of our warehouses in NVIDIA Omniverse,
helping us optimize warehouse 
design, train more intelligent robot assistants,  
and gain operational efficiencies.
In Omniverse, we are uniquely able to aggregate data sets from 
many different CAD applications and visualize 
these massive models in full fidelity realism  
enabled by Omniverse's RTX accelerated ray tracing, materials and physics.
 Digital twins are an integral part of future warehouses and factories  
enabling continuous integration and continuous delivery.
With each new software and layout optimization we can test
in the digital twin before releasing to the physical warehouse  
preventing any system downtime or failure 
while maximizing operational efficiencies.  
Next, packages of every shape, size, weight, and 
material move rapidly through our fulfillment centers.
We use NVIDIA Omniverse to better train 
autonomous robotic sorting and picking solutions. 
Training these robots' perception systems 
accurately enough to prevent system failures  
requires massive amounts of high quality data but 
often the data doesn't exist or there isn't enough.  
When we introduced more reflective tape to our 
packing materials the perception systems failed.  
We retrained the models with physically accurate, 
photoreal synthetic data generated in Omniverse,  
indistinguishable from reality, saving weeks of 
retraining time and increasing model accuracy.  
Finally, with the digital twins of our 
facilities and the ability to quickly  
and accurately train robot perception systems 
we can also better configure human robot workstations  
simulating opportunities for better employee ergonomics.
At Amazon Robotics, NVIDIA Omniverse digital twins are
helping us as we reimagine warehouse logistics from end to end
and capturing significant operational efficiencies which 
enable us to deliver more value to our customers.
Omniverse is available for customers and developers around the world. 
Omniverse Enterprise is on NVIDIA Launchpad which
provides a free two-week trial where you can harness the power of Omniverse on NVIDIA-Certified Systems.
You can also request a free 30-day evaluation period. 
And finally Omniverse for Developers is available today at nvidia.com/omniverse.
 It's a free-to-download and free-to-develop-on platform. 
This has been a brief overview of some of
the features and powerful capabilities of the Omniverse platform. 
I thank you very much for your time today, and I'd
like to end with showing you some of the amazing work our customers have been doing with this platform. 
Let's take a look.
Omniverse technology will transform the way you create.
Title: Hands-on: The Tegra K1-powered Acer Chromebook 13
Publish_date: 2014-08-12
Length: 164
Views: 109097
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/xDWyyP8UAac/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLAaXI1SZDzPooNJVx0M_AT1niYHPQ
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: xDWyyP8UAac

--- Transcript ---

[Music]
hey guys I'm will and today we're
talking about the exciting new Acer
Chromebook 13 powered by the amazing
NVIDIA Tegra k1 mobile processor so
let's start by talking about just how
exceptionally powerful this guy is most
Chromebooks will get you about eight
hours on a single charge this will do 13
hours and at just 18 millimeters and 3.3
pounds it's one of the thinnest and
lightest Chromebooks on market today and
you'll notice there are no fans so it
runs completely silent and you get all
of this in a comfortable 13 inch
notebook that has a clean and modern
design but what really sets the
Chromebook 13 apart from all the others
is the performance of the world's most
advanced mobile processor the NVIDIA
Tegra k1 it's GPU is based on the same
Kepler architecture that drives the
world's fastest supercomputers and most
extreme gaming PC's the graphics
horsepower of the Tegra k1 is 192 core
GPU is simply unmatched in the world of
Chromebooks this means you get next-gen
Chromebook experiences like rich visual
web browsing high-end gaming and
interactive educational content with an
available full 1080p screen the quad
core architecture also makes
multitasking a breeze you can check out
all your favorite websites run
productivity apps and listen to
streaming music all at the same time
without missing a beat and this is one
of the only Chromebooks that's Google
Hangouts optimized so you can enjoy
fluid video chats in HD with up to 10
people now we all know that Chromebooks
are huge in education and they're being
used by students everywhere here's a
perfect example this is bio digital
human it's a searchable 3d model of the
human body that's being used in over
2,500 schools across the US and thanks
to Tegra k1 you get the performance you
need to enjoy perfectly fluid
experiences on next-gen content just
like this another common use case for
Chromebooks is to have a bunch of
different websites open all at once the
quad-core Tegra k1 easily outperforms
dual-core processors like a seller on
that you'll find on this Chromebook
right here and many others like it so
let me show you what that means here we
have two web sites open CNN and ESPN we
also have songs of streaming music in
the background and Google Docs is fired
up right here I'm just gonna run a
DOX calculation on both machines and I'm
just going to start both at the same
time
[Music]
as you can see the dual core Celeron
takes significantly longer to complete
the task compared to the quad-core Tegra
k1 so check it out for yourself and see
how the Tegra k1 processor lets you do
more see more and explore more with Acer
Chromebook 13
Title: Evolve gameplay, overview and interview at E3 2014
Publish_date: 2014-06-12
Length: 137
Views: 9129
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/xf34okGY9lw/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: xf34okGY9lw

--- Transcript ---

we had the opportunity to speak with
someone from Turtle Rock Studios today
about evolved we covered a lot of things
including what it's like to be a hunter
what it's like to be a monster and some
of the amazing and engine technology
that makes this game look so good it's
co-op play against a boss battle only
the boss monster gets to be a player you
can team up you can do co-op and be one
of four classes on the hunter side and
hunt them down a big monster or you can
be the monster player can grows over
time and gets bigger and more powerful
as the game goes on my name is Chris
Ashton I am a co-founder at turok
studios and also design director on
evolve maybe 13 years ago like big game
hunter used to be really big and we're
thought that's kind of crazy that so
many people are buying these hunting
Sims you know and there was something
cool there but we thought well wouldn't
it be it could be cooler if it was some
crazy big monster or something like that
so that was kind of you know this little
seed that was planted you know a long
time ago if you've played any games she
fought a boss at the end and it was like
well it would be fun to fight together
against a boss and then it's it's only a
small little leap from that to say well
what if what if I was the bonus
so we're using CryEngine it's the newest
version of CryEngine we just are at e3
right now showing off the new physically
based shader and so the metals are more
accurate and all that kind of stuff so
what's cool about where the PC is that
is that we're able now to create these
really big lush environments you know we
need the monster be able to hide and so
we need that coverage we need all this
foliage and we need wildlife to feed on
we want that wildlife to be cool the
Predators will hunt prey creatures and
they'll go kill hunters and and so
there's all of this wildlife simulating
in the environment as well while the two
teams are battling and so there's just a
lot going on and we couldn't do that
with sort of the older system so it's
kind of opened the door for us
you
Title: Research at NVIDIA:  New Core AI and Machine Learning Lab
Publish_date: 2018-11-29
Length: 159
Views: 18135
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/xFGGksGUVcg/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: xFGGksGUVcg

--- Transcript ---

and Media Research has started a new lab
on coal machine learning and artificial
intelligence
this lab will push the boundaries of
machine learning techniques to me AI is
a trinity of data algorithms and compute
thanks to the availability of large
amounts of data along with large amounts
of compute from nvidia gpus we've seen
the deep learning revolution take off
but now we want to take you to the next
level and that means new algorithmic
research so if you think about the
current computations in our deep
learning systems they're all based on
linear algebra can we come up with
better paradigms to do multi-dimensional
processing can we do truly tensor
algebraic techniques in our tensor
course and what kind of new
architectures will this realise how can
we use active learning generative
modeling as well as synthetic data from
large scale simulations and computer
graphics to do better machine learning
but also one that will generalize to new
unseen scenarios in a stable and
reliable manner these are all important
questions my lab will tackle currently
we have all of the machine learning
being done on the cloud and all the data
being transmitted to the cloud but if
you think about developing countries or
regions with not enough bandwidth what
kinds of compression algorithms can be
used to send limited amounts of
information and enable much of the local
and federated learning on small devices
that to me ties in very well with my
core mission as well as the mission of
Nvidia to democratize AI increasing
accessibility of current AAA
technologies to diverse populations
across the globe enabling them to do
further innovations apply to solve their
local problems and bring their
creativity into doing better
research is in the very core DNA of
NVIDIA here research drives products
rather than the other way and we've seen
that in so many instances if you think
about the development of CUDA and the
impact it has had on so many large-scale
applications all the development of ray
tracing in computer graphics they all
started out as research projects and
ultimately took over the product
landscape and media being a full-stack
company means these algorithmic
innovations can be translated quickly
into actual gains in hardware and
realized in a diverse set of
applications
[Music]
Title: Concurrent Design and Real-Time Visualization with NVIDIA RTX and Autodesk Revit + Enscape
Publish_date: 2022-05-26
Length: 134
Views: 29476
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/XGacecbnVFA/hq720.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AH-CYAC0AWKAgwIABABGGUgXChSMA8=&rs=AOn4CLBCzOpXXIganBnwN_FjnXxWi4glxA
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: XGacecbnVFA

--- Transcript ---

enscape 3d is a rendering and
visualization software available as a
plugin for autodesk revit enscape
provides superior visualization of revit
models and allows instantaneous changes
made to the design the model in revit
and the visualization and enscape are
both synchronized this provides the
ability to see and evaluate variations
allowing for rapid model iterations and
better overall design decisions adding
enscape as an extension to revit is
quick and easy once installed enscape
will appear as a tab in the revit ribbon
ready to engage and get to work
enscape can take advantage of rtx ray
tracing provided through the integration
of nvidia rtx sdks within the
application this means users can achieve
physically accurate reflections diffuse
indirect lighting better performance
light transport and reflection quality
simply start up enscape and the viewport
will align and be in sync with the 3d
view in revit in enscape simple
adjustments to the time of day feature
changes the scene view to any desired
time and mood here the scene is dialed
into a nighttime scenario for
visualization notice that the lights
will also dynamically turn on and off as
the scene transitions from day to night
exclusive to users with nvidia rtx gpus
are ai-based technologies nvidia deep
learning super sampling or dlss and
nvidia denoiser which together provides
superior image quality and enhanced
performance
data such as bim info associated with
our scene can be adjusted and
transformed and automatically updated in
enscape simple adjustments of props and
revit are updated for us in our rendered
view
with the assets functionality in enscape
it is simple to add props or additional
assets to the scene furthermore when
changing a configuration within the
revit model as shown here when removing
windows or changing exterior lights the
lighting and environment will update as
one would expect in the physical world
enabling creativity and design nvidia
rtx gpu technology together with
autodesk revit and enscape 3d empowers
and enhances iterative design and
visualization workloads
Title: 2 minute Tips:  Getting the most out of SolidWorks RealView
Publish_date: 2013-08-28
Length: 75
Views: 20792
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/XgB75ZajdOs/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: XgB75ZajdOs

--- Transcript ---

hi my name is Brandon Loeb I'm a senior
mechanical design engineer at per septum
ed and a certified SolidWorks expert
today I'd like to talk to you about real
view functionality inside of SolidWorks
first off real view can be enabled by
the toolbar at the top of your screen
and if you're not seeing this option
it's probably because you don't have the
recommended hardware installed from
SolidWorks in this case I'm running a
NVIDIA Quadro k2000 graphics card which
is perfect for my model here so when I
design rear view not only looks great
but can have a business impact as well
sometimes if I need to make a decision
between producing and expensive
injection molded part or maybe having a
metal machine part I can come over here
and change the appearance of how I want
this to look to really showcase the
potential of real view graphics I've
applied a few extra materials to my
model and a zoom in you can really see
the level of detail that real view
enables rear view isn't just for the
engineer it can even be for the
marketing department as these images are
so good that they've been able to be
used for marketing material
Title: GPU Technology Conference Keynote Oct 2020 | Part 6: "AI for Every Company"
Publish_date: 2020-10-05
Length: 362
Views: 151409
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/XiwVziNh_3s/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: XiwVziNh_3s

--- Transcript ---

 
The world's enterprise is awakening to the power of AI.
What AI and machine learning applications will the world's enterprises need?
How will today's enterprise computing environment accommodate the new model of computing – scale-out and microservices?
For the rest of my talk, I will explore some of these.
One of the most important AI models is the recommender – it is the engine of e-commerce, digital ads, social media, news readers, music and
movie services, user generated content platforms, and even search.
Learning from data, a recommender will learn your explicit and implicit preferences, to recommend the trillions of physical and digital items in the world to billions of people.
Recommenders are extreme data processing and machine learning systems.
Large scale services can be collecting petabytes of data a day from interacting with hundreds of millions of users and connecting them to
the continuous stream of new news, new video, and new goods.
Data engineering teams process the data into data frames.
Data scientists engineer from structured data or using deep learning to learn features from unstructured data that may be predictive of your preferences.
The MLOps teams deploy the models at-scale.
New model generates new interactions that will be used to update the model.
Meanwhile, new features are being engineered and new AI models are being explored.
The more predictive, the greater the click-through-rate. A 1% improvement in accuracy can translate to billions more sales or better customer retention.
The recommender system consumes entire data centers.
Iteration loops can be weeks. Companies understandably want it to be hours.
NVIDIA Merlin is the first end-to-end accelerated recommender system ever created.
Merlin is superfast and scalable.
Scalable to as large as you like, it is plumbed from data load, to training, to inference. And with v0.2, we can load data directly from
AWS, GCP, Azure, and Apache Hadoop File System.
We've gotten great feedback on Merlin.
Tencent used Merlin to train a recommender for their video platform.
Their cycle time went from a day to three hours.
The increased cycles of learning allowed them to increase their model accuracy.
Merlin is now in Open Beta.
The core engine of Merlin is NVIDIA RAPIDS, and is fast and scalable.
RAPIDS is the fastest ETL engine on the planet – and it supports multi-GPU and multi-node.
Its API is modelled after Pandas, XGBoost, and ScikitLearn, the hugely popular data science frameworks, so RAPIDS is easy to pick up.
On the industry standard data processing benchmark, running 30 complex database queries on a 10TB dataset, a 16 DGX cluster ran 20 times
faster than the fastest CPU server and is 1/7th the cost and 1/3rd the power.
RAPIDS has been downloaded nearly 500,000 times, with this year nearly 3 times the download over last year.
Adobe is using RAPIDS as the data engine for their intelligent marketing service.
Capital One is using RAPIDS for their critical business operations like fraud detection and chatbot.
Data processing is foundational to all enterprises in the data-driven era.
While some companies are just starting to use deep learning, nearly every company is doing data processing with exponentially growing scale.
Industry leading data science and machine learning platforms and cloud services are integrating RAPIDS.
Cloudera is a leading enterprise data analytics software company. Their platform runs on over 400,000 data center CPU nodes.
Over 2,000 enterprise customers use Cloudera globally.
They serve 8 of the top 10 financial services companies, all 10 of the top 10 telcos, and all 10 of the top 10 healthcare institutions globally.
Cloudera is a hybrid-cloud data platform that lets you manage, secure, analyze, and learn predictive models from your data – the entire lifecycle of your data.
Today we are announcing Cloudera will accelerate their data platform with NVIDIA RAPIDS, NVIDIA AI, and NVIDIA-accelerated Spark.
It will be called Cloudera Data Platform powered by NVIDIA.
Several thousand engineering years of work will instantly be available to their over 2000 enterprise customers.
Notice the lower right box of the Cloudera Data Platform – the Private Cloud.
There are two types of computing environments – bare metal for scale-out and virtualized multi-tenant.
Bare metal is essential to achieve multi-GPU, multi-node scale-out needed for large data processing or training.
Scale-out and virtualized computing cannot share one infrastructure today.
This creates two distinct infrastructures that are managed and operated differently, not sharable, and more difficult to secure.
There is a solution.
NVIDIA and VMware are announcing a second partnership.
We are going to do some serious computer science to create a data center platform that can support GPU-acceleration for all three major
domains of computing today – virtualized, distributed scale-out, and composable microservices – all NVIDIA accelerated.
Enterprises running VMware will be able to enjoy NVIDIA GPU and AI computing in any computing mode.
This is a massively exciting project and every bit as exciting as when VMware first virtualized the data center.
VMware and NVIDIA are going to bring AI to the world's enterprises.
Title: Helping Cameras See Clearly With AI - NVIDIA DRIVE Labs Ep. 3
Publish_date: 2019-05-15
Length: 86
Views: 17009
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/XkSiJPRovXw/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: XkSiJPRovXw

--- Transcript ---

Cameras are basically the eyes of a self-driving
car, so before we even get on the road, we
have to know whether or not they can see and
how well they can see.
That is the purpose of our NVIDIA DRIVE AV
Mission 59C, assessing camera site using ClearSightNet,
deep neural network.
On the left, we see that the input camera
image has some blur due to lens fog, and on
the right, ClearSightNet is detecting that
partial occlusion is shown in green, and in
the top-left corner, it's detecting complete
occlusion, shown in red, due to pixel saturation
from the sun.
In this case, we're detecting a temporary
occlusion as the windshield wiper moves in
and out of the camera's field of view.
Here, we see that there's snow accumulated
on the lens, and as we clean the lens, ClearSightNet
is detecting both complete blockage in red
and reduced visibility in green, and the reduced
visibility remains temporarily because of
the glare of the overhead streetlamp.
As soon as we drive away, full clarity and
visibility return.
Here, the lens is clean, and the network is
detecting reduced visibility in the far distance
due to heavy rain.
These detection results can be used with other
sensor data to compute overall perception
confidence and make safe driving decisions.
And that was DRIVE AV Mission 59C, ClearSightNet
deep neural network, which will be shipping
with an open API in the NVIDIA DRIVE software
9.0 release.
Title: NVIDIA DGX Systems Customer Montage
Publish_date: 2019-05-01
Length: 138
Views: 18424
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/XNYHVbBO7ec/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLCWEf07GrKMtXE9DlN7_P_kVkeMoQ
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: XNYHVbBO7ec

--- Transcript ---

[Music]
the main reason customers come to us is
they have the tremendous amount of data
that they're sitting on from decades and
very very sparse data sets where things
like failure or damage are concerned but
implications are very large
pharmaceutical companies are generating
tons and tons of data those insights
could be new potential drugs to develop
or new therapies for patients or new
diagnostic mechanisms for disease
companies already have a massive amount
of audio data coming in but they only
understand a tiny fraction of that the
solution to this is using end to end
deep learning that can understand a vast
variety and have very high accuracy
while doing we're using Nvidia dgx
station and DG x1 in our data center in
order to train these models it's giving
us the business outcomes that we can
barely even imagine even a few months
ago if not a few years ago things like
three to five times better accuracy in
anomaly detection about four to five
times better accuracy in predicting when
a particular piece of equipment would
fail or even looking at predicting rate
of penetration in a drilling operation
for example all these things get us not
just better accuracy but they also get
us faster than any of our previous
systems could we want everyone to become
native with the GPU to be able to use it
on anything that they work on and then
that allows us to then immediately jump
to the dgx
and scale it up and see if it's
something that we can apply commercially
to what we do we use the dgx station
when we're doing quick tests and would
like to figure out if a model is worth
pursuing deeper and then when we know
that that's the case we switch to DG x1
and train for longer periods of time in
order to probe deeper into that model we
had been searching for several years for
a technology package that was small in
format
and price point wise could allow us to
have the biggest bang for a buck for
high performance computing when the dgx
station came out we were just blown away
by what was possible building systems
that could look at data think about data
integrate knowledge that could actually
imagine we could create whole new areas
where insights could be generated
Title: NVIDIA and ServiceNow: Transforming Enterprise IT with Generative AI
Publish_date: 2023-06-05
Length: 471
Views: 25233
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/XqbXLxqZGhY/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLAhc54w5xuWfTpITTzxUV4goVZQeQ
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: XqbXLxqZGhY

--- Transcript ---

please welcome on stage
CEO president and co-founder of Nvidia
Jensen Wong
[Applause]
[Applause]
[Music]
all right first of all thank you for
coming I'm delighted to be here
our colors also match fantastic yeah
median service now so that's why we have
to stand here looks like we just got
acquired
[Laughter]
I'm okay with it
I'm a little sad but I'm okay with it
behind deep learning
is this truly like you said
an iPhone moment in the AI industry
this is
the biggest
computer industry platform transition of
Our Generation
there's no question about that
generative AI you've been talking about
it all day and and this is the first
computer that can automatically generate
text images videos
proteins genes chemicals anything that
has structure it can learn the language
of learn as representation and uh and
generate based on input that is
surprisingly easy to form human language
and so after all of these years we've
invented
a computer that we can program with
human
and it's really quite quite
extraordinary and it it triggered two
simultaneous
computer industry
transitions the first is accelerated
Computing our our uh our growth is uh
directly tied to what is happening in
the industry after 60 years
I mean literally after 60 years we have
reinvented what a computer is
for the very first time since the IBM
system 360. wow yeah and so this is this
is a gigantic moment for the computer
industry and uh you know we've installed
about a trillion dollars worth of
computers in the world today wow and
that trillion dollars worth of computers
over the next 10 years will all be re
you know turned over into accelerated
Computing systems because it now has a
killer app called generative Ai and so I
think this is a this is just a giant
moment for the industry and what a
phenomenal job by Nvidia not only
inventing gpus or gpus that power Ai and
the Deep learning advances would have
not happened without your vision and
your team's hard work so Jensen
one of the things that this great you
know all of these are our customers as
you see them is pretty large room hi
customers
um
tell us about you know 175 billion
parameters on chat gbt3.x yeah of course
we can do that so your software offering
Nemo
what was the vision behind it and how
you think we are working together so
that we can provide gen AI for all these
customers yeah one of the things that
you said that was I was listening to
your talk earlier and you said thank you
for that yeah I enjoyed it I enjoyed it
it was an excellent keynote
um uh that you said very very
appropriately is that there are there
are general purpose Ai and that's going
to be really terrific uh to help
interpret uh do nuanced or the uh the uh
imprecise uh meaning of people who want
to interact with computers however what
all companies would need to do and what
we've done inside our company is create
highly specialized domain specific AIS
we don't you know it's great to have
intelligence but we have very specific
skills and we have very specific tasks
and informed by proprietary knowledge
and domain uh information that we want
to train our AIS to do in our case want
to train an AI to design chips or to
architect our chips or even design our
algae our kernels our math kernels in a
way that no human possibly can
and so we we come up with AIS that
create AIS that help explore the entire
design space and we run it on our Ai
supercomputers and they collaborate with
our engineers and they come up with
designs that no humans possibly can
and so we we would like to do this and
and your vision is to to do this for all
of the world's Enterprise yeah uh
servicenow is the world's Enterprise
service platform thank you and over time
you will add on top of it uh
domain-specific AIS that would be
optimized for the data and the domain
skills of each of your customers that's
correct we would love to partner with
you to be the back end of that we have
the AI capability the AI expertise we've
developed a state-of-the-art large
language model system which includes
pre-trained models of large medium small
and Tiny sizes you know the reasons the
reason why you need large and small
models just just uh you're probably
reading a lot of press around this right
now you need large and small models
because obviously you want to deploy
into many places in an optimal way but
you need the large models to teach the
medium models correct to teach the small
models to teach the tiny models and so
before you have tiny models you actually
need very large models and so we've
created the whole family of it in a
pre-trained way including the Guard
railing systems the supervised
fine-tuning systems the reinforcement
learning human feedback systems and the
vector databases that will be connecting
your proprietary information and
knowledge base into it and so in
combination the system allows our
collaboration to create specialized
models for your customers that could be
deployed on the servicenow platform and
that is exactly what we are working on
our engineering teams are working with
Jensen's Nvidia teams to have
domain-specific llms we are actually
starting with itsm as a use case
and Jensen's team proudly showed us this
morning the accuracy rates are pretty
high and we cannot wait to roll this out
for you in our releases coming soon and
Jensen you know first of all I'm just
inspired by all the work Nvidia has done
on deep learning and llms one of the
things that many
of us didn't know I didn't know until we
started our relationship with you is you
have an amazing software team
and you have enabled ecosystem around
the world that can Leverage The Power of
Nvidia you delivered the first dgx
personally to open AI team and I mean
just I think about all these Innovations
and how your team behind the scene
sometimes are unsung heroes and that's
what you're going to do with us so that
they can be all heroes in gen AI thank
you very much for being with us thank
you thank you CJ and really looking
forward to this partnership thanks
everybody
thank you
[Music]
Title: AiFi – AI Auto Checkout Solution
Publish_date: 2019-10-28
Length: 70
Views: 11226
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/xqkT8dZyNWc/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: xqkT8dZyNWc

--- Transcript ---

i phi is a global innovator automating
the worst stores our autonomous store
platform consists about a camera
tracking the live inventory management
the delightful checkout for experiences
and i five power stores have becoming
destinations in cities like San
Francisco Paris Shanghai Saddam the EDX
platform is actually the few - I Phi's
autonomous or platform it enables the
multiple person tracking inside the
store and also to view the virtual
shopping carts for each of them they can
actually bring the check out free
experiences to all their shoppers cater
to their demands wherever they want and
whenever they want we can also provide
battery decision-making tools when it
comes to inventory and workforce
management we appreciate and we do
support really enabling us to provide
better services to all of our retail
clients we're automating the war stores
and looking forward to bringing the
checkout free experience to all the
shoppers in the world
Title: NVIDIA Brings GPU-Accelerated AI to Modern Enterprise IT - CEO Jensen Huang at VMworld 2019 Keynote
Publish_date: 2019-09-17
Length: 92
Views: 16850
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/xS9juMlXVlY/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: xS9juMlXVlY

--- Transcript ---

artificial intelligence is the most
powerful technology force of our time
and in fact it has created the greatest
computing challenge of all time machine
learning and deep learning has allowed
us to solve problems that we couldn't
imagine solving just a few years ago
we're working with enterprises all over
the world these companies are using
artificial intelligence to create new
products of services such as smart
retail and smart warehouses to medical
instruments that have AI to self-driving
cars and self driven trucks for the data
scientist we have created a powerful
computer with NVIDIA GPUs and on top of
it a CUDA accelerated data science
framework we called rabbits with
Nvidia's Rapids data scientists are able
to do their data analytics work which
used to take days has now been reduced
to hours for data centers we had to
create a computer that integrates into
the modern IT infrastructure and that
means it has to be virtualized with VM
work so that is manageable flexible and
secure so we did two things first we
created the Nvidia virtual compute
server that virtualizes for the very
first time this GPU compute architecture
and makes it possible for us to run all
of the modern workloads that require our
GPUs and seamlessly second we establish
a partnership with VMware and Amazon so
that this architecture first naturally
from any data center in the world into
the Amazon Cloud I can't tell you how
excited I am about this development and
I hope that all of you enjoy using it
and I can't wait to see the amazing AI
the Ukraine
[Music]
you
Title: Batman: Arkham Knight gameplay, overview and interview at E3 2014
Publish_date: 2014-06-12
Length: 178
Views: 16023
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/xuzwXWe5-mQ/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: xuzwXWe5-mQ

--- Transcript ---

nvidia and warner brothers have a long
history of integrating game-changing
technologies like physics into the
batman arkham series this year we have a
chance to check out the finale in the
franchise batman arkham knight hold on
your pants boys and girls because this
game looks incredible not only are there
a slew of new free flow combat movements
but you get a drive the Batmobile
batman arkham knight is about the epic
conclusion that's coming together
scarecrow has taken charge he's acquired
Arkham Knight new main super villain of
this game they're bringing altima
buildings together they try and take
down about 12 once and for all it's epic
it's big its bold we've got new features
we've got beer takedowns whole host of
new gadgets at the biggest I guess
element so far is the batter people like
that's the big talking point we've got
the super mode we've got battle mode how
we integrate that into Batman's Arsenal
is the biggest challenge for us it is
the epic conclusion we are bringing the
whole story elements to a close players
would have realized that we've gone from
destroying Arkham City to now rebuilding
it from from square one for Arkham
Knight yourself it's the game world's
five times bigger in the original game
was 20 times bigger than Arkham Asylum
was we got a whole host of new super
villains we've got the Arkham Knight
scarecrow uniting trying to take down
the bats and Batman facing his biggest
enemy so far which is the Arkham Knight
himself tonight grasim falls
a city of fear rises well Arkham Knight
all i could say right now is he's a guy
that just wants to kill Batman he's
bringing the fight to Batman told to toe
so he's equipped themselves with the
Batmobile taking on the tanks taking the
drones those guys going to get dirty and
it's going to be crazy it's war so the
gliding ability has been enhanced so
those game well it is much bigger and
there was a lot of votes Kelly in the
game you can use the Batmobile to your
advantage and that respect to so you can
eject out the Batmobile and gained
immense fight using the momentum at the
Batmobile and its force you know we want
to maintain free flow but obviously
introduced a whole new host of enemies
you all have seen people charging a
Batman people kind of let my picking up
pipes using it against them that my
smashing their heads into like
electrical boxes lot more use of the
environment you were seeing the
battement we will take down where he
kind of gets the Batmobile involved
there's a whole host of new stuff there
so just players going to have to just
wait and see what happens and will be
reveal what haven't mentioned so far I
guess fear take down their tech towns
are an awesome way to kind of introduce
Batman into a predator or combat
situation he takes on I'm thugs in very
quick succession you will have seen the
body armor the Batman is now gone its
new sous he uses that to that varnished
kind of quickly get around the level and
just bust out I'm going to take out
these guys in very quick so by my life
from nice coming out in 2015 we cannot
wait
Title: NVIDIA Vid2Vid Cameo Mission AI Possible: NVIDIA Researchers Stealing the Show
Publish_date: 2021-06-24
Length: 85
Views: 45722
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/xzLHZbBvKNQ/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: xzLHZbBvKNQ

--- Transcript ---

[Music]
welcome everyone
we've got a big job ahead of us our
mission is to change online video
conferencing making it way better with
artificial intelligence but we've only
got one shot
so let's get started we're gonna need
all of you so introduce yourselves
i specialize in facial redirection which
uses ai to help people ci2i
facial redirection is about eye contact
and framing the face using a single shot
my air technology enables meeting girls
to till their heads
and move their eyes i make meetings more
animated
with colorful avatars ready for the big
screen using my facial cues as a guide
watch as ai animates a digital avatar
right in front of your eyes with this
technology meeting attendees can quickly
jump on a call using their polished
digital alter egos no red jumpsuit or
salvador dali mask required
with a fraction of the typical bandwidth
i help every meeting run smoothly
besides help make everyone look their
best this ai technique
also helps reduce the video conferencing
bandwidth by up to 10x
without any jitter or lag great thanks
everyone
we're going to need all your
contributions to succeed at this mission
we'll meet again
Title: Feature Tracking for Robust Self-Driving - NVIDIA DRIVE Labs Ep. 12
Publish_date: 2019-09-25
Length: 86
Views: 53103
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/y2X_7KwppoI/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLBIAeJdclfqKtGznUeFhTtLZfr0Ow
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: y2X_7KwppoI

--- Transcript ---

Today in DRIVE Labs, we're talking about estimating
changes in correspondences in groups of pixels
between adjacent video frames.
In computer vision, this is known as feature
tracking.
In this clip, the feature tracking algorithm
is running on a six-camera surround perception
set up with pixel-level feature tracks for
different objects shown in blue.
When both the ego car and the tracked objects
are stationary, feature tracks look like dots
and when there is relative motion they look
like continuous traces.
For the object detections shown by the bounding
boxes, feature tracks are used to compute
object motion and urgency.
Green boxes indicate the object is moving
further away while the yellow and red boxes
indicate it's getting closer.
Here the continuous blue traces indicate that
feature tracking is maintaining a long track
history or tracking pixel-level correspondences
in a stable and robust way over time, in this
case at night.
In this clip, we observe long future track
histories despite glares from the sun, which
causes pixel saturation.
Here we're observing long stable future tracks
for buildings, polls, lane markings, and traffic
lights despite camera image blur.
Feature tracking provides important temporal
and geometric information for object motion
and velocity, estimation, camera self-calibration,
and visual odometry.
Title: NVIDIA Self-Healing Network Technology Enables Unbreakable Data Centers
Publish_date: 2022-11-14
Length: 130
Views: 34644
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/ydULljl9ies/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: ydULljl9ies

--- Transcript ---

At the heart of every InfiniBand data center 
is a centralized subnet manager —
a software mechanism that manages the entire InfiniBand network.
Occasionally, network failures occur, causing 
system downtime and interruptions to critical applications. 
The subnet manager will recognize these failed links.
It will re-calculate and program the switch's 
forwarding tables to resolve the problem,  
but this can take up to 5 seconds for 1,000 nodes
and 30 seconds or more for clusters of 10,000 or more nodes
— simply not fast enough to keep an application running seamlessly.
With autonomous self-healing capabilities,  
the latest generation of NVIDIA Quantum 
InfiniBand switches dramatically improves  
data center resiliency by speeding up fault recovery by 1,000 times.
Let’s see it in action!
When everything is operating correctly, data follows an optimized,  
predefined route through the network.
But what happens when that route is 
interrupted? Let's look at how NVIDIA  
Quantum InfiniBand rapidly recovers 
from two different network failures.
In the simplest case, when a switch has more than 
one forward route to the desired destination,  
that switch can decide to forward the 
packet to an alternate port on a new route.
A more complex scenario occurs when there's a failure,  
and there are no alternate forward routes to the desired endpoint.
In this case, the network must quickly identify 
an alternate (but still optimum) route. 
NVIDIA Quantum InfiniBand's self-healing 
technology lets network components exchange  
real-time information and make smart, immediate 
decisions to overcome application timeouts.
In this case, it determines which switch should 
take responsibility for rerouting the traffic.
The total time required to perform this 
action is about 1 microsecond —
quick enough to ensure the application 
continues to run uninterrupted.
NVIDIA Self-Healing Network technology improves  
data center resiliency by speeding up fault recovery by 1,000 times.  
To learn more , visit the links in this video's description.
Title: How AI Helps Self-Driving Cars Predict the Future - NVIDIA DRIVE Labs Ep. 21
Publish_date: 2020-07-22
Length: 176
Views: 24178
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/yEtH23rKY8Q/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: yEtH23rKY8Q

--- Transcript ---

In complex driving scenarios, such as multi-way intersections
A self-driving car needs to correctly anticipate
or predict, the future actions of other road users in order to drive safely
Today in DRIVE Labs, we are talking about our PredictionNet deep neural network
This is a module trained on both perception and map data to 
predict the future trajectories of road users.
As input training data for PredictionNet,
we use outputs from our lidar perception software stack that uses our Multi-view LidarNet deep neural network (DRIVE Labs episode 18)
These perception results provide accurate top-down 
view information about the position, orientation,  
and size of all dynamic objects on the scene,
while a map, denoted by gray lines,
provides information about the positions of fixed objects and landmarks,
such as lane markings, traffic lights or traffic signs.
Using both the perception and map data as inputs,
we train PredictionNet to infer the future trajectories of dynamic objects.
At inference time, PredictionNet does not require lidar data
can use camera or radar perception data as input.
So on the left, we see vehicle detections from a four-camera surround perception setup
On the right, we see PredictionNet using these results to infer the future trajectories of these vehicles in top-down view.
The dotted white lines represent the trajectory predictions,
while the colorized clouds represent the computed uncertainties for these predictions
so warmer colors correspond to time points that are closer to the present moment,
while cooler colors denote time points further into the future.
In this clip, we see PredictionNet results in an intersection handling scenario.
We see that the DNN is able to predict which cars will proceed straight through the intersection
versus which cars will turn, and which cars will remain stationary or parked.
Here we see PredictionNet reason about an upcoming lane change.
The multi-modal prediction here indicates that, at this moment, the DNN is predicting that this vehicle could either stay in its lane
or lane change to the right, which then converges to a lane change prediction.
Here we see the DNN predict a lane change that happens under an overpass.
We notice that PredictionNet understands that the horizontal 
gray lines represent an overpass bridge on a map
rather than an intersection,
correctly predicts a left lane change from the right-most lane,
without allowing for the incorrect possibility of a turn onto the overpass.
In this clip, we see PredictionNet results for a highway merge scenario,
in which the DNN predicts that the merging car on the right
needs to slow down and wait for blocking traffic to clear before it can complete the merge.
This type of prediction is very useful for planning a smooth speed profile for the merging vehicle.
The perception inputs into this DNN can be based on 
any combination of camera, radar, and lidar data,
which means the DNN model can be leveraged in various sensor configurations.
Moreover, PredictionNet results can be used to train reinforcement learning-based planning and control policies for self-driving cars.
 
Title: Adobe Premiere Pro CC uses NVIDIA GPUs for Debayering and Smoother 4K Workflows
Publish_date: 2014-06-25
Length: 113
Views: 76075
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/Yp5u2Aj3v0w/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLDhvWDnMAzGLpPHfe_zXtFBgehVpw
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: Yp5u2Aj3v0w

--- Transcript ---

hi I'm Sean from Nvidia but now you've
probably heard about how Adobe Premiere
Pro leverages the power of CUDA and
NVIDIA GPUs to deliver real-time video
editing with high-definition content
with each update of Premiere Pro
Creative Cloud video editors effects
artist colorist and other creative
professionals are getting more
performance and value out of Adobe
steady stream of feature enhancements
using GPU acceleration now users can see
those same benefits and smoother more
cost-effective 4k workflows 4k is
happening now and growing fast but it's
challenging to work with all that data
and that can make the post-production
process difficult 4k is actually four
times larger than HD which can bond down
an editing workflow in our hurry that's
where the NVIDIA GPU comes to the rescue
in addition to the data size footage
from cameras like the read 4k cameras
captured in a raw format and needs to be
debarred before it can be viewed this D
bearing step is extremely complex if you
work with 4k raw footage you probably
know that it required a dedicated read
rocket card to handle this process while
it's great at what it does it's an
expensive piece of single-use hardware
that requires space in your computer
chassés but now this complicated D
bearing process can be handled using
your NVIDIA Quadro GPU in fact Premiere
users with a high-performance GPU like
the quadric a6000 in our workstation
indie bear and playback read RAW files
in real time now you can get the
performance of a dedicated read rocket
from the same NVIDIA GPU that currently
accelerates the mercury playback engine
in Premiere Pro this new functionality
is the result of collaboration between
the engineers at Nvidia Adobe and red to
dramatically improve your 4k workflow
with the new version of Adobe Premiere
Pro CC and an NVIDIA Quadro GPU you can
be sure you're getting the best
performance and best interactive video
workflow possible
Title: Ericsson's 5G Digital Twin Simulated in NVIDIA Omniverse
Publish_date: 2022-01-31
Length: 113
Views: 27588
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/yTbUSXJ8M-8/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: yTbUSXJ8M-8

--- Transcript ---

there are 15 million 5g microcells and
towers planned for global deployment in
the next five years
ericsson is using nvidia omniverse to
build digital twin environments to help
determine how to place and configure
each of their sites for the best
coverage and network performance
in omniverse ericsson builds city scale
models that are physically accurate down
to the materials of the buildings
vegetation and foliage then wireless
network components are added including
the precise location height and antenna
pattern of each transmitter
ericsson built a custom omniverse
extension enabling them to integrate
radio propagation data and leverage
omniverse's rtx accelerated ray tracing
to quickly visualize and calculate the
quality of the signal at every point in
the city
because omniverse materials are
physically accurate the intensity of
reflections are precisely determined
antenna beam forming and signal paths
can be accurately simulated and
visualized
in the simulations the lobes signify
transmitter antenna beam forming and the
straight lines or signal paths the
colors of the signal paths denote
strength in decibels and data throughput
from blue is the weakest to red as the
strongest
visualization is a critical capability
for ericsson with omniverse vr network
engineers can virtually explore any part
of the model teleporting to any location
anywhere in the world at one to one
scale as they tune the network for
optimal performance or identify path
disruptions they can literally see the
effects of their adjustments in real
time things which aren't visible in real
life
in omniverse ericsson can perform true
to reality remote simulation of entire
5g networks enabling them to design more
efficient and reliable networks conduct
remote field trials and speed up
deployments
Title: Research at NVIDIA: Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects
Publish_date: 2018-10-29
Length: 88
Views: 30120
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/yVGViBqWtBI/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: yVGViBqWtBI

--- Transcript ---

[Music]
to help robots work more effectively in
complex environments researchers from
Nvidia have developed a deep learning
system that allows a robot to recognize
and pick up household objects with ease
using a camera mounted on the robot the
AI continuously detects and estimates
the full pose of the object knowing the
position and orientation of objects in
the scene often referred to as six
degrees of freedom pose is critical as
it allows robots to manipulate objects
even if they are not in the same place
every time this research which builds on
previous work developed by Nvidia
researchers allows robots to precisely
infer the pose of objects around them
while trained only on synthetic data the
advantage synthetic data has over real
data is that it is now possible to
generate an almost unlimited amount of
labelled training data for deep neural
networks this is the first time that an
algorithm trained only on synthetic data
is able to beat state-of-the-art
networks trained on real images for
object pose estimation this breakthrough
is important because it allows robust
robotic manipulation in the real world
[Music]
Title: The Fast Track to AI-Powered Driving - Roborace | Season 1 Episode 5 | I AM AI Docuseries
Publish_date: 2018-03-12
Length: 355
Views: 15214
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/YVjliDkLSTs/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLCwbFgknUf9fviAHF1SiQjmAi1B4g
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: YVjliDkLSTs

--- Transcript ---

when I think of racing I think of human
precision mechanical excellence and the
thrill of going really fast but what if
the human is taken out of the equation
an AI becomes the driving force behind
the experience is technology alone
enough to feel the same passion for the
sport I'm Arjun dev and this is I am am
[Music]
so breast is the world's first
autonomous racing competition
fundamentally Rober AC is providing a
platform for development of AR drivers
so we do the hardware we do all of the
mechanical design we then work with
partners like in video on the vehicle
intelligence platform we surface data we
surface 8 into AI drivers and that's
what the team's computer is writing the
software that consumes the data see them
the first thing we have to achieve is
providing vehicles to the teams so we
have three of the development vehicles
and we have three of the Rover cars that
are running to validate that the AI
training is correct
Robo race created dev bot a race car
with a seat for a human to act as a
monitor once they feel confident in the
ai's abilities the human can come out
and the AI driver is set out on the
track as the sole control on top of the
vehicle we provide the self-driving
platform and lower level software so the
teams can not focus on low-level stuff
they can focus only on intelligent part
they can think how to process the data
in more efficient way and how to compete
with other players at the track so when
we put dev go out on the track it's got
a whole load of sentences that are
related to the powertrain and the
vehicle dynamics and all of that is
surfaced up into the AI driving layer
that a brain you become a scarecrow I
think that's probably the best reference
and in video is our brain that's inside
these cars that turn effectively a
chassis that can't move into something
that can actually automate itself that's
what's actually consuming all of the
data and then being able to use machine
learning to process the data and make
decisions
in order to make informed driving
decisions it's important to have a more
complete understanding of the world
around the car the robocar has a wealth
of sensors that uses to gather
information about its location and
environment to improve the data
collected by these sensors the racing
team can use a technique called sensor
fusion fusing the data from multiple
sensors helps you increase confidence in
the combined measurements lidar for
example doesn't work well in dense fog
while radar still does when both lidar
and radar sensors are able to detect an
object the multiple measurements will
more accurately predict the object's
distance in this way sensor fusion
provides more complete and accurate
information to influence the decisions
of the AI driver there's a long way to
go before AI can be a level of a human
driver a human driver even if they're
seventeen has a lot of background
knowledge about environments how to
navigate environments and what to expect
and they bring that with them if they've
been go-kart racing since they were four
years old they bring all of that with
them as well so you're talking about 15
years worth of experience is what you're
bringing to the track it sounds like one
of the most important uses of AI is
building those intuitive reflexes that
humans have but self-driving cars and
trucks are already making headway here
so how are these robotic race cars
different
the biggest challenge is the delays and
latencies because ants you go from the
speeds like 100 kph it is fine 200 kph
to 300 kph it is more or less fine but
as you go beyond 300 kph every
millisecond is really important so ants
we go to higher speeds we need to make
sure all our algorithms all all the
sensors can do the things super fast and
perform fast enough to make it possible
to drive at such speeds if they're
providing an identical platform to
compete on what's this competition
really about what will the teams add to
make their AI driver original and the
winner of the race the teams that enter
that right the driving software might
bias one data set compared to another so
what we focus on is situations and
creating situations that test perception
decision-making reasoning judgment to
the limit so you're looking at cognitive
power not mechanical power when you
watch the car perform and you see the
actions that it takes you read the
decisions that it's made you see
personality so how are the fans reacting
to this they're used to supporting the
person that's behind the wheel not the
technology
we completely split the public so you
have part of the public that want to see
a human involved they want to see humans
racing other parts of the public want to
see technology and they want to see
motorsport being used to develop
technology that will keep them safe on
the roads in the future and that's
really what Robo race is about is making
sure we can take technology from this
environment transfer it to the roads and
make the road safer part of that focus
is saying well what situations are you
training as if you're just racing on a
racetrack how's that relevant to me when
I'm on the freeway when I'm in the city
when I'm on an urban road or rural road
or mountain road well they're the
environments that we're going to be
racing it they're the situations that
we're going to be creating so that when
the public watch they see things that
are relevant to their real-life driving
experience and they know that that
software is capable of keeping them safe
in those conditions
Title: Do Your Life’s Work From Anywhere
Publish_date: 2021-02-03
Length: 107
Views: 12491
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/YzYXaeyRhjA/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: YzYXaeyRhjA

--- Transcript ---

[Music]
there's a new normal
shaped by the forces that continue to
change
everything
[Music]
but we're facing these challenges head
on
because there's still work to be done
we're connecting like never before
from
educators are finding new ways to teach
and students to learn
artists are breathing life into
fantastic
worlds to surprise and delight us all
engineers are training fleets of robots
but now in a virtual world
caregivers are watching over those in
need
to keep us healthy and safe what does
that mean
diabetic retinopathy can happen if your
blood sugar
stays too high over a long period of
time
and architects are designing new spaces
to bring us all together when we return
this is the new normal where everything
is different
yet anything is possible
and our life's work can't wait
you
Title: GTC 2019 Keynote with NVIDIA CEO Jensen Huang
Publish_date: 2019-03-19
Length: 9645
Views: 214958
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/Z2XlNfCtxwI/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: Z2XlNfCtxwI

--- Transcript ---

I am AI accelerating your discoveries to
solve the great challenges of our time
I am a visionary
bringing characters to life with more
natural movement generating brilliant
new worlds for them to explore and
inventing new ways to bring out the
creative genius in us all
I am a protector leading the way into
the most dangerous environments
and searching for signs of life
I am a guardian listening for the sounds
of destruction to save our forests
and using satellites to bring freedom to
those who are enslaved
I am a navigator finding safer paths for
cross-country deliveries
and taking a personal travel to new
heights
I am a scientist exploring oceans of
data to understand extreme weather
patterns and studying the building
blocks of life to save a community from
hunger I am a healer
giving hope to those who suffer from the
most challenging diseases
and tapping into the brain to rejuvenate
paralyzed limbs
I am even the composer of the music
you're hearing
I am AI brought to life by Nvidia deep
learning and brilliant minds everywhere
[Music]
ladies and gentlemen please welcome
nvidia founder and CEO jensen huang
[Music]
[Applause]
so this is what it's like to be Kobe
playing in the great stadium hey guys
thanks for coming out this way you know
we got so packed in the old fort the old
stadium that we have to get you guys out
here so I really appreciate you making
the trip I have so much to tell you guys
today so many fun things to tell you I'm
gonna get going right away the
accelerated computing approach that we
pioneered is really taken off if you
take a look at what we've achieved last
year the momentum is absolutely clear
50% more developers on NVIDIA GPUs 50%
more CUDA downloads than a year ago we
now have a hundred and forty
supercomputers powered by Nvidia a 50%
growth the number one fastest
supercomputer in the world the summit
the number one fastest supercomputer in
Europe the Swiss supercomputing Center
does paint and the number one fastest in
Japan 22 of the most energy-efficient
supercomputers are powered by Nvidia we
now have 600 applications that are
powered by CUDA and high-performance
computing 15 of the top 15 applications
most popularly used are now powered by
CUDA and this year we saw some really
important new applications cryo spark
the reconstruction of particles from
like from cryo-electron be microscopy
fun 3d a CFD simulation grow max
molecular dynamics microvolt evolution a
inverse convolution method to enhance
imagery para Brix genomics analysis and
Worf to earth worlds
most popular most frequently used
weather simulator the thing that's
really great about a software-defined
method and an accelerated computing
approach like ours is that we never give
up on the applications and together with
the developers we continue to advance it
using exactly the same GPU the same
exactly same infrastructure applications
continue to improve in performance if
you take a look at the most popular some
of the most popular applications from a
year ago this year over year we improved
the performance from 25 X to almost 40 X
we continue to enhance those
applications continue to refine them
continue to squeeze more performance out
of them so that you can continue to be
more and more productive or simulate
larger and larger simulations
accelerated computing is not just about
the chips the chips is really important
there's no question about that and we
certainly build the world's largest
chips but accelerate computing is a
collaboration it's a co design it's a
continuous optimization between the
architecture to chips the systems the
algorithms and the applications the
Nvidia stack looks basically like this
and today I'm going to refer back to
this repeatedly starting from now we're
going to take all of our libraries I'm
going to put them together into one body
of work one suite if you will one
umbrella name called CUDA X CUDA
acceleration libraries it's built on top
of the architecture that all of you know
very well and for many of you the reason
why you're here call CUDA CUDA runs on
top of all of our GPUs it runs on top of
our graphics GPU called RT X our deep
learning systems called dgx
our hyper scale systems called hgx and
even our autonomous machine systems a
little embedded systems call a GX
it's architectural II compatible with
all of them on top of it we build
domain-specific acceleration libraries
domain-specific acceleration libraries
one domain after another whether it's a
computer graphics or high performance
computing artificial intelligence
autonomous driving dr for drive is for
Isaac robotics CL for Clara healthcare
medical imaging and last one is what
metropolis metropolis for smart cities
all of these domain-specific
applications and acceleration libraries
are now contained as part of what we
call CUDA X and on top of it is NGC the
NVIDIA GPU cloud whenever it's possible
for us to containerize those libraries
we do so and we put it into the NVIDIA
GPU cloud you could download it into any
cloud any data center any computer that
has been certified and it will just run
the entire acceleration stack fully
integrated fully optimized and enhance
all the time now the reason that we do
this is for several several
characteristics that we really love
about accelerated computing and remember
it's not just about the chip it's about
the entire stack and so one way to think
about that is this the first thing you
want to do for accelerated computing is
you want to make it programmable I
created a whole bunch of acronyms here
just to simplify it for you and more
importantly that's right I made it
simple for you more importantly there's
going to be a pop quiz at the end okay
there's going to be a pop quiz at the
end it's programmable well the reason
for that is because most algorithms
today want to be software-defined you
want your computers to be
software-defined because so much
innovation is going into computer
algorithms today and we want to put the
architecture put the computer put the
capabilities of that machine in the
hands of the software developers making
a software-defined makes that computer
forever more powerful improving all the
time if it's software-defined of it's
programmable has great tools then the
time to solution time to finding that
algorithm will be shortened
and of course you will see growth in the
number of domains growth in domains if
it's programmable you could use it for
more and more things the number of
algorithms of course are diverse and
endless but the fundamental
computational methods tend to be very
similar and so by using a
software-defined acceleration approach
we could expand the number of domains
that the computers could be used for
acceleration gives you time to answers
time to answers or size of problems you
can increase the size of the problem
whether you want to take a very large
problem do it as fast as you can or you
have a certain amount of time you want
to increase the size of the problem you
can solve it also gives you the lowest
cost of infrastructure if you solve a
problem quickly if you have a computer
that can achieve the necessary solution
as quickly as possible of course you
could buy fewer computers you've heard
me say the more you buy the more you
save the reason for that is this doing
things efficiently and doing things
quickly it's the most cost effective way
of doing something the number of domains
it turns out a computer is only
successful if there's a whole ecosystem
around it and the number of partners
that you have that take it to the
marketplace is large because the world
they use as computers is huge almost
every field of science of an industry
every company the world uses computing
of some kind and we believe that in the
future they will all be high performance
computing customers they will rely on
high performance computing to achieve
their mission the cost of deployment is
lowest when it has multiple domains and
the reason is very very simple a
computer maker would be more
enthusiastic to take computers out to
the world
if you conserve finance and healthcare
and manufacturing and transportation and
retail and insurance the larger the
number of domains that a computer serves
lower the cost of deployment it is it is
the reason why today the x86 is the
world's most affordable and the most
popular computer because it has so much
use the space of applicability is
gigantic its domain of use is large
programmability gives us the largest
possible domain and also as a result
reduces the cause of infrastructure
again architecture architecture is a
funny word in our company architecture
means this an application that was
written yesterday has to run on a
computer tomorrow and a application that
we enhance today will run on computers
everywhere that has CUDA in it
architectural compatibility has such
enormous and powerful implications it is
the reason why architectures either
succeed or don't succeed
backwards-compatibility large install
base all makes it available to drive the
cost of the infrastructure down so the
architecture gives us continuous
improvement in performance it expands
the domains and drives the cost of
infrastructure down all of these things
create a positive feedback system and
that's what we're experiencing today the
reason why an architecture grows faster
and faster and adoption grows faster and
faster all the time it's because of
these characteristics while these
characteristics as it turns out is
incredibly hard to remember and so as in
every keynote I try to introduce you to
a new word
today's word is that's right
and I come up with it at the very
beginning of every keynote just in time
and so this is this is the word of the
day
Prada at the end of our keynote I will
ask you this word again Prada stands for
programmable acceleration of multiple
domains with one architecture does that
make sense
Prada can you remember this okay say it
out loud
one Prada okay thank you very much my
job is done
have a nice day okay product accelerated
computing the approach that we pioneered
a dozen years ago is really taken off
and it's taken off because we're seeing
that CPUs are not scaling as fast as I
used to and because the number of
workloads that are really really
important to the future of computing and
future of society has just gone berserk
the amount of computation that is
necessary for the future is incredible
and I want to talk about some of those
today but first my talk is in three
chapters three fun-filled chapters the
first chapter is computer graphics
computer graphics the driving force of
our company
it is the simulation of virtual reality
one of the most challenging computation
problems in all of the industry after 25
years of pursuing real-time virtual
reality we have not achieved it not even
close but we're getting closer and
closer every single day today I want to
show you something that's really great
this is by one of our partners and I'll
come back and I'll tell you about it in
just a second guys
invent yourself and then reinvent
yourself don't you swim in the same
Slough invent yourself and then reinvest
yourself
and stay out of the clutches of
mediocrity
invent yourself and then reinvent
yourself change your tone handshape so
often that they can never categorize you
reinvigorate yourself and accept what is
good only
terms that you have invented me
self-taught and reinvent your life
because you must
it is your life and its history and the
present you
okay was that real or virtual reality
let's take a look all right audience
participation okay left is real or right
Israel how many people say right this is
not real so how many people thinks this
is real it's got to be the rest come on
okay so everybody thinks this one's real
all right next one how many people
thinks this one's real getting a little
confused how many people thinks this
one's real okay so this one's real re
this is one of those rock-paper-scissors
things all right how many people thinks
this one's real okay well it turns out
they were on the same side this one's
not real this one is r-tx
that one is real is that amazing
by mimicking the physical properties of
light our TX has made it possible for us
to perform real-time ray tracing for the
very first time it turns out ray tracing
could be done in software it is true and
the reason for that is because the first
implementation of ray tracing was done
in software by one of our researchers a
long time ago and in fact most films are
software ray traced today they're run on
a bunch of CPUs they call them render
farms there's about a million and a half
of those render servers running around
the world making movies all around the
world and so it stands to reason that
ray tracing could be done in software
but as in everything else our goal is to
accelerate this particular domain of
application what we want to do is find a
way to accelerate enough of it as
quickly as we can so that it could be
done in real time chad is going to show
you a couple of these things
Chad yep what are you gonna show us here
so the one thing I'm going to show you
which is a little is the new thing is
what is not very common in that video
solves all those rendered image you
would see would have been done offline
over you know a few minutes a few hours
even days but that video you saw at the
beginning that we did with unity all the
rendered images were done in real time
for honor our TX hardware and so some of
those images were a lot of those scenes
were rendered about 30 fps and what
we're showing you right here now is not
a cut from the move from the commercial
we kind of put together it's actually
unity running real-time Wow
and so that's amazing yeah so this would
be really useful for like a designer a
professional designer who's like I want
to see what my materials look like
simulated it for real using ray tracing
so they could swap materials they can
play with the compartment open them up
see how the lighting works guys this is
all done in real time yeah we can even
go to the exterior you'll see some of
these shots what we're familiar again
you can change the materials make sure
you didn't create any distracted
reflections for other drivers or
anything like that and kind of play
around and we can turn some of the our
she effects on and off to show you that
what's really going on so ray-tracing
gives you like a real shadow and then we
turn it back on and there you go that's
beautiful
Chad thank you very much a quick thing
real real exciting about it is if we're
showing this in unity it looks awesome
it's actually gonna be available and an
experimental package April 4th so it's
coming soon for guys everyone here to
play with April 4th are you guys unity
great work thanks a lot
Danville the you're in the audience you
must be very proud of Chad Chad Dibley
all right we practically raised him okay
so this is the NVIDIA Turing r-tx
architecture it is the greatest leap in
computer graphics in 15 years since we
started the programmable shader
revolution if you take a look at the
imageries today it's really at the
limits of what you could do with
programmable shading and we need to kick
it up a notch take it to the next level
of realism what you saw was a twenty
atti the highest end of the Turing
architecture 18 billion transistors 32
trillion operations per second 16
billion of it 16 trillion of it
floating-point 16 trillion of it an
integer the reason for that is for the
first time Touring's architecture split
the integer and floating-point execution
so we can have we can do them
concurrently the reason for that is this
back in the good old days we would
largely use our GPUs for shading but in
the future if you want to use it for ray
tracing the programs are really
complicated and when you do probe when
you do a lot of complicated program
execution the integer or the address
calculation of the program becomes to
dominate and so by putting it making it
concurrent we could overlap the shading
operations as well as the ray tracing
operations with concurrent floating
float and integer operation we also have
the new technology called variable rate
shading depending on what part of the
screen it is maybe the amount of shading
doesn't have to be nearly as precise
maybe the texture is not as relative
the course maybe it's moving relatively
faster you wouldn't notice it anyways
using variable rate shading we could
reduce the amount of shading necessary
we have this new technology called mesh
shaders allows us to render really
really complicated scenes like like
rocks and geometry geometry of mountains
and an enormous enormous force of of
grass and rocks and just you know stuff
stuff in the world stuff in the world to
make it make it make it more interesting
render stuff it's called the incredibly
fast stuff rendering system otherwise
known as mesh shaders and it allows us
to do things like level of detail
adjustments and of course the big ticket
item is ray tracing our first GPU to be
able to ray tracing and intersection
testing in real time and then lastly one
of the most important features of this
GPU is a tensor core architecture it
allows us to do artificial intelligence
for generating images creating pixels
that otherwise would have had to been
rendered or fully calculated and so
notice the number of things that you can
now do with artificial intelligence with
super resolution if we could find a way
to create algorithms and neural network
models that can process it so fast we
could reduce the number of pixels we
calculate infer infer the rest of it and
as a result achieve very high
performance working on this part it's
called DL SS we're making progress all
the time I'm super excited about its
future there's no question in my mind
this is going to be a huge success we
just have to continuously innovate and
create these new models while creating
the GPU is the first step inventing the
technology is the first step but
ultimately what's going to enhance its
adoption is the entire ecosystem
supporting it and the ecosystem starts
with the most pervasive computing
platform for 3d in the world Microsoft
Windows and Microsoft Windows Direct X
with ray tracing called DX R is now
available but that's just the ape
gie that's the lower level API on top of
it are the game engines the two most
important game engines in the world
unreal and unity represent 90% of the
world's games you just saw just now
unity with DX r and RT X acceleration as
Chad said available april 4th for people
to download an experiment with we also
mentioned before that unreal is
developing their version of game engine
it's Unreal Engine 4 point 2 - which
will incorporate DX R and then lastly
Vulcan RT these represent basically the
entire vast majority of the entire
computer graphics industry and it is all
coming together for a future of ray
tracing then we have to create all the
products we now have Turing our TX
graphics cards from top to bottom from
what you saw earlier which is a 20 atti
we recently announced a touring GPU for
just over $200 $219 the lower end GPUs
have no RT X just the Turing shading
architecture and then for the higher end
GPUs starting with the 2060 whenever it
says RT X it has the ray tracing
acceleration hardware but the Turing
architecture now spans from $219 all the
way to the highest performance in the
world and this year we have more
notebooks than ever the gaming notebook
marketplace is growing super fast
it grew 50% just at in-video last year
year over year and we expect this year
to be pretty great as well 40 new
laptops based on the Turing architecture
has been announced let me show you a
couple more demos it is very clear it is
very clear that ray tracing is the next
step in real-time computer graphics
and the next generation of video games
has started this next game I want to
show you is from a developer called
Nexon and it's built on top of Unreal
Engine 4 point you too
[Music]
[Music]
[Music]
[Music]
[Music]
really really amazing I think it doesn't
matter how the technology works many of
you in the audience understand how the
technology works it's really a marvel
that we're seeing real time ray tracing
but what really matters in the final
analysis is that things just look so
much more beautiful and alive you know
everything is just alive somehow because
the reflections work properly and the
shadows work properly
it just looks much more realistic much
more alive and as a result developers
can tell much better stories it all
started about 20 years ago about 20
years ago for the very first time
Hardware 3d acceleration was possible
because of a game named quake use this
opengl for the very first time frankly
quake if it wasn't because of quake
Nvidia wouldn't be here today every
company needs a bit of a killer app and
in our case it was the video game
industry and the one that really kicked
it into high gear was quick and it was
so hard to render quake back then it was
just so hard to do it and then you need
an accelerated to do it and for the very
first time we were able to do quake in
real time now art our engineers wanted
to do something and make a contribution
technologically to the community that
has since worked on quake it turns out
quake has gone through several different
iterations and what we're going to show
you is quake 2 it's open sourced and and
this is going to carry on some of the
work that was done by Christoph
Christoph she'd one of the interns at
Nvidia working at Nvidia research and we
took it all the way and what we're going
to show you here is an original game an
original game now done with
state-of-the-art computer graphics
manual let's take it let's show it to
him all right so what you're looking at
over here is a resurrection of the
original single core single thread
running on the CPU quake 2 game engine
and this is one of the destination
match maps and at the time our CPUs are
so fast now compared to 20 years ago you
can just render it's off in software
using a CPU go ahead you know of course
at the time on a single CPU you just
really do not have compute power to the
global illumination and instead they
used light Maps so all the lighting is
baked and Static and this is what you
get now what we added on top is all TX
so that we can do this no no no no
ladies and gentlemen wait for it if this
is not the end this is the beginning
wait for it all right now go everything
is dynamic we can change time of day we
can move around all the reflections the
materials are physically based go ahead
keep going look we'll just keep going go
anyone yeah at this point I'm just
letting them run because it's just so
beautiful so Alex you over here is the
lead programmer has added a lot of
features so the first thing we did was
adding adding high dynamic range because
without physical units nothing works
obviously the direct lighting the
indirect lighting the reflections
refractions all based on ray tracing all
based on past racing once again the CPU
rasterizer over here with baked globe
illumination and all T X all right we're
gonna take you to one of the actual game
levels for game play all right so this
is before this is before this is after
as you can see we know how physically
based material we can get glass which
reflects everything around it we also
added little fun things like volumetric
lighting that you can see here with
beautiful light shafts and we have more
interesting materials coming up like
metal grades over here which starts
reflecting all the environment around
them no tricks this is actually real
we're not faking it
and here's not a little Kodak moment
there we go beautiful volumetric lining
very moody again before-and-after
ladies and gentlemen this is the beat
this is a definition of beauty for a
computer scientist and we have one more
little surprise for you something else
that we've added a last minute we
weren't sure if it was going to work
today so I'm going to let them show it
and of course this is not quick without
the BFG
okay thank you
Aleksey good job Manuel good job thank
you
this really incredible work and this is
this is a obviously this is a work of
love we're going to contribute this back
to open source and the engineers are
going to finish it off over the next
month and keep you know stay tuned and
we'll post it as soon as we can
but this this is this is this is
genuinely state-of-the-art computer
science and computer graphics coming
together and we're doing it in real time
and when when we put it out in open
source it's going to be it's going to be
a topic of great research and great
discussion to advance the future of
computer graphics so that's real-time
computer graphics offline computer
graphics offline computer graphics or
rendered using render farm computer
graphics so that we could generate
photorealistic images is used in every
single industry today whether it's an
architecture or product design there's a
whole community of 3d artists who use 3d
graphics and rendering software to
create amazing imagery in imageries this
is created by somebody it's a fan out
there who just loves to create art with
this medium arduously meticulously turns
their imagination into something that
comes alive and of course media and
entertainment using it for making movies
a modern movie a modern movie might have
something along the lines of 2500 3000
shots and each one of those shots are a
few seconds long just a few seconds long
and those few seconds would take a team
of tens and tens of artisan and
designers and it would take them
literally the time of the film making
production budget time to create those
few shots whole bunch of studios were
coming together and stitch all their
shots together and creates what what you
guys know what we all know as a
wonderful movie well this process is
incredibly arduous and we wanted to
bring accelerated computing to the
rendering process
the very first time this has been
largely the effort of CP is because the
algorithms are so complicated the data
size is so large it has taken us
literally 25 years to do this and so
finally this year we announced our TX
and we started working with all the all
of the leading film quality design tool
companies and firms to accelerate their
tools and rendering systems and I'm
happy to announce that as of now over 80
percent of the world's leading tools
makers and film studios have adopted our
TX and by the end of this year we should
have all of them in production we have
some of it in production now Arnold is
ready and we're going to show you some
examples of some of the work this is
from a studio call imagine engine and
this is work that they did for lost in
space I can't wait for the second
episode second second season let's take
a look at it
[Music]
image engine image image engine lost in
space and you could see the work is
arduous as meticulous it takes so many
people working together to create that
one shot it's layered by layered by
layered a meticulous process what we
ultimately see as the final film what
you saw there was really quite amazing
so in just a few seconds it represents a
shot our early adopters our early
developers have now benchmarked it for
us this is um take a look up here this
is the CPU with the dual Lake server ok
dual sky Lakes server 25 nodes
takes 38 hours to render a shot of a few
seconds a shot of course of a few
seconds is 30 frames per second and each
one of those frames are meticulously
rendered 25 nodes
takes 38 hours the power bill is about
$70,000 over the course of five years
and the total cost of that data center
or excuse me of those 25 nodes is about
$250,000 we've now done the same work
that one shot with one node of what we
called an RTX server with for RTX aight
thousands it took six hours instead of
38 hours the cost of the server is
$30,000
not $250,000 and the amazing thing is
this in the power bill that you would
save you could buy a server just with
the power bill that you save I used to
say the more you buy the more you save I
think I was wrong
our team servers are free
[Applause]
and you get another $30,000 to spare at
the end of five years so it's completely
free okay and so I said earlier that
doing work efficiently is the most cost
effective way of doing anything and of
course energy efficiency and getting
things done quicker is really a great
savings if you take a look at a major
film and it costs something like 350
million dollars 300 million dollars to
to produce that film and the vast
majority of it is post-production which
is otherwise known as rendering and it
might take something along the lines of
a year and a half a year to a year and a
half if you could even save one month on
what is otherwise a one year long
project the amount of money you could
possibly save is in the millions and so
this is one of the reasons why this
industry is such in a hurry to find ways
to accelerate the rendering process and
to accelerate the production process
we're working closely with Pixar and
they've they've been sewing then so
excited about the technology I can't
wait to see the first major motion movie
made by Pixar rendered completely on
r-tx the process of creating these
movies are hard it starts with modeling
and of course you got concept you got
modeling you've got texture and got
rigging which is basically putting the
bones into the characters and once you
put the bones into the characters that
character essentially becomes a robot
you can animate that that character you
could deform it you could you could you
could change the shape of the skin you
of course do forward kinematics you
could do inverse kinematics and so as a
result you can animate these characters
you have to light it and then of course
you have to render it make it look
totally perfect and then once you create
the character you have to composite it
with a whole bunch of other characters
and the scene and all the environments
and all the special effects all the
special effects are done in physics
simulation it is so so complicated and I
just told you that a shot may be
assigned to you know a few shots may be
assigned to a studio a few shots can be
assigned to another studio as a result
multiple
in multiple sites are all working on a
movie at the same time there are over
200 animation studios in the world today
because of the nature of how we enjoy
entertainment because so many cultures
are now starting to adopt this form of
this media in this form of entertainment
and because there's just so many
different ways to enjoy content today
the number of studios have gone up
significantly and the amount of pressure
that is put on the studios is increasing
as well they would like to drive their
cost down they would like to increase
their productivity but one of the
greatest things is that we want to do is
find a way to work together if you take
a look at this this map this just
represents a few of them there's over
200 of them and there are sites all over
the world and yet some of the sites are
working on modeling maybe another site
is working on the simulations maybe
another one's working on animation
they're all using different tools some
of it is internal some of this
third-party but it's got to come
together in some kind of a holistic way
could you imagine a world where there
was no way to share documents because
the content is too big because the
workflows are too complicated and yet
each document requires hundreds and
hundreds of human hours to do well we
have four Word documents we have Google
Docs well for 3d content creation we
have nothing we have nothing and so we
wanted to create a tool that made it
possible for Studios around the world to
collaborate from for the workflow the
designers of it worked a single workflow
which has all these different tools and
all these different steps to be able to
collaborate we've been working on this
for some time and I want to show you
this great new technology from our
company it's called the omniverse the
omniverse basically connects up all of
the designers and Studios it works with
every tool it communicates we
communicate with all the tools through
their plugins using USD
and using MDL and we energy we only
exchange all the things that are dynamic
and we put it into this world this
portal that everybody can see from and
so as a result irrespective of what tool
or what part of the workflow you're
working on you will see one version of
the final content that you're creating
in its highest possible fidelity you
could be on the laptop maybe your
computer doesn't have ray tracing maybe
you're using a particular tool it's
renderers not quite up up to the point
of being able to do ray tracing but all
of your working together in one workflow
and everything looks beautiful everybody
has one common understanding of what
everybody is working on at that moment
in time it's an open collaboration tool
it works with all of the major 3d tool
out in the world we're getting great
support people are so excited about this
let me now show it to you ladies and
gentlemen the Nvidia omniverse now we're
going to demonstrate this very quickly
this is this is a this is our team back
there and let's pretend for a second
that they're all working in a different
place it doesn't matter they're here
today but it doesn't matter that they're
in a different place one one you know
that the substance painter could be done
in Montreal the Unreal Engine could be
you know could be done here in
California and the Autodesk Maya could
be done somewhere in England and all
these tools all these studios are global
as I mentioned and they're working on
different parts of the design now notice
in the case of Maya we're doing the
design in the case of unreal we're doing
we're composing the environment in the
case of substance we're painting that
airplane and these three designers have
don't see what the other ones are doing
they can't see what the other ones are
doing until omniverse these three tools
are from three different developers and
three different third-party vendors but
yet we're gonna have them see each other
on the same omniverse or
old okay okay so what you're looking at
over there is omniverse now check it out
while one engineer while one designer is
changing the geometry while another one
is changing the scene while another one
is painting it all shows up in one world
can you guys see that it's all showing
up in one world the geometry is changing
we could change the paint we could
create a different environment
this is omniverse those are all the
independent designers working separately
this is rendered with ray tracing it
doesn't matter what they have and as a
result we have this beautiful world now
omniverse could run on your local
workstation if it has an RT X in it it
could also run in a local workstation
and you could share with other people
you could stream it to other people in
your workgroup you could also put it in
the data center or you could put it in
the in a cloud and we can render all of
that and stream it to you it doesn't
matter how you would like to enjoy it
omniverse is one shared world what do
you guys think
okay so working on this this is thank
you guys let's come back to the slides
the studios we've talked to are so blown
away by this this is something they've
been looking for a whole long time and
finally we have essentially the Google
Docs for 3d design except it also works
across all the heterogeneous tools and
all the data formats are different but
when they communicate with us and
connect into the omniverse this is what
you get to see where we have early
access planning and come to this website
come to the nvidia website for
developers and it's under nvidia
omniverse and let us know if you would
like to have early access to that
rendering is data center graphics
omniverse ultimately will be data center
graphics another data center graphics is
GeForce now we've been working on
GeForce now for a few years in fact
we've been working on it for about six
years and the reason we working we've
been working on it is because it turns
out that the vast majority the gamers in
the world don't have access to our
powerful graphics card we have some 200
million GeForce gamers in the world and
they're growing but yet there's another
billing in PC gamers that don't have the
necessary computer or don't have a
geforce card to be able to play the
games at the level that they should and
so we decided several years ago that we
would create a cloud gaming system not a
service not a store it's not the Netflix
of gaming it's an open platform so that
all the developers get to keep all their
economics all it is is a PC in the cloud
think of it as a GeForce PC in the cloud
so one of our GeForce PC can do 4 games
this can do it's an open platform it
should be able to run everything if it
runs on a GeForce PC on your desk on
your laptop it should be able to run it
in the cloud it is virtualized so that
we could share it across a whole hold
whole data center and to be as efficient
as possible but yet the entire stack has
to be exactly the same as the stack that
we have on PCs because want the software
to just run no porting no onboarding it
just works and it would be streamed to
PCs the other billion pcs that we can't
reach and that's what GeForce now is
about there are five hundred plus games
in there today
basically the games you love and enjoy
on PC if it's on Steam or the epic store
or you play you know all these different
game stores if you've bought it if you
bought the game and you get on top of
GeForce now you should be able to stream
it to a Mac a Chromebook a low-end PC a
laptop a leftover PC from years ago
you'll be able to stream basically very
high quality games today GeForce now has
over 500 games we have 15 data centers
about 300,000 users players are using it
all the time now we've been in beta for
some time and there's a million gamers
on the waiting list a million gamers on
the waiting list and we're trying not to
tell too many people about it because
the waiting list is actually fairly
large and so we're sitting here trying
to improve the quality of our service
improve its reliability it's a very
complicated problem we want to make sure
that latency is as short as possible
never any hitches the sound quality is
excellent the interactivity with the
mouse is great and what we've discovered
is this you could build the world's
largest data center won't make any
difference
you've got to build a whole bunch of
data centers all over the world and it
turns out that the number of gamers
around the world is quite large and
they're spread out all over the world
and so what we need to do is we need to
come up with a system that allows us to
go well beyond these 515 data centers to
support these 300,000 gamers but to find
a way to reach all of the edges of the
world through partnerships and so we
created this idea called the GeForce now
Alliance the GeForce now
basically is us creating a server
architecture developing all the
necessary software and partnering with
telcos around the world who can't wait
to put the service on their 5g networks
the 5g networks are there Wi-Fi networks
or their broadband networks they can't
wait to put additional services on top
of it and so these alliances are all
over the world they could be in
countries all over the place today we're
announcing two big ones we're announcing
today that the first two alliance
partners for GeForce Alliance is
Softbank in Japan and LG U+ in Korea two
of the major gaming markets I want to
thank you for your support
our strategy is to put these alliances
set them up in countries all over the
world and they would have data centers
multiple data centers with GeForce now
servers and we will host a service and
maintain the service for everybody and
the first step therefore is to create
the server we call it the RT X server
does RT X server is super dense it's
super dense 40 GPUs and 8u 40 of our
state-of-the-art GPUs and eight you you
can virtualize it and as a result of
virtualizing it you can share it with
3300 20 different concurrent users it's
optimized end to end so you could use it
for the entire cloud gaming GeForce now
service you could use it for rendering
you could use it for omniverse but that
alone is not enough and turns out the
reason for that is because storage and
networking is a really complicated
problem once you get the data on the
server we can stream it and we can
compute and we understand that problem
super well but it turns out the entire
infrastructure with hundreds of
thousands of gamers all playing at the
same time essentially running the
supercomputer in real time the
interconnects all become bottlenecks
everything becomes an issue and so we
decided to create a pod we create a pod
that can scale up to 32 of these r-tx
servers and within 10 racks we can
support 12 hundred and 80 GPUs it's all
connected with high-speed Mellanox
InfiniBand and we can therefore support
10,000 concurrent gamers at the same
time and this is what it looks like this
is one pod this is one pod and our
alliance partners would install these
pods and basically within a week they
should be up and running and the service
is ready to go and whether it's all the
different countries in Southeast Asia or
Eastern Europe where a big part of the
market market for gamers are emerging we
could set these things up and work with
the telcos as an alliance partner and
bring GeForce now service to all of them
as quickly as possible
these servers can now support three
basic use cases the first chapter ends
here and it's about the fact that
graphics is both number one going to go
to the next level it's going to also go
into the data center and data center of
graphics requires a brand new
architecture in this architecture
already supports three use cases and we
have others to come that I'll share with
you in the future our engineers have
been working on something really cool
and so let me play it for you now ladies
and gentlemen everything you see is in
real-time
[Music]
[Music]
[Music]
[Applause]
[Music]
[Music]
[Music]
[Music]
[Applause]
[Music]
what do you guys think
let's another round of applause for the
engineers and a creative artists Nvidia
very few technology companies get to sit
at the intersection of art and science
and it's such a great pleasure to be
here and to be able to focus on doing
work like this and to bring joy to so
many people it's such a creative process
and it just blows my mind every single
year when they keep raising it raising
it to another level Nvidia is the ILM of
computer graphics real-time computer
graphics and you can really see it here
chapter 2 chapter 2 is about AI and HPC
as I mentioned today we're going to talk
in three chapters the first chapter is
computer graphics now we're going to
switch gears and talk about AI and HPC
if you look at AI and HPC we've been
talking about deep learning for some
time and deep learning is a part is a
new algorithm new type of algorithm and
great breakthrough as part of machine
learning which is used which is used for
natural language understanding and
computer vision which largely built up
AI now underneath AI there's a whole
bunch of other stuff like robotics and
such but in the areas that we've been
talking about this is a taxonomy if you
will data science is the fastest growing
field of computer science today it is
the most sought-after
job it is the most oversubscribed course
in leading universities where there's
Berkeley or Stanford or C CMU or NYU or
it is so oversubscribed and the reason
for that is this it has become because
of several different factors and has
become the fourth pillar of the
scientific method theoretical of course
the eine to einstein method the thought
experiment the experimental method
Newtonian physics through
experimentation simulation methods
today's scientists using it for
molecular dynamics and such
and now we have a data-driven data
science method and it's made possible
because of three factors an enormous
amount of data that we can now collect
sensors everywhere
digital everything phones everywhere
cameras everywhere we can now have great
sensors collect a great amount of
information customers clicking on your
websites customers clicking on your apps
all of that creates data
the second is breakthroughs and machine
learning algorithms whether it's deep
learning or machine learning and the
approaches that has emerged recently and
then lastly computation and we've been
we've been fortunate I've been a big
part of advancing and accelerating the
computation that makes all of this
possible these three factors
continuously feed on each other and then
now data science is a pillar of the
scientific methods that allows us to
solve problems that were previously
impossible we're solving problems that
were just previously impossible now when
we talk about data science when we talk
about deep learning we talk about
machine learning in AI I wanted today to
frame it for you the pipeline to
workflow starts with data and there's a
stage of the first ingest ingesting of
data to bringing up data called data
analytics then it goes and from data
analytics your goal is your goal is to
take data and join it from a whole bunch
of data lakes create what is called a
data frame from that data frame tabular
data the rows the rows or all the all of
the instances if you will and all of the
columns are the features all the columns
are its features the features could be
where you live what your preferences are
for movies the cont connects with all
kinds of things okay and so all of the
features that you might want to learn
from that's called feature engineering
you take all of this data in you do all
of this munching and wrangling and what
comes out of it this gigantic table and
that gigantic table could be anywhere
from gigabytes large two terabytes large
could you imagine a spreadsheet that's
terabytes large first of all it wouldn't
loan on a normal computer
not to mention what would you do with it
at all and so you do that data wrangling
you do all of that processing and
analytics what comes out of it is
engineered features with a large amount
of data that is now in a data set
that you can now use for predictive
analytics to learn from that data set
and that is what we call AI some people
call it AI some people call the machine
learning some people call it deep
learning depending on the field of
science that you're in if you're an
image recognition of course you would
just use camera inputs if you're trying
to understand something with related to
disease the area of the data source
could come from a whole lot of places
including genomics could be medical
imaging it could even your family
history from that you run it through
frameworks and those frameworks
ultimately gives you a model that
predictive model that predictive model
once verified allows you to now predict
the next outcome from other input so the
new input comes you can now predict the
future we call that prediction process
inference so data analytics machine
learning or AI or deep learning and then
inference okay there's some words that
we use and for some of the people in the
audience who are kind of new to data
science this is such an important field
I think it's worthwhile for everybody to
understand it and the words that you
hear the words that you hear are things
like this CSV comma separated values
parque Park is a file format that came
out of Parque as a column or database
notice the features that are in columns
is a much faster way of reading it
Hadoop distributed file system it is the
beginning of Big Data the ability to
distribute to use a large data center
distributed files all over it and use it
for large compute engine ETL extract
transform and load basically the data
analytics process pandas a data
analytics program set of libraries
basically the spreadsheet of big data or
the spreadsheet of data analytics and
machine learning SPARC for a large data
center graph
analytics you hear tensorflow Pytor
Chamonix and makes that psychic learn
actually boost these are all frameworks
for machine learning or deep learning
and then on the inference side you have
tensorflow serving you have onyx and you
have sage maker neo these are some of
the most important the most popular
large scale out inference service and
and backends and so these are the words
that are going to come up over and over
again and I just wanted to place it on a
graph for you so you have a feeling for
how all the stuff works
now our words the words that come that I
use that talks about these areas who I
owe it is about vectorizing and
massively paralyzing the input the
loading of the i/o really complicated
problem and you take it from desk you
put it into system memory and it is now
in memory in the right formats and the
format is in the format of Apache arrow
ku DF is a data frame it is largely
compatible with pandas except it's
gpu-accelerated ku graph graph analytics
ku DNN basically invidious deep learning
API that got this whole revolution going
Kuh ml which is our family of machine
learning algorithms from random forests
and decision trees and you know
k-nearest and k-means and regression
algorithms tensor RT our optimizing
compiler that takes the output of this
which is a model and compile it down to
a target where there's a little tiny
computer or a giant supercomputer or a
cloud computer and our inference server
that runs on top of kubernetes called
tensor RT inference server allows us to
now run it in a hyper scale data center
and make it possible for millions of
people to be doing inferencing at the
same time that's exhausting it was
exhausting to say it must be exhausting
to listen to all right if that wasn't
hard enough as it is this is what it
looks like when you look at it as an
ecosystem it turns out building a chip
building a great chip is a nice
beginning but it turns out it's useless
until the world's developers and users
could take it
and that's why it's so important for us
to work with the ecosystem NVIDIA is an
open platform company we create all
these libraries in a way so that it's
software-defined and integratable into
modern systems
it runs on three computing platforms
personal workstations servers and cloud
this is the way computing is consumed
today if you're not in all of that
you're know where the next is the system
software stack on top CUDA and this
subset of CUDA today and for this
chapter
ai and HPC we call CUDA X AI it has data
analytics has graph analytics machine
learning deep learning for training and
deep learning for inference and it has
to go out into services to the world to
the developers in frameworks in cloud
services and then to be deployed in
cloud platforms and this is the
ecosystem at large
today it turned out we have a ton of
announcements and I just wanted to put
all of the announcements into one slide
so the first announcement is that we now
automatically automatically do the mixed
precision necessary for our tensor core
architecture which is invidious new AI
architecture for GPUs our tensor court
it's now optimized automatically for
support and tensor flow in pi torch and
MX net it turns out that precision is a
very complicated problem
their range issues their precision
issues sometimes you could do it in FP
16 sometimes you can't
sometimes you could do it an innate
sometimes you can't you've just got to
find out when you can and when you can't
well it turns out that compiling problem
is a very complicated problem especially
since you have to wait a week and the
answer comes back and off by a little
tiny bit these convergence problems are
just historically challenging for
high-performance computing
well tensor quote tensor tensor core is
now optimized automatically for these
frameworks and you get the x factor
speed ups
automatically you do nothing number one
announcement number two announcement is
data bricks started by Matteo Hari
Marte I love you all right so Matt a who
was the founder a spark when he was back
at Berkeley started a company called
data bricks and they they have a machine
learning and AI platform that is up in
the cloud is really popular super
successful company they've have now
incorporated and accelerated Rapids
which is the data analytics and the
machine learning part of our framework
of our API into their platform Google
Cloud now has rapid acceleration
Microsoft today announced that they have
rapid acceleration for Azure machine
learning and the next ceinture announced
that we're partnering together they're
one of the world's leading professional
services company and Mike Sutcliffe was
so visionary to lean into data analytics
and AI years ago they've they're just
doing fantastically and it's great to
partner with them
because we now have Rapids and our CUDA
X API and platform integrated into their
services and they're seeing great
results from that they serve three
quarters of the world's top 500 fortune
companies top 500 companies and so it's
really great to partner with Accenture
also announcing today that onyx runtime
is integrated natively with NVIDIA
tensor RT so all of that today also that
you can accelerate all of these
frameworks and all these AI production
systems and development systems so that
your productivity could be as great as
possible so I want to thank all of the
engineers and all the partners for
making this possible thank you
the question is what happens to all this
stuff you know we look at we look at
these things and and they're cnn's and
the industry is benchmarking our and 50
and it's a little bit like benchmarking
quake and it's it's it's now at 974
frames per second and we're benchmarking
our and 50 and we're we're focused on
cnn's
and and yet yet yet all of this all of
this is about ultimately creating
services that are incredible and do
incredible things one of the areas where
in the end we're trying to lean we're
trying to lead to its this idea of a AI
conversational agent that when you pick
up your phone or you're talking to a
speaker a microphone somehow that AI
service is super smart it's super smart
it somehow figures out how to you when
you're talking to it it's got to do the
necessary noise processing speech
recognition it has to understand your
language and convert it to the language
it wants to do processing in it has to
make a recommendation maybe it has to do
a search maybe it has to answer a
question and that question could come
back with another question that answer
could come back with another question
which is you could maybe upload an image
and now it has to upload an image it has
to recognize the image it has to come
back maybe with an answer maybe with
another question in this conversational
back and forth with an AI agent has to
be fast and it has to be really natural
if you take a look at the number of
neural networks that's involved in the
future it's actually quite large people
think there's one AI somehow
running in the cloud is just not true
it's like any other software program
these are all models that are sitting in
containers and they're running all over
the data center and at the end of each
container you're sending an output
through a REST API to another container
running a kubernetes or something like
that on another server and these
containers services are sending inputs
and outputs to each other and it's just
one program one service and it connects
all of these different models it is the
reason why people say that
East and West data traffic in a data
center is exploding and the reason for
that is because machines are talking to
machines a lot more than we're talking
to the machines all of these neural
networks are talking to each other
across containers across servers does
that make sense this is the future and
these models are going to get chained
together and for conversational type
models it is very likely that latency
will ultimately define the quality of
service and therefore we have to process
it incredibly fast here's the challenge
it's not one network there are species
of networks there are different network
types and they're different species of
them and every data centers got a
different one and the reason for that is
because they've architected all
differently and some of them it eight
works some of them in eight work
sometimes an F P 16s got to work the
rest of the time F P 16 always works for
images and sometimes you have to punt
all the way back to F P 32 sometimes the
whole networking runs on F P 32 because
that's all the time you've got to
optimize it sometimes after optimization
there's only so much precision you're
afford to lose and so you've got a mixed
precision and sometimes it's good enough
you might even be able to use four bit
integer sometimes so this data center is
heterogeneous in the number of the types
of models it runs and it has to be
heterogeneous in the type of precision
that it runs and it runs across a whole
bunch of servers that are connected
together this is the future the near
future the near future of AI in the
cloud there's somebody that I've invited
who's going to show you something that's
really really really cool Gohan he's a
good friend thank you very much for
coming here
show us something amazing cool so what
I'd like to show you today
is the Bing app it's running on my phone
right here and for this demo I'm just
gonna be a regular Bing user I'm just
gonna use this app like a regular user
would and I've been thinking about
redecorating a couple of rooms in my
house so I want to kind of change the
look and feel a little bit I want to
kind of make them in freshen them up a
little bit and I've learned that one way
in which I can do this is by changing
the lighting in the room but as a user I
don't know much about lighting so I'm
gonna use the app to help me out here so
the app has three icons three buttons in
the middle there's a camera button on
the left
search by image there's a text button in
the middle and I'm gonna use the
microphone button here on the right and
I'm just gonna ask it about lighting in
different rooms in the house so the
first query goes like this that's right
what are the different types of lighting
for the kitchen according to steal and
home morgue as far as general lighting
in a kitchen there are three main types
surface recessed and pendent fixtures no
color three things that's amazing what's
what just happens so of course of
Corrigan you're gonna explain it more
deeply but for the audience very quickly
what just happened is this your service
recognized your speech it understood
figured out what you were trying to say
it went to find the information it then
presented back to you in two ways one
way is to say it back to you so it has
to do speech synthesis and then the
other way it had to compose this page
that's right to present the information
to you and so look what just happened
okay
speech n what came out was visual and
verbal all right several different
models all happen not to mention the
search algorithm behind it all right go
that's exactly right and so I just
wanted to highlight exactly the
components you said but this card up
here up top we call it the intelligent
answers and that goes through the table
documents that came back for that search
query reasons over them real time in
response to the query and it finds the
best passage from all those documents
all those sentences that answers the
question that was asked and within that
passage it highlights in bold the
specific phrase that we think is the
answer right so that worked for the
kitchen
as a user now I want to learn about
other rooms maybe I'm looking at a
living room so let me try asking about a
living room instead what are the
different types of lighting for the
living room according to this Bruce calm
living rooms require three types of
lighting ambient tasks and accent and
COO hon I gotta tell you I just realized
something as you asked I'm missing one
of those three Lighting's in my house
that's why my eyes going bad honey our
task lighting is in trouble
yep ambient light and the accent lights
working great well I actually have the
opposite problem so I'll find out more
about accent lights in just a second but
the thing I wanted to highlight here is
it's a very consistent experience again
for this query that was speech in the
intelligent answers card gives me the
answer and the speech readout the
text-to-speech service is reading out
the speech but the other thing that I
wanted to call out is there's a little
embedded image in the answer and as a
user I'm visually drawn to that I want
to learn more about images and I
actually don't have accent lights in
this room so I'm gonna learn more about
accent lights show me images of accent
lighting
for the living room images for you so
here you see it has a whole bunch of
images of living rooms with accent
lights but I like one of these I'm gonna
pick the first one and I can actually
look at it in more detail and up top
there there's a button that we call the
visual search button and when I click on
it not only can I see the whole image
but I can actually select different
parts of the image and find some
portions of the image that maybe are
more interesting so in this case well
that's cool I'm just gonna Center in on
maybe the lamp that I'm looking at and
not only as I do that does it find
images that are similar to the lamp I've
selected but now there's this shopping
tab and as a user I can click on it and
I can buy lamps that happen to look
similar to the area that I selected in
this random picture from the Internet
that's pretty awesome okay and and that
is object detection
that's also happening in real time as I
change the selection this is running in
real time and to Jensen's earlier point
the latency is where these things are
super critical if this takes 3 seconds
to refresh no users gonna use it right
so our latency budgets for these models
are super tight and that is kind of a
very key aspect of the experience so
Jensen what I hope I showed you is an
experience here that sort of brings
together four very different types of
models so there was speech recognition
there was the intelligent answers there
was the text to speech read out and
there's object detection right but to me
as the user it was just one immersive
fluid experience as a user I didn't have
to worry about all the AI that happened
in the back end and that's to me the
best kind of AI it just works but of
course in this room we're all
technologists we know how hard it is to
get these things to work seamlessly and
for the big app which by the way you can
try it's available on iOS and Android
devices today for the wing gap we relied
and we use the azure n series Williams
which lie on NVIDIA GPUs that's awesome
thank you ladies and gentlemen
Gohan Seiran argh Jana I like saying
your name alright that is that is an
example of extreme inference extreme
inferencing that pipeline is complicated
it went through a lot of different
models and imagine hundreds of millions
of people doing it this way well we're
working with the industry at large to
accelerate inference and to be able to
deploy services tensor RT has been
downloaded so many times now 300,000
times six times growth in just one year
and some of your some of the best
services some of the most popular
services that you guys know of and a
whole bunch of them that I didn't list
here are now using and being accelerated
by video GPUs as a result of tensor RT
voice search and image search and
recommendation systems and assistance
and news feeds and translation and
e-commerce all of these things have
modules
of AI models in it and their accelerated
by nvidia gpus one of my favorite uses
of inference is medical imaging there
are a hundred thousand radiologists in
here just the United States alone we are
also we also have the best trained
radiologists in the world they used to
be able to study us they used to be able
to look at a study and study it for
twenty minutes now they barely have four
minutes to study the same thing the
pressure on them is incredible it is the
largest operation in the hospital and
yet early detection is the best way the
best way to stop something before it
gets worse
and so the question is how do we apply
deep learning to enable all of these
radiologists and augment them so that
when they're doing their work there's an
assistant sitting next to them helping
them along maybe at the end of the day
they're doing QA and meanwhile meanwhile
recognizing that there is no way that
one institution one group can possibly
train all of the neural networks for all
of these rare diseases many of them
happen very infrequently and so many of
them have experts and specialists in the
field well so what we decided to do is
this we decided instead of being the one
company to solve it all we would help
them create tools and put it in the
hands of radiologists all over the world
give them a AI tools and then give them
an AI infrastructure make it super easy
for them to share work among themselves
but we've got to give them a starting
point so we started with pre trained
models we have 13 pre train models up in
NGC they're incredibly well done we
worked with radiologists in research
hospitals and we dedicated ourselves
make amazing pre train models the second
thing is we have a tool that basically
is an AI that's going to create a eyes
and it helps you with the assistance of
essentially annotating doing the
laborious work of annotation and then we
can do transfer learning we fine-tune
these models with the data that you
provide so you download a pre trained
model you annotate your data you then
fine-tune that
and then we compile that into a model
that you can use and then we make it
possible for you to deploy it easily the
entire framework I described earlier
this entire process end-to-end is
easy-peasy and it's being used all over
easy-peasy that's very technical it's
being used all over the world now we're
so happy that MGH NIH OSU some of the
leading D kfz some of the world's
leading research hospitals are now using
it for assistance of annotation or the
deployment of the model or integrating
it into their own tools all of this is
open sourced and we'll make it available
for researchers all over the world so we
call this the clara AI toolkit and you
can come and download it alright Dave
thank you
data science is the new HPC Challenge
HPC using computers very large computers
to solve very difficult problems because
the data is so large because the
computation necessary is so great this
is the new HPC challenge and just to
show you one of the examples and there's
so many examples during the talks at GTC
and then Walmart's gonna give a great
talk I can't imagine the world's largest
company surely has the world's largest
data and they are doing they have
millions of SKUs millions of SKUs
simultaneously that they're managing and
forecasting and they have stores all
over the place and they have to find the
exact right level of inventory for every
single SKU to arrive at exactly the
right store for exactly the right time
because during every seasons a little
bit differently maybe there's a run on
something special like bananas you know
different types of products have
different different seasons and demands
and they have to monitor and they have
to ingest all of that data and use it to
train a machine learning model so that
they can protect the inventory faster
well because of our platform they could
do it now practically in real time you
should be able to you should go listen
to their talk there's so many other
talks here related to using invidious
machine learning platform to accelerate
data analytics and data science well we
have we have a we have somebody here
Aaron Williams Aaron are you here I
can't see it right here Jeff all right
Aaron
welcome to GTC Thank You Aaron has been
working with charter one of the world's
largest cable operators to apply data
science to solve their problem and so
why don't why don't we change to let's
go to your demo let's take a look at it
yeah sure this is a really exciting
real-world example of putting AI to work
for data scientists as Jensen was saying
we're working with the network operator
that has 25 million subscribers
connecting to half a 500 million sorry
five hundred thousand LTE towers and
Wi-Fi networks those connections are the
the lifeblood of this business and that
data is super important so what we're
gonna show is how we can put AI to work
to help the network operator make
smarter decisions using these predictive
algorithms now the decision just to go
to the end and work backwards charm the
decision they're trying to do is they're
trying to figure out where to place
Wi-Fi access points to offload from the
cellular traffic so that they could
provide the best possible quality of
service and then of course and to
increase the capacity for their company
but you don't want to place it randomly
everywhere
they're adding ten thousand of these
Wi-Fi access points per month that is a
big decision they got to decide where to
put them and they can save a lot of time
and money if they put them in the right
place and so what we're gonna do is help
them predict where to put those Wi-Fi
access points okay that's awesome and so
the first step of course is we have to
take the data in we have to do basically
ETL extract transform and load we have
to do basically data analytics and this
is a lot of data Janson it's a terabyte
of data per day that they've got coming
in now when you're talking about data
scales that big well usually you know
what people do when they have a terabyte
of data that comes in every single day
ignore it that's right that's right yeah
yeah that's right because tomorrow it's
gonna come back again that's right and
you ignore that one too because they'll
come back next week that's right that's
the beautiful thing about recurring data
all right but obviously obviously they
would like to use this data yeah this
care might have a their business yeah
this terabyte of data is not also a
really clean set of data of course it's
coming in from multiple different data
sources they've made some acquisitions
over the years that means they've got
goldmines of data that they all need to
combine into a single now the industry
calls it give it a great name it's
called a data Lake yeah right well the
data Lakes got all these different
formats is all messy data and and some
people call it dark data because you're
never gonna look at it again yeah to
your point what they were doing before
was spending eight days to be able to
transform that data into a usable set of
data and this was all done manually we
had data scientists dozens of data
scientists spending their time
transforming this data to make it user
so let me see if I understand you got a
terabyte of data coming in every day it
takes
you eight days to process one terabyte
of data yeah yeah to make it usable
uh-huh so guess what I pretty soon
you're gonna be infinitely behind guess
how many days of data we're not getting
watched right so here's what we've got
on the left-hand side here you can see
these diff this sort of incoming data
this is the complexity of the data that
they had coming into the system dozens
of different sources all kinds of
different formats we're gonna use a
product called data log a really awesome
product for being able to transform that
data into usable data the first step in
that process is by the way data log is
really fantastic they take it so this is
Tim's company I think Tim is watching
Tim good job
go ahead Erin yeah they are great so
what we're gonna do first is define an
ontology this is basically just a
knowledge graph that helps us transform
what the data is that we have to the
data is that we want and so we've
defined here the different data sets
that we want each of these data sets
then we can use AI to be able to
transform the data we have into this
data set that we want yeah and now
what's really amazing what Erin just
said is this just imagine so you have
you have basically data let's say you
have data sets coming from 10 different
places it's the same data it's about the
same thing but the format's all
different the names that they use the
labels that they use the columns they
use the features what they just the
names are used for the features are
different but just a little different
just a little different you know maybe
your name spelt just a little
differently if you and I were to look at
the database we would be able to tell
that this column and this row is the
same thing but to a computer program
they're basically exactly different now
what we need to do is we need to have
these AIS that are basically looking
onto all of this data as we're taking it
in and realize that you know what you
know what that J and Sen and J and - Hsu
is the same person it's the same person
it's okay yeah all in there let's let's
go look at actually what it looks like
when we train these models so this is an
example of a model that we've trained
for being able to predict these values
you can see that the the different
values that we're coming in on the left
hand side the values that we were
predicting on the on the
up there you can see how successful this
model is at being able to determine the
different kinds of data coming in with
the different data that we need in our
ontology you are off by two yeah so this
is interesting though right because
sometimes you will you'll confuse an IP
address for a different kind of data it
sometimes you know these things are they
look the same
sometimes the good news is we've trained
this model so well that it happened it's
99.9% accurate which is much better than
investing the eight days worth of data
science this time to be able to produce
those same results now it takes it would
take days to do this by hand that's
right
we want to automate to do this by hand
right yeah not to mention that just a
computation of loading it and
reformatting and then joining in and
grouping by this and doing the sequel
processing that you guys are familiar
with doing the sequel processing on it
it takes so much energy to do it
eventually you have this thing called a
data frame that data frame then comes in
to another amazing product this amazing
product is from a company called Omni
size isn't it right I've heard of that
guy yeah yeah yeah yeah you and Todd
started and and Todd is hard out in the
audience you know Todd Todd good job man
good job so so todd todd realized the
importance of GPU accelerated day signs
quite frankly almost before everybody
practically not just momentarily before
me okay and then he then he came and
told me that I realized hold on a second
this is a big this is a high-performance
computing problem and so so we've been
working together for some time I'm the
size you guys are doing amazing work now
talk to you now here's the data frame
yeah so so now we've got all this clean
data right now we want to actually put
it to work so we're gonna bring it into
Omni so I am nice I is a sequel database
that is built to run on GPUs and it also
has a visualization engine let's cut to
the dashboard so we can see what that
looks like now this is 500 million rows
of that network access data that we were
looking at before this covers two
hundred and fifteen thousand access you
got to let that moment sink in you guys
know what a spreadsheet with 500 million
rows look like I know this is 200 and
I've ever seen one that big 215,000
access points across the entire US on
the left-hand side there
you can see this really nice heat map
showing where those access points are
and where they're being used the most
let's let's zoom in to Ohio actually
just because you can see how easy it is
to zoom in and get a much clearer
picture of a specific part of your data
so now look how we've zoomed in now
we're seeing the detail of what's
happening in the state of Ohio I'm gonna
go use the the time chart here in the
middle to zoom in not just by location
but now by time as well see how when we
changed to the the the chart at the
bottom there now we're seeing hourly
results at the top and as we scrub over
to the right we're seeing all of the
charts changing in real time to show us
what happens at different time frames in
that data set now
Jennsen this is the cool part because
what's actually happening behind the
scenes here we're running hundreds of
sequel queries against that database
there is no way you could do this on
CPUs it's only possible because we've
architected to run on it was like
yesterday morning that sequel was a
batch job that's right those guys are
now watching this in real time and it is
this craziness this is no indexing no
aggregation this is 500 million rows of
are you guys getting this sequel used to
be something you would run with Hadoop
and it would be a batch job that's right
that's right
you know now you're doing this
interactively and you're visualizing it
in real time it's incredible now this is
just the first step though Jennsen
because now we've now we're actually
seeing the historical data let's get to
the predictions let's get in our time
machine and start to see what's
happening in the future
we're gonna do that using a jupiter
notebook so we'll switch over to the
jupiter notebook now and this is a
interface that all data scientists are
familiar with it's Python we're gonna
actually in the first step there on the
Left we're gonna actually get a pointer
to that data sitting in the Omni sigh
memory in GPU memory we're gonna use
that pointer to do the feature
engineering that you talked about before
and on the right hand side we're gonna
run our XG boost to do the training to
do our machine learning very familiar
toolkit here of tools that pointer is
exactly a panda's interface which is
what you're looking for if you're a data
scientist right so these tools give us
those predictions
once we get those predictions we're
gonna push them back into Omni size so
that we can make a complete
visualization of what that looks like
did you guys just see this sir first
step is data analytics and within data
analytics you could use on the side to
visualize it just take a look at the
data so you can get a feeling for it
then once you get once you do that
you've done the feature engineering you
take that into a that data frame you put
that into XG boost that XG boosts your
machine learning algorithm that machine
learning algorithm is going to take the
previously collected information and all
of the features and it's going to learn
a model that predicts the future from
those features okay predict the future
from those features it's gonna come up
with a new model and let's not forget
about rapids Rapids is an important
piece of all this that ties it all
together right so we're using that here
with these data frames that you talked
about to be able to do this kind of
prediction okay we push the data back in
Dom nice I let's go back to that
dashboard now so we can see what that
looks like you'll see on the time chart
now we have this dotted line that goes
into the future so as we scrub through
again we see that same interactive
experience of scrubbing through the data
and now we're seeing predicted results
for where we think the usage is going to
be across the state of Ohio and you can
see the same sort of interface the same
interactivity that you get now using
these predicted results coming from
Rapids using ku DF and guys you've just
you just saw in just a really quick
moment here predictive analytics in
practice from the beginning to the end
and and if we could accelerate
everything and and what what what Aaron
was saying was it's called Rapids Rapids
is the machine learning framework the
data analytics that data science
framework that we open sourced
underneath it there's a whole bunch of
engines those engines are basically CUDA
X and Rapids is now open sourced and the
important thing is is you see you see
down here this is where I grew up
Anita Kentucky there's no traffic there
at all I think here somebody type in
Anita Kentucky live it's just Anita
Kentucky let's see
[Laughter]
just one access point come on give us
one access point that's it that's where
I grew up one intersection and not
kidding not kidding all right so
everybody still baffled they're gonna
right that can't be right uh that is
right so so that was my that's why I
came to the United States now and so so
so here we saw the entire the entire
platform what what how long did it take
before and how long does it take now
right so we were talking about eight
days before just to up to a point where
we had some data we could do something
interesting with and then it was taking
us hours to do queries after that right
now we're talking about four minutes to
be able to get data the way we want it
that's near real-time and then what
you're seeing here is absolutely
interactive engagement with that data so
it's just a completely different
paradigm of what's possible with this
data so here we go data scientists the
most sought-after professional in the
world today
they're sought-after in every single
industry and every single company and
then once they get them they make them
sit eight days as they work on data that
is eight days old seven days old okay
and so so so that makes no sense what we
want to do is want to accelerate their
work given the instrument of their
science give them the instrument the
tool of their science so that they could
do their life's work and to do their
life's work as quickly as possible
ladies and gentlemen Aaron Williams
thanks
Todd you guys got something good going
okay so what we need to do is we need to
build we need to build high performance
computers for this whole new area called
data science you know design design
automation computer-aided design styling
median entertainment climate simulation
energy discovery molecular dynamics they
all have high performance computers they
all have workstations that the engineers
and scientists and researchers work on
well there's a whole field now it's
called data scientists there are three
million of them around the world three
million of them around the world as I
mentioned it is oversubscribed in every
single class we taught a hundred
thousand data scientists ourselves last
year our program is called deep learning
institute we taught a hundred thousand
ourselves their classes all over the web
this is an area that is hot and brimming
with excitement and the reason for that
is because the three things that I
mentioned that came together the
availability data the machine learning
algorithms and high performance
computing has made it possible for us
now to use this as the fourth pillar of
scientific discovery we think there
needs to be a new type of computer built
and so we decided that the work station
has to be re-engineered a new type of
work station with very very fast storage
very very fast i/o really fast
computation with very fast memories this
type of architecture a work station for
data scientists is really complicated to
build in fact just us building it
installing and building the software and
tuning the whole computer to deliver the
performance is not easy we're basically
taking what otherwise is a high
performance computing data data center
IT team and shrink it into one box and
make it possible for us to ship these
like appliances all over the world we
came to this idea because some of the
world's leading researchers were
building it themselves and some of the
companies at the forefront of AI were
building these machines themselves and
they were building it right in front of
us and they asked us you know how do we
make this easier it came to us because
customers were asking for it
and so we decided to build the
workstation for data scientists and this
is the performance look at this this box
if you were to run it the old way it
would be the blue bar and you run it
with one RTX 8000 or to RTX 8000 and now
you basically have a performance similar
to what Aaron was showing you the the
industry is so excited about this
because the demand is just right there
all of the world's leading computer
makers have joined us in this endeavor
and today we're announcing a brand new
family of workstations we call it a data
science workstation and it's going to be
available from the world's leading
computer makers Dell HP Lenovo and all
of our partners I want to thank you for
joining us this is a good way to get
going for one data scientist however the
jobs many of them many of the jobs are
so big it's impossible to fit on one
computer and that's why we say that data
science is the new HPC it's the new HPC
now before I go into HPC it's let me
give you a text sauna me also hi
supercomputers and high-performance
computers are created this chart this is
by the way this is jhh mathematics
inside and video that's called CEO math
it is not accurate but it's right yeah
are you guys following me it's not
accurate because you're gonna if you
nitpick it it's gonna be off but if you
study it you go darn it I think he's
dead on ok
all right goes like this basically
high-performance computers are built in
for two fundamental applications on the
one hand you want to build something
with capacity okay with capacity this is
this site the other way this vertical
side capacity capability capability is
building a computer you have a you a
computer the largest possible computer
so that you can run a simulation as fast
as possible or the largest possible
simulation you can imagine
a capability machine that is designed in
a way that we call supercomputers for
very few jobs very very very large ones
and you want to get it done as quickly
as possible or you want to do the
biggest one you can the second is called
a capacity machine this capacity machine
otherwise known as hyper scale uses
efficient cost Computers efficient cost
Computers millions of them and what you
want to do is you want to serve millions
and millions and hundreds of the
millions and ideally billions of people
with small jobs with many small jobs
okay so the architecture that you do you
create for hyper scale in the
architecture you create for super
computers are not the same are not the
same this is called scale out this is
called scale up this has maybe a million
nodes at a data center this has tens of
thousands supercomputers what I show
here is the compute it is the
computation load not not the flops but
the flops as in instances not time okay
the number of instances of compute is a
billion ped flops for example a billion
petaflop sware the s is units not time
and this is hundreds of millions of
people concurrently data centers of two
types both are high-performance
computing data centers this is where
supercomputers go capability machines
scale-up architecture this is where
hyper scale goes capacity machines what
people is call scale art architecture
are we okay
all right here's where data science goes
data science goes right here these are
tough problems to solve engineers love
problems here engineers love problems
here these ones are hard because it's
not quite this or that
is not quite this or that and the reason
why it's not quite this or that is
because in the case of data science
notice the amount of data is gigantic
and if you want to train a network or
you want to do data analytics on it you
do computation for days those are
characteristics of a supercomputer a few
people using a large cluster for days
otherwise known as a supercomputer on
the other hand on the other hand data
scientists is in the millions
we don't have millions of weather
simulation experts so we don't have
millions of molecular dynamics
scientists we have millions of data
scientists and so all of a sudden the
concurrency of data science is both
large and the computation requirement is
also large of course not everybody is
the same way and so we need to have
multiple architectures to solve this
this is where dgx goes we created D GX
which basically takes a supercomputer
and we turned it into an appliance we
integrated everything
lots of GPUs it's the scale-up
architecture 16 Voltas 16 v-102 petaflop
sat computing half a terabyte of
high-speed memory essentially 16
terabytes per second of aggregated
bandwidth and we have eight Mellanox
infinite bands on here to get the
fastest possible access to the network
into storage this is where DG x2 goes
the next step of our journey is to
accelerate hyper scale to do essentially
the same at for us with DG X but we have
to do it for hyper scale the software
stack is different the architecture is
different the whole solution stack is
different the go-to-market is different
everything is different about it it took
two years longer to do this than that
and you'll see in the second why and so
this is called scale allowed
acceleration when it accelerates scale
out and we're going to address the
millions of data scientist engineers to
the upper to the bottom right of that
Center bubble
and the solution for that is this GPU
we've been making called T 4 T 4 is the
first GPU tensor quart is our second
generation test record GPU
it's literally 70 watts it's the size of
a candy bar it fits into every single of
the high-volume most popular data center
servers in the world it can fit in a
blade it fits in a hyper scaled server
it fits in an enterprise data center
server 44s gives you about 260 teraflops
FP 16 so it's R it's a supercomputer
essentially and it comes with Mellanox
or infinite or Broadcom Ethernet NICs in
the end the software stack is the
complexity and it basically looks like
this you notice earlier when I was
talking about inference one container
the output through a REST API goes into
another container and it goes into
another container eventually comes back
out and it recognizes your speech
answers your question those are
containers those are all containers on
kubernetes what you're seeing here is
distributed computing notice a whole
bunch of users are using a server
containers are communicating with each
other data is going back and forth over
here over here a whole bunch of servers
a whole clusters working together as one
compute engine it's running one job okay
Hadoop started it they used it for of
course crawling the internet and doing
search Hadoop is a disk in disk
computation system in this computation
system uses commodity off-the-shelf was
a brilliant strategy billion
architecture basically you could take
terabytes and terabytes and terabytes
100 terabytes of data to internet put it
into your disks and stream it out to the
disk doing map and then reduce ok
basically Hadoop is HDFS filesystem the
yarn distributed scheduling system and
MapReduce compute engine that
revolutionized Big Data however because
most of the data is sitting
disk the computation was slow and then
spark came along and spark came along
and read everything into memory so now
instead of small amounts of memory on
all these servers now you have big
amounts of memory and it loads
everything into memory and now it could
interact on it and iterate on it in real
time first time people were able to do
basically interactive sequel searches
sequel processing in a data center
however the story goes eventually
Moore's law started to slow and the data
kept getting bigger and bigger and
bigger and now we have to accelerate
that and that's where Rapids came Rapids
is an effort that started about six
years ago and the industry has been
working together on that and it came in
several different layers the first layer
is to completely re-engineer the memory
system the in-memory architecture is now
called Apache spark Apache Aero the
second part of it is a scheduling system
called desk and the third the new
compute engine the new compute engine
called Rapids and Aaron was talking
about that that's integrated into the
work that they're doing these three
stacks Hadoop for disk oriented low-cost
systems built on top of that spark on
top of HDFS built another version of it
which is the GPU accelerated version of
it call now Rapids and you saw earlier
Rapids has seen great success has been
adopted into Microsoft Azure has been
adopted into Google Cloud it's been just
been used all over the place it's just
fantastic
and this is how we're gonna accelerate
basically your distributed computing
capability the one thing that you can
notice about this is that between these
containers between these containers is a
lot of traffic that's going back and
forth what the industry calls east and
west networking traffic the east and
west networking traffic is going up
exponentially what the industry calls
north and south basically data center to
cloud is not growing exponentially and
the reason for that is because the
number of people in the world's not
growing exponentially is growing but not
exponentially the amount of traffic
inside the arc into that data center is
growing X
potentially second when we create these
large distributed computing systems the
broadcast the collecting the reducing
all of that Ness that all of that those
primitives necessary to do distributed
computing is causing an enormous amount
of traffic inside so it turns out in the
future the way you design a data center
is going to change instead of a whole
bunch of compute nodes that are
connected essentially by networking the
networking and the compute will become
one continuous computing fabric the
network is going to become really really
important and that's one of the reasons
why recently we announced that we're
acquiring Mellanox and let me let me do
this let's introduce the CEO of Mellanox
yell Waldman please
ladies and gentlemen a visionary a giant
in the industry and pretty big guy thank
you for the introduction J so so so yeah
you know we've been working together for
some time
and we've been working on supercomputers
for some time in fact in fact I think
we've been working on it for about a you
know a dozen years you've been working
on it for almost 20 years yes and so so
we've been building supercomputers
together and what what are the trends
that you're seeing in the world of
high-performance computing right so I
think like Johnson said we're seeing a
great growth in data an exponential
growth and we also are starting to see
that the old program of program centric
data center is changing into data
centric data center which basically
means that data will flow and create the
program instead of us creating a program
using the data the data will start
creating the program using the data
itself and this is things we can work on
and actually have very synergistic
architecture solutions for the future
for the data center mm-hmm and so you
know if you take a look at look at our
journey together we started in
supercomputing and and almost almost all
the major supercomputers we worked on
you guys worked on as well and our
engineers were close hand-in-hand and
the reason for that is because you know
when you have all these different
compute nodes working together the the
synchronization of the information the
sharing of the information into one
large simulation is very intensive but
we're seeing the same thing happening
now in hyper scale data centers and
we're seeing the same thing happening in
enterprises and what are you what are
you guys seeing and what are the
dynamics that is causing that you think
all right so I think if you look at the
big hyper scale one of their big
advantage is the compute engine the
supercomputer they have in the data
centers worldwide to serve hundreds of
millions of people simultaneously what
we help is actually connect that compute
to computer and computer storage in the
most efficient way with the lowest
latency and highest scalability and this
is why we increase the productivity the
efficiency of those data centers
significantly some of the things you
showed here is that latency is one of
the most important parameters in terms
of then scalability and then efficient
productivity and that's what we do best
we have the lowest latency interconnect
on the planet both on InfiniBand and
Ethernet and we're just improving this
now with 200 HDR InfiniBand and also 200
and 400 Gigabit Ethernet they will
continue to develop and evermore
synergistic products in the future yeah
so between the neck to switch the
latency of your system is just really
incredible the other thing that that
that what you were well ahead at the
time was the concept of CPU offload in
our DMA and and and and we we also felt
the same way of course we didn't call
the CPU off flow we call it acceleration
but in a way you were in a network
accelerating company all along right so
we found out that you know doing
programs is great by the CPU but then
doing very tedious IO operations by the
CPU is very not efficient so we took
this task on Mellanox and we do it
mainly on the endpoints on the knee
connect CA with InfiniBand then what we
found out there is we can put computing
inside the switch and this is something
we're done with Nvidia sharp to actually
to accelerate we have like an AI offload
machine floating-point machines inside
the switch to increase the efficiency of
artificial intelligence programs in the
data center distance is something we're
working together I think you have very
recent results I don't know if you've
shown this recently but we are seeing
more and more offload we can take away
from the CPU and GPU into the network
and then synergize this into the whole
data center solution for AI yeah and
that's that's our path forward you have
to find a way beyond the time when CPU
scaling is going to continue to progress
now that it's slowed down we have to
take as much workload as we can and of
course moving it moving it into an
accelerator is one thing but moving into
a network is a completely another thing
and we should do both we will execute on
that fantastic thank you thank you well
being on stage here Thank You yo ladies
and gentlemen yo Waldman incredible guy
we we've got a bunch of Mellanox
switches and Nick's all over our
products and so so I now we've created
these machines we've created these
machines in these systems we've got to
take them to market and the market is
fragmenting very
in the sense that the number of
customers that use data analytics and
data science is growing so rapidly it's
gone from research it's gone from
research to the world's Internet
companies what we call hyper scale
companies cloud service providers it's
moving very quickly into supercomputing
physics inspired neural networks in AI
and moving of course into enterprise and
we're trying to take these various
different kinds of computers to the
world and so we have two for each one of
these markets and each one of these
segments we have to use a different go
to market one of the ways we're going to
market we're taking the dgx and
basically these AI supercomputing
appliances with all the storage and all
the switching which is really
complicated and bringing it to the
enterprise we have a suite of partners
that we're working with really great
partners D D and L UMC IBM NetApp pure
storage they know a ton about storage
and they happen to be working already
with all of the people who use a lot of
big data and so partnering with them
creating essentially a pod and these
pods are fully integrated what the
industry calls essentially
hyper-converged you could take all that
you install it literally within a day we
come in or somebody comes in I don't
come in but somebody comes in between
zero I was almost going to take credit
for it but I don't think I should
maleeh heavy and and so you bring it in
zero to AI is basically in a day a few
hours and so really fantastic work with
Arista and Cisco on switches and
Mellanox and switches together this
represents a large part of the
high-performance computing industry and
we now take these capabilities into the
enterprise the second thing is now the
larger enterprises the dgx pods that I
just mentioned comes in to the upper
left-hand side upper left-hand side of
that bubble of data science now we want
to take it into the bottom where the top
in the bottom goes the computation the
computation is three orders of magnitude
apart but yet the market size the number
of engineers the number of data
scientists is also three orders of
magnitude instead of one
instead of 1,000 now we're talking about
1 million instead of thousands we're
talking about millions of people and so
we're gonna need a large large network
of partners to be able to take these
architectures which are relatively
complicated which is only possible
really today in the cloud service
providers to bring it out into the
world's enterprise and so that they
could set it up easily and to be a run
these workloads as easily as possible
we're announcing today that nearly all
of the world's leading computer makers
for enterprise has joined us to take
this new architecture this data science
server powered by T fours
the CUDA x AI stack and all of the
machine learning frameworks that I've
already spoken about to take it to
market and so I want to if I could ask
you guys to congratulate them and thank
you for joining our team let me show you
what these servers look like so just now
you saw what a what a what a workstation
can do now this is a gigantic data a
gigantic data set and this is what it
looks like in the case of this data set
as several hundred gigabytes large
end-to-end it takes thirty five minutes
to do on a cluster of ten servers okay
and on a cluster of 10 servers with one
T 4 inside it's almost zero and so
basically we take it from half an hour
end to end to three minutes barely
enough time to get up to get a cup of
coffee ok so so that in the future you
will get all the data scientists in the
world are going to be substantially less
caffeinated we're going to get a lot
more work done here's here's an
interesting this is this is M X net
training using the same distributed
servers and notice it starts to plateau
this is the problem of of distributed
computing this is the reason why
scale-up is better than scale out for
some very large
simulation jobs because by adding more
and more servers notice the return on
that investment starts to decline and
the reason for that is because you're
spending too much time communicating and
so this is with the fastest ethernet and
this is with the fastest Ethernet with
our DMA the technology that Mellanox
invented and now it's an industry
standard called Rocky ok so this is the
reason why networking bandwidth is so
important and why networking offload is
important and why software integration
of the stack is so important what I see
here what I show here you here it looks
simple but the amount of software that
goes into making long as possible is
really incredible
well that's enterprise if we want to
reach a lot of people if we want to
reach a lot of people and want to reach
a lot of people fast with the single
largest compute engine on the planet
there's one way to do it and it's the
only way to do it ladies and gentlemen
is the Amazon way to do it and so if I
could just invite Matt Garmin a partner
of ours I've been working together for
years Matt how's it going have you here
thank you it's great to have you here we
started working together seven years ago
you put a Fermi GPU in the cloud we did
yes
what were you thinking but anyways man
put a fermi GP in the cloud and and
opened it up as a GPU as a service and
that was the first time and actually to
be honest when you first did it I was
kind of going yeah I don't know where
this is gonna go a lot of people you
know yeah I'm enthusiastic I'm happy I'm
happy to sell any GPUs but but but it
was just it was it was so it's so cool
to see and then you were the first to
launch Volta into the cloud and then you
were the first to launch an eighth
instance hgx to Volta into the cloud and
we've been working together ever since
we have it's and and so so it's great to
have you here you guys have been doing
AI in the cloud you've got this lay
layer called the service called sage
maker and it's a fantastic tool you know
I'm gonna
and us to you you got a couple slides
but can you tell can you tell the
audience about some of the work that you
guys are doing and sure yeah happy to
and so and I've actually been an Amazon
an AWS about 13 years since we started
AWS back in 2006 and over that time you
know really our goal is to develop and
what we've done is develop the most
reliable fastest cloud for anybody out
there and really how we think about that
is we wanted to deliver services across
the world for everybody wanted compute
services storage services and we want
data analytic services and networking
services across 61 availability zones
and 20 regions all around the world and
particularly ml services and that's a
lot about what you've been talking here
today and when you think about ml
services one of the things that's really
exciting is machine learning is a great
fit for the cloud a lot of our customers
and we have more machine learning is
done in AWS in the cloud than anywhere
and one of the reasons for that is
because a lot of our customers are still
trying to figure out a lot of these
people here trying to figure out how
exactly do you incorporate machine
learning into your applications right
they're still trying to figure out
exactly what are the best ways to do it
and iterating on that well the cloud is
a perfect fit for that we have customers
who come launch large clusters of p3
servers running Voltas and they'll run
their training applications and they'll
spin them up and they'll test them they
see how they go they spin up lots of
computers and lots of servers so they
can get the work done quickly and then
they shut it down
they don't have they paid don't pay for
any of that infrastructure and then they
go look at their results they iterate
they try some new algorithms and then
they go spin it up again when they're
ready to do it
and in fact one of the services that
we've delivered as you mentioned is
called sage maker sage maker is an
end-to-end fully managed machine
learning service that makes it easy for
data scientists developers even machine
learning experts to easily and scalable
e launch their machine learning
applications in the cloud all in Nvidia
and ec2 technologies now what are some
of the customers that use your stuff
yeah and tell about tell us tell the
audience about some other things that
they use it for thank you and yeah it's
so you know AWS and Amazon we always
start with a customer that's that's
really kind of where we think about it
and we have thousands and thousands and
thousands of customers using ml in the
cloud today I picked up a couple of them
here some well-known customers and I'll
highlight a few of them for you
and we have some customers that are
actually doing traditional HPC
we have customers like Western Digital
who use P threes and Voltas to look at a
wide range of factors from material
science properties to magnetic and heat
flows and all sorts of things to really
improve the quality of their disk drives
we've got a lot of customers that are
using your stuff for seismic processing
we do they're doing all sorts of they
don't have if you look at a different
industry we have cell gene cell gene is
a bio pharma company recently bought by
this bristol-myers Squibb and they have
AI and they're looking at a bunch of
different drug designs to see what's
going to happen fastest they used to
have a cluster on premise that would run
their applications it took them two
months to run this complicated
application they move to the cloud
scaled out and now they're able to do it
in six hours
Wow and there's a couple of Bay Area
companies two months to six hours when
you say that out loud for a second it's
amazing it's actually yeah shocking yeah
and so think about think about not just
the cost that they're saving but the to
your point the most precious resource is
that data scientist time and that ml
scientist time and the iterations that
they're able to make are incredible for
their business yeah and then many of the
top technology companies here in the Bay
Area from Salesforce using it in their
einstein vision application for their
developers finding image recognition for
brands that are online people like
Airbnb making it easier for their hosts
to figure out how much they should
charge for their property or even lift
lifts recently announced that they're
all in on AWS everything that they're
running is all on AWS for their 50th or
50 million riders per month and they use
al-rai and ml running on p3s and Voltas
together with sage maker to calculate
everything from estimating fares to
better drop off and pick up things to
fraud detection in the cloud just a
couple of examples that's just amazing
and one of the things these are all kind
of training applications but many of
them my favorite app Yelp exactly
working great that's incredible yeah II
know it's about the future before does
it go from here yes so these are many of
these customers are doing their big huge
training sets in south of AWS and happy
to announce that many of them are also
doing as you mentioned through that
pipeline that you had up here earlier
they do their training but they also
then spread out inference yeah and many
of the customers tell us that actually
80 to 90 percent of the cost of machine
learning at scale ends up being in front
yeah this is the big market so super
excited to announce this today
a new instance coming out in AWS coming
soon the G for instance it'll be
featuring Nvidia t4 processors and
really designed for machine learning
inference it's really designed to help
our customers really shrink the time
that it takes to do inference at the
edge where that response time really
mattered matters but also really reduce
the cost they have to run fewer nodes in
addition to machine learning where this
is a really great fit you mentioned you
can also do graphics processing on these
and so we're really excited and many
customers are excited to do that as well
we have customers that are looking to do
a high-end video workstations in the
cloud we have customers that are looking
to do video kit transcoding and media
processing as well as video games
streaming all of that via the cloud on
our new g4 instances I've got two great
stacks for you one's called g-force now
the other ones call omniverse awesome
yeah so you'd go build a whole bunch of
cloud GPUs I'll be there we will nothing
got to be more excited when he said 61
countries that's right 61 availability
regions but we're getting to 61
countries so we're super excited I want
to thank you hope you have a good night
oh and thank you for inviting us thank
you Matt thanks for the partnership yeah
appreciate it thank you Matt Garmin
Amazon AWS the largest computer on the
planet alright chapter 3 robotics here
we're gonna talk about robotics the
first part of robotics is Jetson we
created this little tiny computer called
Jetson and we put it out it's it's based
on Linux it runs the entire Nvidia CUDA
X stack the amazing thing is there are
200,000 developers across 2000 companies
building things everywhere warehouse
logistics robots little delivery robots
agriculture robots John Deere for
example retail robots and assistants
industrial robots robots everywhere
augmenting our capability doing things
that are hard for us to do and this area
is just rich with research this is of
course the ultimate AI today
we're announcing a brand-new robotics
computer a brand-new robotics computer
we're so proud of this one
it is the smallest computer our company
has ever built it's called the Jetson
Nano I have one here I've been wearing
it all day
[Applause]
well you don't get it it took me days to
get it they kept putting the slide in
front of me and I get come on it's not
quite right it's not quite right
his head's too big turns out they were
it was my head that's a terrible joke
you guys ladies and gentlemen Jetson
nano here's the amazing thing about this
little tiny thing it's $99 the whole
computer and and it's like you know if
you if you use a Raspberry Pi and you
just don't have enough computer
performance then you get yourself one of
these and it runs the entire CUDA X
stack it runs it runs computer vision if
we're in speech recognition because it's
architectural II compatible our company
is that way and so you've got rich
software and all the AIS that you've
created that runs on DG X's you know
when you compile it again it runs on
this we we care so much about the
robotics industry we decided to create a
whole set of tools for the robot
robotics to foster the robotics
ecosystem and so today we're opening
several things the first is the Isaac
robot engine basically the entire stack
that's available on top of it to create
robots it creates three different robots
the kaya robot the Carter robot in the
link robot and one of them could use
little Jetson Nano the other one may use
Xavier we also created a robotic
simulator so that the robots could learn
how to be robots inside this virtual
reality environment and it's kind of
look like real to the robot so that when
the robot is done we take the artificial
intelligent into the real physical robot
it sees the world perceives the world in
the same way the robotics loop basically
has three things and go on talked about
it okay it all sits all everybody talked
about it it's exactly the same way
however you think about one perception
two reasoning and three planning
perception reasoning and planning
perception reasoning and planning these
robots are all doing that the internet
services the AI chat conversational
agents are doing that
the machine learning prediction systems
are basically doing that you're
perceiving from all the data your
reasoning about what to do and you take
action you take action you make a
recommendation or otherwise known in the
world of physical worlds called planning
and so we would we would let the robot
learn how to perceive the world so the
world has to look right that's the
reason about what to do based on what
it's asked to do and then has to do the
planning of the motion the articulation
of it and do the work we also wanted to
learn the machine learning algorithm the
AI algorithm by itself because some of
these programs are just impossible to
write how do you look at something go
and pick it up and object is changing in
shape sometimes it's different shapes
all the time it's in a different
position all the time so the program is
not exactly the same every single time
but you would like to have an agent and
AI algorithm that goes and picks it up
by itself and so we created a gym where
it can learn through reinforcement
learning how to be a robot we put all
these things together and hopefully the
the ecosystem the community can use this
platform to create amazing robots for
the future you can go to developer down
and video.com isaac sdk and well just
you know it's it's open and feel free to
use it give us feedback and let me let
me show you I saw here we go and and
there's these reference robots you just
kind of you know here come just yeah
just stay put sit
all right so this is kaya and this has a
Jetson in it and it's got rich sensors
in this case depth camera in the larger
computers the larger robots has lidar
and so they have wonderful sensors and
we can support very high resolution
sensors and so if you wanted to make a
little toy you could but if you wanted
to make something that's actually real
and you do real work you can and they
can be wonderful robots and the whole
stack from vision to speech and all of
the AI that we've been talking about
this whole time is available on here in
a ship run okay and so hey good job you
guys okay yeah okay the guys the guys
made you a short movie let's take a look
at it
[Music]
[Music]
[Music]
ladies and gentlemen Isaac robotics okay
good job you guys let me talk to you
about one of the most important robots
of all your self-driving car everything
that moves in the future will have
robotics technology inside will have
some level of Tamas capability our
company is deep in the middle of the
autonomous vehicle revolution but we're
not building a self-driving car we're
creating a system and the infrastructure
and the design capability necessary for
the whole industry to build a
self-driving car these are basically the
components of our drive initiative we
start with of course the Saturn what we
call dgx Saturn 5 our own supercomputer
necessary for doing deep learning and
training these a is the output of these
AIS
whole bunch of them that are then
ensemble together to compose the three
layers the three groups of algorithms
perception localization and path
planning perception localization and
path planning some people combine
localization and mapping and so
perception localization mapping and
planning these three parts of the
computing AI computing stack robotics
TAC is inside the drive platform all of
those algorithms are in Samba together
to essentially create what we call drive
AV constellation is our simulation
platform as an engineer nothing is right
unless you can simulate it you want to
be able to simulate the corner
conditions you want to simulate the rare
conditions you want to be able to
regress and repeat old scenarios this
car not only does it drive by itself but
it has to communicate with you just as
when goo han was communicating with the
cloud there has to be visual back input
they've had feedback as well as audio
feedback and so dr IX is our intelligent
intelligent user experience and it
basically shows you what is in the mind
of the self-driving car and it
communicates with you the driving
computer itself scalable from level 2
all the way to robot taxis
we have a Risa Malaysian
effort that is going I'm going to show
you a video of that and then ultimately
we've got to drive the car we're gonna
drive the car until it drives by itself
and it's just it's perfectly ready for
production and then we open up the
entire platform people could use it at
the computer level at the meet
middleware software level or the fully
integrated application level they could
take advantage of our server system for
developing the AI the simulation system
and whatever infrastructure we've
created this is an open system the
future of autonomous vehicles has to be
software-defined and the reason for that
is because look at how far we have to go
and how many different diversity of
systems we have to support it is
impossible to design a specific specific
widget for one particular car we have to
be software-defined it has to be open
you have to take advantage of the entire
ecosystem and I really appreciate all of
the partners for joining us thank you
today we're announcing our release 9
release 9 we're still a year or so away
from having a production car but this is
our release 9 high functioning level 2
gives you basically it gives you off
ramp on ramp and off ramp last year we
showed you I showed you how we drove 50
miles without touching the steering
wheel we have some round perception so
that we could have the ability to do
auto lane change we localized to all of
the world's major maps we do real-time
mapping ourselves and the reason for
that is because these maps cover about
80% of the world but most of the routes
that we drive ourselves the last 20% of
the world it turns out to be the most
frequent miles and so we our car could
map fuse it all together and turn it
into our own personal HD map then we can
localize to in the future we also
integrate everything into an AR and VR
system that I'll show you in just a
second it's really quite incredible and
then we have integrated driver
monitoring and voice recognition
basically your car becomes an AI guys
let's show it to them
last year I showed you guys and to end
50 miles this year I'm showing you this
is map routing our car is doing dynamic
mapping right now you drive the routes
and then it fuses the routes together
into a map not it doesn't record your
driving it's Greek creating the map and
now notice on the right hand side it's
creating a map it fuses it together
because of the routes that we drive
these days there's a lot of
intersections a lot of complicated
intersections we have to teach it what
are the different context and where
should you stop every intersection is a
little bit different counting on red
lights alone is not good enough counting
on signs alone is not good enough we
fuse everything together we have this
great technology this is so great we use
radar to localize essentially radar
turns into a lidar very coarse lidar and
it works even in the rain and fog at
night and as supplements our camera we
simulate everything this is a virtual
reality simulator now automatic Lane
changing in a crowded environment and
this one is a technology called safety
forcefield apply to break
[Music]
and then takes us home back to
headquarters
lots of intersections lots of
complicated corners we drive it several
times we fuse it together in to
reconstruct our own map because the HD
map of the map providers don't don't go
here okay in the interest thank you guys
let me show you something that's really
cool so we do so the first part that you
guys know we do incredibly well is
perception the second part we do
incredibly well is localization we've
been building up the stack from the
foundation up now we're going to
introduce our path planning our path
planning has several components the
first component is an ensemble of neural
networks and different computer vision
approaches to estimate the paths that
the car should take we call that path
planning it is really really robust and
it's fantastic on top of that we have a
prediction algorithm to predict all of
our surroundings detect the surrounding
predict its path future path estimate
their speed that action perceiving the
surroundings and predicting their future
is important to safe driving and the
reason for that is this you want to be
able to have a computationally robust
algorithm to predict that whatever you
decide to do you will do no harm in the
future and so everybody else around you
put assuming that they're they're well
behaving agents that you yourself will
computationally not cause any harm so
you are essentially in a safety cocoon
we have a method a computational method
that detects the surrounding cars
predict their future path of course
knowing our own path and computationally
avoid the surrounding traffic we call it
the safety force field this is the first
of its kind is completely computational
it has the ability to be computationally
validated we have researchers around the
world who are as safety experts looking
at the algorithms
we're getting great feedback from it
it's going to be an open open system so
you guys can take the algorithm and you
can implement it yourself
and let me show you how it works so this
is basically you've now predicted that
based on their trajectory which is not
moving that you should apply your brakes
and this is using this algorithm you
achieved what's called automatic
emergency braking automatically
this is intersection handling we're now
predicting that car is now taken a turn
and so we have to apply our brakes and
we did it just in time okay and this is
intelligent steering we're now
estimating where everybody else is
and because we're blocked we have to
find the next closest route okay and so
that's part of the safety forcefield it
figures out computationally that there's
a lane next to us and we can use it and
this you can just congested traffic
using computational methods we detect
where all of the surrounding cars are we
want to change lanes it is okay to do so
and the reason for that is because we
predicted the car velocity and
trajectory of the car next to us behind
us and we come to the conclusion that if
we change lanes it will not hit us if it
were to if we were to change our
computation and we determined that it
will collide with us of course we'll
veer back into our own lane okay so
safety force field
safety force field it's computationally
verified and it is simulated and that's
why simulation is so vital to us this
algorithm we're super proud of it it's
now it's now going to be in the open and
you can read the white papers and as we
as we continue to progress and we'll
make it available to you and so the
third stage of really really great
self-driving cars is the path planning
algorithms and both in comfort and in of
course accuracy and comfort as well as
in safety simulation is really vital
today we're announcing that
constellation is available we've been
working on constellation for some time
this the architecture of the
the image generator is complicated it's
bit accurate meaning the hardware it's
basically a virtual car and you take the
software we throw it into our data
center and you essentially have a
virtual car it's like having a a virtual
fleet of autonomous vehicles in your
data center the workflow is in the cloud
you could program your workflow we're
going to show it to you in just a second
and then you can of course stream it
from the cloud in the future instead of
having thousands of Av test cars we're
gonna have thousands of these
constellation systems they're going to
be so much more programmable we can
create conditions that we otherwise
can't and we have our own system I'll
wracked up okay so now mark is going to
show it to you mark
okay let me show you a little bit about
what how we deploy our system on
constellation you're looking at drive
sim which is our simulation platform the
Center for screens are our simulator
right left front and rear view Thank You
vana over to the left side is the
perception this is the drive a V stack
taking the sensor input from the
simulator and perceiving what's going on
in the world and and as well as the
lanes and the cars and everything that
are in the world and giving us control
information back to the simulator and so
the computers are driving the car look
ma no hands over here on the right side
it's what we show the driver or the the
human in the car so that the human is
well aware that we're aware of what's
happening in the environment is a
confidence view as a driver you could
decide to see this augmented reality
view right here or you can decide to see
a virtual reality view right there and
the ability to see this gives you so
much confidence that the AV computer is
recognizing perceiving the right things
and about to do the right things okay
mark up it fantastic so I want to show
you how we actually use this the
workflow of using Drive Sim so if we can
switch the developer view real quick
over here on the right side we're seeing
and yet another camera in the scene this
is a spectator camera that a developer
would use we're running the simulation
the entire time that we're editing this
we also have the
Jupiter notebook Python interface
two-hour drive sim so we can make some
changes let's let's make some weather
changes go ahead let's uh night time go
through a few of them
sunset rain so much easier to do this in
virtual reality than to do it in real
life like we try to we try to hope for
rain but it just doesn't rain for months
[Laughter]
okay fantastic
so we've changed some environments in
the world we've actually been adding
traffic as we've been playing along all
this while the simulation is running all
while we're running hardware in the loop
so we're talking to the actual computer
in the car as the exact software that we
would actually put into the car is
actually running on constellation bit
accurate exactly the same perfect yes so
let's make one more modification while
we're here all this traffic is
essentially their agents they're they're
running a pass based upon rules but we
can we can occupy the mind of one of
these agents let's take it over and and
control it so we've grabbed one of the
cars in the world let's uh let's do it
have it do a lane change so I can
locally modify any one of these BOTS of
traffic I can possess it and control it
I can set up any scenario I want
interactively finally the best way to
test our car isn't to do it all of this
interactively I really want some
randomized versions of these scenarios
that we just created and I want to
deploy them on a whole fleet of not real
cars but in this case constellation
boxes in our data center so let's do
that let's switch to the constellation
view here we go we've got in this case
twelve versions of that exact same setup
but now with randomized variables of
weather time of day timing of the
traffic so that we can we can test every
possible iteration of that scenario
that's awesome thanks a lot mark thank
you very much ladies and gentlemen
constellation
not only can you use it for simulation
you could use it for rese emission watch
this this is really cool what's actually
happening here is we're taking a
previous Drive and we're taking the data
and we're pumping it into constellation
we're sitting inside of inside a test
car stationary outside it's in GT C as
far as your as far as the car is
concerned it's basically running a
regression and so the detection is
working the lane detections are working
and it's recommended paths are all
working and whenever there's any
deviations from previous drives this is
how you can regress do regression
testing ok good job guys thank you
constellation I have one more thing to
tell you so it turns out that autonomous
vehicles is one of the greatest
challenges safety is a great concern the
technology is really complicated the
software that we have to develop is
still quite significant and and it's a
computing challenge it's an artificial
intelligence challenge it is a system
integration challenge of cars there's
all kinds of challenges involved this is
really one of the world's great
computational challenges and the world's
largest car company the world's largest
color company is making enormous
endeavors in this in this area
ladies and gentleman today we're
announcing that Toyota the world's
largest car company the largest
transportation company is partnering
with us from end to end
from deep learning systems the
simulation systems to Inc our computers
to collaboration with AI ladies and
gentlemen Toyota and the t.ri advanced
developments team is partnering with
Nvidia to create the future of
autonomous vehicles
this is this is so exciting and this is
how we can make a real a real real
difference in the future of
Transportation
let me quickly now summarize we just
several things today we talked about
accelerated computing in the path
forward ladies and gentlemen
what does Prada stand for that's right
programmable acceleration of multiple
domains with one architecture
okay so programmable acceleration domain
architecture that is what accelerated
computing is about and that's how we
move forward we work across the entire
acceleration stack the second thing is
the future of graphics the future of
games is unquestionably ray tracing
you're gonna talk you're gonna hear
about ray tracing on and on and on and
on and on this week turns out to be Game
Developers Conference and all they're
gonna be talking about is very tracing
we announced several things a server
graphics in the data center graphics in
the data center a very complicated
server architecture with three stacks on
it for rendering
omniverse computer design and three
cloud gaming on cloud gaming we have a
strat we have a we have a platform we
call the GeForce now Alliance and we're
partnering with Softbank LG U+ is our
first partners and I'm so excited that
we're announcing omniverse something
we've been working on for quite a long
time it's just really great technology
the next in data science it's the new
HPC is the fourth pillar of the
scientific method this is our ecosystem
approach and today we're also announcing
two computers two new computers designed
for the future of data sciences and for
enterprises all over the world to be
able to access this technology data
science workstations and a data science
server based on the T 4 and our
partnership with AWS to take this entire
stack on top of the world's largest
computers to the world's data scientist
and of course our partnership and our
acquisition of Mellanox something super
excited about because the future of
computing extends out of the computing
node and into the networking fabric and
then lastly in robotics Jetson
smallest little tiny computer - cutest
little thing right there I'll just do it
one more time for you guys for your
enjoyment I think it's like right there
and so so and then constellation and
then our partnership we tell you how to
ladies and gentlemen this is our GTC I
want to thank all of you for coming
today have a great GTC
[Applause]
Title: Optimize Route Planning with NVIDIA cuOpt
Publish_date: 2021-11-10
Length: 58
Views: 15274
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/z5-gKQFqE_4/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLCLH-S0TrYlraWC6mvMOud3SVvpcA
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: z5-gKQFqE_4

--- Transcript ---

route planning is an extremely hard
logistics problem
at industrial scale even small routing
optimizations can save billions of
dollars
this example uses a virtual warehouse
and nvidia omniverse to show the impact
of optimized routing in an automated
order picking scenario
an optimized plan results in orders
picked in half the time and half the
distance traveled current routing
solvers can take hours to rerun and
respond to incoming new orders
nvidia reopt runs continuously and
dynamically re-optimizes in real time
when new orders come in after the robots
deploy
re-optimize
when a robot goes offline re-optimize
and video reopt responds in seconds and
scales for thousands of locations for
the first time we can now capture the
dynamic behavior of the real world and
respond not just intelligently but
optimally re-optimize your logistics
today
Title: Omniverse Connectors - Part 5: Platform Overview | NVIDIA Omniverse Tutorials
Publish_date: 2020-12-18
Length: 130
Views: 6196
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/ZC1A-eEk_3U/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: ZC1A-eEk_3U

--- Transcript ---

[Music]
hello everyone and welcome back
okay so let's get to one of the most
exciting aspects of the omniverse
connectors omniverse connectors are
plug-ins for popular digital content
creation
applications they create conduits
through a nucleus to other dcc
connectors
and omniverse apps the result is the
connection of any number of dcc
applications omniverse apps and users
to the same content at the same time
this innovative approach is a key
component to real-time collaboration on
the omniverse
now and for the first time tools like
maya
revit sketchup ue4 and others
can all work together on a single
project sharing the same data
at the same time additionally the
omniverse can do this non-destructively
live and across the globe what's more
there is already a significant number of
connectors today
and more are being added to the roster
all the time
if you are a professional working in 3d
chances are
a connector already exists for you tools
like maya
3ds max revit rhino sketchup
unreal engine and more can now send live
sync
edit and open usd data and enjoy first
class status on the omniverse for many
users
these connectors will be the primary
method for interacting with the
omniverse
install the connector fire up your dcc
application
log in and send with minimal effort or
change in typical workflow
your content can enjoy all the benefits
of the omniverse platform
you
Title: NVIDIA GeForce NOW CES - PC Beta Now Available
Publish_date: 2018-01-07
Length: 70
Views: 196389
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/zceUpFI16UE/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: zceUpFI16UE

--- Transcript ---

it's like magic except it's not science
this is a great example of how newer
technology can mean that you don't have
to leave older hardware behind it is the
future of the gaming industry this is a
two hundred dollar MacBook straight from
2009 it runs pretty well here with
GeForce now this allows you to run a
bunch of games at high settings at the
resolution that your computer can
support
I didn't have to do any driver
installation or anything it actually is
just working out of the box this is very
quick in terms of the mouse movement
registration it's almost lag free or
latency free even more graphic intensive
games I can crunch through it at any
difficulties even at Ultra and high
settings GeForce now gives you on-demand
access to a GeForce GTX gaming PC in the
cloud it plays games better than my
gaming laptop does plus I don't have to
put any of them on my hard drive and
take up a precious precious storage
space I'm really impressed with this
it's crazy just how far cloud gaming is
calm
you
Title: The Top Reasons You Can't Afford to Miss GTC Europe 2018
Publish_date: 2018-06-14
Length: 72
Views: 197416
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/ze2lNxkDmpI/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: ze2lNxkDmpI

--- Transcript ---

[Music]
this is a great new era of computing I
want to welcome all of you to cheeky see
over 3,000 attendees more than 200
speaking sessions over 80 exhibitors
with brilliant demos which really are
interesting to see how deep learning is
taking part of the Nvidia offering he's
met so many people from all over the
world breakthrough in the fields of
artificial intelligence deep learning
virtual reality high performance
computing autonomous machines and much
more
the always very fantastic we've been a
super happy to be here great platform to
meet with other companies good
networking options
I would recommend a colleague to come to
GGC next year it was just great to be
part of the post keep learning is a big
topic so yeah we are interested in
what's going on what's the future that's
why I'm here
[Music]
you
Title: NVIDIA Mellanox 400G: For the AI and Supercomputing Community It Means a Lot
Publish_date: 2020-11-16
Length: 108
Views: 22413
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/Ze54CXGhf4Q/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: Ze54CXGhf4Q

--- Transcript ---

[Music]
to most people
nvidia mellanox infiniband doesn't mean
much but changing the world
means a lot to most people battling a
disease
the impact of two times faster data
throughput isn't obvious
but getting their medical analysis twice
as fast
means everything and to astronomy
enthusiasts
innovations like four times higher mpi
performance
with new in-network computing engines
might not spark the imagination
but finding the right path to land on
mars does
if you would ask people living in small
villages about five times higher switch
system throughput
at 1.6 petabits per second you might not
get them thrilled
but tell them they could get a hurricane
alert on time
and you've got their attention to most
visually impaired people
6.5 times higher scalability connecting
more than 1 million gpus at 400 gigabits
per second
may not seem to make a difference but
being able to operate an autonomous
vehicle
probably will in these challenging times
32 times more ai acceleration power with
nvidia melanox sharp
may not mean much to many of us
accelerating supercomputer processing to
help the world fight a pandemic faster
should mean a lot
introducing the seventh generation of
nvidia melanox infiniband
now at ndr 400 gigabit per second speed
the world's only in-network acceleration
platform
with ultra low latency for the super
computing community
it means the world
Title: NVIDIA SHIELD Showcase: Hardware specs, preloaded games, and more...
Publish_date: 2013-09-13
Length: 96
Views: 47728
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/Zi3JcWNLDAw/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: Zi3JcWNLDAw

--- Transcript ---

hey guys I'm will with NVIDIA and this
is shield Nvidia shield is our first
portable gaming device it features a
5-inch 720p HD multi-touch display
attached to a console grid game
controller with analog joysticks
shoulder-mounted triggers a d-pad and a
bx y buttons and in the center there's
this dedicated backlit nvidia
multifunction button at the heart of
shield you'll find the Tegra 4 quad core
mobile processor 2 gigabytes of ram 16
gigs of storage which is expandable with
the microsd card gps 802 11 2 x 2 mimo
game speed Wi-Fi bluetooth 3.0 and a
custom bass reflex tuned audio port
system with built-in microphone all
running on the latest android jelly bean
operating system turn Nvidia shield
around and you'll find a 3.5 millimeter
headphone jack a micro USB 2.0 port a
micro SDXC card slot and a mini hdmi
port now shield will come pre-loaded
with to our favorite Tegra for enhanced
games their Sonic 4 episode 2 and
expendable rearmed that way you'll be up
and running the best android games right
out of the box and on the pc side you
can play your favorite pc titles on
nvidia shield through our pc game
streaming service which we're launching
as a beta feature that's it for now make
sure to keep your eyes on youtube.com
slash in video form or shield demos
you
Title: NVIDIA Fans Get a Sneak Peek of VR Funhouse
Publish_date: 2016-07-07
Length: 67
Views: 13754
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/ZJ_AxVge2ak/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: ZJ_AxVge2ak

--- Transcript ---

see look I can see the controller oh yes
I like bow and arrow this is cool this
is a game where you can actually shoot a
bow and arrow and it actually feels like
you're shooting I do archery in real
life so this is like as you're drawing
each of the controllers rumble oh I got
the gun
join John Wayne had these pretty good
work out here yeah just the realism it
looks like i have actually holding guns
in my hand this is a lot more than i
expected the art of 80 there's an idiot
i right here
Title: GeForce Experience Early Access Share Beta: New Features Rundown
Publish_date: 2015-08-20
Length: 181
Views: 172757
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/ZqsT4Og28lI/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: ZqsT4Og28lI

--- Transcript ---

hi I'm Andrew with Nvidia and today I'm
excited to share with you the latest
beta update to GeForce experience some
of you have been asking for an easier
way to access the features while in game
and we're releasing an overlay tool that
will allow you to access the broadcast
and share controls while playing your
games let's go ahead and take a look at
how the overlay works so i'm playing
witcher 3 and using shadow play to
capture 4k 60fps video in very little
performance shadow plays a couple
different modes including a manual mode
and a DVR function we can always on
capture the last five minutes of
gameplay like killing this Griffith when
I get a kill I can instantly save my
video and upload it directly to YouTube
from within the game simply open the
share overlay and select upload trim and
title the video and send it right to
YouTube there's even a gallery option to
view and upload recently captured videos
so let's take a look at one of the
newest features called game stream co-op
this feature allows you to stream your
game to another player over the Internet
so we're gonna switch over to a game
called shrine 3 to check out what this
feature is capable
so now I've reached a point in shrine 3
where I don't know how to beat this boss
it's what I'm gonna do is use the new
feature to invite a friend to come into
my game and help me beat them I'm going
to use the new overlay click stream and
invite my friend to join my game and
hopefully she knows how to beat this guy
hey NAT hey brought you in to my session
using game stream co-op because I need
your help with this philosophy and I
know you're trying 3 Pro so I am
desperate
sure so on this part you definitely need
to use the archer she has the skills to
get that secret little weak spot right
there then you get this Knight guy he
dropped his arm off first then you got
to switch right back to the archer shoot
him in the face and now you're going to
switch back to this guy
yeah well now when he comes over here
he's hopin you're the knees smacking
around a little bit that's it thanks
Natalie
so easy right trying three is also a
co-op game so we can play the next level
together all I have to do is change the
overlay settings from play as me to play
alongside me and then we can go beat
these guys up see if you can jump in
here
whoa all right what's next do you know
we're gonna have to defeat a bunch of
little guys over here
cool well this is awesome using game
stream co-op with GeForce experience you
can either play a co-op game with a
buddy get help with a tough boss fight
or do a one-on-one real-time stream with
a friend to get the latest update go to
GeForce comm to update GeForce
experience thanks for watching
you
Title: NVIDIA GTC May 2020 Keynote Pt4: NVIDIA Merlin for Recommendation Systems
Publish_date: 2020-05-14
Length: 375
Views: 74031
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/zWe02O2po1U/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: zWe02O2po1U

--- Transcript ---

recommenders is one of the most
important and complex machine learning
pipelines creating recommenders is
incredibly complex however the benefits
are enormous to Internet services it
enhances their user engagement the
quality of service and for many of them
it dramatically shapes their economics
recommenders consists of a couple of
different algorithms collaborative
filtering and content filtering
collaborative filtering tries to predict
user preferences from other user
interactions that are similar and
content filtering tries to predict what
items could be preferred based on
similar items the first stage of the
recommender systems consists of taking
high dimensional information of users
and items and encodes it into a low
dimensional vector in the process
computing the similarities of users to
other users and items to other items the
reduction of high dimensional
information of users into low
dimensional vectors and the high
dimensional descriptions of items into
low dimensional vectors is called
embedding that is a computationally
intensive process and requires deep
learning to do so the embeddings are
then used by the recommender systems to
learn to predict user preferences so
when the new user comes into a service
and queries about a particular item
which could be a movie or song or a book
or a product or grocery the first stage
generates a large number from billions
of options to thousands of candidates
from those thousands of candidates it is
then ranked by a neural network that has
learned this particular users preference
to provide a ranked list of preferred
next items and that's the reason why
when you're watching a movie the next
movie that comes up is similar to the
previous and it could recommend if you
liked this movie you might like this
movie if you liked the song this would
represent a great playlist if you
enjoyed this book you might consider
these books if you just bought this item
you might consider also buy
this item that ability to predict user
preference based on your interaction and
based on your queries is one of the
major reasons why internet is so
personalized and is one of the reasons
why the Internet is so useful to us the
recommender system is the engine of the
personalized Internet it is the reason
why we're going to be able to enjoy an
Internet that seems to be practically
designed for us it's almost as if the
recommender is going to be an AI that
reads your mind it could predict what
you prefer based on the context your
circumstance maybe some vacation you're
on a birthday of somebody you love and
based on that it would recommend a
product or service to you the
recommender system is vitally important
to the future of the Internet there are
trillions of items in the world hundreds
of billions of websites hundreds of
millions of books and songs it is
impossible to search it is impossible to
know what's out there and so the
Internet has to understand what you
prefer and make a recommendation to you
four companies will be your recommender
system for sales automation or marketing
automation for healthcare it might
recommend therapies over time the
recommender system is going to be vital
to every industry whether it's shopping
or supply chain management or customer
service call centers will have
recommender systems in the background
recommending solutions or recommending a
path or journey through the dialog
system recommender systems is
foundational to all industries however
building recommender systems is
enormously complex this is the endeavor
of large Internet companies what we
would love to do is to simplify and to
codify the complexity of modern
recommender systems and put it into an
application framework so that we can
democratize this capability for all
industries ladies and gentlemen we've
done just that and we call it Nvidia
Merlin Nvidia Merlin is a deep learning
application framework we've taken what
is otherwise a very large system a very
complicated data processing and machine
learning algorithm encoding codified it
into an application framework that's
easy to use the first stage of data
processing
with envy tabular with just a few lines
of code you could create a data frame
and do data processing for extract
transform and load from terabytes to
hundreds of terabytes and the scaling
and the partitioning is done
automatically you don't have to think
about the system underneath you don't
have to think about the system software
all of it is done in MD tabular and it's
easy to use you then pass off the data
frame that you've processed into huge
CTR that allows you to learn embedding
and the ranking systems automatically
all of that is codified into these
libraries it sits on top of Rapids it
sits on top of KU d n n it has been
optimized with the highest performance
engineering that's been done and we call
that Nvidia Merlin well the results are
really quite amazing here I'm showing
you a one terabyte ads database from
curt a Oh what would otherwise take a
couple of days of processing on a CPU
all of a Sun with this framework not
only is it easier to use just a few
lines of code it now runs like a speed
of light all of a sudden from a couple
of days of processing into minutes now
this is for one terabyte of data if you
were to scale this up a thousand times
you could just imagine the complexity a
thousand times basically is a one
petabyte database and that one petabyte
database is just around the corner and
so companies are now able to think about
creating these really complex
recommender systems built on top of high
performance computing servers and have
this application framework that is
really easy to use we call that Nvidia
Merlin
Title: GeForce Garage: Cross Desk Series, Video 8  – How To Set Up Multiple Monitors
Publish_date: 2015-04-16
Length: 402
Views: 200896
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/ZWy4EppFgk0/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLBMcjt0kGhPmZvxRwPT_XFPWElsrg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: ZWy4EppFgk0

--- Transcript ---

hi I'm Andrew with NVIDIA and you're
watching GeForce garage after weeks of
bending cutting drilling sawing and
customizing all the way down to color
matched SSDs and lighting strips the red
harbinger cross desk is cocked
overclocked and ready to rock well
almost ready despite all this prodigious
Hardware it's still just a fancy beer
pong table without displays all four of
them why four displays you may ask well
because we can and my buddies Matt and
Dwight are going to show us exactly how
we set ours up hey I'm Matt from area
five and I'm Dwight also known as the
mighty potato on YouTube
normally I'm behind the camera and I'm
usually helping with the build so in our
previous episodes a Geforce garage we
spent a lot of time and effort putting
this thing together but the one thing
that it's missing of course is a monitor
in order to do that we're going to use
four 4kg sync monitors that we're going
to throw in to surround in order to do
that we're going to be using the ergo
tek apex mounting system so every
monitor mounting system is different
you're going to need to follow the
directions to the letter but no matter
which one you choose you're going to
need some friends to help you out we
brought along and crew with us we're
going to get this thing set up and
together then we're gonna play some
games let's do it
you
now that we've mounted our one over
three displays it's time to get rockin
and rollin and do some game
alright guys that's the end of the red
harbinger cross test series but
definitely not the end of G Force garage
in fact this is just the first of many
series to come so if you haven't checked
out our other amazing content go to G
Force comm slash garage for a bunch of
written guides and other videos in case
you haven't noticed you're watching the
G Force garage the ultimate resource
center for designing building and
customizing your g-force PC
you
Title: GeForce GTX Battlebox PCs Powering Next-Gen Experiences At Gamescom 2015
Publish_date: 2015-08-13
Length: 130
Views: 27396
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/ZW_UrVCt-iU/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: ZW_UrVCt-iU

--- Transcript ---

Gamescom attendees treating themselves
to a game of call of duty black ops
three gorgeous 4k will discover that
they're playing with some seriously
powerful Nvidia hardware namely the new
range of GeForce GTX BattleBots pcs what
makes the battle box differ from other
pcs well here's Nvidia's atom quote to
enlighten you know seeking a definitive
no compromise experience we've worked
with the best system builders in the
world
to create the GeForce GTX battle box PC
handcrafted by specialists using the
best possible PC components money can
buy each GeForce GTX Patton box PC uses
as a minimum geforce gtx 980ti to give
you the perfect for K DirectX 12 VR
experience this makes these rigs perfect
for playing games such as Metal Gear
Solid 5 Tom Clancy's Rainbow six seeds
and call of duty black ops 3 to find out
what goes into the creation of a battle
box PC we sponsored James go hold of UK
battle box builders scan computers
sorry fuzzy Pakistan means building the
very best
that doesn't mean trying the kitchen
thing
clever and research and development to
find the very best combination of
hardware a lot of Africans are engineers
in the performance
customization that I am
I got a heart when I'm old
also folks experience
we'd like to find out more about the
full range of battle box PCs
it's a Geforce of co dot uk' check back
tomorrow for even more Gamescom action
Title: NVIDIA at CES 2018
Publish_date: 2018-01-10
Length: 147
Views: 476531
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/_kZ_K4EZLWY/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: _kZ_K4EZLWY

--- Transcript ---

[Music]
after capturing headlines with our CS
opening keynote here in Las Vegas the
midea is showing off its latest magic on
the show floor and Beyond we're right in
the middle of the North Hall where the
auto companies are featured and many of
the show's 180,000 visitors pass through
our booth is bigger than ever and fully
dedicated to our market leading
autonomous driving innovations drawing
huge attention is the CES debut of the
robocar the Nvidia powered race car will
participate in a future autonomous
series sanctioned by formula e what's
really capturing the crowds attention is
our holodeck demonstration our VR room
contains a classic VW bus and by putting
on a head mounted VR display you'll be
transported to a holodeck demonstration
where we're showing off our just
announced partnership with Volkswagen
the new VW ID buzz which will be using
Nvidia AI technology Xavier is here on
display the most complex and powerful
processor ever built soon to be in our
customers hands it's at the core of
Nvidia Drive a platform for self-driving
vehicles nearby demos show how our new
Nvidia Drive technology is bringing
self-driving cars closer to your
driveway including Nvidia Drive IX for
safety comfort and convenience and drive
a arm to help build your confidence in
the cars capabilities our eight mile
demo shows how an Nvidia powered
self-driving car can easily maneuver
along a complicated urban route complete
with numerous blind intersections
traffic and sharp turns down the Las
Vegas Strip is our decked out gaming
suite there we're showing off our new BF
GD or big format gaming display from HP
Asus and Acer
these 65 inch g-sync monitors integrate
shield to deliver the world's first
giant screen PC gaming experience
we're also demoing our new GeForce now
beta for PC letting you stream games to
virtually any laptop even underpowered
models with integrated graphics GeForce
now has been further expanded to include
new play light and thin notebooks are
sleeker than ever with three max-q
notebooks on display even as thin as 18
millimeters you'll get a great gaming
experience with a GTX 1080 inside also
on display are new features for GeForce
experience including new ways to mod
your PC G
during life play with features like
freestyle that provides 15 filters and
38 settings compatible with a hundred
games for a deeper dive and all
invidious CS news check out Nvidia comm
[Music]
Title: GeForce Showcase: Most anticipated VR experiences for HTC Vive
Publish_date: 2016-04-19
Length: 196
Views: 22476
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/_m0ZxmxraA8/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: _m0ZxmxraA8

--- Transcript ---

as the latest headsets hit the shelves
and pre-orders begin to ship you need to
make sure your PC is ready for virtual
reality with an NVIDIA GeForce GTX 970
GPU or higher and you need to know which
games to pick up so here's our handy
guide to the most anticipated VR games
coming to the HTC vive first up is the
breathlessly terrifying Everest VR
developer so far studios transports you
over 8000 meters above sea level to
experience the raw unforgiving and
deadly environment of the world's
highest peak with an unparalleled degree
of accuracy using Nvidia's cutting-edge
VR Works technologies including
turbulence effects that deeply immersed
climbers in realistically simulated snow
storms at the summit of Everest soul
Farah pushing back the boundaries of
just what is possible with VR the
whirling snow blinding whiteness sheer
drops and rickety ladder crossings will
make you feel as though you've really
been there let's just hope they can't
simulate frostbite yet if you fancy
something a little closer to sea level
then spy thriller budget cuts might be
just the game for you laid off from your
job as a spy thanks to recent cutbacks
you must sneak around the offices of the
trans corporation making the most of
their teleportation technology to find
and approve your job application loving
pointy things at robots along the way
what's not to like available on the HTC
vive from launch and making you feel
like a kid again is final approach this
air traffic controller come Airport
manager sim takes advantage of Rome
scale VR like nothing else with headset
on and controllers in hand you can walk
around your various airports gliding
planes into land completing challenges
and even tossing luggage around in a
truly hyper realistic manner father
Junkers on the other hand blocks you
down in a futuristic wasteland grim and
dry enough to rival Mad Max here you
must fortify and defend your own hover
junker by battling other players and
salvaging scrap along way taking
advantage of room scale VR you'll need
to duck and dodge in real life to make
the most of your virtual cover and avoid
incoming fire the gallery call of the
star seed is the first installment of an
episodic VR fantasy adventure and it is
perfect if you really want something
with
deeper narrative after coming across a
strange message left by your twin sister
you must seek out an ancient cosmic
machine meeting bizarre characters along
the way and solving puzzles in a bid to
track her down another interactive
adventure title landing on the HTC vive
this summer is Thunderbird created by
one of the team behind missed sequel
Riven this VR puzzle adventure is
shrouded in mystery and promises to
bring together eastern and western
mythology along with science and magic
to create a unique and memorable
experience truly making the most of the
HTC vibes unique motion controller
system and great fun to play with
friends is fabulous
physics bending construction game
fantastic contraption the aim of the
game is simple use all manner of bits
and bobs at your disposal to build a
contraption capable of reaching the goal
at the end of the level just what kind
of abomination you create though is
entirely up to your own deranged
imagination if you're planning to order
an HTC vive then make sure you have an
NVIDIA GeForce GTX 970 GPU or higher
powering your PC
Title: NVIDIA RTX Powers Real-Time Visualization with 3DEXPERIENCE CATIA
Publish_date: 2019-07-30
Length: 104
Views: 14396
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/_noPcyFh1Ro/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLAFyBQ6sZw_xwQ7MmItrRX5Iq_UUA
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: _noPcyFh1Ro

--- Transcript ---

I tell you Melkonyan I'm managing the
Quecha design portfolio in the system
this is a portfolio for the creative
designers you need to do sketching
clearly modeling visualization and
immersive experiences so this year at
SIGGRAPH we are showing a showcase of
the 3d experience for creative designers
in collaboration with Nvidia and Renault
Dacia and what we're showing is our
real-time visualization experience with
of Stella render engine based on ethics
for creative designers visualization is
critical and to make it fast it's also
very important because they need to take
accurate decision but they need to take
it very quickly while even sometimes
while they are designing changing the
shape they have to see how it looks like
to take a decision and also when they
are going to visit some customers or
chief designer they need to decide on on
the spot so visualization and real-time
visualization is critical for them what
is new here what we are showing with the
3d experience capture design is really
to do this kind of visualization
I am visualization on the real data the
exact geometry so even while you're
designing you can see your data there is
no preparation nothing a to reach is
kind of performance we have to rely on
very super powerful graphic boards like
the ethics today that we're showing at
SIGGRAPH with nvidia and we're showing
two things the ethics on the other
workstation you have a good real-time
experience while also showing a demo
with a cluster of active servers with 8
ft 6000 here you can have a full we have
some experience with full waitressing
globalization
you
Title: NVIDIA RTX Extreme: Real-Time Automotive Rendering
Publish_date: 2022-10-10
Length: 237
Views: 58091
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/_OGayLObEZ4/hq720.jpg
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: _OGayLObEZ4

--- Transcript ---

[Music]
hi everyone my name is David Bayless and
today we'll be reviewing together the
RTX a6000 and see how we can push
real-time Graphics to achieve
photorealistic results in automotive
rendering the key aspects that I look
for in my work is the highest Fidelity
outputs out of real time using features
such as real-time Ray tracing and being
able to Output high resolution renders
in a short amount of time the main
software is I use are 3ds Max where I
typically do the pre-setup of my scene
and do all the necessary work on the car
before importing into Unreal Engine now
once inside unreal this is where I do
all the lighting and shading for final
render and also experiments with
different car paints and see what fits
best having all of this happening in
real time is extremely helpful so here
are my current PC specs I have an Intel
I9 12900k for the CPU equipped with 32GB
sides of ddr5 RAM and finally the RTX
a6000 for the GPU which are rely the
most for my programs
this is equipped with 48 gigabyte of
vram which will enable us to load very
high resolution textures as well as some
high poly assets
10752 Cuda cores and finally 84 second
gen RT cores which will prove themselves
really useful for real-time Ray tracing
so a few interesting stats in my current
scene we are currently running at around
60 frames per second in pass Tracer mode
the a6000 doing a great job maintaining
that high frame rate for me we are
having no problem switching between the
three different views here the building
that I'm using as a background is a scan
assets of over 2 million triangles
containing 10 sets of 16k textures again
no issues here for the a6000 the overall
poly count of my scene is almost 50
million triangles and we are currently
using just on the 50 vram capacity here
so we still have some room if we need
some more assets in the scene okay so
this is my previous card which is the
a5000 and what I can find doing with the
a6000 is render at a much higher
resolution without hitting the vram
limits as I'll be demonstrating right
now in my scene here in ue5 using past
Tracer using around 15 divides a vram
what I'll be doing here is render one
still frame in pass Tracer mode at 8K
resolution we can see that we are using
24 gigabytes of eram
now what happens if I want to push this
higher let's say a 12K resolution hit
render and now we are using close to 35
gigabytes of vram which wouldn't have
been possible to render on the a5000
here is why rendering at a higher
resolution is important for me a 4K
render when zoomed in appears blurry
when we triple the resolution size to
12K notice how sharp my details remain
making it ideal for editors to dial in
extra details or even printing this as a
large poster this 12K render took just
under 10 minutes to render on the a6000
now we've seen the benefits of using a
single a6000 in my scene for real time
let's see what happens if I introduce a
second a6000 to the mix and see how this
can enhance my workflow using the NV
link to connect these two for Combined
total vram memory of 90 six gigabytes be
sure to check out part 2 of this video
to see how my dual RTX a6000 perform
real-time rendering
Title: I AM AI: Digital Avatar Made Easy SIGGRAPH Real-Time Live Demo
Publish_date: 2021-08-12
Length: 406
Views: 43629
Rating: None
Author: NVIDIA
Description: None
Thumbnail_url: https://i.ytimg.com/vi/__Za-BP20eM/hqdefault.jpg?sqp=-oaymwExCJADEOABSFryq4qpAyMIARUAAIhCGAHwAQH4Af4OgAK4CIoCDAgAEAEYZSBXKFYwDw==&rs=AOn4CLA_QSzXxLmPUIFT-hfKuWp7i8Tfvw
Keywords: 
Channel_id: UCHuiy8bXnmK5nisYHUd1J5g
Channel_url: https://www.youtube.com/channel/UCHuiy8bXnmK5nisYHUd1J5g
Age_restricted: False
Video_id: __Za-BP20eM

--- Transcript ---

making digital avatars is a notoriously
difficult tedious and expensive process
it requires a team of digital artists
and engineers with professional hardware
and software to make a lifelike digital
character wouldn't it be great if we had
a tool that could create a digital
avatar instantly by using just a single
photo of a person
in this demo we will present a
technology that does just that and its
application to video conferencing
storytelling and beyond
hi this is kevin yeah ready for the
interview
yeah
so it says you're presenting a new ai
driven digital avatar technology to me
today tell me all about it
all right before we begin can you tell
me how i look on your screen
um looks like you're wearing a blue
collared shirt and have recently shaved
would you believe me if i told you i'm
actually wearing a t-shirt hat and mask
and chilling in a coffee shop
wait no that's definitely not what i'm
seeing on my screen
yes what you're seeing is the new ai
driven digital avatar technology i want
to demonstrate to you today the digital
avatar you're speaking to is created by
using just a single photo of me i was
never 3d scanned nor were my photos used
during training the digital avatar
creation is instantaneous so i can
quickly create a different avatar by
using a different photo
in fact let me quickly create an avatar
here's one using a photo of me in a suit
and here's another one
it's using a photo taken last week
wait how are you getting all that facial
motion out of just a still photo
sure let me explain
it all starts with speech
we transmit only my voice to your
machine or the internet the voice is
then used to drive an ai model called
audio to face which is the technology
that takes my speeches input and
generates lip sync facial motions and
expressions for a 3d head model in real
time
the system generates natural 3d facial
motion including emotions lips eyes and
head motion
the motion of the 3d head model is then
fed to another ai model called victobit
which can animate a photo of a person
since it takes audio as input we can
drive facial animation using any voice
for example i can say hey hey this is my
funny voice or in any language hello
miranam arun
by switching to a different photo we
have a different avatar
this is the power of having an air model
that can create a digital avatar by just
using a single photo
we can even create cartoon avatars with
this technology
this is what we get by simply plugging
in a cartoon navita image
by using stylegan we can generate a wide
range of 2d avatars
using this technology we can bring them
to life like what you're seeing right
now
nice so how does it work with so many
different avatars
vidtuvid achieves this amazing
capability because it was trained with a
large number of videos which included
various facial appearance features and
motions
now let me change back to a photo of the
real me
wow so it's still really hard to believe
that you got this working so smoothly in
a video conferencing pipeline and also
our deep network's really slow and like
what about the bandwidth
our ai models are optimized for gpus
allowing us to easily achieve 30 frames
per second using a pc with an rtx gpu
it's also extremely bandwidth efficient
because we never have to transfer a full
image over the network
since we are not even using a webcam
without this technology all you'd be
getting is just my voice
nice well that sounds perfect for my
next my next business trip
i'm glad you like it
uh sorry the coffee grinder in the
background is making a lot of noise let
me mute my mic and quickly switch to
text
wait text don't you just want to do you
want to call me back when you're
somewhere quieter
hi kevin hope you can hear me yeah loud
and clear but you said you were texting
yes my microphone is on mute and i'm
typing on my laptop
let me introduce the third exciting
technology rad tds
rad tts is our latest text-to-speech
method based on deep learning it is
robust and can synthesize a lifelike
speech from arbitrary text inputs in
milliseconds
well that's awesome so with all the
stuff you've shown me today you could
just be like calling in from a beach and
no one will know
absolutely this digital avatar creation
technology can bring video conferencing
to the next level yeah i can see that
are there any other applications you
want to show me today
it also allows for virtual assistants
and accessible content creation all you
need is a single picture
rad tts can even personalize the voice
meaning that we can even narrate a
famous storyline with our own characters
for example i am going to create an
avatar to represent a famous character
from the novel a song of ice and fire
and make her speak her signature
sentence in the book
i am daenerys stormborn of the house
targaryen the first of her name the
unburnt queen of meereen queen of the
andals and the rhoynar and the first men
khaleesi of the great grass sea breaker
of chains and mother of dragons
wow um so how much control do you have
over the speech output
not only does it do casual conversation
but it can sink too check this out
please stand up please stand up please
stand up because i'm some shitty yes i'm
the real you other slim shades are just
imitating so won't the real slim shake
please stand up
well i certainly wasn't expecting that
well hey thanks for interviewing with us
today we'd love to integrate your work
into our technology
um when when can you start
as soon as i'm done with my coffee
i hope we've convinced you that there's
an easy way to create digital avatars
for real people as well as cartoon
characters it can be used for video
conferencing storytelling virtual
assistants and many other applications
we can't wait to put this technology in
your hands and see what you invent with
it
